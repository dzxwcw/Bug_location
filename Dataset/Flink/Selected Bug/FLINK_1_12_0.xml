<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  
  
  <bug fixdate="2020-3-10 01:00:00" id="16516" opendate="2020-3-10 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Avoid codegen user-defined function for Python UDF</summary>
      <description>Currently we make use of codegen to generate PythonScalarFunction and PythonTableFunction, but it is unnecessary. We can directly create a static PythonScalarFunction and PythonTableFunction.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.utils.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.SimplePythonFunction.java</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.catalog.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-25 01:00:00" id="16768" opendate="2020-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart hangs</summary>
      <description>Logs: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6584&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=d26b3528-38b0-53d2-05f7-37557c2405e42020-03-24T15:52:18.9196862Z "main" #1 prio=5 os_prio=0 tid=0x00007fd36c00b800 nid=0xc21 runnable [0x00007fd3743ce000]2020-03-24T15:52:18.9197235Z java.lang.Thread.State: RUNNABLE2020-03-24T15:52:18.9197536Z at java.net.SocketInputStream.socketRead0(Native Method)2020-03-24T15:52:18.9197931Z at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)2020-03-24T15:52:18.9198340Z at java.net.SocketInputStream.read(SocketInputStream.java:171)2020-03-24T15:52:18.9198749Z at java.net.SocketInputStream.read(SocketInputStream.java:141)2020-03-24T15:52:18.9199171Z at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)2020-03-24T15:52:18.9199840Z at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:593)2020-03-24T15:52:18.9200265Z at sun.security.ssl.InputRecord.read(InputRecord.java:532)2020-03-24T15:52:18.9200663Z at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975)2020-03-24T15:52:18.9201213Z - locked &lt;0x00000000927583d8&gt; (a java.lang.Object)2020-03-24T15:52:18.9201589Z at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933)2020-03-24T15:52:18.9202026Z at sun.security.ssl.AppInputStream.read(AppInputStream.java:105)2020-03-24T15:52:18.9202583Z - locked &lt;0x0000000092758c00&gt; (a sun.security.ssl.AppInputStream)2020-03-24T15:52:18.9203029Z at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137)2020-03-24T15:52:18.9203558Z at org.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198)2020-03-24T15:52:18.9204121Z at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176)2020-03-24T15:52:18.9204626Z at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)2020-03-24T15:52:18.9205121Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9205679Z at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)2020-03-24T15:52:18.9206164Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9206786Z at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125)2020-03-24T15:52:18.9207361Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9207839Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9208327Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9208809Z at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)2020-03-24T15:52:18.9209273Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9210003Z at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107)2020-03-24T15:52:18.9210658Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9211154Z at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$read$3(S3AInputStream.java:445)2020-03-24T15:52:18.9211631Z at org.apache.hadoop.fs.s3a.S3AInputStream$$Lambda$42/1936375962.execute(Unknown Source)2020-03-24T15:52:18.9212044Z at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)2020-03-24T15:52:18.9212553Z at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:260)2020-03-24T15:52:18.9212972Z at org.apache.hadoop.fs.s3a.Invoker$$Lambda$23/1457226878.execute(Unknown Source)2020-03-24T15:52:18.9213408Z at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)2020-03-24T15:52:18.9213866Z at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:256)2020-03-24T15:52:18.9214273Z at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:231)2020-03-24T15:52:18.9214701Z at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:441)2020-03-24T15:52:18.9215443Z - locked &lt;0x00000000926e88b0&gt; (a org.apache.hadoop.fs.s3a.S3AInputStream)2020-03-24T15:52:18.9215852Z at java.io.DataInputStream.read(DataInputStream.java:149)2020-03-24T15:52:18.9216305Z at org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.read(HadoopDataInputStream.java:94)2020-03-24T15:52:18.9216781Z at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)2020-03-24T15:52:18.9217187Z at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)2020-03-24T15:52:18.9217571Z at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)2020-03-24T15:52:18.9218108Z - locked &lt;0x00000000926ea000&gt; (a java.io.InputStreamReader)2020-03-24T15:52:18.9218475Z at java.io.InputStreamReader.read(InputStreamReader.java:184)2020-03-24T15:52:18.9218876Z at java.io.BufferedReader.fill(BufferedReader.java:161)2020-03-24T15:52:18.9219261Z at java.io.BufferedReader.readLine(BufferedReader.java:324)2020-03-24T15:52:18.9219890Z - locked &lt;0x00000000926ea000&gt; (a java.io.InputStreamReader)2020-03-24T15:52:18.9220256Z at java.io.BufferedReader.readLine(BufferedReader.java:389)2020-03-24T15:52:18.9220914Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getContentsOfFile(HadoopS3RecoverableWriterITCase.java:423)2020-03-24T15:52:18.9221704Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:411)2020-03-24T15:52:18.9222457Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364)2020-03-24T15:52:18.9223222Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:330)2020-03-24T15:52:18.9223817Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-03-24T15:52:18.9224232Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-03-24T15:52:18.9224729Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-03-24T15:52:18.9225160Z at java.lang.reflect.Method.invoke(Method.java:498)2020-03-24T15:52:18.9225675Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-03-24T15:52:18.9226171Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-03-24T15:52:18.9226682Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-03-24T15:52:18.9227187Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-03-24T15:52:18.9227661Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2020-03-24T15:52:18.9228145Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2020-03-24T15:52:18.9228718Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2020-03-24T15:52:18.9229112Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-03-24T15:52:18.9229582Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-03-24T15:52:18.9230029Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-03-24T15:52:18.9230525Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-03-24T15:52:18.9230963Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-03-24T15:52:18.9231546Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-03-24T15:52:18.9231999Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-03-24T15:52:18.9232432Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-03-24T15:52:18.9232862Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-03-24T15:52:18.9233307Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2020-03-24T15:52:18.9233833Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2020-03-24T15:52:18.9234284Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-03-24T15:52:18.9234700Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-03-24T15:52:18.9235076Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-03-24T15:52:18.9235599Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-03-24T15:52:18.9236124Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-03-24T15:52:18.9236648Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-03-24T15:52:18.9237167Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-03-24T15:52:18.9237688Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-03-24T15:52:18.9238244Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-03-24T15:52:18.9238745Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-03-24T15:52:18.9239202Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2020-03-24T15:52:18.9239549Z 2020-03-24T15:52:18.9239794Z "VM Thread" os_prio=0 tid=0x00007fd36c260800 nid=0xc58 runnable  </description>
      <version>1.10.0,1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.watchdog.sh</file>
      <file type="M">tools.ci.test.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-26 01:00:00" id="16795" opendate="2020-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End to end tests timeout on Azure</summary>
      <description>Example: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6650&amp;view=logs&amp;j=08866332-78f7-59e4-4f7e-49a56faa3179 or https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6637&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5##[error]The job running on agent Azure Pipelines 6 ran longer than the maximum time of 200 minutes. For more information, see https://go.microsoft.com/fwlink/?linkid=2077134and ##[error]The operation was canceled.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-26 01:00:00" id="16796" opendate="2020-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix The Bug of Python UDTF in SQL Query</summary>
      <description>When executes Python UDTF in sql query, it will cause some problem.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCorrelate.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-2 01:00:00" id="16945" opendate="2020-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute CheckpointFailureManager.FailJobCallback directly in main thread executor</summary>
      <description>Since we have put all non-IO operations of CheckpointCoordinator into main thread executor, the CheckpointFailureManager.FailJobCallback could be executed directly now. In this way execution graph would fail immediately when CheckpointFailureManager invokes the callback. We could avoid the inconsistent scenario of FLINK-13497.</description>
      <version>1.10.0,1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-2 01:00:00" id="16946" opendate="2020-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update user documentation for job manager memory model</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.memory.mem.tuning.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.md</file>
      <file type="M">docs.ops.memory.mem.setup.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.md</file>
      <file type="M">docs.ops.memory.mem.detail.zh.md</file>
      <file type="M">docs.ops.memory.mem.detail.md</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-2 01:00:00" id="16947" opendate="2020-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ArtifactResolutionException: Could not transfer artifact. Entry [...] has not been leased from this pool</summary>
      <description>https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6982&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5Build of flink-metrics-availability-test failed with:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (end-to-end-tests) on project flink-metrics-availability-test: Unable to generate classpath: org.apache.maven.artifact.resolver.ArtifactResolutionException: Could not transfer artifact org.apache.maven.surefire:surefire-grouper:jar:2.22.1 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): Entry [id:13][route:{s}-&gt;https://maven-central-eu.storage-download.googleapis.com:443][state:null] has not been leased from this pool[ERROR] org.apache.maven.surefire:surefire-grouper:jar:2.22.1[ERROR] [ERROR] from the specified remote repositories:[ERROR] google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/, releases=true, snapshots=false),[ERROR] apache.snapshots (https://repository.apache.org/snapshots, releases=false, snapshots=true)[ERROR] Path to dependency:[ERROR] 1) dummy:dummy:jar:1.0[ERROR] 2) org.apache.maven.surefire:surefire-junit47:jar:2.22.1[ERROR] 3) org.apache.maven.surefire:common-junit48:jar:2.22.1[ERROR] 4) org.apache.maven.surefire:surefire-grouper:jar:2.22.1[ERROR] -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException[ERROR] [ERROR] After correcting the problems, you can resume the build with the command[ERROR] mvn &lt;goals&gt; -rf :flink-metrics-availability-test</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-13 01:00:00" id="1695" opendate="2015-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create machine learning library</summary>
      <description>Create the infrastructure for Flink's machine learning library. This includes the creation of the module structure and the implementation of basic types such as vectors and matrices.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-18 01:00:00" id="1716" opendate="2015-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CoCoA algorithm to flink-ml</summary>
      <description>Add the communication efficient distributed dual coordinate ascent algorithm to the flink machine learning library. See CoCoA for the implementation details.I propose to first implement it with hinge loss and l2-norm. This way, it will allow us to train SVMs in parallel.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.recommendation.ALSITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.SparseVectorSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.SparseMatrixSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.recommendation.ALS.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Matrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Learner.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.LabeledVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.FlinkTools.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedTransformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedLearner.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-27 01:00:00" id="17404" opendate="2020-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Running HA per-job cluster (rocks, non-incremental) gets stuck killing a non-existing pid in Hadoop 3 build profile</summary>
      <description>CI log: https://api.travis-ci.org/v3/job/678609505/log.txtWaiting for text Completed checkpoint [1-9]* for job 00000000000000000000000000000000 to appear 2 of times in logs...grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directorygrep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directoryStarting standalonejob daemon on host travis-job-e606668f-b674-49c0-8590-e3508e22b99d.grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directorygrep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directoryKilled TM @ 18864kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]Killed TM @ No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-receivedThe build has been terminated</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-27 01:00:00" id="17406" opendate="2020-4-27 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add documentation about dynamic table options</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sql.index.zh.md</file>
      <file type="M">docs.dev.table.sql.index.md</file>
      <file type="M">docs.dev.table.config.zh.md</file>
      <file type="M">docs.dev.table.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-27 01:00:00" id="17408" opendate="2020-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce GPUDriver</summary>
      <description>Introduce GPUDriver for GPU resource.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-28 01:00:00" id="17424" opendate="2020-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client end-to-end test (Old planner) Elasticsearch (v7.5.1) failed due to download error</summary>
      <description>`SQL Client end-to-end test (Old planner) Elasticsearch (v7.5.1)` failed in release-1.10 crone job with below error:Preparing Elasticsearch(version=7)...Downloading Elasticsearch from https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.5.1-linux-x86_64.tar.gz ... % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 4 276M 4 13.3M 0 0 28.8M 0 0:00:09 --:--:-- 0:00:09 28.8M 42 276M 42 117M 0 0 80.7M 0 0:00:03 0:00:01 0:00:02 80.7M 70 276M 70 196M 0 0 79.9M 0 0:00:03 0:00:02 0:00:01 79.9M 89 276M 89 248M 0 0 82.3M 0 0:00:03 0:00:03 --:--:-- 82.4Mcurl: (56) GnuTLS recv error (-54): Error in the pull function. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (7) Failed to connect to localhost port 9200: Connection refused[FAIL] Test script contains errors.https://api.travis-ci.org/v3/job/680222168/log.txt</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-5-28 01:00:00" id="17428" opendate="2020-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SupportsProjectionPushDown in planner</summary>
      <description>Support the SupportsProjectionPushDown interface for ScanTableSource.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSourceSinks.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.ScanTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.ScanTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-28 01:00:00" id="17431" opendate="2020-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement table DDLs for Hive dialect part 1</summary>
      <description>Will cover CREATE, DROP, DESCRIBE, SHOW table in this ticket.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.type.ExtendedSqlRowTypeNameSpec.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.type.ExtendedSqlCollectionTypeNameSpec.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dql.SqlRichDescribeTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.constraint.SqlTableConstraint.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlCreateHiveDatabase.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlAlterHiveDatabaseOwner.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlAlterHiveDatabase.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.HiveDDLUtils.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-13 01:00:00" id="17646" opendate="2020-5-13 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Reduce the python package size of PyFlink</summary>
      <description>Currently the python package size of PyFlink has increased to about 320MB, which exceeds the size limit of pypi.org (300MB). We need to remove unnecessary jars to reduce the package size.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-19 01:00:00" id="17810" opendate="2020-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for K8s application mode</summary>
      <description>Add document for how to start/stop K8s application cluster.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-19 01:00:00" id="17814" opendate="2020-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate native kubernetes document to Chinese</summary>
      <description>https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/native_kubernetes.html Translate the native kubernetes document to Chinese.English updated in 7723774a0402e10bc914b1fa6128e3c80678dafe</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-19 01:00:00" id="17816" opendate="2020-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Latency Marker to work with "scheduleAtFixedDelay" instead of "scheduleAtFixedRate"</summary>
      <description>Latency Markers and other periodic timers are scheduled with scheduleAtFixedRate. That means every X time the callable is called. If it blocks (backpressure) is can be called immediately again.I would suggest to switch this to scheduleAtFixedDelay to avoid calling for a lot of latency marker injections when there is no way to actually execute the injection call.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-25 01:00:00" id="17931" opendate="2020-5-25 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document fromValues clause</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-26 01:00:00" id="17936" opendate="2020-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement type inference for AS</summary>
      <description>Type information gets lost due to the legacy planner expressions. The user might experience unexpected exceptions.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.fieldExpression.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-6-1 01:00:00" id="18051" opendate="2020-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fail Maven setup on AZP if download fails</summary>
      <description>Setup maven task is green even though the install was not a success: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2481&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=7f98ac96-cfb0-5c1a-969b-c2a0e48a2291</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.maven-utils.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-8-3 01:00:00" id="18081" opendate="2020-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix broken links in "Kerberos Authentication Setup and Configuration" doc</summary>
      <description>The config.html#kerberos-based-security is not valid now.</description>
      <version>1.10.1,1.11.0,1.12.0</version>
      <fixedVersion>1.10.3,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.security-kerberos.zh.md</file>
      <file type="M">docs.ops.security-kerberos.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-3 01:00:00" id="18082" opendate="2020-6-3 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>UnsignedTypeConversionITCase stalls in ch.vorburger.mariadb4j.DB.stop</summary>
      <description>CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2582&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=03dca39c-73e8-5aaf-601d-328ae5c35f202020-06-02T19:01:31.8486456Z ==============================================================================2020-06-02T19:01:31.8487052Z Printing stack trace of Java process 86532020-06-02T19:01:31.8487424Z ==============================================================================2020-06-02T19:01:31.8541169Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError2020-06-02T19:01:32.1665740Z 2020-06-02 19:01:322020-06-02T19:01:32.1666470Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):2020-06-02T19:01:32.1666735Z 2020-06-02T19:01:32.1667614Z "Attach Listener" #537 daemon prio=9 os_prio=0 tid=0x00007f61f8001000 nid=0x3b9f waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1668130Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1668311Z 2020-06-02T19:01:32.1668958Z "flink-akka.actor.default-dispatcher-193" #535 prio=5 os_prio=0 tid=0x00007f6034001000 nid=0x3af7 waiting on condition [0x00007f61a25b8000]2020-06-02T19:01:32.1669418Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1669730Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1670301Z - parking to wait for &lt;0x0000000080c51528&gt; (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)2020-06-02T19:01:32.1670791Z at akka.dispatch.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)2020-06-02T19:01:32.1671329Z at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)2020-06-02T19:01:32.1671763Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2020-06-02T19:01:32.1672211Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2020-06-02T19:01:32.1672491Z 2020-06-02T19:01:32.1673104Z "flink-akka.actor.default-dispatcher-191" #533 prio=5 os_prio=0 tid=0x00007f619801e000 nid=0x3ae1 waiting on condition [0x00007f60770f1000]2020-06-02T19:01:32.1673564Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1673839Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1674422Z - parking to wait for &lt;0x0000000080c51528&gt; (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)2020-06-02T19:01:32.1674865Z at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)2020-06-02T19:01:32.1675305Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2020-06-02T19:01:32.1675751Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2020-06-02T19:01:32.1676046Z 2020-06-02T19:01:32.1676669Z "jobmanager-future-thread-2" #466 daemon prio=5 os_prio=0 tid=0x00007f6124001000 nid=0x3795 waiting on condition [0x00007f61a23b6000]2020-06-02T19:01:32.1677316Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1677617Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1678220Z - parking to wait for &lt;0x00000000816a4c90&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1678702Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1679209Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1679822Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1680422Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1680962Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1681424Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1682062Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1682445Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1682656Z 2020-06-02T19:01:32.1683271Z "Flink-DispatcherRestEndpoint-thread-4" #349 daemon prio=5 os_prio=0 tid=0x00007f618c00a000 nid=0x29a4 waiting on condition [0x00007f61a029f000]2020-06-02T19:01:32.1683750Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1684057Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1684648Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1685145Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1685673Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-06-02T19:01:32.1686400Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-06-02T19:01:32.1687200Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1687724Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1688211Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1688679Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1689076Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1689266Z 2020-06-02T19:01:32.1689923Z "FlinkCompletableFutureDelayScheduler-thread-1" #123 daemon prio=5 os_prio=0 tid=0x00007f60e801d000 nid=0x277b waiting on condition [0x00007f61104dd000]2020-06-02T19:01:32.1690403Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1690698Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1691291Z - parking to wait for &lt;0x00000000879019c0&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1691779Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1692314Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1692905Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)2020-06-02T19:01:32.1693506Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1694040Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1694500Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1694988Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1695372Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1695583Z 2020-06-02T19:01:32.1696176Z "Flink-DispatcherRestEndpoint-thread-3" #84 daemon prio=5 os_prio=0 tid=0x00007f614c003800 nid=0x26ca waiting on condition [0x00007f6113bfa000]2020-06-02T19:01:32.1696685Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1697117Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1697737Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1698205Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1698749Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1699339Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1699942Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1700582Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1701042Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1701528Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1701926Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1702114Z 2020-06-02T19:01:32.1702714Z "mysql-cj-abandoned-connection-cleanup" #83 daemon prio=5 os_prio=0 tid=0x00007f61625c9000 nid=0x26c7 in Object.wait() [0x00007f6113dfc000]2020-06-02T19:01:32.1703201Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1703530Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1703857Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)2020-06-02T19:01:32.1704416Z - locked &lt;0x00000000814a4b00&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-06-02T19:01:32.1704939Z at com.mysql.cj.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:85)2020-06-02T19:01:32.1705464Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2020-06-02T19:01:32.1705950Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1706329Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1706566Z 2020-06-02T19:01:32.1707083Z "Exec Stream Pumper" #76 daemon prio=5 os_prio=0 tid=0x00007f6118001000 nid=0x269c runnable [0x00007f6113ffe000]2020-06-02T19:01:32.1707485Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1707773Z at java.io.FileInputStream.readBytes(Native Method)2020-06-02T19:01:32.1708117Z at java.io.FileInputStream.read(FileInputStream.java:255)2020-06-02T19:01:32.1708522Z at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)2020-06-02T19:01:32.1708944Z at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)2020-06-02T19:01:32.1709357Z at java.io.BufferedInputStream.read(BufferedInputStream.java:345)2020-06-02T19:01:32.1709977Z - locked &lt;0x0000000081555688&gt; (a java.lang.UNIXProcess$ProcessPipeInputStream)2020-06-02T19:01:32.1710386Z at java.io.FilterInputStream.read(FilterInputStream.java:107)2020-06-02T19:01:32.1710779Z at org.apache.commons.exec.StreamPumper.run(StreamPumper.java:107)2020-06-02T19:01:32.1711147Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1711643Z 2020-06-02T19:01:32.1711987Z "Exec Default Executor" #75 prio=5 os_prio=0 tid=0x00007f63009ee800 nid=0x269a in Object.wait() [0x00007f61a019e000]2020-06-02T19:01:32.1712409Z java.lang.Thread.State: WAITING (on object monitor)2020-06-02T19:01:32.1712729Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1713189Z - waiting on &lt;0x00000000816a4ab0&gt; (a java.lang.UNIXProcess)2020-06-02T19:01:32.1713521Z at java.lang.Object.wait(Object.java:502)2020-06-02T19:01:32.1713849Z at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)2020-06-02T19:01:32.1714370Z - locked &lt;0x00000000816a4ab0&gt; (a java.lang.UNIXProcess)2020-06-02T19:01:32.1714778Z at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:364)2020-06-02T19:01:32.1715244Z at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)2020-06-02T19:01:32.1715710Z at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)2020-06-02T19:01:32.1716078Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1716292Z 2020-06-02T19:01:32.1716642Z "process reaper" #71 daemon prio=10 os_prio=0 tid=0x00007f61a805f000 nid=0x2654 runnable [0x00007f61a3441000]2020-06-02T19:01:32.1717218Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1717519Z at java.lang.UNIXProcess.waitForProcessExit(Native Method)2020-06-02T19:01:32.1717906Z at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:289)2020-06-02T19:01:32.1718285Z at java.lang.UNIXProcess$$Lambda$7/861659238.run(Unknown Source)2020-06-02T19:01:32.1718721Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2020-06-02T19:01:32.1719318Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1719700Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1719908Z 2020-06-02T19:01:32.1720537Z "Flink-DispatcherRestEndpoint-thread-2" #66 daemon prio=5 os_prio=0 tid=0x00007f614c002800 nid=0x2506 waiting on condition [0x00007f61a2bbc000]2020-06-02T19:01:32.1721013Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1721288Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1721899Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1722366Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1722891Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1723582Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1724166Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1724705Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1725166Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1725653Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1726050Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1726238Z 2020-06-02T19:01:32.1727051Z "mini-cluster-io-thread-10" #64 daemon prio=5 os_prio=0 tid=0x00007f613c00b000 nid=0x236e waiting on condition [0x00007f61a06a1000]2020-06-02T19:01:32.1727500Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1727795Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1728406Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1728888Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1729397Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1729941Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1730413Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1730874Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1731360Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1731740Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1731947Z 2020-06-02T19:01:32.1732513Z "mini-cluster-io-thread-9" #63 daemon prio=5 os_prio=0 tid=0x00007f613c006800 nid=0x236d waiting on condition [0x00007f61a07a2000]2020-06-02T19:01:32.1732963Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1733237Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1733844Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1734329Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1734836Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1735379Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1735833Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1736314Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1737055Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1737598Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1737787Z 2020-06-02T19:01:32.1738405Z "jobmanager-future-thread-1" #62 daemon prio=5 os_prio=0 tid=0x00007f613c01c000 nid=0x236c waiting on condition [0x00007f61a08a3000]2020-06-02T19:01:32.1738863Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1739148Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1739755Z - parking to wait for &lt;0x00000000816a4c90&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1740227Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1740771Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-06-02T19:01:32.1741390Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-06-02T19:01:32.1742053Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1742594Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1743076Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1743544Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1743942Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1744129Z 2020-06-02T19:01:32.1744722Z "mini-cluster-io-thread-8" #59 daemon prio=5 os_prio=0 tid=0x00007f6190005000 nid=0x2369 waiting on condition [0x00007f61a0ba6000]2020-06-02T19:01:32.1745147Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1745440Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1746027Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1746553Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1747227Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1747753Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1748230Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1748691Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1749186Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1749585Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1749771Z 2020-06-02T19:01:32.1750347Z "mini-cluster-io-thread-7" #58 daemon prio=5 os_prio=0 tid=0x00007f6190003000 nid=0x2368 waiting on condition [0x00007f61a0ca7000]2020-06-02T19:01:32.1750793Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1751092Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1751686Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1752170Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1752679Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1753226Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1753703Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1754167Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1754652Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1755031Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1755240Z 2020-06-02T19:01:32.1755806Z "mini-cluster-io-thread-6" #57 daemon prio=5 os_prio=0 tid=0x00007f6190001800 nid=0x2367 waiting on condition [0x00007f61a0da8000]2020-06-02T19:01:32.1756340Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1756674Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1757414Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1757914Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1758437Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1758963Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1759434Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1759895Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1760468Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1760864Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1761052Z 2020-06-02T19:01:32.1761621Z "mini-cluster-io-thread-5" #56 daemon prio=5 os_prio=0 tid=0x00007f6150008000 nid=0x2366 waiting on condition [0x00007f61a0ea9000]2020-06-02T19:01:32.1762063Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1762352Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1762938Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1763417Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1763926Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1764464Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1764944Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1765409Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1765886Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1766284Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1766511Z 2020-06-02T19:01:32.1767253Z "mini-cluster-io-thread-4" #55 daemon prio=5 os_prio=0 tid=0x00007f6300311000 nid=0x2365 waiting on condition [0x00007f61a0faa000]2020-06-02T19:01:32.1767692Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1767965Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1768575Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1769051Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1769558Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1770111Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1770563Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1771042Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1771526Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1771906Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1772094Z 2020-06-02T19:01:32.1772676Z "mini-cluster-io-thread-3" #54 daemon prio=5 os_prio=0 tid=0x00007f6198013800 nid=0x2364 waiting on condition [0x00007f61a10ab000]2020-06-02T19:01:32.1773113Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1773526Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1774138Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1774697Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1775231Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1775770Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1776225Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1776747Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1777357Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1777752Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1777943Z 2020-06-02T19:01:32.1778554Z "mini-cluster-io-thread-2" #53 daemon prio=5 os_prio=0 tid=0x00007f630030c800 nid=0x2363 waiting on condition [0x00007f61a11ac000]2020-06-02T19:01:32.1779065Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1779372Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1779968Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1780447Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1780980Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1781503Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1781977Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1782451Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1782920Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1783316Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1783510Z 2020-06-02T19:01:32.1784119Z "Flink-DispatcherRestEndpoint-thread-1" #52 daemon prio=5 os_prio=0 tid=0x00007f6161fad000 nid=0x2362 waiting on condition [0x00007f61a12ad000]2020-06-02T19:01:32.1784565Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1784856Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1785445Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1785925Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1786475Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1787228Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1787826Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1788354Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1788830Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1789300Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1789691Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1789880Z 2020-06-02T19:01:32.1790488Z "mini-cluster-io-thread-1" #51 daemon prio=5 os_prio=0 tid=0x00007f6161faa800 nid=0x2361 waiting on condition [0x00007f61a13ae000]2020-06-02T19:01:32.1790911Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1791202Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1791786Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1792265Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1792794Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1793408Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1793880Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1794359Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1794829Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1795224Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1795411Z 2020-06-02T19:01:32.1796007Z "flink-rest-server-netty-boss-thread-1" #50 daemon prio=5 os_prio=0 tid=0x00007f6161fa5000 nid=0x2360 runnable [0x00007f61a14af000]2020-06-02T19:01:32.1796466Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1796778Z at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)2020-06-02T19:01:32.1797363Z at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)2020-06-02T19:01:32.1797790Z at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)2020-06-02T19:01:32.1798206Z at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)2020-06-02T19:01:32.1798866Z - locked &lt;0x0000000081f004e8&gt; (a org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet)2020-06-02T19:01:32.1799476Z - locked &lt;0x0000000081f00500&gt; (a java.util.Collections$UnmodifiableSet)2020-06-02T19:01:32.1799991Z - locked &lt;0x0000000081f004a0&gt; (a sun.nio.ch.EPollSelectorImpl)2020-06-02T19:01:32.1800359Z at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)2020-06-02T19:01:32.1800875Z at org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)2020-06-02T19:01:32.1801498Z at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:806)2020-06-02T19:01:32.1802036Z at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:454)2020-06-02T19:01:32.1802624Z at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)2020-06-02T19:01:32.1803260Z at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)2020-06-02T19:01:32.1803690Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1803895Z 2020-06-02T19:01:32.1804241Z "IOManager reader thread #1" #45 daemon prio=5 os_prio=0 tid=0x00007f63014fa000 nid=0x235d waiting on condition [0x00007f61a1db0000]2020-06-02T19:01:32.1804685Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1804981Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1805571Z - parking to wait for &lt;0x0000000081901368&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1806053Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1806604Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1807373Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1807892Z at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:354)2020-06-02T19:01:32.1808204Z 2020-06-02T19:01:32.1808550Z "IOManager writer thread #1" #44 daemon prio=5 os_prio=0 tid=0x00007f63014f9000 nid=0x235c waiting on condition [0x00007f61a1eb1000]2020-06-02T19:01:32.1808996Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1809285Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1809897Z - parking to wait for &lt;0x0000000081977a50&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1810382Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1810893Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1811530Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1812047Z at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:460)2020-06-02T19:01:32.1812361Z 2020-06-02T19:01:32.1812888Z "Timer-2" #42 daemon prio=5 os_prio=0 tid=0x00007f63014c0000 nid=0x235b in Object.wait() [0x00007f61a1fb2000]2020-06-02T19:01:32.1813326Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1813641Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1814113Z - waiting on &lt;0x0000000081901c00&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1814450Z at java.util.TimerThread.mainLoop(Timer.java:552)2020-06-02T19:01:32.1814940Z - locked &lt;0x0000000081901c00&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1815270Z at java.util.TimerThread.run(Timer.java:505)2020-06-02T19:01:32.1815469Z 2020-06-02T19:01:32.1816092Z "Timer-1" #40 daemon prio=5 os_prio=0 tid=0x00007f63014be000 nid=0x235a in Object.wait() [0x00007f61a20b3000]2020-06-02T19:01:32.1816569Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1817028Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1817513Z - waiting on &lt;0x0000000081901570&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1817846Z at java.util.TimerThread.mainLoop(Timer.java:552)2020-06-02T19:01:32.1818339Z - locked &lt;0x0000000081901570&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1818650Z at java.util.TimerThread.run(Timer.java:505)2020-06-02T19:01:32.1818862Z 2020-06-02T19:01:32.1819196Z "BLOB Server listener at 39424" #36 daemon prio=5 os_prio=0 tid=0x00007f63014bb000 nid=0x2359 runnable [0x00007f61a21b4000]2020-06-02T19:01:32.1819610Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1819903Z at java.net.PlainSocketImpl.socketAccept(Native Method)2020-06-02T19:01:32.1820304Z at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)2020-06-02T19:01:32.1820726Z at java.net.ServerSocket.implAccept(ServerSocket.java:560)2020-06-02T19:01:32.1821107Z at java.net.ServerSocket.accept(ServerSocket.java:528)2020-06-02T19:01:32.1821511Z at org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:262)2020-06-02T19:01:32.1821756Z 2020-06-02T19:01:32.1822269Z "Timer-0" #37 daemon prio=5 os_prio=0 tid=0x00007f63014a5800 nid=0x2358 in Object.wait() [0x00007f61a22b5000]2020-06-02T19:01:32.1822709Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1823030Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1823478Z - waiting on &lt;0x0000000081901740&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1823816Z at java.util.TimerThread.mainLoop(Timer.java:552)2020-06-02T19:01:32.1824304Z - locked &lt;0x0000000081901740&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1824629Z at java.util.TimerThread.run(Timer.java:505)2020-06-02T19:01:32.1824823Z 2020-06-02T19:01:32.1825397Z "flink-metrics-scheduler-1" #32 prio=5 os_prio=0 tid=0x00007f6301468000 nid=0x2354 waiting on condition [0x00007f61a26b9000]2020-06-02T19:01:32.1825831Z java.lang.Thread.State: TIMED_WAITING (sleeping)2020-06-02T19:01:32.1826134Z at java.lang.Thread.sleep(Native Method)2020-06-02T19:01:32.1826559Z at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)2020-06-02T19:01:32.1827253Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)2020-06-02T19:01:32.1827792Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2020-06-02T19:01:32.1828191Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1828392Z 2020-06-02T19:01:32.1829014Z "flink-scheduler-1" #27 prio=5 os_prio=0 tid=0x00007f6300ec1800 nid=0x22f5 waiting on condition [0x00007f61a2fbe000]2020-06-02T19:01:32.1829625Z java.lang.Thread.State: TIMED_WAITING (sleeping)2020-06-02T19:01:32.1830040Z at java.lang.Thread.sleep(Native Method)2020-06-02T19:01:32.1830784Z at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)2020-06-02T19:01:32.1831508Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)2020-06-02T19:01:32.1832253Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2020-06-02T19:01:32.1832842Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1833195Z 2020-06-02T19:01:32.1833740Z "process reaper" #24 daemon prio=10 os_prio=0 tid=0x00007f61a8048000 nid=0x2222 waiting on condition [0x00007f61a3a81000]2020-06-02T19:01:32.1834332Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1834709Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1835502Z - parking to wait for &lt;0x0000000080ba0518&gt; (a java.util.concurrent.SynchronousQueue$TransferStack)2020-06-02T19:01:32.1836129Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1837204Z at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)2020-06-02T19:01:32.1837935Z at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)2020-06-02T19:01:32.1838607Z at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)2020-06-02T19:01:32.1839184Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)2020-06-02T19:01:32.1839801Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1840433Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1840957Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1841215Z 2020-06-02T19:01:32.1842084Z "surefire-forkedjvm-ping-30s" #23 daemon prio=5 os_prio=0 tid=0x00007f63003c3000 nid=0x221f waiting on condition [0x00007f61b07c6000]2020-06-02T19:01:32.1842680Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1843116Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1843930Z - parking to wait for &lt;0x0000000080b92410&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1844585Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1845348Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-06-02T19:01:32.1846221Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-06-02T19:01:32.1847278Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1848018Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1848653Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1849341Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1849850Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1850130Z 2020-06-02T19:01:32.1850983Z "surefire-forkedjvm-command-thread" #22 daemon prio=5 os_prio=0 tid=0x00007f63003ac000 nid=0x221c runnable [0x00007f61b0ad1000]2020-06-02T19:01:32.1851572Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1851989Z at java.io.FileInputStream.readBytes(Native Method)2020-06-02T19:01:32.1852454Z at java.io.FileInputStream.read(FileInputStream.java:255)2020-06-02T19:01:32.1852980Z at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)2020-06-02T19:01:32.1853504Z at java.io.BufferedInputStream.read(BufferedInputStream.java:265)2020-06-02T19:01:32.1854228Z - locked &lt;0x0000000080c53790&gt; (a java.io.BufferedInputStream)2020-06-02T19:01:32.1854701Z at java.io.DataInputStream.readInt(DataInputStream.java:387)2020-06-02T19:01:32.1855323Z at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)2020-06-02T19:01:32.1856252Z at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)2020-06-02T19:01:32.1857036Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1857304Z 2020-06-02T19:01:32.1857755Z "Service Thread" #21 daemon prio=9 os_prio=0 tid=0x00007f63002d5000 nid=0x221a runnable [0x0000000000000000]2020-06-02T19:01:32.1858280Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1858532Z 2020-06-02T19:01:32.1858991Z "C1 CompilerThread14" #20 daemon prio=9 os_prio=0 tid=0x00007f63002c8000 nid=0x2219 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1859548Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1859786Z 2020-06-02T19:01:32.1860247Z "C1 CompilerThread13" #19 daemon prio=9 os_prio=0 tid=0x00007f63002c6000 nid=0x2218 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1860802Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1861042Z 2020-06-02T19:01:32.1861653Z "C1 CompilerThread12" #18 daemon prio=9 os_prio=0 tid=0x00007f63002c4000 nid=0x2217 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1862227Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1862465Z 2020-06-02T19:01:32.1862931Z "C1 CompilerThread11" #17 daemon prio=9 os_prio=0 tid=0x00007f63002c2000 nid=0x2216 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1863458Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1863711Z 2020-06-02T19:01:32.1864153Z "C1 CompilerThread10" #16 daemon prio=9 os_prio=0 tid=0x00007f63002c0000 nid=0x2215 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1864696Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1864932Z 2020-06-02T19:01:32.1865389Z "C2 CompilerThread9" #15 daemon prio=9 os_prio=0 tid=0x00007f63002bd000 nid=0x2214 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1865921Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1866175Z 2020-06-02T19:01:32.1866632Z "C2 CompilerThread8" #14 daemon prio=9 os_prio=0 tid=0x00007f63002bb800 nid=0x2213 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1867418Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1867660Z 2020-06-02T19:01:32.1868123Z "C2 CompilerThread7" #13 daemon prio=9 os_prio=0 tid=0x00007f63002b9000 nid=0x2212 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1868652Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1868908Z 2020-06-02T19:01:32.1869440Z "C2 CompilerThread6" #12 daemon prio=9 os_prio=0 tid=0x00007f63002b7800 nid=0x2211 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1869969Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1870199Z 2020-06-02T19:01:32.1870646Z "C2 CompilerThread5" #11 daemon prio=9 os_prio=0 tid=0x00007f63002b5000 nid=0x2210 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1871166Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1871396Z 2020-06-02T19:01:32.1871860Z "C2 CompilerThread4" #10 daemon prio=9 os_prio=0 tid=0x00007f63002b3000 nid=0x220f waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1872387Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1872635Z 2020-06-02T19:01:32.1873060Z "C2 CompilerThread3" #9 daemon prio=9 os_prio=0 tid=0x00007f63002a9000 nid=0x220e waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1873590Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1873815Z 2020-06-02T19:01:32.1874267Z "C2 CompilerThread2" #8 daemon prio=9 os_prio=0 tid=0x00007f63002a6800 nid=0x220d waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1874805Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1875035Z 2020-06-02T19:01:32.1875466Z "C2 CompilerThread1" #7 daemon prio=9 os_prio=0 tid=0x00007f63002a4800 nid=0x220c waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1875992Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1876218Z 2020-06-02T19:01:32.1876676Z "C2 CompilerThread0" #6 daemon prio=9 os_prio=0 tid=0x00007f63002a2800 nid=0x220b waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1877580Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1877815Z 2020-06-02T19:01:32.1878222Z "Signal Dispatcher" #5 daemon prio=9 os_prio=0 tid=0x00007f63002a0800 nid=0x220a runnable [0x0000000000000000]2020-06-02T19:01:32.1878748Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1878977Z 2020-06-02T19:01:32.1879472Z "Surrogate Locker Thread (Concurrent GC)" #4 daemon prio=9 os_prio=0 tid=0x00007f630029f000 nid=0x2209 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1880026Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1880278Z 2020-06-02T19:01:32.1880699Z "Finalizer" #3 daemon prio=8 os_prio=0 tid=0x00007f630026e800 nid=0x2208 in Object.wait() [0x00007f620958e000]2020-06-02T19:01:32.1881124Z java.lang.Thread.State: WAITING (on object monitor)2020-06-02T19:01:32.1881408Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1882065Z - waiting on &lt;0x0000000080c53a60&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-06-02T19:01:32.1882609Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)2020-06-02T19:01:32.1883144Z - locked &lt;0x0000000080c53a60&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-06-02T19:01:32.1883522Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)2020-06-02T19:01:32.1883906Z at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)2020-06-02T19:01:32.1884151Z 2020-06-02T19:01:32.1884675Z "Reference Handler" #2 daemon prio=10 os_prio=0 tid=0x00007f630026a000 nid=0x2207 in Object.wait() [0x00007f620968f000]2020-06-02T19:01:32.1885086Z java.lang.Thread.State: WAITING (on object monitor)2020-06-02T19:01:32.1885390Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1885872Z - waiting on &lt;0x0000000080c53a50&gt; (a java.lang.ref.Reference$Lock)2020-06-02T19:01:32.1886186Z at java.lang.Object.wait(Object.java:502)2020-06-02T19:01:32.1886587Z at java.lang.ref.Reference.tryHandlePending(Reference.java:191)2020-06-02T19:01:32.1887443Z - locked &lt;0x0000000080c53a50&gt; (a java.lang.ref.Reference$Lock)2020-06-02T19:01:32.1887838Z at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)2020-06-02T19:01:32.1888077Z 2020-06-02T19:01:32.1888380Z "main" #1 prio=5 os_prio=0 tid=0x00007f630000b800 nid=0x21cf waiting on condition [0x00007f6306d43000]2020-06-02T19:01:32.1888757Z java.lang.Thread.State: TIMED_WAITING (sleeping)2020-06-02T19:01:32.1889050Z at java.lang.Thread.sleep(Native Method)2020-06-02T19:01:32.1889437Z at org.apache.commons.exec.DefaultExecuteResultHandler.waitFor(DefaultExecuteResultHandler.java:121)2020-06-02T19:01:32.1889917Z at ch.vorburger.exec.ManagedProcess.destroy(ManagedProcess.java:344)2020-06-02T19:01:32.1890289Z at ch.vorburger.mariadb4j.DB.stop(DB.java:327)2020-06-02T19:01:32.1890764Z - locked &lt;0x00000000816a5170&gt; (a ch.vorburger.mariadb4j.DB)2020-06-02T19:01:32.1891151Z at ch.vorburger.mariadb4j.junit.MariaDB4jRule.after(MariaDB4jRule.java:64)2020-06-02T19:01:32.1891571Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:50)2020-06-02T19:01:32.1892001Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-06-02T19:01:32.1892425Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-06-02T19:01:32.1892802Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-06-02T19:01:32.1893180Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-06-02T19:01:32.1893594Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-06-02T19:01:32.1894085Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-06-02T19:01:32.1894575Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-06-02T19:01:32.1895059Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-06-02T19:01:32.1895567Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-06-02T19:01:32.1896211Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-06-02T19:01:32.1896744Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-06-02T19:01:32.1897423Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)</description>
      <version>1.12.0,1.15.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-4 01:00:00" id="18117" opendate="2020-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Kerberized YARN per-job on Docker test" fails with "Could not start hadoop cluster."</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2683&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f52020-06-04T06:03:53.2844296Z Creating slave1 ... [32mdone [0m2020-06-04T06:03:53.4981251Z [1BWaiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...2020-06-04T06:03:58.5980181Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...2020-06-04T06:04:03.6997087Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...2020-06-04T06:04:08.7910791Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...2020-06-04T06:04:13.8921621Z Waiting for hadoop cluster to come up. We have been trying for 20 seconds, retrying ...2020-06-04T06:04:18.9648844Z Waiting for hadoop cluster to come up. We have been trying for 25 seconds, retrying ...2020-06-04T06:04:24.0381851Z Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...2020-06-04T06:04:29.1220264Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...2020-06-04T06:04:34.1882187Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...2020-06-04T06:04:39.2784948Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...2020-06-04T06:04:44.3843337Z Waiting for hadoop cluster to come up. We have been trying for 51 seconds, retrying ...2020-06-04T06:04:49.4703561Z Waiting for hadoop cluster to come up. We have been trying for 56 seconds, retrying ...2020-06-04T06:04:54.5463207Z Waiting for hadoop cluster to come up. We have been trying for 61 seconds, retrying ...2020-06-04T06:04:59.6650405Z Waiting for hadoop cluster to come up. We have been trying for 66 seconds, retrying ...2020-06-04T06:05:04.7500168Z Waiting for hadoop cluster to come up. We have been trying for 71 seconds, retrying ...2020-06-04T06:05:09.8177904Z Waiting for hadoop cluster to come up. We have been trying for 76 seconds, retrying ...2020-06-04T06:05:14.9751297Z Waiting for hadoop cluster to come up. We have been trying for 81 seconds, retrying ...2020-06-04T06:05:20.0336417Z Waiting for hadoop cluster to come up. We have been trying for 87 seconds, retrying ...2020-06-04T06:05:25.1627704Z Waiting for hadoop cluster to come up. We have been trying for 92 seconds, retrying ...2020-06-04T06:05:30.2583315Z Waiting for hadoop cluster to come up. We have been trying for 97 seconds, retrying ...2020-06-04T06:05:35.3283678Z Waiting for hadoop cluster to come up. We have been trying for 102 seconds, retrying ...2020-06-04T06:05:40.4184029Z Waiting for hadoop cluster to come up. We have been trying for 107 seconds, retrying ...2020-06-04T06:05:45.5388372Z Waiting for hadoop cluster to come up. We have been trying for 112 seconds, retrying ...2020-06-04T06:05:50.6155334Z Waiting for hadoop cluster to come up. We have been trying for 117 seconds, retrying ...2020-06-04T06:05:55.7225186Z Command: start_hadoop_cluster failed. Retrying...2020-06-04T06:05:55.7237999Z Starting Hadoop cluster2020-06-04T06:05:56.5188293Z kdc is up-to-date2020-06-04T06:05:56.5292716Z master is up-to-date2020-06-04T06:05:56.5301735Z slave2 is up-to-date2020-06-04T06:05:56.5306179Z slave1 is up-to-date2020-06-04T06:05:56.6800566Z Waiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...2020-06-04T06:06:01.7668291Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...2020-06-04T06:06:06.8620265Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...2020-06-04T06:06:11.9753596Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...2020-06-04T06:06:17.0402846Z Waiting for hadoop cluster to come up. We have been trying for 21 seconds, retrying ...2020-06-04T06:06:22.1650005Z Waiting for hadoop cluster to come up. We have been trying for 26 seconds, retrying ...2020-06-04T06:06:27.2500179Z Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...2020-06-04T06:06:32.3133809Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...2020-06-04T06:06:37.4432923Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...2020-06-04T06:06:42.5658250Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...2020-06-04T06:06:47.6682536Z Waiting for hadoop cluster to come up. We have been trying for 51 seconds, retrying ...2020-06-04T06:06:52.7810371Z Waiting for hadoop cluster to come up. We have been trying for 56 seconds, retrying ...2020-06-04T06:06:57.8860269Z Waiting for hadoop cluster to come up. We have been trying for 61 seconds, retrying ...2020-06-04T06:07:03.0337979Z Waiting for hadoop cluster to come up. We have been trying for 67 seconds, retrying ...2020-06-04T06:07:08.1080310Z Waiting for hadoop cluster to come up. We have been trying for 72 seconds, retrying ...2020-06-04T06:07:13.2297578Z Waiting for hadoop cluster to come up. We have been trying for 77 seconds, retrying ...2020-06-04T06:07:18.3779034Z Waiting for hadoop cluster to come up. We have been trying for 82 seconds, retrying ...2020-06-04T06:07:23.4789495Z Waiting for hadoop cluster to come up. We have been trying for 87 seconds, retrying ...2020-06-04T06:07:28.6063062Z Waiting for hadoop cluster to come up. We have been trying for 92 seconds, retrying ...2020-06-04T06:07:33.8220409Z Waiting for hadoop cluster to come up. We have been trying for 97 seconds, retrying ...2020-06-04T06:07:38.9439231Z Waiting for hadoop cluster to come up. We have been trying for 102 seconds, retrying ...2020-06-04T06:07:44.0193849Z Waiting for hadoop cluster to come up. We have been trying for 108 seconds, retrying ...2020-06-04T06:07:49.1241642Z Waiting for hadoop cluster to come up. We have been trying for 113 seconds, retrying ...2020-06-04T06:07:54.2425087Z Waiting for hadoop cluster to come up. We have been trying for 118 seconds, retrying ...2020-06-04T06:07:59.3835321Z Command: start_hadoop_cluster failed. Retrying...2020-06-04T06:07:59.3847275Z Starting Hadoop cluster2020-06-04T06:08:00.1959109Z kdc is up-to-date2020-06-04T06:08:00.1968717Z master is up-to-date2020-06-04T06:08:00.1982811Z slave1 is up-to-date2020-06-04T06:08:00.1988143Z slave2 is up-to-date2020-06-04T06:08:00.4014781Z Waiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...2020-06-04T06:08:05.5168483Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...2020-06-04T06:08:10.6759355Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...2020-06-04T06:08:15.8307550Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...2020-06-04T06:08:21.0143341Z Waiting for hadoop cluster to come up. We have been trying for 21 seconds, retrying ...2020-06-04T06:08:26.0932297Z Waiting for hadoop cluster to come up. We have been trying for 26 seconds, retrying ...2020-06-04T06:08:31.2526775Z Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...2020-06-04T06:08:36.4356124Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...2020-06-04T06:08:41.5607530Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...2020-06-04T06:08:46.6407963Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...2020-06-04T06:08:51.8464789Z Waiting for hadoop cluster to come up. We have been trying for 51 seconds, retrying ...2020-06-04T06:08:56.9735817Z Waiting for hadoop cluster to come up. We have been trying for 56 seconds, retrying ...2020-06-04T06:09:02.1023842Z Waiting for hadoop cluster to come up. We have been trying for 62 seconds, retrying ...2020-06-04T06:09:07.2390427Z Waiting for hadoop cluster to come up. We have been trying for 67 seconds, retrying ...2020-06-04T06:09:12.4433329Z Waiting for hadoop cluster to come up. We have been trying for 72 seconds, retrying ...2020-06-04T06:09:17.5390800Z Waiting for hadoop cluster to come up. We have been trying for 77 seconds, retrying ...2020-06-04T06:09:22.7020537Z Waiting for hadoop cluster to come up. We have been trying for 82 seconds, retrying ...2020-06-04T06:09:27.8754909Z Waiting for hadoop cluster to come up. We have been trying for 87 seconds, retrying ...2020-06-04T06:09:33.0447274Z Waiting for hadoop cluster to come up. We have been trying for 93 seconds, retrying ...2020-06-04T06:09:38.1804596Z Waiting for hadoop cluster to come up. We have been trying for 98 seconds, retrying ...2020-06-04T06:09:43.3636590Z Waiting for hadoop cluster to come up. We have been trying for 103 seconds, retrying ...2020-06-04T06:09:48.4975410Z Waiting for hadoop cluster to come up. We have been trying for 108 seconds, retrying ...2020-06-04T06:09:53.6117328Z Waiting for hadoop cluster to come up. We have been trying for 113 seconds, retrying ...2020-06-04T06:09:58.7785946Z Waiting for hadoop cluster to come up. We have been trying for 118 seconds, retrying ...2020-06-04T06:10:03.9748663Z Command: start_hadoop_cluster failed. Retrying...2020-06-04T06:10:03.9808244Z Command: start_hadoop_cluster failed 3 times.2020-06-04T06:10:03.9823071Z ERROR: Could not start hadoop cluster. Aborting...Frequent, suspicious logs2020-06-04T06:10:04.5032658Z 20/06/04 06:05:42 WARN ipc.Client: Failed to connect to server: master.docker-hadoop-cluster-network/172.19.0.3:9000: try once and fail.2020-06-04T06:10:04.5033211Z java.net.ConnectException: Connection refused...2020-06-04T06:10:04.6867876Z 20/06/04 06:04:11 ERROR namenode.NameNode: Failed to start namenode.2020-06-04T06:10:04.6868640Z java.net.BindException: Port in use: 0.0.0.0:504702020-06-04T06:10:04.6869062Z at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:998)2020-06-04T06:10:04.6869702Z at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:935)2020-06-04T06:10:04.6870199Z at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:171)2020-06-04T06:10:04.6870740Z at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:842)2020-06-04T06:10:04.6871235Z at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:693)2020-06-04T06:10:04.6871728Z at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:906)2020-06-04T06:10:04.6872202Z at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:885)2020-06-04T06:10:04.6872699Z at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1626)2020-06-04T06:10:04.6873701Z at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1694)2020-06-04T06:10:04.6874100Z Caused by: java.net.BindException: Address already in use2020-06-04T06:10:04.6901805Z at sun.nio.ch.Net.bind0(Native Method)2020-06-04T06:10:04.6902168Z at sun.nio.ch.Net.bind(Net.java:433)2020-06-04T06:10:04.6902478Z at sun.nio.ch.Net.bind(Net.java:425)2020-06-04T06:10:04.6902847Z at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)2020-06-04T06:10:04.6903296Z at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)2020-06-04T06:10:04.6903744Z at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)2020-06-04T06:10:04.6904395Z at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:993)2020-06-04T06:10:04.6904727Z ... 8 more2020-06-04T06:10:04.6905005Z 20/06/04 06:04:11 INFO util.ExitUtil: Exiting with status 12020-06-04T06:10:04.6905401Z 20/06/04 06:04:11 INFO namenode.NameNode: SHUTDOWN_MSG:</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.config.hdfs-site.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.Dockerfile</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.bootstrap.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-4 01:00:00" id="18125" opendate="2020-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip CI execution on documentation pull requests</summary>
      <description>In order to save some resources, we can skip the CI execution on documentation-only changes (whole changeset).</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.free.disk.space.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-5 01:00:00" id="18140" opendate="2020-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for ORC format</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.parquet.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.parquet.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-5 01:00:00" id="18141" opendate="2020-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Parquet format</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-8 01:00:00" id="18173" opendate="2020-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bundle flink-csv and flink-json jars in lib</summary>
      <description>The biggest problem for distributions I see is the variety of problems caused by users' lack of format dependency. These three formats are very small and no third party dependence, and they are widely used by table users. Actually, we don't have any other built-in table formats now.. flink-csv-1.10.0.jar flink-json-1.10.0.jar We can just bundle them in "flink/lib/". It not solve all problems and it is independent of "fat" and "slim". But also improve usability.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.release-notes.flink-1.11.zh.md</file>
      <file type="M">docs.release-notes.flink-1.11.md</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">docs.dev.table.connectors.formats.avro.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.avro.md</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-9 01:00:00" id="18222" opendate="2020-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Avro Confluent Schema Registry nightly end-to-end test" unstable with "Kafka cluster did not start after 120 seconds"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3045&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f52020-06-09T15:16:48.1427795Z ==============================================================================2020-06-09T15:16:48.1428609Z Running 'Avro Confluent Schema Registry nightly end-to-end test'2020-06-09T15:16:48.1429204Z ==============================================================================2020-06-09T15:16:48.1438117Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-481432981702020-06-09T15:16:48.2985167Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT2020-06-09T15:16:48.3157575Z Downloading Kafka from https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz2020-06-09T15:16:48.3214487Z % Total % Received % Xferd Average Speed Time Time Time Current2020-06-09T15:16:48.3215154Z Dload Upload Total Spent Left Speed2020-06-09T15:16:48.3215597Z 2020-06-09T15:16:48.3528820Z 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 02020-06-09T15:16:49.3421526Z 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 02020-06-09T15:16:50.3415678Z 8 35.8M 8 2960k 0 0 2896k 0 0:00:12 0:00:01 0:00:11 2896k2020-06-09T15:16:51.3406836Z 23 35.8M 23 8544k 0 0 4226k 0 0:00:08 0:00:02 0:00:06 4225k2020-06-09T15:16:51.6553485Z 70 35.8M 70 25.2M 0 0 8550k 0 0:00:04 0:00:03 0:00:01 8548k2020-06-09T15:16:51.6555606Z 100 35.8M 100 35.8M 0 0 10.7M 0 0:00:03 0:00:03 --:--:-- 10.7M2020-06-09T15:16:51.9818041Z Downloading confluent from http://packages.confluent.io/archive/3.2/confluent-oss-3.2.0-2.11.tar.gz2020-06-09T15:16:51.9880242Z % Total % Received % Xferd Average Speed Time Time Time Current2020-06-09T15:16:51.9880983Z Dload Upload Total Spent Left Speed2020-06-09T15:16:51.9914252Z 2020-06-09T15:16:52.3398614Z 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 02020-06-09T15:16:53.3399552Z 9 398M 9 39.5M 0 0 111M 0 0:00:03 --:--:-- 0:00:03 111M2020-06-09T15:16:53.9149276Z 47 398M 47 188M 0 0 139M 0 0:00:02 0:00:01 0:00:01 138M2020-06-09T15:16:53.9150980Z 100 398M 100 398M 0 0 206M 0 0:00:01 0:00:01 --:--:-- 206M2020-06-09T15:17:04.3565942Z Waiting for broker...2020-06-09T15:17:12.4215170Z Waiting for broker...2020-06-09T15:17:14.3012835Z Waiting for broker...2020-06-09T15:17:16.1965074Z Waiting for broker...2020-06-09T15:17:18.1102274Z Waiting for broker...2020-06-09T15:17:19.9929632Z Waiting for broker...2020-06-09T15:17:21.8607172Z Waiting for broker...2020-06-09T15:17:23.7802949Z Waiting for broker...2020-06-09T15:17:25.6695260Z Waiting for broker...2020-06-09T15:17:27.5536417Z Waiting for broker...2020-06-09T15:17:29.4327778Z Waiting for broker...2020-06-09T15:17:31.3203091Z Waiting for broker...2020-06-09T15:17:33.1987150Z Waiting for broker...2020-06-09T15:17:35.0694860Z Waiting for broker...2020-06-09T15:17:36.9595576Z Waiting for broker...2020-06-09T15:17:38.9243558Z Waiting for broker...2020-06-09T15:17:40.7984064Z Waiting for broker...2020-06-09T15:17:42.6676095Z Waiting for broker...2020-06-09T15:17:44.5628797Z Waiting for broker...2020-06-09T15:17:46.4374532Z Waiting for broker...2020-06-09T15:17:48.3086761Z Waiting for broker...2020-06-09T15:17:50.1574336Z Waiting for broker...2020-06-09T15:17:52.0432952Z Waiting for broker...2020-06-09T15:17:53.9406541Z Waiting for broker...2020-06-09T15:17:55.8162052Z Waiting for broker...2020-06-09T15:17:57.7090015Z Waiting for broker...2020-06-09T15:17:59.5747770Z Waiting for broker...2020-06-09T15:18:01.4601854Z Waiting for broker...2020-06-09T15:18:03.3332039Z Waiting for broker...2020-06-09T15:18:05.2210453Z Waiting for broker...2020-06-09T15:18:07.1133675Z Waiting for broker...2020-06-09T15:18:09.0132417Z Waiting for broker...2020-06-09T15:18:10.8769511Z Waiting for broker...2020-06-09T15:18:12.7601639Z Waiting for broker...2020-06-09T15:18:14.6389770Z Waiting for broker...2020-06-09T15:18:16.5210725Z Waiting for broker...2020-06-09T15:18:18.4088216Z Waiting for broker...2020-06-09T15:18:20.2732225Z Waiting for broker...2020-06-09T15:18:22.1558390Z Waiting for broker...2020-06-09T15:18:24.0400570Z Waiting for broker...2020-06-09T15:18:25.9134038Z Waiting for broker...2020-06-09T15:18:27.7922350Z Waiting for broker...2020-06-09T15:18:29.6748679Z Waiting for broker...2020-06-09T15:18:31.5340996Z Waiting for broker...2020-06-09T15:18:33.3998472Z Waiting for broker...2020-06-09T15:18:35.2718135Z Waiting for broker...2020-06-09T15:18:37.1426082Z Waiting for broker...2020-06-09T15:18:39.1282264Z Waiting for broker...2020-06-09T15:18:41.0029183Z Waiting for broker...2020-06-09T15:18:42.8700037Z Waiting for broker...2020-06-09T15:18:44.7531621Z Waiting for broker...2020-06-09T15:18:46.6465173Z Waiting for broker...2020-06-09T15:18:48.9504192Z Waiting for broker...2020-06-09T15:18:50.4165383Z Waiting for broker...2020-06-09T15:18:52.2931688Z Waiting for broker...2020-06-09T15:18:54.1669857Z Waiting for broker...2020-06-09T15:18:56.0238505Z Waiting for broker...2020-06-09T15:18:57.8931143Z Waiting for broker...2020-06-09T15:18:59.7607751Z Kafka cluster did not start after 120 seconds. Printing Kafka logs:There's a lot of log output I didn't analyze yet.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-11 01:00:00" id="18246" opendate="2020-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink e2e fails with java version mismatch on JDK11 nightly build</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3213&amp;view=logs&amp;j=6caf31d6-847a-526e-9624-468e053467d6&amp;t=679407b1-ea2c-5965-2c8d-1467777fff88Preparing transaction: ...working... doneVerifying transaction: ...working... doneExecuting transaction: ...working... doneError: A JNI error has occurred, please check your installation and try againException in thread "main" java.lang.UnsupportedClassVersionError: org/apache/flink/client/cli/CliFrontend has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0 at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)No taskexecutor daemon (pid: 123813) is running anymore on fv-az670.No standalonesession daemon to stop on host fv-az670.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-11 01:00:00" id="18248" opendate="2020-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update data type documentation for 1.11</summary>
      <description>Update the data type documentation for 1.11.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.types.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-15 01:00:00" id="18291" opendate="2020-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming File Sink s3 end-to-end test stalls</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3444&amp;view=logs&amp;j=5c8e7682-d68f-54d1-16a2-a09310218a49&amp;t=f508e270-48d6-5f1e-3138-42a17e0714f02020-06-12T21:55:57.6277963Z Number of produced values 10870/600002020-06-12T21:57:10.5467073Z Number of produced values 22960/600002020-06-12T21:58:01.0025226Z Number of produced values 59650/600002020-06-12T21:58:52.5624619Z Number of produced values 60000/600002020-06-12T21:58:53.2407133Z Cancelling job 9412dcb358631ab461a3a1e851417b9e.2020-06-12T21:58:54.0819168Z Cancelled job 9412dcb358631ab461a3a1e851417b9e.2020-06-12T21:58:54.1097745Z Waiting for job (9412dcb358631ab461a3a1e851417b9e) to reach terminal state CANCELED ...2020-06-13T00:00:35.0502923Z ##[error]The operation was canceled.2020-06-13T00:00:35.0522780Z ##[section]Finishing: Run e2e tests</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.file.sink.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-15 01:00:00" id="18307" opendate="2020-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace "slave" file name with "workers"</summary>
      <description>See parent issue for a discussion of the rationale.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkDistribution.java</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.slaves</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
      <file type="M">docs.ops.deployment.cluster.setup.zh.md</file>
      <file type="M">docs.ops.deployment.cluster.setup.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-16 01:00:00" id="18323" opendate="2020-6-16 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement a Kafka Source based on new Source API</summary>
      <description>This is an umbrella ticket for a new Kafka Source implementation based on the new Source API.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-16 01:00:00" id="18324" opendate="2020-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate updated data type and function page into Chinese</summary>
      <description>The Chinese translations of the pages updated in FLINK-18248 and FLINK-18065 need an update.</description>
      <version>None</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.types.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-17 01:00:00" id="18343" opendate="2020-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable DEBUG logging for java e2e tests</summary>
      <description>Java e2e tests run with the default logging configuration, which only logs on INFO.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.main.java.org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkDistribution.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-18 01:00:00" id="18361" opendate="2020-6-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support username and password options for new Elasticsearch connector</summary>
      <description>We support username and password options for ES connector in FLINK-16788. As we introduce new ES connector in FLINK-17027, we should also add this support to the new ES connector.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchOptions.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchConfiguration.java</file>
      <file type="M">docs.dev.table.connectors.elasticsearch.zh.md</file>
      <file type="M">docs.dev.table.connectors.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-18 01:00:00" id="18368" opendate="2020-6-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HadoopRecoverableWriterOldHadoopWithNoTruncateSupportTest.createHDFS fails with "Running in secure mode, but config doesn&amp;#39;t have a keytab"</summary>
      <description>https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8184&amp;view=logs&amp;j=66592496-52df-56bb-d03e-37509e1d9d0f&amp;t=ae0269db-6796-5583-2e5f-d84757d711aa</description>
      <version>1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-hadoop-fs.src.test.java.org.apache.flink.runtime.util.HadoopUtilsTest.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2020-7-4 01:00:00" id="18485" opendate="2020-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kerberized YARN per-job on Docker test failed during unzip jce_policy-8.zip</summary>
      <description>Instance on 1.11 branch: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4230&amp;view=logs&amp;j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&amp;t=94459a52-42b6-5bfc-5d74-690b5d3c6de8+ curl -LOH Cookie: oraclelicense=accept-securebackup-cookie http://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0100 429 100 429 0 0 1616 0 --:--:-- --:--:-- --:--:-- 1616 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0100 7073 100 7073 0 0 20139 0 --:--:-- --:--:-- --:--:-- 20139+ unzip jce_policy-8.zipArchive: jce_policy-8.zip End-of-central-directory signature not found. Either this file is not a zipfile, or it constitutes one disk of a multi-part archive. In the latter case the central directory and zipfile comment will be found on the last disk(s) of this archive.unzip: cannot find zipfile directory in one of jce_policy-8.zip or jce_policy-8.zip.zip, and cannot find jce_policy-8.zip.ZIP, period.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.Dockerfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-5 01:00:00" id="18486" opendate="2020-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the &amp;#39;%&amp;#39; modulus function</summary>
      <description>This is a follow-up issue of FLINK-18240 to add documentation for the new system operator %.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.systemFunctions.zh.md</file>
      <file type="M">docs.dev.table.functions.systemFunctions.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-8 01:00:00" id="18532" opendate="2020-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Beta tag from MATCH_RECOGNIZE docs</summary>
      <description/>
      <version>1.12.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.match.recognize.zh.md</file>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-15 01:00:00" id="18600" opendate="2020-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kerberized YARN per-job on Docker test failed to download JDK 8u251</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4514&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529+ mkdir -p /usr/java/default+ curl -Ls https://download.oracle.com/otn-pub/java/jdk/8u251-b08/3d5a2bb8f8d4428bbe94aed7ec7ae784/jdk-8u251-linux-x64.tar.gz -H Cookie: oraclelicense=accept-securebackup-cookie+ tar --strip-components=1 -xz -C /usr/java/default/gzip: stdin: not in gzip formattar: Child returned status 1</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.Dockerfile</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-16 01:00:00" id="18619" opendate="2020-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update training to use WatermarkStrategy</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.learn-flink.streaming.analytics.zh.md</file>
      <file type="M">docs.learn-flink.streaming.analytics.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-24 01:00:00" id="18699" opendate="2020-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow variables for column names in Scala Table API</summary>
      <description>User have reported that the Scala API lacks a way to reference columns via a name that is stored in a variable. String interpolation is inconvenient in this case:We should allow this also in Scala:tab.select($(keyVar), $(valueVar))</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.api.ExpressionsConsistencyCheckTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.WordCountTable.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.StreamSQLExample.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-24 01:00:00" id="18708" opendate="2020-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The links of the connector sql jar of Kafka 0.10 and 0.11 are extinct</summary>
      <description>The links of the connector sql jar of Kafka 0.10 and 0.11 are extinct. I will fix it as soon as possible.</description>
      <version>1.11.1,1.12.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-29 01:00:00" id="18755" opendate="2020-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RabbitMQ QoS Chinese Documentation</summary>
      <description>Please add documentation for the new QoS settings in the RabbitMQ connector. The added English documentation can be found in the PR here: https://github.com/apache/flink/pull/12729/files#diff-6b432359b51642a8fad3050c4b73f47cR134-R167  </description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.rabbitmq.zh.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-8-30 01:00:00" id="18760" opendate="2020-7-30 00:00:00" resolution="Done">
    <buginformation>
      <summary>Redundant task managers should be released when there&amp;#39;s no job running in session cluster</summary>
      <description>In FLINK-18625, we introduced redundant task managers, as backup resources for speeding up job recovery in cases of task manager lost.For a session cluster, when there's no job running, it would be better to not keep such redundant resources. Currently, Flink session cluster will not request redundant task managers until a job is submitted, but it also will not release redundant task managers after all jobs terminated.This ticket proposes to check and release task managers if there are redundant task managers only.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskManagerCheckInSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-30 01:00:00" id="18764" opendate="2020-7-30 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support from_collection for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.utils.python.PythonTableUtils.scala</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.api.common.python.PythonBridgeUtils.java</file>
      <file type="M">flink-python.pyflink.pyflink.gateway.server.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-30 01:00:00" id="18765" opendate="2020-7-30 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support map() and flat_map() for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamPythonStatelessFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonStatelessFunctionFlatMap.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-30 01:00:00" id="18766" opendate="2020-7-30 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support add_sink() for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.util.DataStreamTestCollectSink.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.util.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-7-31 01:00:00" id="18776" opendate="2020-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"compile_cron_scala212" failed to compile</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5060&amp;view=logs&amp;j=ed6509f5-1153-558c-557a-5ee0afbcdf24&amp;t=241b1e5e-1a8e-5e6a-469a-a9b8cad87065[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-avro-confluent-registry ---[WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:Found Banned Dependency: com.typesafe:ssl-config-core_2.11:jar:0.3.7Found Banned Dependency: com.typesafe.akka:akka-slf4j_2.11:jar:2.5.21Found Banned Dependency: com.typesafe.akka:akka-actor_2.11:jar:2.5.21Found Banned Dependency: org.scala-lang.modules:scala-java8-compat_2.11:jar:0.7.0Found Banned Dependency: com.typesafe.akka:akka-protobuf_2.11:jar:2.5.21Found Banned Dependency: org.apache.flink:flink-table-api-java-bridge_2.11:jar:1.12-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-table-runtime-blink_2.11:jar:1.12-SNAPSHOTFound Banned Dependency: com.typesafe.akka:akka-stream_2.11:jar:2.5.21Found Banned Dependency: com.github.scopt:scopt_2.11:jar:3.5.0Found Banned Dependency: org.apache.flink:flink-runtime_2.11:jar:1.12-SNAPSHOTFound Banned Dependency: org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.1.1Found Banned Dependency: com.twitter:chill_2.11:jar:0.7.6Found Banned Dependency: org.clapper:grizzled-slf4j_2.11:jar:1.3.2Found Banned Dependency: org.apache.flink:flink-streaming-java_2.11:jar:1.12-SNAPSHOTUse 'mvn dependency:tree' to locate the source of the banned dependencies.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-8-5 01:00:00" id="18831" opendate="2020-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the Python documentation about the operations in Table</summary>
      <description>Currently, there are a few documentation is out of date and should be updated. For example, Python UDTF has been already supported in Python Table API and we could use examples of Python UDTF instead of Java UDTF in the Python doc.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-7 01:00:00" id="18849" opendate="2020-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the code tabs of the Flink documents</summary>
      <description>Currently there are some minor problems on the code tabs of the Flink documents: There are some tab labels like `data-lang="Java/Scala"`, which can not be changed synchronously with the label `data-lang="Java"` and `data-lang="Scala"`. Case sensitive. If one code tab has a label `data-lang="java"` and another has the label `data-lang="Java"` in one page. They would not change synchronously. Duplicated content. Many contents in the "Java" tab are the same as the "Scala" tab.I would like to improve the situation by following way:      1. When parsing the label like `data-lang="Java/Scala"`, we can clone the tab content, let one has the label `data-lang="Java"`, another has the label `data-lang="Scala"`.      2. Then force the first character of the data-lang value to be upper case. i.e. if the label is `data-lang="java"`, it will be modified to `data-lang="Java"`.      3. Add a new attribute "data-hide-tabs" to the "tabcontents" to hide the tab headers, so that we can let the text above the codes changes synchronously.      4. Add a new url parameter "code_tab" to set the default code tab to display when entering the page.This way we can remove the duplicated content via merge them into one element with a `data-lang="Java/Scala"` label. And all the tab can be changed synchronously when they are clicked. </description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.page.js.flink.js</file>
      <file type="M">docs.dev.table.types.zh.md</file>
      <file type="M">docs.dev.table.types.md</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-10-7 01:00:00" id="18851" opendate="2020-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add checkpoint type to checkpoint history entries in Web UI</summary>
      <description>It would be helpful to users to better understand checkpointing times, if the type of the checkpoint is displayed in the checkpoint history.Possible types are savepoint, aligned checkpoint, unaligned checkpoint.A possible place can be seen in the screenshot</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointingStatisticsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointStatistics.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-checkpoint.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-10 01:00:00" id="18866" opendate="2020-8-10 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support filter() operation for Python DataStream API.</summary>
      <description>Support filter() interface for Python DataStream API.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-12 01:00:00" id="18902" opendate="2020-8-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot serve results of asynchronous REST operations in per-job mode</summary>
      <description>Due to the changes introduced with FLINK-18663 a Flink per-job cluster can no longer properly shut down. The problem is that we no longer serve asynchronous results (e.g. resulting from a cancel-with-savepoint operation) while the cluster waits to shut down.In order to solve this problem, Flink needs to serve REST request also while it waits for shutting itself down.</description>
      <version>1.10.2,1.11.2,1.12.0</version>
      <fixedVersion>1.10.2,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.InFlightRequestTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.InFlightRequestTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractHandler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-8-13 01:00:00" id="18936" opendate="2020-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation about user-defined aggregate functions</summary>
      <description>The documentation needs an update because all functions support the new type inference now.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-13 01:00:00" id="18937" opendate="2020-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Environment Setup" section to the "Installation" document</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.installation.zh.md</file>
      <file type="M">docs.dev.python.installation.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-15 01:00:00" id="18965" opendate="2020-8-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ExecutionContextTest.testCatalogs failed with "ClassNotFoundException: org.apache.hadoop.fs.BlockStoragePolicySpi"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5541&amp;view=logs&amp;j=a8bc9173-2af6-5ba8-775c-12063b4f1d54&amp;t=46a16c18-c679-5905-432b-9be5d8e27bc62020-08-14T21:10:09.3503802Z [ERROR] testCatalogs(org.apache.flink.table.client.gateway.local.ExecutionContextTest) Time elapsed: 0.148 s &lt;&lt;&lt; ERROR!2020-08-14T21:10:09.3505006Z org.apache.flink.table.client.gateway.SqlExecutionException: Could not create execution context.2020-08-14T21:10:09.3505856Z at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:870)2020-08-14T21:10:09.3506790Z at org.apache.flink.table.client.gateway.local.ExecutionContextTest.createExecutionContext(ExecutionContextTest.java:324)2020-08-14T21:10:09.3508011Z at org.apache.flink.table.client.gateway.local.ExecutionContextTest.createCatalogExecutionContext(ExecutionContextTest.java:360)2020-08-14T21:10:09.3509273Z at org.apache.flink.table.client.gateway.local.ExecutionContextTest.testCatalogs(ExecutionContextTest.java:133)2020-08-14T21:10:09.3510548Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-08-14T21:10:09.3511496Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-08-14T21:10:09.3512417Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-08-14T21:10:09.3513883Z at java.lang.reflect.Method.invoke(Method.java:498)2020-08-14T21:10:09.3514563Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-08-14T21:10:09.3515604Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-08-14T21:10:09.3516643Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-08-14T21:10:09.3517498Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-08-14T21:10:09.3518189Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-08-14T21:10:09.3519625Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-08-14T21:10:09.3520621Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-08-14T21:10:09.3521328Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-08-14T21:10:09.3521978Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-08-14T21:10:09.3522787Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-08-14T21:10:09.3523469Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-08-14T21:10:09.3524045Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-08-14T21:10:09.3524652Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-08-14T21:10:09.3525307Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-08-14T21:10:09.3526086Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-08-14T21:10:09.3526996Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-08-14T21:10:09.3527737Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-08-14T21:10:09.3528564Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-08-14T21:10:09.3529381Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-08-14T21:10:09.3530153Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-08-14T21:10:09.3530883Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2020-08-14T21:10:09.3531641Z Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to create Hive Metastore client2020-08-14T21:10:09.3532669Z at org.apache.flink.table.catalog.hive.client.HiveShimV230.getHiveMetastoreClient(HiveShimV230.java:52)2020-08-14T21:10:09.3533609Z at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createMetastoreClient(HiveMetastoreClientWrapper.java:240)2020-08-14T21:10:09.3534606Z at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.&lt;init&gt;(HiveMetastoreClientWrapper.java:71)2020-08-14T21:10:09.3535549Z at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientFactory.create(HiveMetastoreClientFactory.java:35)2020-08-14T21:10:09.3536359Z at org.apache.flink.table.catalog.hive.HiveCatalog.open(HiveCatalog.java:224)2020-08-14T21:10:09.3537715Z at org.apache.flink.table.client.gateway.local.DependencyTest$TestHiveCatalogFactory.createCatalog(DependencyTest.java:276)2020-08-14T21:10:09.3538628Z at org.apache.flink.table.client.gateway.local.ExecutionContext.createCatalog(ExecutionContext.java:378)2020-08-14T21:10:09.3539739Z at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$null$5(ExecutionContext.java:626)2020-08-14T21:10:09.3540476Z at java.util.HashMap.forEach(HashMap.java:1289)2020-08-14T21:10:09.3541368Z at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$initializeCatalogs$6(ExecutionContext.java:625)2020-08-14T21:10:09.3542770Z at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:264)2020-08-14T21:10:09.3544224Z at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeCatalogs(ExecutionContext.java:624)2020-08-14T21:10:09.3545180Z at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeTableEnvironment(ExecutionContext.java:523)2020-08-14T21:10:09.3546096Z at org.apache.flink.table.client.gateway.local.ExecutionContext.&lt;init&gt;(ExecutionContext.java:183)2020-08-14T21:10:09.3547025Z at org.apache.flink.table.client.gateway.local.ExecutionContext.&lt;init&gt;(ExecutionContext.java:136)2020-08-14T21:10:09.3547910Z at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:859)2020-08-14T21:10:09.3548486Z ... 28 more2020-08-14T21:10:09.3548911Z Caused by: java.lang.reflect.InvocationTargetException2020-08-14T21:10:09.3549464Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-08-14T21:10:09.3550087Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-08-14T21:10:09.3550860Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-08-14T21:10:09.3551523Z at java.lang.reflect.Method.invoke(Method.java:498)2020-08-14T21:10:09.3552430Z at org.apache.flink.table.catalog.hive.client.HiveShimV230.getHiveMetastoreClient(HiveShimV230.java:50)2020-08-14T21:10:09.3553356Z ... 43 more2020-08-14T21:10:09.3553943Z Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient2020-08-14T21:10:09.3554764Z at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1708)2020-08-14T21:10:09.3555577Z at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:83)2020-08-14T21:10:09.3556451Z at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)2020-08-14T21:10:09.3557374Z at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:89)2020-08-14T21:10:09.3557975Z ... 48 more2020-08-14T21:10:09.3558396Z Caused by: java.lang.reflect.InvocationTargetException2020-08-14T21:10:09.3558941Z at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)2020-08-14T21:10:09.3559664Z at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)2020-08-14T21:10:09.3560509Z at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)2020-08-14T21:10:09.3561281Z at java.lang.reflect.Constructor.newInstance(Constructor.java:423)2020-08-14T21:10:09.3561995Z at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1706)2020-08-14T21:10:09.3564984Z ... 51 more2020-08-14T21:10:09.3565646Z Caused by: MetaException(message:java.lang.NoClassDefFoundError: org/apache/hadoop/fs/BlockStoragePolicySpi)2020-08-14T21:10:09.3566467Z at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:83)2020-08-14T21:10:09.3567399Z at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)2020-08-14T21:10:09.3568474Z at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6891)2020-08-14T21:10:09.3569795Z at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:164)2020-08-14T21:10:09.3570366Z ... 56 more2020-08-14T21:10:09.3570953Z Caused by: java.lang.RuntimeException: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/BlockStoragePolicySpi2020-08-14T21:10:09.3572567Z at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:91)2020-08-14T21:10:09.3573320Z at org.apache.hadoop.hive.metastore.ObjectStore.getDataSourceProps(ObjectStore.java:480)2020-08-14T21:10:09.3574237Z at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:279)2020-08-14T21:10:09.3574929Z at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)2020-08-14T21:10:09.3575654Z at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)2020-08-14T21:10:09.3576395Z at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:58)2020-08-14T21:10:09.3577207Z at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)2020-08-14T21:10:09.3578029Z at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)2020-08-14T21:10:09.3578863Z at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)2020-08-14T21:10:09.3579708Z at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)2020-08-14T21:10:09.3580543Z at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)2020-08-14T21:10:09.3581365Z at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)2020-08-14T21:10:09.3582048Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-08-14T21:10:09.3582783Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-08-14T21:10:09.3583560Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-08-14T21:10:09.3584236Z at java.lang.reflect.Method.invoke(Method.java:498)2020-08-14T21:10:09.3584931Z at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)2020-08-14T21:10:09.3585749Z at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)2020-08-14T21:10:09.3586543Z at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:79)2020-08-14T21:10:09.3587215Z ... 59 more2020-08-14T21:10:09.3587710Z Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/BlockStoragePolicySpi2020-08-14T21:10:09.3588331Z at java.lang.ClassLoader.defineClass1(Native Method)2020-08-14T21:10:09.3588892Z at java.lang.ClassLoader.defineClass(ClassLoader.java:757)2020-08-14T21:10:09.3589529Z at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)2020-08-14T21:10:09.3590215Z at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)2020-08-14T21:10:09.3590838Z at java.net.URLClassLoader.access$100(URLClassLoader.java:74)2020-08-14T21:10:09.3591457Z at java.net.URLClassLoader$1.run(URLClassLoader.java:369)2020-08-14T21:10:09.3592015Z at java.net.URLClassLoader$1.run(URLClassLoader.java:363)2020-08-14T21:10:09.3592677Z at java.security.AccessController.doPrivileged(Native Method)2020-08-14T21:10:09.3593215Z at java.net.URLClassLoader.findClass(URLClassLoader.java:362)2020-08-14T21:10:09.3593778Z at java.lang.ClassLoader.loadClass(ClassLoader.java:419)2020-08-14T21:10:09.3594353Z at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)2020-08-14T21:10:09.3594945Z at java.lang.ClassLoader.loadClass(ClassLoader.java:352)2020-08-14T21:10:09.3595460Z at java.lang.Class.forName0(Native Method)2020-08-14T21:10:09.3595914Z at java.lang.Class.forName(Class.java:348)2020-08-14T21:10:09.3597121Z at org.apache.hadoop.hive.shims.Hadoop23Shims.&lt;init&gt;(Hadoop23Shims.java:113)2020-08-14T21:10:09.3597790Z at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)2020-08-14T21:10:09.3598487Z at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)2020-08-14T21:10:09.3599362Z at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)2020-08-14T21:10:09.3600126Z at java.lang.reflect.Constructor.newInstance(Constructor.java:423)2020-08-14T21:10:09.3600971Z at java.lang.Class.newInstance(Class.java:442)2020-08-14T21:10:09.3601577Z at org.apache.hadoop.hive.shims.ShimLoader.createShim(ShimLoader.java:130)2020-08-14T21:10:09.3602593Z at org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:124)2020-08-14T21:10:09.3603611Z at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:88)2020-08-14T21:10:09.3604096Z ... 77 more2020-08-14T21:10:09.3604616Z Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.BlockStoragePolicySpi2020-08-14T21:10:09.3605271Z at java.net.URLClassLoader.findClass(URLClassLoader.java:382)2020-08-14T21:10:09.3605867Z at java.lang.ClassLoader.loadClass(ClassLoader.java:419)2020-08-14T21:10:09.3606454Z at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)2020-08-14T21:10:09.3607164Z at java.lang.ClassLoader.loadClass(ClassLoader.java:352)2020-08-14T21:10:09.3608030Z ... 100 more2020-08-14T21:10:09.3608257Z</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-15 01:00:00" id="18966" opendate="2020-8-15 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support key_by() on ConnectedStreams for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.datastream.runtime.operators.python.DataStreamTwoInputPythonStatelessFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-19 01:00:00" id="18998" opendate="2020-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>No watermark is shown on Flink UI when ProcessingTime is used</summary>
      <description>As stated in the subject, no watermark is shown on Flink UI when ProcessingTime is used, see the attached screenshot. It is better to be more specific, like "Watermarks are only available if EventTime is used." </description>
      <version>1.11.1,1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.humanize-watermark.pipe.ts</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-1-21 01:00:00" id="19014" opendate="2020-8-21 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Investigate minimum e2e test duration</summary>
      <description>Currently, the minimum time for an e2e test is roughly 30 seconds.This even applies to tests that start a tiny cluster and only run a WordCount.It would be good to understand what exactly causes these tests to run for such a long time.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-25 01:00:00" id="19041" opendate="2020-8-25 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add dependency management for ConnectedStream in Python DataStream API.</summary>
      <description>We failed to set merged configurations into DataStreamTwoInputPythonStatelessFunctionOperator when finally generating the StreamGraph.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-9-28 01:00:00" id="19070" opendate="2020-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector should throw a meaningful exception if user reads/writes ACID tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-9-28 01:00:00" id="19086" opendate="2020-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression 2020-08-27 in globalWindow benchmark</summary>
      <description>http://codespeed.dak8s.net:8000/timeline/?ben=globalWindow&amp;env=2http://codespeed.dak8s.net:8000/timeline/#/?exe=1,3&amp;ben=tumblingWindow&amp;env=2&amp;revs=200&amp;equid=off&amp;quarts=on&amp;extr=on The results started to decrease 2 days before decomissioning of an old jenkins node.The other tests, however, were stable. cc: pnowojski</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.runtime.PojoSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-2 01:00:00" id="19121" opendate="2020-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid accessing HDFS frequently in HiveBulkWriterFactory</summary>
      <description>In HadoopPathBasedBulkWriter, getSize will invoke `FileSystem.exists` and `FileSystem.getFileStatus`, but it is invoked per record.There will be lots of visits to HDFS, may make HDFS pressure too high.</description>
      <version>1.11.1,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-4 01:00:00" id="19137" opendate="2020-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Apache Parquet to 1.11.1</summary>
      <description>Apache Parquet 1.11.1 fixed some important issues: https://issues.apache.org/jira/browse/PARQUET-1309 https://issues.apache.org/jira/browse/PARQUET-1510 https://issues.apache.org/jira/browse/PARQUET-1485Now Flink master branch relies parquet 1.10.0, and flink-sql-parquet artifact shaded parquet class files into flink-sql-parquet.jar. So this may lead to direct memory leak in PARQUET-1485 or parquet properties bug in PARQUET-1309 or repeat values with dictionary encoding error in PARQUET-1510. For example in PARQUET-1309:then in Flink:https://github.com/C08061/flink/blob/master/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetInputFormat.java#L166  </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-parquet.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-9-8 01:00:00" id="19163" opendate="2020-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add building py38 wheel package of PyFlink in Azure CI</summary>
      <description>Add building py38 wheel package of PyFlink in Azure CI</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.build-wheels.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-9-10 01:00:00" id="19179" opendate="2020-9-10 00:00:00" resolution="Done">
    <buginformation>
      <summary>Implement the managed memory fraction calculation logic</summary>
      <description>This also means migrating the batch operator use cases.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.BufferDataOverWindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.String2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.String2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.RandomSortMergeInnerJoinTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.Int2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.TableStreamOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.BatchAggTestBase.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-10 01:00:00" id="19182" opendate="2020-9-10 00:00:00" resolution="Done">
    <buginformation>
      <summary>Update document for intra-slot managed memory sharing</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.memory.mem.setup.tm.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-10 01:00:00" id="19184" opendate="2020-9-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Batch Physical Pandas Group Aggregate Rule and RelNode</summary>
      <description>Add Batch Physical Pandas Group Aggregate Rule and RelNode</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedAggFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSortAggRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecHashAggRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-10 01:00:00" id="19187" opendate="2020-9-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a new basic Table API example</summary>
      <description>Add a new basic Table API example that does not require a connector and can be used for simply play around with operators.It should show: How to get started without any prior knowledge. How to work with Table Environment. How to work with Table How to print/collect. How to perform simple ETL and some operations.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.WordCountTable.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.WordCountSQL.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.TPCHQuery3Table.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.StreamTableExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.StreamSQLExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-10 01:00:00" id="19188" opendate="2020-9-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a new streaming SQL examples</summary>
      <description>Add a new streaming SQL example. It should work entirely with SQL. It should show:How to get started with a connector (CSV or DataGen?) and event-time.How the DDL works.How to work with Table Environment and SQL.How a complex SQL query looks like. Windows, joins, maybe Top N.How to use pure streaming operators vs. changelog producing operators</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-10-11 01:00:00" id="19201" opendate="2020-9-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink e2e tests is instable and failed with "Connection broken: OSError"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6452&amp;view=logs&amp;j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&amp;t=6945d9e3-ebef-5993-0c44-838d8ad079c02020-09-10T21:37:42.9988117Z install conda ... [SUCCESS]2020-09-10T21:37:43.0018449Z install miniconda... [SUCCESS]2020-09-10T21:37:43.0082244Z installing python environment...2020-09-10T21:37:43.0100408Z installing python3.5...2020-09-10T21:37:58.7214400Z install python3.5... [SUCCESS]2020-09-10T21:37:58.7253792Z installing python3.6...2020-09-10T21:38:06.5855143Z install python3.6... [SUCCESS]2020-09-10T21:38:06.5903358Z installing python3.7...2020-09-10T21:38:11.5444706Z 2020-09-10T21:38:11.5484852Z ('Connection broken: OSError("(104, \'ECONNRESET\')")', OSError("(104, 'ECONNRESET')"))2020-09-10T21:38:11.5513130Z 2020-09-10T21:38:11.8044086Z conda install 3.7 failed. You can retry to exec the script.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-15 01:00:00" id="19247" opendate="2020-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Chinese documentation after removal of Kafka 0.10 and 0.11</summary>
      <description>In FLINK-19152 I removed Kafka 0.10 and 0.11. Most of the Chinese documentation was updated for this but some parts were too hard to fix for me as a non-chinese-speaker.Take a look at the PR for FLINK-19152 to see what changes still need to be applied to the Chinese documentation.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.project-configuration.zh.md</file>
      <file type="M">docs.dev.connectors.kafka.zh.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-9-21 01:00:00" id="19304" opendate="2020-9-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add feature toggle for declarative resource management</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.JobMasterBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.DefaultSlotPoolFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-10-21 01:00:00" id="19308" opendate="2020-9-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SlotTracker</summary>
      <description>Move the slot bookkeeping into a separate data-structure.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TestingTaskManagerSlotInformation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskManagerSlotInformation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskManagerSlot.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-10-21 01:00:00" id="19324" opendate="2020-9-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Map requested/allocated containers with priority on YARN</summary>
      <description>In the design doc of FLINK-14106, there was a discussion on how we map allocated containers with the requested ones on YARN. We rejected the design option that uses container priorities for mapping containers of different resources, because we do not want to priorities different container requests (which is the original purpose for this field). As a result, we have to interpret how the requested container request would be normalized by Yarn, and map the allocated/requested containers accordingly, which is complicated and fragile. See also FLINK-19151.Recently in our POC for fine grained resource management, we surprisingly discovered that Yarn actually doesn't work with container requests same priority and different resources. I do not find this described as an official protocol in any Yarn's documents. The issue has been raised in early Yarn versions (YARN-314) and has not been fixed util Hadoop 2.9 when allocationRequestId is introduced. In Hadoop 2.8, Yarn scheduler is still internally using priority as the key of a container request (see AppSchedulingInfo#updateResourceRequests ), thus requests same priority and different resources would overwrite each other.The new discovery suggests that, if we want to support containers with different resources on Hadoop 2.8 and earlier versions, we have to give them different priorities anyway. Thus, I would suggest to get rid of the container normalization simulation and go back to the previously rejected priority based design option.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerDriverTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.TestingRegisterApplicationMasterResponse.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.TaskExecutorProcessSpecContainerResourceAdapterTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManagerDriver.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.TaskExecutorProcessSpecContainerResourceAdapter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-10-22 01:00:00" id="19339" opendate="2020-9-22 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support Avro&amp;#39;s unions with logical types</summary>
      <description>Avro 1.9.x introduced yet another mechanism for registering/looking up conversions for logical types. See https://issues.apache.org/jira/browse/AVRO-1891If a logical type is part of a union a static field MODEL$ of type SpecificData will be added to the generated Avro class with registered conversions for a logical type. We should use that SpecificData in AvroSerializer and Avro(De)SerializationSchema whenever available.</description>
      <version>None</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.resources.avro.user.avsc</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.utils.AvroTestUtils.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroSerializerSnapshot.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroFactory.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-9-23 01:00:00" id="19372" opendate="2020-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Pandas Batch Over Window Aggregation</summary>
      <description>We will add Batch Physical Pandas Over Window RelNode, Python Operation and PandasOverWindowFunctionCoder to support Pandas Batch Over Window Aggregation</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecOverAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-10-25 01:00:00" id="19412" opendate="2020-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-layer Python Operation Make it Possible to Provide only Python implementation</summary>
      <description>Now whenever we introduce a Python Operation, we need to provide the implementation of python version and cython version, but in many cases, we only need to provide the implementation of Python. We need to re-layer Python Operation make it possible to provide only Python implementation.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pxd</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-25 01:00:00" id="19414" opendate="2020-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce ParquetColumnarRowInputFormat</summary>
      <description>Introduce ParquetColumnarRowInputFormat for new FileSource API.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.util.RecyclableIterator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.RowPartitionComputer.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.utils.ParquetWriterUtil.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetDecimalVector.java</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.util.Utils.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.FileSourceParquetColumnarITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceTextLinesITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.AbstractFileSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-25 01:00:00" id="19417" opendate="2020-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug of the method from_data_stream in table_environement</summary>
      <description>The parameter fields should be str or expression *, not the current list &amp;#91;str&amp;#93;. And the table_env object passed to the Table object should be Python's TableEnvironment, not Java's TableEnvironment</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-28 01:00:00" id="19436" opendate="2020-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TPC-DS end-to-end test (Blink planner) failed during shutdown</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7009&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=2b7514ee-e706-5046-657b-3430666e7bd92020-09-27T22:37:53.2236467Z Stopping taskexecutor daemon (pid: 2992) on host fv-az655.2020-09-27T22:37:53.4450715Z Stopping standalonesession daemon (pid: 2699) on host fv-az655.2020-09-27T22:37:53.8014537Z Skipping taskexecutor daemon (pid: 11173), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8019740Z Skipping taskexecutor daemon (pid: 11561), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8022857Z Skipping taskexecutor daemon (pid: 11849), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8023616Z Skipping taskexecutor daemon (pid: 12180), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8024327Z Skipping taskexecutor daemon (pid: 12950), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8025027Z Skipping taskexecutor daemon (pid: 13472), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8025727Z Skipping taskexecutor daemon (pid: 16577), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8026417Z Skipping taskexecutor daemon (pid: 16959), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8027086Z Skipping taskexecutor daemon (pid: 17250), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8027770Z Skipping taskexecutor daemon (pid: 17601), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8028400Z Stopping taskexecutor daemon (pid: 18438) on host fv-az655.2020-09-27T22:37:53.8029314Z /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/bin/taskmanager.sh: line 99: 18438 Terminated "${FLINK_BIN_DIR}"/flink-daemon.sh $STARTSTOP $ENTRYPOINT "${ARGS[@]}"2020-09-27T22:37:53.8029895Z [FAIL] Test script contains errors.2020-09-27T22:37:53.8032092Z Checking for errors...2020-09-27T22:37:55.3713368Z No errors in log files.2020-09-27T22:37:55.3713935Z Checking for exceptions...2020-09-27T22:37:56.9046391Z No exceptions in log files.2020-09-27T22:37:56.9047333Z Checking for non-empty .out files...2020-09-27T22:37:56.9064402Z No non-empty .out files.2020-09-27T22:37:56.9064859Z 2020-09-27T22:37:56.9065588Z [FAIL] 'TPC-DS end-to-end test (Blink planner)' failed after 16 minutes and 54 seconds! Test exited with exit code 1</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.4,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-28 01:00:00" id="19441" opendate="2020-9-28 00:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>Performance regression on 24.09.2020</summary>
      <description>A couple of benchmarks are showing a small performance regression on 24.09.2020:http://codespeed.dak8s.net:8000/timeline/?ben=globalWindow&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=tupleKeyBy&amp;env=2</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ConsumableNotifyingResultPartitionWriterDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-29 01:00:00" id="19445" opendate="2020-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Several tests for HBase connector 1.4 failed with "NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7042&amp;view=logs&amp;j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&amp;t=bfbc6239-57a0-5db0-63f3-41551b4f7d512020-09-28T21:28:29.4171075Z Running org.apache.flink.connector.hbase1.HBaseTablePlanTest2020-09-28T21:28:31.0367584Z Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.62 sec &lt;&lt;&lt; FAILURE! - in org.apache.flink.connector.hbase1.HBaseTablePlanTest2020-09-28T21:28:31.0368925Z testProjectionPushDown(org.apache.flink.connector.hbase1.HBaseTablePlanTest) Time elapsed: 0.031 sec &lt;&lt;&lt; ERROR!2020-09-28T21:28:31.0369805Z org.apache.flink.table.api.ValidationException: 2020-09-28T21:28:31.0370409Z Unable to create a source for reading table 'default_catalog.default_database.hTable'.2020-09-28T21:28:31.0370707Z 2020-09-28T21:28:31.0370976Z Table options are:2020-09-28T21:28:31.0371204Z 2020-09-28T21:28:31.0371528Z 'connector'='hbase-1.4'2020-09-28T21:28:31.0371871Z 'table-name'='my_table'2020-09-28T21:28:31.0372255Z 'zookeeper.quorum'='localhost:2021'2020-09-28T21:28:31.0372812Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125)2020-09-28T21:28:31.0373359Z at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.buildTableScan(CatalogSourceTable.scala:135)2020-09-28T21:28:31.0373905Z at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:78)2020-09-28T21:28:31.0374390Z at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)2020-09-28T21:28:31.0375224Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)2020-09-28T21:28:31.0375867Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)2020-09-28T21:28:31.0376479Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl$$anon$1.convertFrom(FlinkPlannerImpl.scala:181)2020-09-28T21:28:31.0377077Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)2020-09-28T21:28:31.0377593Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl$$anon$1.convertFrom(FlinkPlannerImpl.scala:181)2020-09-28T21:28:31.0378114Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)2020-09-28T21:28:31.0378622Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)2020-09-28T21:28:31.0379132Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)2020-09-28T21:28:31.0379872Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)2020-09-28T21:28:31.0380477Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:196)2020-09-28T21:28:31.0381128Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:154)2020-09-28T21:28:31.0381666Z at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:823)2020-09-28T21:28:31.0382264Z at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:795)2020-09-28T21:28:31.0382968Z at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:250)2020-09-28T21:28:31.0383550Z at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)2020-09-28T21:28:31.0384172Z at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:640)2020-09-28T21:28:31.0384700Z at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:346)2020-09-28T21:28:31.0385201Z at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyPlan(TableTestBase.scala:271)2020-09-28T21:28:31.0385717Z at org.apache.flink.connector.hbase1.HBaseTablePlanTest.testProjectionPushDown(HBaseTablePlanTest.java:124)2020-09-28T21:28:31.0386166Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-09-28T21:28:31.0386575Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-09-28T21:28:31.0387257Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-09-28T21:28:31.0387822Z at java.lang.reflect.Method.invoke(Method.java:498)2020-09-28T21:28:31.0388229Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-09-28T21:28:31.0388718Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-09-28T21:28:31.0389198Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-09-28T21:28:31.0389745Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-09-28T21:28:31.0390262Z at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)2020-09-28T21:28:31.0390732Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2020-09-28T21:28:31.0391179Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-09-28T21:28:31.0391582Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-09-28T21:28:31.0391964Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-09-28T21:28:31.0392382Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-09-28T21:28:31.0393053Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-09-28T21:28:31.0393617Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-09-28T21:28:31.0393997Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-09-28T21:28:31.0394407Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-09-28T21:28:31.0394817Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-09-28T21:28:31.0395211Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-09-28T21:28:31.0395608Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-09-28T21:28:31.0396041Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)2020-09-28T21:28:31.0396517Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)2020-09-28T21:28:31.0397026Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-09-28T21:28:31.0397512Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)2020-09-28T21:28:31.0398245Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)2020-09-28T21:28:31.0398778Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)2020-09-28T21:28:31.0399251Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)2020-09-28T21:28:31.0399838Z Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V2020-09-28T21:28:31.0400340Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)2020-09-28T21:28:31.0400756Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)2020-09-28T21:28:31.0401304Z at org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.createDynamicTableSource(HBase1DynamicTableFactory.java:113)2020-09-28T21:28:31.0401869Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:122)2020-09-28T21:28:31.0402307Z ... 50 more2020-09-28T21:28:31.0402624Z 2020-09-28T21:28:31.0402949Z Running org.apache.flink.connector.hbase1.HBaseDescriptorTest2020-09-28T21:28:31.0416116Z Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec - in org.apache.flink.connector.hbase1.HBaseDescriptorTest2020-09-28T21:28:31.4448287Z 2020-09-28T21:28:31.4448950Z Results :2020-09-28T21:28:31.4449082Z 2020-09-28T21:28:31.4449270Z Tests in error: 2020-09-28T21:28:31.4450556Z HBaseDynamicTableFactoryTest.testTableSourceFactory:104-&gt;createTableSource:332 Â» Validation2020-09-28T21:28:31.4451232Z HBaseTableFactoryTest.testTableSourceFactory:101 Â» NoSuchMethod com.google.com...2020-09-28T21:28:31.4451851Z HBaseTablePlanTest.testProjectionPushDown:124 Â» Validation Unable to create a ...</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-10-29 01:00:00" id="19459" opendate="2020-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-dist won&amp;#39;t build locally with newer (3.3+) maven versions</summary>
      <description>flink-dist will fail on non Maven 3.2.5 versions because of banned dependencies.These are the messages you'll see:[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (ban-unsafe-snakeyaml) @ flink-dist_2.11 ---[WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:Found Banned Dependency: org.yaml:snakeyaml:jar:1.24Use 'mvn dependency:tree' to locate the source of the banned dependencies.[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary for Flink : 1.12-SNAPSHOT:...[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M1:enforce (ban-unsafe-snakeyaml) on project flink-dist_2.11: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -&gt; [Help 1]</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch7-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch6-test.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-10-1 01:00:00" id="19483" opendate="2020-10-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink Table end-to-end test failed with "FileExistsError: [Errno 17] File exists: &amp;#39;/home/vsts/work/1/s/flink-python/dev/.conda/pkgs&amp;#39;"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7130&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d3555292020-09-30T17:13:14.7489481Z Collecting package metadata (current_repodata.json): ...working... failed2020-09-30T17:13:14.7699351Z 2020-09-30T17:13:14.7699995Z # &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;2020-09-30T17:13:14.7700398Z 2020-09-30T17:13:14.7700782Z Traceback (most recent call last):2020-09-30T17:13:14.7702095Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/update.py", line 107, in touch2020-09-30T17:13:14.7702736Z mkdir_p_sudo_safe(dirpath)2020-09-30T17:13:14.7703608Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/__init__.py", line 84, in mkdir_p_sudo_safe2020-09-30T17:13:14.7704221Z os.mkdir(path)2020-09-30T17:13:14.7704992Z FileExistsError: [Errno 17] File exists: '/home/vsts/work/1/s/flink-python/dev/.conda/pkgs'2020-09-30T17:13:14.7705512Z 2020-09-30T17:13:14.7705956Z During handling of the above exception, another exception occurred:2020-09-30T17:13:14.7706402Z 2020-09-30T17:13:14.7706789Z Traceback (most recent call last):2020-09-30T17:13:14.7707615Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 185, in _load2020-09-30T17:13:14.7708341Z mtime = getmtime(self.cache_path_json)2020-09-30T17:13:14.7709527Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 153, in cache_path_json2020-09-30T17:13:14.7710340Z return self.cache_path_base + '.json'2020-09-30T17:13:14.7711227Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 144, in cache_path_base2020-09-30T17:13:14.7711832Z create_cache_dir(),2020-09-30T17:13:14.7712821Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 645, in create_cache_dir2020-09-30T17:13:14.7715308Z cache_dir = join(PackageCacheData.first_writable(context.pkgs_dirs).pkgs_dir, 'cache')2020-09-30T17:13:14.7715986Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/package_cache_data.py", line 162, in first_writable2020-09-30T17:13:14.7716407Z created = create_package_cache_directory(package_cache.pkgs_dir)2020-09-30T17:13:14.7717084Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/create.py", line 435, in create_package_cache_directory2020-09-30T17:13:14.7717522Z touch(join(pkgs_dir, PACKAGE_CACHE_MAGIC_FILE), mkdir=True, sudo_safe=sudo_safe)2020-09-30T17:13:14.7718150Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/update.py", line 125, in touch2020-09-30T17:13:14.7718694Z raise NotWritableError(path, e.errno, caused_by=e)2020-09-30T17:13:14.7719040Z conda.exceptions.NotWritableError: The current user does not have write permissions to a required path.2020-09-30T17:13:14.7719797Z path: /home/vsts/work/1/s/flink-python/dev/.conda/pkgs/urls.txt2020-09-30T17:13:14.7720054Z uid: 10012020-09-30T17:13:14.7720217Z gid: 1182020-09-30T17:13:14.7720375Z 2020-09-30T17:13:14.7720625Z If you feel that permissions on this path are set incorrectly, you can manually2020-09-30T17:13:14.7720898Z change them by executing2020-09-30T17:13:14.7721072Z 2020-09-30T17:13:14.7721513Z $ sudo chown 1001:118 /home/vsts/work/1/s/flink-python/dev/.conda/pkgs/urls.txt2020-09-30T17:13:14.7721778Z 2020-09-30T17:13:14.7722334Z In general, it's not advisable to use 'sudo conda'.2020-09-30T17:13:14.7722539Z 2020-09-30T17:13:14.7722680Z 2020-09-30T17:13:14.7722946Z During handling of the above exception, another exception occurred:2020-09-30T17:13:14.7723200Z 2020-09-30T17:13:14.7723407Z Traceback (most recent call last):2020-09-30T17:13:14.7724000Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/exceptions.py", line 1062, in __call__2020-09-30T17:13:14.7724516Z return func(*args, **kwargs)2020-09-30T17:13:14.7725075Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/main.py", line 84, in _main2020-09-30T17:13:14.7725408Z exit_code = do_call(args, p)2020-09-30T17:13:14.7725983Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/conda_argparse.py", line 82, in do_call2020-09-30T17:13:14.7726556Z exit_code = getattr(module, func_name)(args, parser)2020-09-30T17:13:14.7727157Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/main_install.py", line 20, in execute2020-09-30T17:13:14.7727688Z install(args, parser, 'install')2020-09-30T17:13:14.7728274Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/install.py", line 256, in install2020-09-30T17:13:14.7728663Z force_reinstall=context.force_reinstall or context.force,2020-09-30T17:13:14.7729601Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py", line 112, in solve_for_transaction2020-09-30T17:13:14.7729964Z force_remove, force_reinstall)2020-09-30T17:13:14.7730791Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py", line 150, in solve_for_diff2020-09-30T17:13:14.7731132Z force_remove)2020-09-30T17:13:14.7731878Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py", line 249, in solve_final_state2020-09-30T17:13:14.7732267Z ssc = self._collect_all_metadata(ssc)2020-09-30T17:13:14.7732844Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/common/io.py", line 88, in decorated2020-09-30T17:13:14.7733199Z return f(*args, **kwds)2020-09-30T17:13:14.7733780Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py", line 389, in _collect_all_metadata2020-09-30T17:13:14.7734177Z index, r = self._prepare(prepared_specs)2020-09-30T17:13:14.7734922Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py", line 974, in _prepare2020-09-30T17:13:14.7735307Z self.subdirs, prepared_specs, self._repodata_fn)2020-09-30T17:13:14.7736067Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/index.py", line 214, in get_reduced_index2020-09-30T17:13:14.7736435Z repodata_fn=repodata_fn)2020-09-30T17:13:14.7737001Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 91, in query_all2020-09-30T17:13:14.7737494Z result = tuple(concat(executor.map(subdir_query, channel_urls)))2020-09-30T17:13:14.7738148Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/_base.py", line 586, in result_iterator2020-09-30T17:13:14.7738507Z yield fs.pop().result()2020-09-30T17:13:14.7739046Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/_base.py", line 425, in result2020-09-30T17:13:14.7739578Z return self.__get_result()2020-09-30T17:13:14.7740341Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result2020-09-30T17:13:14.7740685Z raise self._exception2020-09-30T17:13:14.7741217Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/thread.py", line 57, in run2020-09-30T17:13:14.7741767Z result = self.fn(*self.args, **self.kwargs)2020-09-30T17:13:14.7742581Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 87, in &lt;lambda&gt;2020-09-30T17:13:14.7743099Z package_ref_or_match_spec))2020-09-30T17:13:14.7743680Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 96, in query2020-09-30T17:13:14.7744019Z self.load()2020-09-30T17:13:14.7744563Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 160, in load2020-09-30T17:13:14.7744932Z _internal_state = self._load()2020-09-30T17:13:14.7745837Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 188, in _load2020-09-30T17:13:14.7746162Z self.cache_path_json)2020-09-30T17:13:14.7746724Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 153, in cache_path_json2020-09-30T17:13:14.7747244Z return self.cache_path_base + '.json'2020-09-30T17:13:14.7748013Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 144, in cache_path_base2020-09-30T17:13:14.7748381Z create_cache_dir(),2020-09-30T17:13:14.7749159Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 646, in create_cache_dir2020-09-30T17:13:14.7749537Z mkdir_p_sudo_safe(cache_dir)2020-09-30T17:13:14.7750896Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/__init__.py", line 84, in mkdir_p_sudo_safe2020-09-30T17:13:14.7751274Z os.mkdir(path)2020-09-30T17:13:14.7751795Z FileExistsError: [Errno 17] File exists: '/home/vsts/work/1/s/flink-python/dev/.conda/pkgs/cache'</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-1 01:00:00" id="19485" opendate="2020-10-1 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Decouple runtime operator implementation from DataStream operations</summary>
      <description>Although DataStream is going to be the unified API for Batch and Streaming applications, some operations, e.g. Sinks, may need to have different runtime implementations depending on if they are intended to run on bounded or unbounded data. This is not necessarily only for optimisations but also for the exposed semantics, i.e. correctness.So far, DataStream had a 1-to-1 mapping between an API call and an operator. In a sense, the DataStream API was an "explicit" API. With this addition, we will decouple the API calls from the actual runtime implementations and thus allow different operations to have more than one runtime implementations, depending (for now) on the execution.runtime-mode.</description>
      <version>1.12.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.UnionTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.TwoInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SourceTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SinkTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SideOutputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.PartitionTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.OneInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.LegacySourceTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.FeedbackTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.CoFeedbackTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.AbstractMultipleInputTransformation.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.dag.TransformationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.dag.Transformation.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.DataGenerators.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch7.Elasticsearch7UpsertTableSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6UpsertTableSinkFactoryTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-10-2 01:00:00" id="19496" opendate="2020-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataGen source DECIMAL always returns null</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.DataGenTableSourceFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.RandomGeneratorVisitor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-6 01:00:00" id="19516" opendate="2020-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PerJobMiniClusterFactoryTest.testJobClient()</summary>
      <description>Log:https://dev.azure.com/sewen0794/19b23adf-d190-4fb4-ae6e-2e92b08923a3/_apis/build/builds/151/logs/137Exception:[ERROR] testJobClient(org.apache.flink.client.program.PerJobMiniClusterFactoryTest) Time elapsed: 0.392 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: Expected: is &lt;false&gt; but: was &lt;true&gt; at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8) at org.apache.flink.client.program.PerJobMiniClusterFactoryTest.assertThatMiniClusterIsShutdown(PerJobMiniClusterFactoryTest.java:161) at org.apache.flink.client.program.PerJobMiniClusterFactoryTest.testJobClient(PerJobMiniClusterFactoryTest.java:93)</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterJobClient.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-7 01:00:00" id="19520" opendate="2020-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add reliable test randomization for checkpointing</summary>
      <description>With the larger refactoring of checkpoint alignment and the additional of more unaligned checkpoint settings, it becomes increasingly important to provide a large test coverage.Unfortunately, adding sufficient test cases in a test matrix appears to be unrealistic: many of the encountered issues were subtle, sometimes caused by race conditions or unusual test configurations and often only visible in e2e tests.Hence, we like to rely on all existing Flink tests to provide a sufficient coverage for checkpointing. However, as more and more options in unaligned checkpoint are going to be implemented in this and the upcoming release, running all Flink tests - especially e2e - in a test matrix is prohibitively expensive, even for nightly builds.Thus, we want to introduce test randomization for all tests that do not use a specific checkpointing mode. In a similar way, we switched from aligned checkpoints by default in tests to unaligned checkpoint during the last release cycle.To not burden the developers of other components too much, we set the following requirements: Randomization should be seeded in a way that both builds on Azure pipelines and local builds will result in the same settings to ease debugging and ensure reproducibility. Randomized options should be shown in the test log. Execution order of test cases will not influence the randomization. Randomization is hidden, no change on any test is needed. Randomization only happens during local/remote test execution. User deployments are not affected. Test developers are able to avoid randomization by explicitly providing checkpoint configs.</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.TestStreamEnvironment.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.TestLogger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.EnvironmentInformation.java</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-10-9 01:00:00" id="19553" opendate="2020-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The format of checkpoint Completion Time and Failure Time should be changed from HH:mm:ss to yyyy-MM-dd HH:mm:ss</summary>
      <description>As shown in the picture below, The latest completed checkpoint ID is 11113, but the latest failed checkpoint ID is 5370. The two IDs are too far apart .The failure time in HH:mm:ss format is difficult to determine the specific failure date of the checkpoint. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-9 01:00:00" id="19554" opendate="2020-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unified testing framework for connectors</summary>
      <description>As the community and eco-system of Flink growing up, more and more Flink-owned and third party connectors are developed and added into Flink community. In order to provide a standardized quality controlling for all connectors, it's necessary to develop a unified connector testing framework to simplify and standardize end-to-end test of connectors. </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.CommonTestUtils.java</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
      <file type="M">tools.ci.java-ci-tools.src.main.resources.modules-skipping-deployment.modulelist</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceTestEnv.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.subscriber.KafkaSubscriberTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.KafkaEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-test-utils-parent.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkContainer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-29 01:00:00" id="1956" opendate="2015-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Runtime context not initialized in RichWindowMapFunction</summary>
      <description>Trying to access the runtime context in a rich window map function results in an exception. The following snippet demonstrates the bug: env.generateSequence(0, 1000) .window(Count.of(10)) .mapWindow(new RichWindowMapFunction&lt;Long, Tuple2&lt;Long, Long&gt;&gt;() { @Override public void mapWindow(Iterable&lt;Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception { long self = getRuntimeContext().getIndexOfThisSubtask(); for (long value : input) { out.collect(new Tuple2&lt;&gt;(self, value)); } } }).flatten().print();</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowMapper.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowFolder.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-12 01:00:00" id="19572" opendate="2020-10-12 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the TwoInputTransformation translator</summary>
      <description/>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-12 01:00:00" id="19573" opendate="2020-10-12 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the MultipleInputTransformation translator</summary>
      <description/>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-12 01:00:00" id="19574" opendate="2020-10-12 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the SourceTransformation translator</summary>
      <description/>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-12 01:00:00" id="19575" opendate="2020-10-12 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the LegacySourceTransformation translator</summary>
      <description/>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-12 01:00:00" id="19576" opendate="2020-10-12 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the SinkTransformation translator</summary>
      <description/>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-12 01:00:00" id="19577" opendate="2020-10-12 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the UnionTransformation translator</summary>
      <description/>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-12 01:00:00" id="19578" opendate="2020-10-12 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the PartitionTransformation translator</summary>
      <description/>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-12 01:00:00" id="19579" opendate="2020-10-12 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the SideOutputTransformation translator</summary>
      <description/>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-10-12 01:00:00" id="19586" opendate="2020-10-12 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement StreamCommitterOperator for new Sink API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TestHarnessUtil.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-9-29 01:00:00" id="1960" opendate="2015-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add comments and docs for withForwardedFields and related operators</summary>
      <description>The withForwardedFields and related operators have no docs for the Scala API. It would be useful to have code comments and example usage in the docs.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  <bug fixdate="2020-10-13 01:00:00" id="19616" opendate="2020-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink : Formats : Parquet compilation failure</summary>
      <description>https://dev.azure.com/pnowojski/Flink/_build/results?buildId=175&amp;view=logs&amp;j=66592496-52df-56bb-d03e-37509e1d9d0f&amp;t=ae0269db-6796-5583-2e5f-d84757d711aa[WARNING] [PROTOC] Unable to invoke protoc, will retry 1 time(s)org.codehaus.plexus.util.cli.CommandLineException: Error while executing process. at org.codehaus.plexus.util.cli.Commandline.execute(Commandline.java:680) at org.codehaus.plexus.util.cli.CommandLineUtils.executeCommandLineAsCallable(CommandLineUtils.java:136) at org.codehaus.plexus.util.cli.CommandLineUtils.executeCommandLine(CommandLineUtils.java:106) at org.codehaus.plexus.util.cli.CommandLineUtils.executeCommandLine(CommandLineUtils.java:89) at org.xolstice.maven.plugin.protobuf.Protoc.execute(Protoc.java:190) at org.xolstice.maven.plugin.protobuf.AbstractProtocMojo.execute(AbstractProtocMojo.java:529) at org.xolstice.maven.plugin.protobuf.AbstractProtocTestCompileMojo.execute(AbstractProtocTestCompileMojo.java:31) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355) at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155) at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584) at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216) at org.apache.maven.cli.MavenCli.main(MavenCli.java:160) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289) at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415) at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)Caused by: java.io.IOException: Cannot run program "/__w/1/s/flink-formats/flink-parquet/target/protoc-plugins/protoc-3.5.1-linux-x86_64.exe": error=13, Permission denied at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) at java.lang.Runtime.exec(Runtime.java:621) at org.codehaus.plexus.util.cli.Commandline.execute(Commandline.java:660) ... 27 moreCaused by: java.io.IOException: error=13, Permission denied at java.lang.UNIXProcess.forkAndExec(Native Method) at java.lang.UNIXProcess.&lt;init&gt;(UNIXProcess.java:247) at java.lang.ProcessImpl.start(ProcessImpl.java:134) at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.unpack.build.artifact.sh</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-14 01:00:00" id="19636" opendate="2020-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add declarative SlotPool</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-15 01:00:00" id="19646" opendate="2020-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamExecutionEnvironment support new Source interface based on FLIP-27</summary>
      <description>StreamExecutionEnvironment currently supports new Source interface based on FLIP-27 in Java, but doesn't support in Python. PyFlink StreamExecutionEnvironment should add methods related to new Source interface like from_source etc.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-15 01:00:00" id="19654" opendate="2020-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the execution time of PyFlink end-to-end tests</summary>
      <description>Thanks for the sharing from rmetzger, currently the test duration for PyFlink end-to-end test is as following:test caseaverage execution-timemaximum execution-timePyFlink Table end-to-end test1340s1877sPyFlink DataStream end-to-end test387s575sKubernetes PyFlink application test606s694sWe need to investigate how to improve them to reduce the execution time.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-python-test.python.add.one.py</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.table.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.datastream.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-15 01:00:00" id="19664" opendate="2020-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upload logs before end to end tests time out</summary>
      <description>Due to a bug in azure pipelines, we can not see the e2e output when a run times out.This ticket is to add some tooling for rescuing the logs before it's too late</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-16 01:00:00" id="19682" opendate="2020-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Actively timeout checkpoint barriers on the inputs</summary>
      <description>After receiving the first checkpoint barrier announcement, we should some kind of register a processing time timeout to switch to unaligned checkpoint.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TestSubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskCancellationBarrierTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.MockInputGate.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.ValidatingCheckpointHandler.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.UnalignedControllerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.UnalignedControllerCancellationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.TestBarrierHandlerFactory.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.InputProcessorUtilTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierTrackerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingControllerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlignedControllerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlignedControllerMassiveRandomTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.UnalignedController.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierBehaviourController.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingController.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AlignedController.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.TestStreamEnvironment.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2020-10-19 01:00:00" id="19716" opendate="2020-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to use Assume Role with EFO record publisher</summary>
      <description>Reproduction Steps Setup an application to consume from a Kinesis Stream Use ASSUME_ROLE credential provider consumerConfig.setProperty(AWSConfigConstants.AWS_CREDENTIALS_PROVIDER, ASSUME_ROLE.name());consumerConfig.setProperty(AWSConfigConstants.AWS_ROLE_ARN, "&lt;role-arn&gt;");consumerConfig.setProperty(AWSConfigConstants.AWS_ROLE_SESSION_NAME, "test-efo"); Expected Result Consumer is able to authorise and consume from the streamActual Result The following error is thrown (full stack attached) Caused by: org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.exception.SdkClientException: Unable to load an HTTP implementation from any provider in the chain. You must declare a dependency on an appropriate HTTP implementation or pass in an SdkHttpClient explicitly to the client builder. DiagnosisThis issue occurs because Assume Role credential provider requires a Sync HTTP Client. The Apache HTTP Client is on the classpath but it is not detected due to the shading relocation. It is looking for: org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.SdkHttpServiceHowever the service manifest is defining: software.amazon.awssdk.http.SdkHttpServiceSolution Add a service manifest such that the shaded HTTP client is used Also needed to update the HTTP client/core version due to incompatibilities TestingTested using EFO and POLLING record consumer  </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-20 01:00:00" id="19720" opendate="2020-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce new Providers and parallelism API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.sink.DynamicTableSink.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-10-20 01:00:00" id="19732" opendate="2020-10-20 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Disable Checkpointing when in BATCH mode</summary>
      <description>When we are executing a pipeline in BATCH mode, given that scheduling happens using independent regions and not all tasks are present at the same time, we should disable checkpointing.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorExecutionModeDetectionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.CheckpointConfig.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-20 01:00:00" id="19734" opendate="2020-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace &amp;#39;collection&amp;#39; connector by &amp;#39;values&amp;#39; connector for temporal join plan tests</summary>
      <description>Currently, both COLLECTION and VALUES connectors are `LookupTableSoure`, we can add a non lookup table source connector to cover  scan-only source.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-20 01:00:00" id="19739" opendate="2020-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CompileException when windowing in batch mode: A method named "replace" is not declared in any enclosing class nor any supertype</summary>
      <description>Example script:from pyflink.table import EnvironmentSettings, BatchTableEnvironmentfrom pyflink.table.window import Tumbleenv_settings = ( EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build())table_env = BatchTableEnvironment.create(environment_settings=env_settings)table_env.execute_sql( """ CREATE TABLE table1 ( amount INT, ts TIMESTAMP(3), WATERMARK FOR ts AS ts - INTERVAL '5' SECOND ) WITH ( 'connector.type' = 'filesystem', 'format.type' = 'csv', 'connector.path' = '/home/alex/work/test-flink/data1.csv' )""")table1 = table_env.from_path("table1")table = ( table1 .window(Tumble.over("5.days").on("ts").alias("__window")) .group_by("__window") .select("amount.sum"))print(table.to_pandas())Output:WARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by org.apache.flink.api.python.shaded.io.netty.util.internal.ReflectionUtil (file:/home/alex/work/flink/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/opt/flink-python_2.11-1.12-SNAPSHOT.jar) to constructor java.nio.DirectByteBuffer(long,int)WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.python.shaded.io.netty.util.internal.ReflectionUtilWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operationsWARNING: All illegal access operations will be denied in a future release/* 1 *//* 2 */ public class LocalHashWinAggWithoutKeys$59 extends org.apache.flink.table.runtime.operators.TableStreamOperator/* 3 */ implements org.apache.flink.streaming.api.operators.OneInputStreamOperator, org.apache.flink.streaming.api.operators.BoundedOneInput {/* 4 *//* 5 */ private final Object[] references;/* 6 */ /* 7 */ private static final org.slf4j.Logger LOG$2 =/* 8 */ org.slf4j.LoggerFactory.getLogger("LocalHashWinAgg");/* 9 */ /* 10 */ private transient org.apache.flink.table.types.logical.LogicalType[] aggMapKeyTypes$5;/* 11 */ private transient org.apache.flink.table.types.logical.LogicalType[] aggBufferTypes$6;/* 12 */ private transient org.apache.flink.table.runtime.operators.aggregate.BytesHashMap aggregateMap$7;/* 13 */ org.apache.flink.table.data.binary.BinaryRowData emptyAggBuffer$9 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 14 */ org.apache.flink.table.data.writer.BinaryRowWriter emptyAggBufferWriterTerm$10 = new org.apache.flink.table.data.writer.BinaryRowWriter(emptyAggBuffer$9);/* 15 */ org.apache.flink.table.data.GenericRowData hashAggOutput = new org.apache.flink.table.data.GenericRowData(2);/* 16 */ private transient org.apache.flink.table.data.binary.BinaryRowData reuseAggMapKey$17 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 17 */ private transient org.apache.flink.table.data.binary.BinaryRowData reuseAggBuffer$18 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 18 */ private transient org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry reuseAggMapEntry$19 = new org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry(reuseAggMapKey$17, reuseAggBuffer$18);/* 19 */ org.apache.flink.table.data.binary.BinaryRowData aggMapKey$3 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 20 */ org.apache.flink.table.data.writer.BinaryRowWriter aggMapKeyWriter$4 = new org.apache.flink.table.data.writer.BinaryRowWriter(aggMapKey$3);/* 21 */ private boolean hasInput = false;/* 22 */ org.apache.flink.streaming.runtime.streamrecord.StreamRecord element = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord((Object)null);/* 23 */ private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);/* 24 *//* 25 */ public LocalHashWinAggWithoutKeys$59(/* 26 */ Object[] references,/* 27 */ org.apache.flink.streaming.runtime.tasks.StreamTask task,/* 28 */ org.apache.flink.streaming.api.graph.StreamConfig config,/* 29 */ org.apache.flink.streaming.api.operators.Output output,/* 30 */ org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {/* 31 */ this.references = references;/* 32 */ aggMapKeyTypes$5 = (((org.apache.flink.table.types.logical.LogicalType[]) references[0]));/* 33 */ aggBufferTypes$6 = (((org.apache.flink.table.types.logical.LogicalType[]) references[1]));/* 34 */ this.setup(task, config, output);/* 35 */ if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {/* 36 */ ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)/* 37 */ .setProcessingTimeService(processingTimeService);/* 38 */ }/* 39 */ }/* 40 *//* 41 */ @Override/* 42 */ public void open() throws Exception {/* 43 */ super.open();/* 44 */ aggregateMap$7 = new org.apache.flink.table.runtime.operators.aggregate.BytesHashMap(this.getContainingTask(),this.getContainingTask().getEnvironment().getMemoryManager(),computeMemorySize(), aggMapKeyTypes$5, aggBufferTypes$6);/* 45 */ /* 46 */ /* 47 */ emptyAggBufferWriterTerm$10.reset();/* 48 */ /* 49 */ /* 50 */ if (true) {/* 51 */ emptyAggBufferWriterTerm$10.setNullAt(0);/* 52 */ } else {/* 53 */ emptyAggBufferWriterTerm$10.writeInt(0, ((int) -1));/* 54 */ }/* 55 */ /* 56 */ emptyAggBufferWriterTerm$10.complete();/* 57 */ /* 58 */ }/* 59 *//* 60 */ @Override/* 61 */ public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {/* 62 */ org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();/* 63 */ /* 64 */ org.apache.flink.table.data.binary.BinaryRowData currentAggBuffer$8;/* 65 */ int field$11;/* 66 */ boolean isNull$11;/* 67 */ int field$12;/* 68 */ boolean isNull$12;/* 69 */ boolean isNull$13;/* 70 */ int result$14;/* 71 */ org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.LookupInfo lookupInfo$20;/* 72 */ org.apache.flink.table.data.TimestampData field$21;/* 73 */ boolean isNull$21;/* 74 */ boolean isNull$22;/* 75 */ long result$23;/* 76 */ boolean isNull$24;/* 77 */ long result$25;/* 78 */ boolean isNull$26;/* 79 */ long result$27;/* 80 */ boolean isNull$28;/* 81 */ long result$29;/* 82 */ boolean isNull$30;/* 83 */ long result$31;/* 84 */ boolean isNull$32;/* 85 */ long result$33;/* 86 */ boolean isNull$34;/* 87 */ boolean result$35;/* 88 */ boolean isNull$36;/* 89 */ long result$37;/* 90 */ boolean isNull$38;/* 91 */ long result$39;/* 92 */ boolean isNull$40;/* 93 */ long result$41;/* 94 */ boolean isNull$42;/* 95 */ long result$43;/* 96 */ boolean isNull$44;/* 97 */ long result$45;/* 98 */ boolean isNull$46;/* 99 */ long result$47;/* 100 */ boolean isNull$48;/* 101 */ long result$49;/* 102 */ boolean isNull$50;/* 103 */ long result$51;/* 104 */ boolean isNull$52;/* 105 */ long result$53;/* 106 */ boolean isNull$55;/* 107 */ long result$56;/* 108 */ boolean isNull$57;/* 109 */ long result$58;/* 110 */ /* 111 */ /* 112 */ if (!in1.isNullAt(1)) {/* 113 */ hasInput = true;/* 114 */ // input field access for group key projection, window/pane assign/* 115 */ // and aggregate map update/* 116 */ isNull$11 = in1.isNullAt(0);/* 117 */ field$11 = -1;/* 118 */ if (!isNull$11) {/* 119 */ field$11 = in1.getInt(0);/* 120 */ }/* 121 */ isNull$21 = in1.isNullAt(1);/* 122 */ field$21 = null;/* 123 */ if (!isNull$21) {/* 124 */ field$21 = in1.getTimestamp(1, 3);/* 125 */ }/* 126 */ // assign timestamp(window or pane)/* 127 */ /* 128 */ /* 129 */ /* 130 */ /* 131 */ /* 132 */ isNull$22 = isNull$21;/* 133 */ result$23 = -1L;/* 134 */ if (!isNull$22) {/* 135 */ /* 136 */ result$23 = field$21.getMillisecond();/* 137 */ /* 138 */ }/* 139 */ /* 140 */ /* 141 */ isNull$24 = isNull$22 || false;/* 142 */ result$25 = -1L;/* 143 */ if (!isNull$24) {/* 144 */ /* 145 */ result$25 = (long) (result$23 * ((long) 1L));/* 146 */ /* 147 */ }/* 148 */ /* 149 */ isNull$26 = isNull$21;/* 150 */ result$27 = -1L;/* 151 */ if (!isNull$26) {/* 152 */ /* 153 */ result$27 = field$21.getMillisecond();/* 154 */ /* 155 */ }/* 156 */ /* 157 */ /* 158 */ isNull$28 = isNull$26 || false;/* 159 */ result$29 = -1L;/* 160 */ if (!isNull$28) {/* 161 */ /* 162 */ result$29 = (long) (result$27 * ((long) 1L));/* 163 */ /* 164 */ }/* 165 */ /* 166 */ /* 167 */ isNull$30 = isNull$28 || false;/* 168 */ result$31 = -1L;/* 169 */ if (!isNull$30) {/* 170 */ /* 171 */ result$31 = (long) (result$29 - ((long) 0L));/* 172 */ /* 173 */ }/* 174 */ /* 175 */ /* 176 */ isNull$32 = isNull$30 || false;/* 177 */ result$33 = -1L;/* 178 */ if (!isNull$32) {/* 179 */ /* 180 */ result$33 = (long) (result$31 % ((long) 432000000L));/* 181 */ /* 182 */ }/* 183 */ /* 184 */ /* 185 */ isNull$34 = isNull$32 || false;/* 186 */ result$35 = false;/* 187 */ if (!isNull$34) {/* 188 */ /* 189 */ result$35 = result$33 &lt; ((int) 0);/* 190 */ /* 191 */ }/* 192 */ /* 193 */ long result$54 = -1L;/* 194 */ boolean isNull$54;/* 195 */ if (result$35) {/* 196 */ /* 197 */ /* 198 */ /* 199 */ /* 200 */ /* 201 */ /* 202 */ isNull$36 = isNull$21;/* 203 */ result$37 = -1L;/* 204 */ if (!isNull$36) {/* 205 */ /* 206 */ result$37 = field$21.getMillisecond();/* 207 */ /* 208 */ }/* 209 */ /* 210 */ /* 211 */ isNull$38 = isNull$36 || false;/* 212 */ result$39 = -1L;/* 213 */ if (!isNull$38) {/* 214 */ /* 215 */ result$39 = (long) (result$37 * ((long) 1L));/* 216 */ /* 217 */ }/* 218 */ /* 219 */ /* 220 */ isNull$40 = isNull$38 || false;/* 221 */ result$41 = -1L;/* 222 */ if (!isNull$40) {/* 223 */ /* 224 */ result$41 = (long) (result$39 - ((long) 0L));/* 225 */ /* 226 */ }/* 227 */ /* 228 */ /* 229 */ isNull$42 = isNull$40 || false;/* 230 */ result$43 = -1L;/* 231 */ if (!isNull$42) {/* 232 */ /* 233 */ result$43 = (long) (result$41 % ((long) 432000000L));/* 234 */ /* 235 */ }/* 236 */ /* 237 */ /* 238 */ isNull$44 = isNull$42 || false;/* 239 */ result$45 = -1L;/* 240 */ if (!isNull$44) {/* 241 */ /* 242 */ result$45 = (long) (result$43 + ((long) 432000000L));/* 243 */ /* 244 */ }/* 245 */ /* 246 */ isNull$54 = isNull$44;/* 247 */ if (!isNull$54) {/* 248 */ result$54 = result$45;/* 249 */ }/* 250 */ }/* 251 */ else {/* 252 */ /* 253 */ /* 254 */ /* 255 */ /* 256 */ /* 257 */ isNull$46 = isNull$21;/* 258 */ result$47 = -1L;/* 259 */ if (!isNull$46) {/* 260 */ /* 261 */ result$47 = field$21.getMillisecond();/* 262 */ /* 263 */ }/* 264 */ /* 265 */ /* 266 */ isNull$48 = isNull$46 || false;/* 267 */ result$49 = -1L;/* 268 */ if (!isNull$48) {/* 269 */ /* 270 */ result$49 = (long) (result$47 * ((long) 1L));/* 271 */ /* 272 */ }/* 273 */ /* 274 */ /* 275 */ isNull$50 = isNull$48 || false;/* 276 */ result$51 = -1L;/* 277 */ if (!isNull$50) {/* 278 */ /* 279 */ result$51 = (long) (result$49 - ((long) 0L));/* 280 */ /* 281 */ }/* 282 */ /* 283 */ /* 284 */ isNull$52 = isNull$50 || false;/* 285 */ result$53 = -1L;/* 286 */ if (!isNull$52) {/* 287 */ /* 288 */ result$53 = (long) (result$51 % ((long) 432000000L));/* 289 */ /* 290 */ }/* 291 */ /* 292 */ isNull$54 = isNull$52;/* 293 */ if (!isNull$54) {/* 294 */ result$54 = result$53;/* 295 */ }/* 296 */ }/* 297 */ isNull$55 = isNull$24 || isNull$54;/* 298 */ result$56 = -1L;/* 299 */ if (!isNull$55) {/* 300 */ /* 301 */ result$56 = (long) (result$25 - result$54);/* 302 */ /* 303 */ }/* 304 */ /* 305 */ /* 306 */ isNull$57 = isNull$55 || false;/* 307 */ result$58 = -1L;/* 308 */ if (!isNull$57) {/* 309 */ /* 310 */ result$58 = (long) (result$56 - ((long) 0L));/* 311 */ /* 312 */ }/* 313 */ /* 314 */ // process each input/* 315 */ /* 316 */ // build aggregate map key/* 317 */ /* 318 */ /* 319 */ aggMapKeyWriter$4.reset();/* 320 */ /* 321 */ /* 322 */ if (false) {/* 323 */ aggMapKeyWriter$4.setNullAt(0);/* 324 */ } else {/* 325 */ aggMapKeyWriter$4.writeLong(0, result$58);/* 326 */ }/* 327 */ /* 328 */ aggMapKeyWriter$4.complete();/* 329 */ /* 330 */ // aggregate by each input with assigned timestamp/* 331 */ // look up output buffer using current key (grouping keys ..., assigned timestamp)/* 332 */ lookupInfo$20 = aggregateMap$7.lookup(aggMapKey$3);/* 333 */ currentAggBuffer$8 = lookupInfo$20.getValue();/* 334 */ if (!lookupInfo$20.isFound()) {/* 335 */ /* 336 */ // append empty agg buffer into aggregate map for current group key/* 337 */ try {/* 338 */ currentAggBuffer$8 =/* 339 */ aggregateMap$7.append(lookupInfo$20, emptyAggBuffer$9);/* 340 */ } catch (java.io.EOFException exp) {/* 341 */ /* 342 */ LOG$2.info("BytesHashMap out of memory with {} entries, output directly.", aggregateMap$7.getNumElements());/* 343 */ // hash map out of memory, output directly/* 344 */ /* 345 */ org.apache.flink.util.MutableObjectIterator&lt;org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry&gt; iterator =/* 346 */ aggregateMap$7.getEntryIterator();/* 347 */ while (iterator.next(reuseAggMapEntry$19) != null) {/* 348 */ /* 349 */ /* 350 */ /* 351 */ hashAggOutput.replace(reuseAggMapKey$17, reuseAggBuffer$18);/* 352 */ /* 353 */ output.collect(outElement.replace(hashAggOutput));/* 354 */ }/* 355 */ /* 356 */ // retry append/* 357 */ /* 358 */ // reset aggregate map retry append/* 359 */ aggregateMap$7.reset();/* 360 */ lookupInfo$20 = aggregateMap$7.lookup(aggMapKey$3);/* 361 */ try {/* 362 */ currentAggBuffer$8 =/* 363 */ aggregateMap$7.append(lookupInfo$20, emptyAggBuffer$9);/* 364 */ } catch (java.io.EOFException e) {/* 365 */ throw new OutOfMemoryError("BytesHashMap Out of Memory.");/* 366 */ }/* 367 */ /* 368 */ /* 369 */ }/* 370 */ }/* 371 */ // aggregate buffer fields access/* 372 */ isNull$12 = currentAggBuffer$8.isNullAt(0);/* 373 */ field$12 = -1;/* 374 */ if (!isNull$12) {/* 375 */ field$12 = currentAggBuffer$8.getInt(0);/* 376 */ }/* 377 */ // do aggregate and update agg buffer/* 378 */ int result$16 = -1;/* 379 */ boolean isNull$16;/* 380 */ if (isNull$11) {/* 381 */ /* 382 */ isNull$16 = isNull$12;/* 383 */ if (!isNull$16) {/* 384 */ result$16 = field$12;/* 385 */ }/* 386 */ }/* 387 */ else {/* 388 */ int result$15 = -1;/* 389 */ boolean isNull$15;/* 390 */ if (isNull$12) {/* 391 */ /* 392 */ isNull$15 = isNull$11;/* 393 */ if (!isNull$15) {/* 394 */ result$15 = field$11;/* 395 */ }/* 396 */ }/* 397 */ else {/* 398 */ /* 399 */ /* 400 */ /* 401 */ isNull$13 = isNull$12 || isNull$11;/* 402 */ result$14 = -1;/* 403 */ if (!isNull$13) {/* 404 */ /* 405 */ result$14 = (int) (field$12 + field$11);/* 406 */ /* 407 */ }/* 408 */ /* 409 */ isNull$15 = isNull$13;/* 410 */ if (!isNull$15) {/* 411 */ result$15 = result$14;/* 412 */ }/* 413 */ }/* 414 */ isNull$16 = isNull$15;/* 415 */ if (!isNull$16) {/* 416 */ result$16 = result$15;/* 417 */ }/* 418 */ }/* 419 */ if (isNull$16) {/* 420 */ currentAggBuffer$8.setNullAt(0);/* 421 */ } else {/* 422 */ currentAggBuffer$8.setInt(0, result$16);/* 423 */ }/* 424 */ /* 425 */ }/* 426 */ }/* 427 *//* 428 */ /* 429 */ @Override/* 430 */ public void endInput() throws Exception {/* 431 */ org.apache.flink.table.data.binary.BinaryRowData currentAggBuffer$8;/* 432 */ int field$11;/* 433 */ boolean isNull$11;/* 434 */ int field$12;/* 435 */ boolean isNull$12;/* 436 */ boolean isNull$13;/* 437 */ int result$14;/* 438 */ org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.LookupInfo lookupInfo$20;/* 439 */ org.apache.flink.table.data.TimestampData field$21;/* 440 */ boolean isNull$21;/* 441 */ boolean isNull$22;/* 442 */ long result$23;/* 443 */ boolean isNull$24;/* 444 */ long result$25;/* 445 */ boolean isNull$26;/* 446 */ long result$27;/* 447 */ boolean isNull$28;/* 448 */ long result$29;/* 449 */ boolean isNull$30;/* 450 */ long result$31;/* 451 */ boolean isNull$32;/* 452 */ long result$33;/* 453 */ boolean isNull$34;/* 454 */ boolean result$35;/* 455 */ boolean isNull$36;/* 456 */ long result$37;/* 457 */ boolean isNull$38;/* 458 */ long result$39;/* 459 */ boolean isNull$40;/* 460 */ long result$41;/* 461 */ boolean isNull$42;/* 462 */ long result$43;/* 463 */ boolean isNull$44;/* 464 */ long result$45;/* 465 */ boolean isNull$46;/* 466 */ long result$47;/* 467 */ boolean isNull$48;/* 468 */ long result$49;/* 469 */ boolean isNull$50;/* 470 */ long result$51;/* 471 */ boolean isNull$52;/* 472 */ long result$53;/* 473 */ boolean isNull$55;/* 474 */ long result$56;/* 475 */ boolean isNull$57;/* 476 */ long result$58;/* 477 */ /* 478 */ org.apache.flink.util.MutableObjectIterator&lt;org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry&gt; iterator =/* 479 */ aggregateMap$7.getEntryIterator();/* 480 */ while (iterator.next(reuseAggMapEntry$19) != null) {/* 481 */ /* 482 */ /* 483 */ /* 484 */ hashAggOutput.replace(reuseAggMapKey$17, reuseAggBuffer$18);/* 485 */ /* 486 */ output.collect(outElement.replace(hashAggOutput));/* 487 */ }/* 488 */ /* 489 */ }/* 490 */ /* 491 *//* 492 */ @Override/* 493 */ public void close() throws Exception {/* 494 */ super.close();/* 495 */ aggregateMap$7.free();/* 496 */ /* 497 */ }/* 498 *//* 499 */ /* 500 */ }/* 501 */ Traceback (most recent call last): File "/home/alex/.config/JetBrains/PyCharm2020.2/scratches/scratch_903.py", line 32, in &lt;module&gt; print(table.to_pandas()) File "/home/alex/work/flink/flink-python/pyflink/table/table.py", line 829, in to_pandas if batches.hasNext(): File "/home/alex/.cache/pypoetry/virtualenvs/relycomply-WA1zLZ2n-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py", line 1285, in __call__ return_value = get_return_value( File "/home/alex/work/flink/flink-python/pyflink/util/exceptions.py", line 147, in deco return f(*a, **kw) File "/home/alex/.cache/pypoetry/virtualenvs/relycomply-WA1zLZ2n-py3.8/lib/python3.8/site-packages/py4j/protocol.py", line 326, in get_return_value raise Py4JJavaError(py4j.protocol.Py4JJavaError: An error occurred while calling o51.hasNext.: java.lang.RuntimeException: Failed to fetch next result at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:77) at org.apache.flink.table.planner.sinks.SelectTableSinkBase$RowIteratorWrapper.hasNext(SelectTableSinkBase.java:115) at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:355) at org.apache.flink.table.runtime.arrow.ArrowUtils$1.hasNext(ArrowUtils.java:644) at org.apache.flink.table.runtime.arrow.ArrowUtils$2.hasNext(ArrowUtils.java:666) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282) at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79) at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238) at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.io.IOException: Failed to fetch job execution result at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:126) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:103) ... 16 moreCaused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2022) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:172) ... 18 moreCaused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147) at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:119) at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:680) at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658) at java.base/java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:2094) at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:117) ... 19 moreCaused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:217) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:210) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:204) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:526) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:413) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: Could not instantiate generated class 'LocalHashWinAggWithoutKeys$59' at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:67) at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40) at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:70) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:613) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:583) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:526) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:574) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:526) at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:164) at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:485) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:533) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) at java.base/java.lang.Thread.run(Thread.java:834)Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68) at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78) at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:65) ... 13 moreCaused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66) ... 15 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81) at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ... 18 moreCaused by: org.codehaus.commons.compiler.CompileException: Line 351, Column 33: A method named "replace" is not declared in any enclosing class nor any supertype, nor through a static import at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12124) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8997) at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5060) at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4421) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4394) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062) at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394) at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5575) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3781) at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3760) at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3732) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3732) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2871) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1842) at org.codehaus.janino.UnitCompiler.access$2200(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1498) at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$WhileStatement.accept(Java.java:3052) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileTryCatch(UnitCompiler.java:3136) at org.codehaus.janino.UnitCompiler.compileTryCatchFinally(UnitCompiler.java:2966) at org.codehaus.janino.UnitCompiler.compileTryCatchFinallyWithResources(UnitCompiler.java:2770) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2742) at org.codehaus.janino.UnitCompiler.access$2300(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitTryStatement(UnitCompiler.java:1499) at org.codehaus.janino.UnitCompiler$6.visitTryStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$TryStatement.accept(Java.java:3238) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$IfStatement.accept(Java.java:2947) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$IfStatement.accept(Java.java:2947) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:78) ... 24 moreHowever it works fine in streaming mode:env_settings = ( EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build())table_env = StreamTableEnvironment.create(environment_settings=env_settings)How the table is created seems irrelevant - this raises the same error:from datetime import datetimefrom pyflink.table import DataTypes, BatchTableEnvironment, EnvironmentSettingsfrom pyflink.table.window import Tumbleenv_settings = ( EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build())table_environment = BatchTableEnvironment.create(environment_settings=env_settings)transactions = table_environment.from_elements( [ (1, datetime(2000, 1, 1, 0, 0, 0)), (-2, datetime(2000, 1, 2, 0, 0, 0)), (3, datetime(2000, 1, 3, 0, 0, 0)), (-4, datetime(2000, 1, 4, 0, 0, 0)), ], DataTypes.ROW( [ DataTypes.FIELD("amount", DataTypes.BIGINT()), DataTypes.FIELD("ts", DataTypes.TIMESTAMP(3)), ] ),)table = ( transactions .window(Tumble.over("5.days").on("ts").alias("__window")) .group_by("__window") .select("amount.sum"))print(table.to_pandas())</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashWindowCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-22 01:00:00" id="19758" opendate="2020-10-22 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement a new unified File Sink based on the new Sink API</summary>
      <description/>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicyTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.DateTimeBucketAssignerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.WriterProperties.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.RowWisePartWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.RowWiseBucketWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.OnCheckpointRollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.CheckpointRollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.PartFileInfo.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.OutputStreamBasedPartFileWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.OutputFileConfig.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.InProgressFileWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BulkPartWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BulkBucketWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.SimpleVersionedStringSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.DateTimeBucketAssigner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.BasePathBucketAssigner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketAssigner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.AbstractPartFileWriter.java</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-10-23 01:00:00" id="19781" opendate="2020-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons_codec to 1.13 or newer</summary>
      <description>A user reported a dependency vulnerability which affects commons_codec &amp;#91;1&amp;#93;. We should try to upgrade this version to 1.13 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-23 01:00:00" id="19782" opendate="2020-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade antlr to 4.7.1 or newer</summary>
      <description>A user reported dependency vulnerabilities which affect antlr &amp;#91;1&amp;#93;. We should upgrade this dependency to 4.7.1 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-23 01:00:00" id="19783" opendate="2020-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade mesos to 1.7 or newer</summary>
      <description>A user reported a dependency vulnerability which affects mesos &amp;#91;1&amp;#93;. We should upgrade mesos to 1.7.0 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-mesos.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-23 01:00:00" id="19785" opendate="2020-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons-io to 2.7 or newer</summary>
      <description>A user reported a dependency vulnerability which affects commons-io &amp;#91;1&amp;#93;. We should try to upgrade this dependency to 2.7 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-10-23 01:00:00" id="19792" opendate="2020-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Interval join with equal time attributes is not recognized</summary>
      <description>A user reported that interval joins with equal time attribute predicate are not recognized, instead a regular inner join is used:For example:table1 = table_env.from_path("table1")table2 = table_env.from_path("table2")print(table1.join(table2).where("ts = ts2 &amp;&amp; id = id2").select("id, ts")The documentation clearly states that this should be supported:For example, the following predicates are valid interval join conditions:ltime === rtimeltime &gt;= rtime &amp;&amp; ltime &lt; rtime + 10.minutesSource: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html#joinsSee also the discussion here:https://stackoverflow.com/q/64445207/806430</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.IntervalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalJoinRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalIntervalJoinRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-26 01:00:00" id="19814" opendate="2020-10-26 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Disable FeedbackTransformation for BATCH mode</summary>
      <description>For now we do not support iterations in BATCH mode.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorBatchExecutionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-26 01:00:00" id="19815" opendate="2020-10-26 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Disable CoFeedbackTransformation for BATCH mode</summary>
      <description>Iterations are not supported in BATCH mode.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorBatchExecutionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-10-27 01:00:00" id="19836" opendate="2020-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SimpleVersionedSerializerTypeSerializerProxy</summary>
      <description>This allows using a SimpleVersionedSerializer, which the Source and Sink API provide, in places where we need a TypeSerializer, such as when setting the serializer that is used for records in a DataStream/that are sent between operators.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.ClosureCleanerTest.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-28 01:00:00" id="19844" opendate="2020-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Python UDAF</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-28 01:00:00" id="19849" opendate="2020-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check NOTICE files for 1.12 release</summary>
      <description>This will be automated through FLINK-19810</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-ml-parent.flink-ml-uber.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.licenses.LICENSE.javax.activation</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.licenses.LICENSE.javax.activation</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-state-machine.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-11-30 01:00:00" id="19894" opendate="2020-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use iloc for positional slicing instead of direct slicing in from_pandas</summary>
      <description>When you use floats are index of pandas, it produces a wrong results: &gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; t_env.from_pandas(pd.DataFrame({'a': [1, 2, 3]}, index=[2., 3., 4.])).to_pandas() a0 11 2 This is because direct slicing uses the value as index when the index contains floats: &gt;&gt;&gt; pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.])[2:] a2.0 13.0 24.0 3&gt;&gt;&gt; pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.]).iloc[2:] a4.0 3&gt;&gt;&gt; pd.DataFrame({'a': [1,2,3]}, index=[2, 3, 4])[2:] a4 3 </description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.conversion.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-30 01:00:00" id="19896" opendate="2020-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve first-n-row fetching in the rank operator</summary>
      <description>Currently Deduplicate operator only supports first-row deduplication (ordered by proc-time). In scenario of first-n-rows deduplication, the planner has to resort to Rank operator. However, Rank operator is less efficient than Deduplicate due to larger state and more state access.This issue proposes to extend DeduplicateKeepFirstRowFunction to support first-n-rows deduplication. And the original first-row deduplication would be a special case of first-n-rows deduplication.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.RankJsonPlanITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-30 01:00:00" id="19897" opendate="2020-10-30 00:00:00" resolution="Done">
    <buginformation>
      <summary>Improve UI related to FLIP-102</summary>
      <description>This ticket collects issues that came up after merging FLIP-102 related changes into master. The following issues should be fixed. Add Tooltip to Heap metrics cell pointing out that the max metrics might differ from the configured maximum value. This tooltip could be made optional and only appears if heap max is different from the configured value. Here's a proposal for the tooltip text: The maximum heap displayed might differ from the configured values depending on the used GC algorithm for this process. Rename "Network Memory Segments" into "Netty Shuffle Buffers" Rename "Network Garbage Collection" into "Garbage Collection"</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.styles.rewrite.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-30 01:00:00" id="19899" opendate="2020-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Kinesis][EFO] Optimise error handling to use a separate exception delivery mechanism</summary>
      <description>BackgroundThere is a queue used to pass events between the network client and consumer application. When an error is thrown in the network thread, the queue is cleared to make space for the error event. This means that records will be thrown away to make space for errors (the records would be subsequently reloaded from the shard).ScopeAdd a new mechanism to pass exceptions between threads, meaning data does not need to be discarded. When an error is thrown, the error event will be processed by the consumer once all of the records have been processed.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.FakeKinesisFanOutBehavioursFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2Test.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-31 01:00:00" id="19908" opendate="2020-10-31 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>FlinkLogicalTableSourceScan and CommonPhysicalTableSourceScan should respect source reuse config option</summary>
      <description>Currently we have the table.optimizer.reuse-sub-plan-enabled config option to configure the reuse of sources. However FlinkLogicalTableSourceScan and CommonPhysicalTableSourceScan do not respect this option and are always reused.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.reuse.SubplanReuser.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-8 01:00:00" id="1992" opendate="2015-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add convergence criterion to SGD optimizer</summary>
      <description>Currently, Flink's SGD optimizer runs for a fixed number of iterations. It would be good to support a dynamic convergence criterion, too.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.optimization.RegularizationITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.optimization.LossFunctionITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.optimization.GradientDescentITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.optimization.Solver.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.optimization.Regularization.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.optimization.LossFunction.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.optimization.GradientDescent.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-2 01:00:00" id="19924" opendate="2020-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disallow unaligned checkpoints for iterative jobs</summary>
      <description>For rescaling of Unaligned Checkpoints added in 1.12, EndOfChannelRecovery and upstream alignment will be added.However, this alignment won't work for iterative jobs.There is a need to pass this info to subtasks (whether is iterative) validate on startup a flag to skip validation</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.CheckpointConfig.java</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">docs..includes.generated.execution.checkpointing.configuration.html</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-3-3 01:00:00" id="19938" opendate="2020-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement shuffle data read scheduling for sort-merge blocking shuffle</summary>
      <description>As described in https://cwiki.apache.org/confluence/display/FLINK/FLIP-148%3A+Introduce+Sort-Merge+Based+Blocking+Shuffle+to+Flink. shuffle IO scheduling is important for performance. We'd like to Introduce it to sort-merge shuffle first.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriteReadTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPoolTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeSubpartitionReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPool.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-3 01:00:00" id="19940" opendate="2020-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task names on web UI should change when an operator chain is chained with sources</summary>
      <description>Currently task names on web UI doesn't change even if the operator chain is chained with sources. We should change its name to show that the sources are chained.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-3-3 01:00:00" id="19944" opendate="2020-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support sink parallelism configuration to Hive connector</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveDynamicTableFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.filesystem.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-12-3 01:00:00" id="19947" opendate="2020-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support sink parallelism configuration to Print connector</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.table.PrintConnectorITCase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.PrintTableSinkFactory.java</file>
      <file type="M">docs.dev.table.connectors.print.zh.md</file>
      <file type="M">docs.dev.table.connectors.print.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-4 01:00:00" id="19974" opendate="2020-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQLClientKafkaITCase.testKafka times out while creating topic caused by "PyFlink end-to-end test"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8967&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d3555292020-11-04T12:00:36.0501135Z Nov 04 12:00:36 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 57.959 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.tests.util.kafka.SQLClientKafkaITCase2020-11-04T12:00:36.0538557Z Nov 04 12:00:36 [ERROR] testKafka[0: kafka-version:2.4.1 kafka-sql-version:universal](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 57.949 s &lt;&lt;&lt; ERROR!2020-11-04T12:00:36.0539781Z Nov 04 12:00:36 java.io.IOException: Process failed due to timeout.2020-11-04T12:00:36.0540659Z Nov 04 12:00:36 at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:130)2020-11-04T12:00:36.0548817Z Nov 04 12:00:36 at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:108)2020-11-04T12:00:36.0549582Z Nov 04 12:00:36 at org.apache.flink.tests.util.AutoClosableProcess.runBlocking(AutoClosableProcess.java:70)2020-11-04T12:00:36.0550231Z Nov 04 12:00:36 at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.createTopic(LocalStandaloneKafkaResource.java:261)2020-11-04T12:00:36.0550909Z Nov 04 12:00:36 at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:136)</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-4 01:00:00" id="19979" opendate="2020-11-4 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Sanity check after bash e2e tests for no leftover processes</summary>
      <description>As seen in FLINK-19974, if an e2e test is not cleaning up properly, other e2e tests might fail with difficult to diagnose issues.I propose to check that no leftover processes (including docker containers) are running after each bash e2e test.</description>
      <version>1.12.0</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e.uploading.watchdog.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-5 01:00:00" id="19984" opendate="2020-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add TypeSerializerTestCoverageTest to check whether tests based on SerializerTestBase and TypeSerializerUpgradeTestBase</summary>
      <description>Currently, we have TypeInfoTestCoverageTest which checks that we have a test that extends TypeInformationTestBase for all type infos. But TypeSerializer doesn’t have the same thing that would verify that TypeSerializer has tests that extend SerializerTestBase and TypeSerializerUpgradeTestBase. Therefore we don’t know if test coverage of TypeSerializer is good.This would add TypeSerializerTestCoverageTest to check whether to have tests based on SerializerTestBase and TypeSerializerUpgradeTestBase because all serializers should have tests based on both of them.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.test.java.org.apache.flink.api.scala.typeutils.ScalaOptionSerializerUpgradeTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-5 01:00:00" id="19987" opendate="2020-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBaseDynamicTableFactoryTest.testTableSourceFactory failed with "NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9020&amp;view=logs&amp;j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&amp;t=bfbc6239-57a0-5db0-63f3-41551b4f7d512020-11-04T22:30:58.3273231Z testTableSourceFactory(org.apache.flink.connector.hbase1.HBaseDynamicTableFactoryTest) Time elapsed: 0.894 sec &lt;&lt;&lt; ERROR! 2020-11-04T22:30:58.3274285Z org.apache.flink.table.api.ValidationException: 2020-11-04T22:30:58.3274931Z Unable to create a source for reading table 'default.default.t1'. 2020-11-04T22:30:58.3275159Z 2020-11-04T22:30:58.3275364Z Table options are: 2020-11-04T22:30:58.3275523Z 2020-11-04T22:30:58.3275882Z 'connector'='hbase-1.4' 2020-11-04T22:30:58.3276283Z 'table-name'='testHBastTable' 2020-11-04T22:30:58.3276712Z 'zookeeper.quorum'='localhost:2181' 2020-11-04T22:30:58.3277158Z 'zookeeper.znode.parent'='/flink' 2020-11-04T22:30:58.3277553Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125) 2020-11-04T22:30:58.3278358Z at org.apache.flink.connector.hbase1.HBaseDynamicTableFactoryTest.createTableSource(HBaseDynamicTableFactoryTest.java:332) 2020-11-04T22:30:58.3279046Z at org.apache.flink.connector.hbase1.HBaseDynamicTableFactoryTest.testTableSourceFactory(HBaseDynamicTableFactoryTest.java:104) 2020-11-04T22:30:58.3279623Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 2020-11-04T22:30:58.3280067Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 2020-11-04T22:30:58.3280703Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 2020-11-04T22:30:58.3281149Z at java.lang.reflect.Method.invoke(Method.java:498) 2020-11-04T22:30:58.3281615Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) 2020-11-04T22:30:58.3282135Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 2020-11-04T22:30:58.3282637Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) 2020-11-04T22:30:58.3283151Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 2020-11-04T22:30:58.3283749Z at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) 2020-11-04T22:30:58.3284221Z at org.junit.rules.RunRules.evaluate(RunRules.java:20) 2020-11-04T22:30:58.3284624Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) 2020-11-04T22:30:58.3285091Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) 2020-11-04T22:30:58.3285590Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) 2020-11-04T22:30:58.3286053Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) 2020-11-04T22:30:58.3286487Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) 2020-11-04T22:30:58.3286911Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) 2020-11-04T22:30:58.3287348Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) 2020-11-04T22:30:58.3287779Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) 2020-11-04T22:30:58.3288262Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363) 2020-11-04T22:30:58.3288721Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367) 2020-11-04T22:30:58.3289254Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274) 2020-11-04T22:30:58.3289781Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) 2020-11-04T22:30:58.3290312Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161) 2020-11-04T22:30:58.3291103Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290) 2020-11-04T22:30:58.3291649Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242) 2020-11-04T22:30:58.3292158Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121) 2020-11-04T22:30:58.3292703Z Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V 2020-11-04T22:30:58.3293230Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357) 2020-11-04T22:30:58.3293876Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338) 2020-11-04T22:30:58.3294441Z at org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.createDynamicTableSource(HBase1DynamicTableFactory.java:113) 2020-11-04T22:30:58.3295032Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:122) 2020-11-04T22:30:58.3295401Z ... 28 more</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-5 01:00:00" id="19989" opendate="2020-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add collect operation in Python DataStream API</summary>
      <description>DataStream.executeAndCollect() has already been supported in FLINK-19508. We should also support it in the Python DataStream API.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.api.common.python.PythonBridgeUtils.java</file>
      <file type="M">flink-python.pyflink.table.utils.py</file>
      <file type="M">flink-python.pyflink.table.table.result.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.pyflink.common.typeinfo.py</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TupleTypeInfoBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.RowTypeInfo.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-5 01:00:00" id="19992" opendate="2020-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate new orc to Hive source</summary>
      <description>After introducing `OrcColumnarRowFileInputFormat`We need integrate it to Hive, including Hive 2+ and Hive 1.X</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-5 01:00:00" id="19997" opendate="2020-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement an e2e test for sql-client with Confluent Registry Avro format</summary>
      <description>We should add an e2e test that would verify the format as well as packaging of the format sql jar.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.util.FileUtils.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.SQLJobSubmission.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.LocalStandaloneFlinkResourceFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-11 01:00:00" id="2000" opendate="2015-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SQL-style aggregations for Table API</summary>
      <description>Right now, the syntax for aggregations is "a.count, a.min" and so on. We could in addition offer "COUNT(a), MIN(a)" and so on.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggreagationsITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.parser.ExpressionParser.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-6 01:00:00" id="20021" opendate="2020-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup several "Dispatcher"/"Jobmanager" misuses in the docs</summary>
      <description>There are a few places in the ops/monitoring docs where we use "Dispatcher" instead of "JobManager", and "JobManager" instead of "JobMaster".</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.security-ssl.zh.md</file>
      <file type="M">docs.ops.security-ssl.md</file>
      <file type="M">docs.ops.deployment.mesos.zh.md</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
      <file type="M">docs.ops.deployment.local.zh.md</file>
      <file type="M">docs.ops.deployment.local.md</file>
      <file type="M">docs.monitoring.rest.api.zh.md</file>
      <file type="M">docs.monitoring.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-6 01:00:00" id="20022" opendate="2020-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move statebackend tradeoffs to from production readiness checklist to statebackend page</summary>
      <description>The production readiness checklist contains a section for choosing the right statebackend, providing pros/const for each statebackend.This should be moved to the actual statebackend page, which is rather light on the topic.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.production.ready.zh.md</file>
      <file type="M">docs.ops.production.ready.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-6 01:00:00" id="20029" opendate="2020-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix computed column can&amp;#39;t be defined on the metadata column</summary>
      <description>Currenlty it's not allowed to define the computed column on metadata. It's very useful when users extract the metadata from record and use the information as watermark. </description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SourceWatermarkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.MergeTableLikeUtilTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.MergeTableLikeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-12 01:00:00" id="2003" opendate="2015-5-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Building on some encrypted filesystems leads to "File name too long" error</summary>
      <description>The classnames generated from the build system can be too long.Creating too long filenames in some encrypted filesystems is not possible, including encfs which is what Ubuntu uses.This the same as this Spark issueThe workaround (taken from the linked issue) is to add in Maven under the compile options: + &lt;arg&gt;-Xmax-classfile-name&lt;/arg&gt;+ &lt;arg&gt;128&lt;/arg&gt;And in SBT add:+ scalacOptions in Compile ++= Seq("-Xmax-classfile-name", "128"),</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.building.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-3-7 01:00:00" id="20042" opendate="2020-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end-to-end tests for Kinesis Table sources and sinks</summary>
      <description>Follow-up issue to add end-to-end tests for the recently added KinesisDynamicSource and KinesisDynamicSink. See the discussion in PR #13770 for details.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.KinesisPubsubClient.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-8 01:00:00" id="20046" opendate="2020-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamTableAggregateTests.test_map_view_iterate is instable</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9279&amp;view=logs&amp;j=821b528f-1eed-5598-a3b4-7f748b13f261&amp;t=4fad9527-b9a5-5015-1b70-8356e5c914902020-11-07T22:50:57.4180758Z _______________ StreamTableAggregateTests.test_map_view_iterate ________________2020-11-07T22:50:57.4181301Z 2020-11-07T22:50:57.4181965Z self = &lt;pyflink.table.tests.test_aggregate.StreamTableAggregateTests testMethod=test_map_view_iterate&gt;2020-11-07T22:50:57.4182348Z 2020-11-07T22:50:57.4182535Z def test_map_view_iterate(self):2020-11-07T22:50:57.4182812Z test_iterate = udaf(TestIterateAggregateFunction())2020-11-07T22:50:57.4183320Z self.t_env.get_config().set_idle_state_retention(datetime.timedelta(days=1))2020-11-07T22:50:57.4183763Z self.t_env.get_config().get_configuration().set_string(2020-11-07T22:50:57.4297555Z "python.fn-execution.bundle.size", "2")2020-11-07T22:50:57.4297922Z # trigger the cache eviction in a bundle.2020-11-07T22:50:57.4308028Z self.t_env.get_config().get_configuration().set_string(2020-11-07T22:50:57.4308653Z "python.state.cache-size", "2")2020-11-07T22:50:57.4308945Z self.t_env.get_config().get_configuration().set_string(2020-11-07T22:50:57.4309382Z "python.map-state.read-cache-size", "2")2020-11-07T22:50:57.4309676Z self.t_env.get_config().get_configuration().set_string(2020-11-07T22:50:57.4310428Z "python.map-state.write-cache-size", "2")2020-11-07T22:50:57.4310701Z self.t_env.get_config().get_configuration().set_string(2020-11-07T22:50:57.4311130Z "python.map-state.iterate-response-batch-size", "2")2020-11-07T22:50:57.4311361Z t = self.t_env.from_elements(2020-11-07T22:50:57.4311691Z [(1, 'Hi_', 'hi'),2020-11-07T22:50:57.4312004Z (1, 'Hi', 'hi'),2020-11-07T22:50:57.4312316Z (2, 'hello', 'hello'),2020-11-07T22:50:57.4312639Z (3, 'Hi_', 'hi'),2020-11-07T22:50:57.4312975Z (3, 'Hi', 'hi'),2020-11-07T22:50:57.4313285Z (4, 'hello', 'hello'),2020-11-07T22:50:57.4313609Z (5, 'Hi2_', 'hi'),2020-11-07T22:50:57.4313908Z (5, 'Hi2', 'hi'),2020-11-07T22:50:57.4314238Z (6, 'hello2', 'hello'),2020-11-07T22:50:57.4314558Z (7, 'Hi', 'hi'),2020-11-07T22:50:57.4315053Z (8, 'hello', 'hello'),2020-11-07T22:50:57.4315396Z (9, 'Hi2', 'hi'),2020-11-07T22:50:57.4315773Z (13, 'Hi3', 'hi')], ['a', 'b', 'c'])2020-11-07T22:50:57.4316023Z self.t_env.create_temporary_view("source", t)2020-11-07T22:50:57.4316299Z table_with_retract_message = self.t_env.sql_query(2020-11-07T22:50:57.4316615Z "select LAST_VALUE(b) as b, LAST_VALUE(c) as c from source group by a")2020-11-07T22:50:57.4316919Z result = table_with_retract_message.group_by(t.c) \2020-11-07T22:50:57.4317197Z .select(test_iterate(t.b).alias("a"), t.c) \2020-11-07T22:50:57.4317619Z .select(col("a").get(0).alias("a"),2020-11-07T22:50:57.4318111Z col("a").get(1).alias("b"),2020-11-07T22:50:57.4318357Z col("a").get(2).alias("c"),2020-11-07T22:50:57.4318586Z col("a").get(3).alias("d"),2020-11-07T22:50:57.4318814Z t.c.alias("e"))2020-11-07T22:50:57.4319023Z assert_frame_equal(2020-11-07T22:50:57.4319208Z &gt; result.to_pandas(),2020-11-07T22:50:57.4319408Z pd.DataFrame([2020-11-07T22:50:57.4319872Z ["hello,hello2", "1,3", 'hello:3,hello2:1', 2, "hello"],2020-11-07T22:50:57.4320398Z ["Hi,Hi2,Hi3", "1,2,3", "Hi:3,Hi2:2,Hi3:1", 3, "hi"]],2020-11-07T22:50:57.4321047Z columns=['a', 'b', 'c', 'd', 'e']))2020-11-07T22:50:57.4321198Z 2020-11-07T22:50:57.4321385Z pyflink/table/tests/test_aggregate.py:468: 2020-11-07T22:50:57.4321648Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 2020-11-07T22:50:57.4322040Z pyflink/table/table.py:807: in to_pandas2020-11-07T22:50:57.4322299Z .collectAsPandasDataFrame(self._j_table, max_arrow_batch_size)2020-11-07T22:50:57.4322794Z .tox/py35-cython/lib/python3.5/site-packages/py4j/java_gateway.py:1286: in __call__2020-11-07T22:50:57.4323103Z answer, self.gateway_client, self.target_id, self.name)2020-11-07T22:50:57.4323351Z pyflink/util/exceptions.py:147: in deco2020-11-07T22:50:57.4323537Z return f(*a, **kw)2020-11-07T22:50:57.4323783Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 2020-11-07T22:50:57.4323963Z 2020-11-07T22:50:57.4324225Z answer = 'xro8653'2020-11-07T22:50:57.4324496Z gateway_client = &lt;py4j.java_gateway.GatewayClient object at 0x7fe5c619db70&gt;2020-11-07T22:50:57.4324943Z target_id = 'z:org.apache.flink.table.runtime.arrow.ArrowUtils'2020-11-07T22:50:57.4325312Z name = 'collectAsPandasDataFrame'2020-11-07T22:50:57.4325439Z 2020-11-07T22:50:57.4325839Z def get_return_value(answer, gateway_client, target_id=None, name=None):2020-11-07T22:50:57.4326420Z """Converts an answer received from the Java gateway into a Python object.2020-11-07T22:50:57.4326648Z 2020-11-07T22:50:57.4326881Z For example, string representation of integers are converted to Python2020-11-07T22:50:57.4327193Z integer, string representation of objects are converted to JavaObject2020-11-07T22:50:57.4327451Z instances, etc.2020-11-07T22:50:57.4327614Z 2020-11-07T22:50:57.4327819Z :param answer: the string returned by the Java gateway2020-11-07T22:50:57.4328157Z :param gateway_client: the gateway client used to communicate with the Java2020-11-07T22:50:57.4329738Z Gateway. Only necessary if the answer is a reference (e.g., object,2020-11-07T22:50:57.4330018Z list, map)2020-11-07T22:50:57.4330273Z :param target_id: the name of the object from which the answer comes from2020-11-07T22:50:57.4330588Z (e.g., *object1* in `object1.hello()`). Optional.2020-11-07T22:50:57.4330873Z :param name: the name of the member from which the answer comes from2020-11-07T22:50:57.4331170Z (e.g., *hello* in `object1.hello()`). Optional.2020-11-07T22:50:57.4331375Z """2020-11-07T22:50:57.4331542Z if is_error(answer)[0]:2020-11-07T22:50:57.4331761Z if len(answer) &gt; 1:2020-11-07T22:50:57.4331954Z type = answer[1]2020-11-07T22:50:57.4332222Z value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)2020-11-07T22:50:57.4332531Z if answer[1] == REFERENCE_TYPE:2020-11-07T22:50:57.4332757Z raise Py4JJavaError(2020-11-07T22:50:57.4333016Z "An error occurred while calling {0}{1}{2}.\n".2020-11-07T22:50:57.4333303Z &gt; format(target_id, ".", name), value)2020-11-07T22:50:57.4333700Z E py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame.2020-11-07T22:50:57.4334558Z E : java.lang.RuntimeException: Could not remove element ',,,1,hi', should never happen.2020-11-07T22:50:57.4335019Z E at org.apache.flink.table.runtime.arrow.ArrowUtils.filterOutRetractRows(ArrowUtils.java:708)2020-11-07T22:50:57.4335479Z E at org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame(ArrowUtils.java:635)2020-11-07T22:50:57.4336238Z E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-11-07T22:50:57.4336645Z E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-11-07T22:50:57.4337099Z E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-11-07T22:50:57.4337485Z E at java.lang.reflect.Method.invoke(Method.java:498)2020-11-07T22:50:57.4337911Z E at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)2020-11-07T22:50:57.4338410Z E at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)2020-11-07T22:50:57.4338859Z E at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)2020-11-07T22:50:57.4339324Z E at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)2020-11-07T22:50:57.4339810Z E at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)2020-11-07T22:50:57.4340260Z E at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)2020-11-07T22:50:57.4340651Z E at java.lang.Thread.run(Thread.java:748)</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-9 01:00:00" id="20053" opendate="2020-11-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for file compaction</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.filesystem.zh.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-10 01:00:00" id="20074" opendate="2020-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix can&amp;#39;t generate plan when joining on changelog source without updates</summary>
      <description>INSTANCE: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9379&amp;view=logs&amp;s=ae4f8708-9994-57d3-c2d7-b892156e7812&amp;j=e25d5e7e-2a9c-5589-4940-0b638d75a414</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-11 01:00:00" id="20086" opendate="2020-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the open method of UserDefinedFunction</summary>
      <description>According to the questions asked by PyFlink users so far, many users are not aware that there is a open method in UserDefinedFunction where they could perform initialization work. This method is especially useful for ML users where they could perform ML mode initialization.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.python.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-11 01:00:00" id="20093" opendate="2020-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a download page for all optional sql client components</summary>
      <description>It would be nice to have a single page that lists all optional binaries for sql client. We could link such a page from the "Optional components" section in the https://flink.apache.org/downloads.html</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.kinesis.md</file>
      <file type="M">docs.dev.table.connectors.kafka.md</file>
      <file type="M">docs.dev.table.connectors.jdbc.md</file>
      <file type="M">docs.dev.table.connectors.hbase.md</file>
      <file type="M">docs.dev.table.connectors.formats.parquet.md</file>
      <file type="M">docs.dev.table.connectors.formats.orc.md</file>
      <file type="M">docs.dev.table.connectors.formats.json.md</file>
      <file type="M">docs.dev.table.connectors.formats.debezium.md</file>
      <file type="M">docs.dev.table.connectors.formats.csv.md</file>
      <file type="M">docs.dev.table.connectors.formats.canal.md</file>
      <file type="M">docs.dev.table.connectors.formats.avro.md</file>
      <file type="M">docs.dev.table.connectors.formats.avro-confluent.md</file>
      <file type="M">docs.dev.table.connectors.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-11 01:00:00" id="20096" opendate="2020-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up PyFlink documentation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.fig.pyflink.svg</file>
      <file type="M">docs.dev.python.table-api-users-guide.index.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.index.md</file>
      <file type="M">docs.try-flink.python.api.zh.md</file>
      <file type="M">docs.try-flink.python.api.md</file>
      <file type="M">docs.try-flink.index.zh.md</file>
      <file type="M">docs.index.zh.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.dev.python.index.zh.md</file>
      <file type="M">docs.dev.python.index.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.conversion.of.pandas.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-11 01:00:00" id="20098" opendate="2020-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t add flink-connector-files to flink-dist, make dependencies explicit</summary>
      <description>We currently add both flink-connector-files and flink-connector-base to flink-dist. This implies, that users should use the dependency like this:&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-files&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;which differs from other connectors where users don't need to specify &lt;scope&gt;provided&lt;/scope&gt;.Also, flink-connector-files has flink-connector-base as a provided dependency, which means that examples that use this dependency will not run out-of-box in IntelliJ because transitive provided dependencies will not be considered.I propose to just remove the dependencies from flink-dist and let users use the File Connector like any other connector.I believe the initial motivation for "providing" the File Connector in flink-dist was to allow us to use the File Connector under the hood in methods such as StreamExecutionEnvironment.readFile(...). We could decide to deprecate and remove those methods or re-add the File Connector as an explicit (non-provided) dependency again in the future.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-uber.pom.xml</file>
      <file type="M">flink-table.flink-table-uber-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-12 01:00:00" id="20102" opendate="2020-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update HBase connector documentation for HBase 2.x supporting</summary>
      <description>Currently, the HBase connector page says it only supports HBase 1.4.x.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.index.md</file>
      <file type="M">docs..data.sql-connectors.yml</file>
      <file type="M">docs.dev.table.connectors.hbase.zh.md</file>
      <file type="M">docs.dev.table.connectors.hbase.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-3-12 01:00:00" id="20130" opendate="2020-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ZStandard format to inputs</summary>
      <description>Allow Flink to read files compressed in ZStandard (.zst)</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.compression.ZStandardInputStreamFactory.java</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.GenericCsvInputFormatTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.FileInputFormat.java</file>
      <file type="M">docs.content.zh.docs.dev.dataset.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-13 01:00:00" id="20140" opendate="2020-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation of TableResult.collect for Python Table API</summary>
      <description>The documentation of TableResult.collect is missing for Python Table API. This API is very useful for users and we should add clear documentation about it.</description>
      <version>1.12.0</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.result.py</file>
      <file type="M">flink-python.pyflink.table.table.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">docs.content.docs.dev.python.table.intro.to.table.api.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.intro.to.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-13 01:00:00" id="20144" opendate="2020-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change link format to "{% link ... %}" in docs/ops dir</summary>
      <description>Some documents' link format in docs/ops is "{{site.baseurl}/... }". But it is preferred to use "{% link ... %}" format.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.savepoints.md</file>
      <file type="M">docs.ops.upgrading.zh.md</file>
      <file type="M">docs.ops.upgrading.md</file>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.state.savepoints.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.config.md</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.deployment.cluster.setup.md</file>
      <file type="M">docs.ops.deployment.docker.md</file>
      <file type="M">docs.ops.deployment.docker.zh.md</file>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
      <file type="M">docs.ops.deployment.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.local.md</file>
      <file type="M">docs.ops.deployment.local.zh.md</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
      <file type="M">docs.ops.deployment.mesos.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.zh.md</file>
      <file type="M">docs.ops.filesystems.azure.md</file>
      <file type="M">docs.ops.filesystems.azure.zh.md</file>
      <file type="M">docs.ops.filesystems.index.md</file>
      <file type="M">docs.ops.filesystems.index.zh.md</file>
      <file type="M">docs.ops.filesystems.oss.md</file>
      <file type="M">docs.ops.filesystems.oss.zh.md</file>
      <file type="M">docs.ops.filesystems.s3.md</file>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.md</file>
      <file type="M">docs.ops.memory.mem.setup.jobmanager.md</file>
      <file type="M">docs.ops.memory.mem.setup.jobmanager.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.tuning.md</file>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.production.ready.md</file>
      <file type="M">docs.ops.production.ready.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.security-kerberos.md</file>
      <file type="M">docs.ops.security-kerberos.zh.md</file>
      <file type="M">docs.ops.security-ssl.md</file>
      <file type="M">docs.ops.security-ssl.zh.md</file>
      <file type="M">docs.ops.state.checkpoints.md</file>
      <file type="M">docs.ops.state.checkpoints.zh.md</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">docs.ops.state.large.state.tuning.zh.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-17 01:00:00" id="20180" opendate="2020-11-17 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Translation the FileSink Document into Chinese</summary>
      <description>Translate the newly added FileSink documentation into Chinese</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
      <file type="M">docs.dev.connectors.index.zh.md</file>
      <file type="M">docs.dev.connectors.file.sink.zh.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-17 01:00:00" id="20184" opendate="2020-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update hive streaming read and temporal table documents</summary>
      <description>The hive streaming read and temporal table document has been out of style, we need to update it.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.hive.hive.read.write.zh.md</file>
      <file type="M">docs.dev.table.connectors.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-17 01:00:00" id="20191" opendate="2020-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for FLIP-95 ability interfaces</summary>
      <description/>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.abilities.SupportsWatermarkPushDown.java</file>
      <file type="M">docs.dev.table.sourceSinks.zh.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
      <file type="M">docs.dev.table.connectors.upsert-kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.upsert-kafka.md</file>
      <file type="M">docs.dev.table.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-18 01:00:00" id="20216" opendate="2020-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move "Configuration" and "Environment Variables" under "Python API" section</summary>
      <description>"Configuration" and "Environment Variables" are currently under the documentation of "Python Table API". However, these sections apply for both Python DataStream API and Python Table API and so we should move them under "Python API".</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.python.config.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.python.config.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.intro.to.table.api.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.intro.to.table.api.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.index.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.index.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.environment.variables.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.environment.variables.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.conversion.of.pandas.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.conversion.of.pandas.md</file>
      <file type="M">docs.dev.python.installation.zh.md</file>
      <file type="M">docs.dev.python.installation.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-19 01:00:00" id="20235" opendate="2020-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing Hive dependencies</summary>
      <description>I tried following the setup here: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/#dependenciesI put the flink-sql-connector-hive-2.3.6 in the \lib directory and tried running queries (as described in https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/hive_streaming.html) via sql-client.SET table.sql-dialect=hive;CREATE TABLE hive_table ( user_id STRING, order_amount DOUBLE) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES ( 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00', 'sink.partition-commit.trigger'='partition-time', 'sink.partition-commit.delay'='1 s', 'sink.partition-commit.policy.kind'='metastore,success-file');SET table.sql-dialect=default;SELECT * FROM hive_table;It fails with:Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.flink.hive.shaded.parquet.format.converter.ParquetMetadataConverter at org.apache.flink.hive.shaded.formats.parquet.ParquetVectorizedInputFormat.createReader(ParquetVectorizedInputFormat.java:112) at org.apache.flink.hive.shaded.formats.parquet.ParquetVectorizedInputFormat.createReader(ParquetVectorizedInputFormat.java:73) at org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter.createReader(HiveBulkFormatAdapter.java:99) at org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter.createReader(HiveBulkFormatAdapter.java:62) at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.checkSplitOrStartNext(FileSourceSplitReader.java:110) at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:68) at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:136) at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:100) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ... 1 more</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-sql-parquet.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-parquet.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-19 01:00:00" id="20245" opendate="2020-11-19 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document how to create a Hive catalog from DDL</summary>
      <description>I'd appreciate if the documentation contained a description how to create the hive catalog from DDL. What I am missing especially are the options that HiveCatalog expects (type, conf-dir). We should have a table somewhere with a description possible values etc. the same way as we have such tables for other connectors and formats. See e.g. https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/kafka.html#connector-options</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.hive.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.hive.index.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-22 01:00:00" id="20275" opendate="2020-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The path delimiter for add_jars and add_classpaths in Python StreamExecutionEnvironment should be ;</summary>
      <description>Currently, the path delimiter for add_jars and add_classpaths in Python StreamExecutionEnvironment is ",", this would cause the rest client fail to upload the specified jars and stuck forever without errors. It should be ";" instead.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-23 01:00:00" id="20278" opendate="2020-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw a meaningful exception if the Python DataStream API job executes in batch mode</summary>
      <description>Currently, the Python DataStream job still doesn't support batch mode. We should thrown a meaningful exception if it runs in batch mode.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonProcessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonKeyedProcessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-23 01:00:00" id="20292" opendate="2020-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the document about table formats overlap in user fat jar</summary>
      <description>When testing the Flink 1.12 in a standalone mode cluster, I found that if the user job jar contains both flink-avro and flink-parquet/flink-orc, the FileSystemTableSink would not be able to load the corresponding format factory correctly. But if only one format is dependent it works.The test project located in here and the test class is FileCompactionTest.The conflict does not seem to affect the local runner, but only has problem when submitted to the standalone cluster.If the problem does exists, we might need to fix it or give user some tips about the conflicts. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-23 01:00:00" id="20300" opendate="2020-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Flink 1.12 release notes</summary>
      <description/>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-23 01:00:00" id="20304" opendate="2020-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fail hard when trying to run job with broadcast state in BATCH execution mode</summary>
      <description>Contrary to the documentation it is currently possible to run a job with broadcast state in BATCH execution mode. Since accessing the keyed state from the broadcast side fails, we shouldn't allow the submissions of these kind of jobs in the first place. Hence, I would suggest to fail hard if one tries to run a job using the broadcast state pattern in BATCH execution mode.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.TwoInputTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.BroadcastStateTransformationTranslator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.BroadcastStateITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.AbstractOneInputTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.BroadcastConnectedStream.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-23 01:00:00" id="20307" opendate="2020-11-23 00:00:00" resolution="Invalid">
    <buginformation>
      <summary>Improve the documentation about the temporal table join syntax</summary>
      <description>A query like:SELECT p.name, p.qty * r.rate AS price, p.`tstamp` FROM Products p JOIN versioned_rates r FOR SYSTEM_TIME AS OF p.`tstamp` ON p.currency = r.currency;fails with:[ERROR] Could not execute SQL statement. Reason:org.apache.flink.sql.parser.impl.ParseException: Encountered "FOR" at line 1, column 108.Was expecting one of: &lt;EOF&gt; "EXCEPT" ... "FETCH" ... "GROUP" ... "HAVING" ... "INTERSECT" ... "LIMIT" ... "OFFSET" ... "ON" ... "ORDER" ... "MINUS" ... "TABLESAMPLE" ... "UNION" ... "USING" ... "WHERE" ... "WINDOW" ... "(" ... "NATURAL" ... "JOIN" ... "INNER" ... "LEFT" ... "RIGHT" ... "FULL" ... "CROSS" ... "," ... "OUTER" ...When I do not alias the versioned_rates table everything works as expected. Therefore query like just runs:SELECT p.name, p.qty * versioned_rates.rate AS price, p.`tstamp` FROM Products p JOIN versioned_rates FOR SYSTEM_TIME AS OF p.`tstamp` ON p.currency = versioned_rates.currency;</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-24 01:00:00" id="20310" opendate="2020-11-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Debezium, Canal, Raw support for Filesystem connector</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.zh.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-24 01:00:00" id="20314" opendate="2020-11-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Empty Calc is not removed by CalcRemoveRule</summary>
      <description>My DDL：create table if not exists t_order(id int PRIMARY KEY comment '订单id',timestamps bigint comment '订单创建时间',orderInformationId string comment '订单信息ID',userId string comment '用户ID',categoryId int comment '商品类别',productId int comment '商品ID',price decimal(10,2) comment '单价',productCount int comment '购买数量',priceSum decimal(10,2) comment '订单总价',shipAddress string comment '商家地址',receiverAddress string comment '收货地址',ts AS TO_TIMESTAMP(FROM_UNIXTIME(timestamps/1000)),WATERMARK FOR ts AS ts - INTERVAL '3' SECOND)with('connector' = 'kafka','format' = 'debezium-avro-confluent', 'debezium-avro-confluent.schema-registry.url' = 'http://hostname:8081','topic' = 'ods.userAnalysis.order','properties.bootstrap.servers' = 'hostname:9092','properties.group.id' = 'flink-analysis','scan.startup.mode' = 'latest-offset') query is ok  when using the following SQLs：select * from t_orderselect receiverAddress from t_orderselectid,timestamps,orderInformationId,userId,categoryId,productId,price,productCount,priceSum,shipAddressfrom t_orderbut when I add the receiveraddress field to the third sql like:selectid,timestamps,orderInformationId,userId,categoryId,productId,price,productCount,priceSum,shipAddress,receiverAddressfrom t_orderit throws an exception：Exception in thread "main" org.apache.flink.table.api.TableException: This calc has no useful projection and no filter. It should be removed by CalcRemoveRule.Exception in thread "main" org.apache.flink.table.api.TableException: This calc has no useful projection and no filter. It should be removed by CalcRemoveRule. at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateProcessCode(CalcCodeGenerator.scala:166) at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateCalcOperator(CalcCodeGenerator.scala:59) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:84) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:39) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan(ExecNode.scala:59) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan$(ExecNode.scala:57) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.translateToPlan(StreamExecCalcBase.scala:38) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToTransformation(StreamExecLegacySink.scala:158) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:82) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:48) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan(ExecNode.scala:59) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan$(ExecNode.scala:57) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlan(StreamExecLegacySink.scala:48) at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:66) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) at scala.collection.Iterator.foreach(Iterator.scala:937) at scala.collection.Iterator.foreach$(Iterator.scala:937) at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) at scala.collection.IterableLike.foreach(IterableLike.scala:70) at scala.collection.IterableLike.foreach$(IterableLike.scala:69) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike.map(TraversableLike.scala:233) at scala.collection.TraversableLike.map$(TraversableLike.scala:226) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:65) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:167) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1261) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:702) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:1065) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:664) at com.bugboy.analysis.AnalysisCase$.main(AnalysisCase.scala:161) at com.bugboy.analysis.AnalysisCase.main(AnalysisCase.scala)    </description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SourceWatermarkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-24 01:00:00" id="20316" opendate="2020-11-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update the deduplication section of query page</summary>
      <description>We have supported deduplication in row time and deduplicate in mini-batch mode, but the document did not update, we need to update the doc.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-24 01:00:00" id="20317" opendate="2020-11-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Format Overview page to mention the supported connector for upsert-kafka</summary>
      <description>Currently, the Format Overview page only mentions and links "Apache Kafka". We should update the table for "upsert-kafka" connector.https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-25 01:00:00" id="20333" opendate="2020-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink standalone cluster throws metaspace OOM after submitting multiple PyFlink UDF jobs.</summary>
      <description>Currently the Flink standalone cluster will throw metaspace OOM after submitting multiple PyFlink UDF jobs. The root cause is that currently the PyFlink classes are running in user classloader and so each job creates a separate user class loader to load PyFlink related classes. There are many soft references and Finalizers in memory (introduced by the underlying Netty), which prevents the garbage collection of the user classloader of already finished PyFlink jobs. Due to their existence, it needs multiple full gc to reclaim the classloader of the completed job. If only one full gc is performed before the metaspace space is insufficient, then OOM will occur. </description>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-18 01:00:00" id="2034" opendate="2015-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add vision and roadmap for ML library to docs</summary>
      <description>We should have a document describing the vision of the Machine Learning library in Flink and an up to date roadmap.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.libs.ml.optimization.md</file>
      <file type="M">docs.libs.ml.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-25 01:00:00" id="20342" opendate="2020-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revisit page structure in documentation</summary>
      <description>Clean up page structure</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.debugging.classloading.md</file>
      <file type="M">docs.monitoring.README.md</file>
      <file type="M">docs.monitoring.logging.zh.md</file>
      <file type="M">docs.monitoring.logging.md</file>
      <file type="M">docs.monitoring.index.zh.md</file>
      <file type="M">docs.monitoring.index.md</file>
      <file type="M">docs.ops.state.savepoints.zh.md</file>
      <file type="M">docs.ops.state.savepoints.md</file>
      <file type="M">docs.ops.state.checkpoints.zh.md</file>
      <file type="M">docs.ops.state.checkpoints.md</file>
      <file type="M">docs.dev.python.table.api.tutorial.zh.md</file>
      <file type="M">docs.dev.python.table.api.tutorial.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.md</file>
      <file type="M">docs.dev.python.faq.zh.md</file>
      <file type="M">docs.dev.python.faq.md</file>
      <file type="M">docs.dev.python.datastream.tutorial.zh.md</file>
      <file type="M">docs.dev.python.datastream.tutorial.md</file>
      <file type="M">docs.dev.python.datastream-api-users-guide.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.datastream-api-users-guide.dependency.management.md</file>
      <file type="M">docs.dev.packaging.zh.md</file>
      <file type="M">docs.dev.packaging.md</file>
      <file type="M">docs.dev.local.execution.zh.md</file>
      <file type="M">docs.dev.local.execution.md</file>
      <file type="M">docs.dev.datastream.api.zh.md</file>
      <file type="M">docs.dev.datastream.api.md</file>
      <file type="M">docs.dev.cluster.execution.zh.md</file>
      <file type="M">docs.dev.cluster.execution.md</file>
      <file type="M">docs.ops.security.security-ssl.zh.md</file>
      <file type="M">docs.ops.security.security-ssl.md</file>
      <file type="M">docs.ops.security.security-kerberos.zh.md</file>
      <file type="M">docs.ops.security.security-kerberos.md</file>
      <file type="M">docs.ops.security.index.zh.md</file>
      <file type="M">docs.ops.security.index.md</file>
      <file type="M">docs.ops.resource-providers.yarn.setup.zh.md</file>
      <file type="M">docs.ops.resource-providers.yarn.setup.md</file>
      <file type="M">docs.ops.resource-providers.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.ops.resource-providers.mesos.zh.md</file>
      <file type="M">docs.ops.resource-providers.mesos.md</file>
      <file type="M">docs.ops.resource-providers.local.zh.md</file>
      <file type="M">docs.ops.resource-providers.local.md</file>
      <file type="M">docs.ops.resource-providers.kubernetes.zh.md</file>
      <file type="M">docs.ops.resource-providers.kubernetes.md</file>
      <file type="M">docs.ops.resource-providers.index.zh.md</file>
      <file type="M">docs.ops.resource-providers.index.md</file>
      <file type="M">docs.ops.resource-providers.hadoop.zh.md</file>
      <file type="M">docs.ops.resource-providers.hadoop.md</file>
      <file type="M">docs.ops.resource-providers.docker.zh.md</file>
      <file type="M">docs.ops.resource-providers.docker.md</file>
      <file type="M">docs.ops.resource-providers.cluster.setup.zh.md</file>
      <file type="M">docs.ops.resource-providers.cluster.setup.md</file>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
      <file type="M">docs.ops.filesystems.s3.md</file>
      <file type="M">docs.ops.filesystems.oss.zh.md</file>
      <file type="M">docs.ops.filesystems.oss.md</file>
      <file type="M">docs.ops.filesystems.common.zh.md</file>
      <file type="M">docs.ops.filesystems.common.md</file>
      <file type="M">docs.ops.filesystems.azure.zh.md</file>
      <file type="M">docs.ops.filesystems.azure.md</file>
      <file type="M">docs.ops.external.resources.zh.md</file>
      <file type="M">docs.ops.external.resources.md</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.zh.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.md</file>
      <file type="M">docs.dev.connectors.streamfile.sink.zh.md</file>
      <file type="M">docs.dev.connectors.streamfile.sink.md</file>
      <file type="M">docs.dev.connectors.file.sink.zh.md</file>
      <file type="M">docs.dev.connectors.file.sink.md</file>
      <file type="M">docs.dev.batch.connectors.zh.md</file>
      <file type="M">docs.dev.batch.connectors.md</file>
      <file type="M">docs.ops.scala.shell.zh.md</file>
      <file type="M">docs.ops.scala.shell.md</file>
      <file type="M">docs.release-notes.flink-1.9.zh.md</file>
      <file type="M">docs.release-notes.flink-1.9.md</file>
      <file type="M">docs.redirects.metrics.md</file>
      <file type="M">docs.ops.monitoring.index.zh.md</file>
      <file type="M">docs.ops.monitoring.index.md</file>
      <file type="M">docs.ops.debugging.index.zh.md</file>
      <file type="M">docs.ops.debugging.index.md</file>
      <file type="M">docs.ops.debugging.debugging.event.time.zh.md</file>
      <file type="M">docs.ops.debugging.debugging.event.time.md</file>
      <file type="M">docs.deployment.metric.reporters.zh.md</file>
      <file type="M">docs.deployment.metric.reporters.md</file>
      <file type="M">docs.redirects.mapr.md</file>
      <file type="M">docs.redirects.gce.setup.md</file>
      <file type="M">docs.redirects.aws.md</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.flinkDev.building.zh.md</file>
      <file type="M">docs.flinkDev.building.md</file>
      <file type="M">docs.dev.table.connectors.hive.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.hive.index.md</file>
      <file type="M">docs.dev.project-configuration.zh.md</file>
      <file type="M">docs.dev.project-configuration.md</file>
      <file type="M">docs.dev.batch.hadoop.compatibility.zh.md</file>
      <file type="M">docs.dev.batch.hadoop.compatibility.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.zh.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.mesos.zh.md</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
      <file type="M">docs.ops.deployment.local.zh.md</file>
      <file type="M">docs.ops.deployment.local.md</file>
      <file type="M">docs.ops.deployment.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
      <file type="M">docs.ops.deployment.index.zh.md</file>
      <file type="M">docs.ops.deployment.index.md</file>
      <file type="M">docs.ops.deployment.hadoop.zh.md</file>
      <file type="M">docs.ops.deployment.hadoop.md</file>
      <file type="M">docs.ops.deployment.docker.zh.md</file>
      <file type="M">docs.ops.deployment.docker.md</file>
      <file type="M">docs.ops.deployment.cluster.setup.zh.md</file>
      <file type="M">docs.ops.deployment.cluster.setup.md</file>
      <file type="M">docs.deployment.logging.zh.md</file>
      <file type="M">docs.deployment.logging.md</file>
      <file type="M">docs.deployment.historyserver.zh.md</file>
      <file type="M">docs.deployment.historyserver.md</file>
      <file type="M">docs.redirects.scala.shell.md</file>
      <file type="M">docs.redirects.python.shell.md</file>
      <file type="M">docs.redirects.oss.md</file>
      <file type="M">docs.redirects.filesystems.md</file>
      <file type="M">docs.redirects.cli.md</file>
      <file type="M">docs.ops.monitoring.rest.api.zh.md</file>
      <file type="M">docs.ops.monitoring.rest.api.md</file>
      <file type="M">docs.ops.metrics.zh.md</file>
      <file type="M">docs.ops.metrics.md</file>
      <file type="M">docs.monitoring.rest.api.zh.md</file>
      <file type="M">docs.monitoring.rest.api.md</file>
      <file type="M">docs.monitoring.checkpoint.monitoring.zh.md</file>
      <file type="M">docs.monitoring.checkpoint.monitoring.md</file>
      <file type="M">docs.monitoring.back.pressure.zh.md</file>
      <file type="M">docs.monitoring.back.pressure.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.metrics.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.metrics.md</file>
      <file type="M">docs.deployment.security.index.zh.md</file>
      <file type="M">docs.deployment.security.index.md</file>
      <file type="M">docs.deployment.filesystems.plugins.zh.md</file>
      <file type="M">docs.deployment.filesystems.plugins.md</file>
      <file type="M">docs.deployment.plugins.zh.md</file>
      <file type="M">docs.deployment.plugins.md</file>
      <file type="M">docs.deployment.filesystems.index.zh.md</file>
      <file type="M">docs.deployment.filesystems.index.md</file>
      <file type="M">docs.deployment.external.resources.zh.md</file>
      <file type="M">docs.deployment.external.resources.md</file>
      <file type="M">docs.ops.security-ssl.zh.md</file>
      <file type="M">docs.ops.security-ssl.md</file>
      <file type="M">docs.ops.security-kerberos.zh.md</file>
      <file type="M">docs.ops.security-kerberos.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.zh.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.md</file>
      <file type="M">docs.ops.plugins.zh.md</file>
      <file type="M">docs.ops.plugins.md</file>
      <file type="M">docs.ops.filesystems.index.zh.md</file>
      <file type="M">docs.ops.filesystems.index.md</file>
      <file type="M">docs.redirects.windows.local.setup.md</file>
      <file type="M">docs.redirects.setup.quickstart.md</file>
      <file type="M">docs.redirects.local.setup.tutorial.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.deployment.resource-providers.local.zh.md</file>
      <file type="M">docs.deployment.resource-providers.local.md</file>
      <file type="M">docs.deployment.resource-providers.kubernetes.zh.md</file>
      <file type="M">docs.deployment.resource-providers.kubernetes.md</file>
      <file type="M">docs.deployment.repls.python.shell.zh.md</file>
      <file type="M">docs.deployment.repls.python.shell.md</file>
      <file type="M">docs.deployment.index.zh.md</file>
      <file type="M">docs.deployment.index.md</file>
      <file type="M">docs.concepts.flink-architecture.zh.md</file>
      <file type="M">docs.concepts.flink-architecture.md</file>
      <file type="M">docs.monitoring.debugging.event.time.zh.md</file>
      <file type="M">docs.monitoring.debugging.event.time.md</file>
      <file type="M">docs.monitoring.application.profiling.zh.md</file>
      <file type="M">docs.monitoring.application.profiling.md</file>
      <file type="M">docs.ops.state.index.zh.md</file>
      <file type="M">docs.ops.state.index.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.index.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.index.md</file>
      <file type="M">docs.release-notes.flink-1.10.zh.md</file>
      <file type="M">docs.release-notes.flink-1.10.md</file>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.memory.mem.tuning.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.md</file>
      <file type="M">docs.ops.memory.mem.setup.jobmanager.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.jobmanager.md</file>
      <file type="M">docs.ops.memory.mem.setup.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.md</file>
      <file type="M">docs.ops.memory.index.zh.md</file>
      <file type="M">docs.ops.memory.index.md</file>
      <file type="M">docs.ops.upgrading.zh.md</file>
      <file type="M">docs.ops.upgrading.md</file>
      <file type="M">docs.ops.production.ready.zh.md</file>
      <file type="M">docs.ops.production.ready.md</file>
      <file type="M">docs.try-flink.flink-operations-playground.zh.md</file>
      <file type="M">docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.redirects.rest.api.md</file>
      <file type="M">docs.redirects.back.pressure.md</file>
      <file type="M">docs.ops.state.large.state.tuning.zh.md</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">docs.monitoring.monitoring.rest.api.zh.md</file>
      <file type="M">docs.monitoring.monitoring.rest.api.md</file>
      <file type="M">docs.monitoring.monitoring.index.zh.md</file>
      <file type="M">docs.monitoring.monitoring.index.md</file>
      <file type="M">docs.monitoring.monitoring.checkpoint.monitoring.zh.md</file>
      <file type="M">docs.monitoring.monitoring.checkpoint.monitoring.md</file>
      <file type="M">docs.monitoring.monitoring.back.pressure.zh.md</file>
      <file type="M">docs.monitoring.monitoring.back.pressure.md</file>
      <file type="M">docs.monitoring.debugging.index.zh.md</file>
      <file type="M">docs.monitoring.debugging.index.md</file>
      <file type="M">docs.monitoring.debugging.debugging.event.time.zh.md</file>
      <file type="M">docs.monitoring.debugging.debugging.event.time.md</file>
      <file type="M">docs.monitoring.debugging.debugging.classloading.zh.md</file>
      <file type="M">docs.monitoring.debugging.debugging.classloading.md</file>
      <file type="M">docs.monitoring.debugging.application.profiling.zh.md</file>
      <file type="M">docs.monitoring.debugging.application.profiling.md</file>
      <file type="M">docs.learn-flink.fault.tolerance.zh.md</file>
      <file type="M">docs.learn-flink.fault.tolerance.md</file>
      <file type="M">docs.dev.event.time.zh.md</file>
      <file type="M">docs.dev.event.time.md</file>
      <file type="M">docs.deployment.memory.index.zh.md</file>
      <file type="M">docs.deployment.memory.index.md</file>
      <file type="M">docs.deployment.config.zh.md</file>
      <file type="M">docs.deployment.config.md</file>
      <file type="M">docs.release-notes.flink-1.7.zh.md</file>
      <file type="M">docs.release-notes.flink-1.7.md</file>
      <file type="M">docs.release-notes.flink-1.5.zh.md</file>
      <file type="M">docs.release-notes.flink-1.5.md</file>
      <file type="M">docs.release-notes.flink-1.11.zh.md</file>
      <file type="M">docs.release-notes.flink-1.11.md</file>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">docs.monitoring.historyserver.zh.md</file>
      <file type="M">docs.monitoring.historyserver.md</file>
      <file type="M">docs.monitoring.debugging.classloading.zh.md</file>
      <file type="M">docs.ops.index.md</file>
      <file type="M">docs.ops.index.zh.md</file>
      <file type="M">docs.deployment.cli.md</file>
      <file type="M">docs.deployment.cli.zh.md</file>
      <file type="M">docs.deployment.jobmanager.high.availability.md</file>
      <file type="M">docs.deployment.jobmanager.high.availability.zh.md</file>
      <file type="M">docs.deployment.memory.mem.migration.md</file>
      <file type="M">docs.deployment.memory.mem.migration.zh.md</file>
      <file type="M">docs.deployment.memory.mem.setup.md</file>
      <file type="M">docs.deployment.memory.mem.setup.zh.md</file>
      <file type="M">docs.deployment.memory.mem.setup.jobmanager.md</file>
      <file type="M">docs.deployment.memory.mem.setup.jobmanager.zh.md</file>
      <file type="M">docs.deployment.memory.mem.setup.tm.md</file>
      <file type="M">docs.deployment.memory.mem.setup.tm.zh.md</file>
      <file type="M">docs.deployment.memory.mem.trouble.md</file>
      <file type="M">docs.deployment.memory.mem.trouble.zh.md</file>
      <file type="M">docs.deployment.memory.mem.tuning.md</file>
      <file type="M">docs.deployment.memory.mem.tuning.zh.md</file>
      <file type="M">docs.deployment.resource-providers.cluster.setup.md</file>
      <file type="M">docs.deployment.resource-providers.cluster.setup.zh.md</file>
      <file type="M">docs.deployment.resource-providers.docker.md</file>
      <file type="M">docs.deployment.resource-providers.docker.zh.md</file>
      <file type="M">docs.deployment.resource-providers.mesos.md</file>
      <file type="M">docs.deployment.resource-providers.mesos.zh.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.zh.md</file>
      <file type="M">docs.deployment.resource-providers.yarn.setup.md</file>
      <file type="M">docs.deployment.resource-providers.yarn.setup.zh.md</file>
      <file type="M">docs.deployment.security.security-kerberos.md</file>
      <file type="M">docs.deployment.security.security-kerberos.zh.md</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
      <file type="M">docs.dev.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.execution.configuration.md</file>
      <file type="M">docs.dev.execution.configuration.zh.md</file>
      <file type="M">docs.dev.parallel.md</file>
      <file type="M">docs.dev.parallel.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.zh.md</file>
      <file type="M">docs.dev.stream.operators.index.md</file>
      <file type="M">docs.dev.stream.operators.index.zh.md</file>
      <file type="M">docs.dev.stream.state.checkpointing.md</file>
      <file type="M">docs.dev.stream.state.checkpointing.zh.md</file>
      <file type="M">docs.dev.stream.state.queryable.state.md</file>
      <file type="M">docs.dev.stream.state.queryable.state.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.index.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-25 01:00:00" id="20343" opendate="2020-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add overview / reference architecture documentation page</summary>
      <description>To properly guide users, we should add some generic overview of the deployment concepts.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.deployment.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-25 01:00:00" id="20349" opendate="2020-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query fails with "A conflict is detected. This is unexpected."</summary>
      <description>The test case to reproduce: @Test public void test() throws Exception { tableEnv.executeSql("create table src(key string,val string)"); tableEnv.executeSql("SELECT sum(char_length(src5.src1_value)) FROM " + "(SELECT src3.*, src4.val as src4_value, src4.key as src4_key FROM src src4 JOIN " + "(SELECT src2.*, src1.key as src1_key, src1.val as src1_value FROM src src1 JOIN src src2 ON src1.key = src2.key) src3 " + "ON src3.src1_key = src4.key) src5").collect(); }</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.MultipleInputITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.processors.utils.InputPriorityGraphGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.processors.utils.InputPriorityGraphGenerator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.processors.utils.InputPriorityConflictResolver.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.processors.utils.InputOrderCalculator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-25 01:00:00" id="20352" opendate="2020-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework command line interface documentation page</summary>
      <description>The command line interface documentation page is quite out-dated and not very easy to read. A large part is simply the help message from the CLI which is wall of text. Ideally, we can loosen the page a bit up and update the examples.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.deployment.cli.zh.md</file>
      <file type="M">docs.deployment.cli.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-25 01:00:00" id="20354" opendate="2020-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework standalone deployment documentation page</summary>
      <description>Similar to FLINK-20347 we need to update the standalone deployment documentation page. Additionally, we need to verify that everything we state on the documentation works.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.redirects.windows.local.setup.zh.md</file>
      <file type="M">docs.redirects.windows.local.setup.md</file>
      <file type="M">docs.redirects.windows.zh.md</file>
      <file type="M">docs.redirects.windows.md</file>
      <file type="M">docs.redirects.tutorials.local.setup.zh.md</file>
      <file type="M">docs.redirects.tutorials.local.setup.md</file>
      <file type="M">docs.redirects.tutorials.flink.on.windows.zh.md</file>
      <file type="M">docs.redirects.tutorials.flink.on.windows.md</file>
      <file type="M">docs.redirects.setup.quickstart.zh.md</file>
      <file type="M">docs.redirects.setup.quickstart.md</file>
      <file type="M">docs.redirects.local.setup.tutorial.zh.md</file>
      <file type="M">docs.redirects.local.setup.tutorial.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.local.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.local.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.kubernetes.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.index.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.docker.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.zh.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.deployment.repls.python.shell.zh.md</file>
      <file type="M">docs.deployment.repls.python.shell.md</file>
      <file type="M">docs.deployment.cli.zh.md</file>
      <file type="M">docs.deployment.cli.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-25 01:00:00" id="20355" opendate="2020-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework K8s deployment documentation page</summary>
      <description>Similar to FLINK-20347, we need to update the K8s deployment documentation page. Additionally, we should ensure that everything works which is stated in the documentation.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.deployment.resource-providers.standalone.docker.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.zh.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-25 01:00:00" id="20356" opendate="2020-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework Mesos deployment documentation page</summary>
      <description>Similar to FLINK-20347, we need to rework the Mesos deployment documentation page. Additionally, we should validate that everything which is stated in the documentation actually works.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.deployment.resource-providers.mesos.zh.md</file>
      <file type="M">docs.deployment.resource-providers.mesos.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-25 01:00:00" id="20357" opendate="2020-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework HA documentation page</summary>
      <description>We need to rework the HA documentation page. The first step is to split the existing documentation into general concepts as an overview page and HA service implementation specific sub pages. For the implementation specific sub pages we need to add Zookeeper and the K8s HA services.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.deployment.resource-providers.standalone.kubernetes.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.index.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.index.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.zh.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.deployment.ha.zookeeper.ha.zh.md</file>
      <file type="M">docs.deployment.ha.zookeeper.ha.md</file>
      <file type="M">docs.deployment.ha.kubernetes.ha.zh.md</file>
      <file type="M">docs.deployment.ha.kubernetes.ha.md</file>
      <file type="M">docs.deployment.ha.index.zh.md</file>
      <file type="M">docs.deployment.ha.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-26 01:00:00" id="20371" opendate="2020-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add docs for outer interval join</summary>
      <description>By looking at the docs, it looks like we only support inner interval joins but we also support outer joins according to the tests.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-27 01:00:00" id="20387" opendate="2020-11-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support defining event time attribute on TIMESTAMP_LTZ column</summary>
      <description>Currently, only TIMESTAMP type can be used as rowtime attribute. Would be better to support TIMESTAMP WITH LOCAL ZONE TIME as well.As a workaround, users can cast the TIMESTAMP WITH LOCAL ZONE TIME into TIMESTAMP, CAST(ts AS TIMESTAMP).</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.WindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.TimeWindowUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.TypeCheckUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.PlannerTypeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.WindowOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.WindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.triggers.Trigger.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.triggers.ProcessingTimeTriggers.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.triggers.EventTimeTriggers.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.slicing.SlicingWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.slicing.SliceAssigners.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.MergingWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.InternalWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.combines.WindowCombineFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.window.WindowRankOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.window.processors.WindowRankProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.window.combines.TopNRecordsCombiner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.processors.SliceSharedWindowAggProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.processors.AbstractWindowAggProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.LocalSlicingWindowAggOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.combines.GlobalAggAccCombiner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.combines.AggRecordsCombiner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.buffers.WindowBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.buffers.RecordsWindowBuffer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.DateTimeTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowRankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowDistinctAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TimeAttributeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SourceWatermarkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.WindowAggregateUseDaylightTimeHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.WindowAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.TemporalTableJoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.UnionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.schema.TimeIndicatorRelDataTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TemporalTableFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowTableFunctionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowRankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ExpandWindowTableFunctionTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PassThroughPythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.DefaultSchemaResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.SchemaResolutionTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.Schema.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeChecks.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.api.TableSchemaTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.CatalogSchemaTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.PlannerRowtimeAttribute.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLegacySink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanAcrossCalcRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.WatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalGroupWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalIntervalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TimeIndicatorRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.GroupWindowAggregateJsonITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.TemporalSortJsonITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.WindowAggregateJsonITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.DeduplicationJsonPlanTest.jsonplan.testDeduplication.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testProcessingTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortProcessingTime.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-27 01:00:00" id="20391" opendate="2020-11-27 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Set FORWARD_EDGES_PIPELINED for BATCH ExecutionMode</summary>
      <description>It would be better to treat the rescale operation similar to keyBy or rebalance and make it a possible pipeline border.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.api.datastream.DataStreamBatchExecutionITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorExecutionModeDetectionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-12-30 01:00:00" id="20419" opendate="2020-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert fails due to failure to generate execution plan</summary>
      <description>Test case to reproduce: @Test public void test() throws Exception { tableEnv.executeSql("create table src(x int)"); tableEnv.executeSql("create table dest(x int) partitioned by (p string,q string)"); tableEnv.executeSql("insert into dest select x,'0','0' from src order by x").await(); }</description>
      <version>None</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSinkRule.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-30 01:00:00" id="20423" opendate="2020-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove usage of {{site.baseurl}} from markdown files</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.execution.configuration.zh.md</file>
      <file type="M">docs.dev.execution.configuration.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.sql.gettingStarted.zh.md</file>
      <file type="M">docs.dev.table.sql.gettingStarted.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-3 01:00:00" id="20467" opendate="2020-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the Example in Python DataStream Doc</summary>
      <description>Currently the example of MapFunction can't work. We need to fix it.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.datastream-api-users-guide.operators.zh.md</file>
      <file type="M">docs.dev.python.datastream-api-users-guide.operators.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-5-4 01:00:00" id="20487" opendate="2020-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to consume retractions for window aggregate operator</summary>
      <description>EXCEPTION: org.apache.flink.table.api.TableException: Group Window Aggregate: Retraction on windowed GroupBy Aggregate is not supported yet. org.apache.flink.table.api.TableException: Group Window Aggregate: Retraction on windowed GroupBy Aggregate is not supported yet. please re-check sql grammar. Note: Windowed GroupBy Aggregate should not follow anon-windowed GroupBy aggregation. at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregateBase.translateToPlanInternal(StreamExecGroupWindowAggregateBase.scala:138) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregateBase.translateToPlanInternal(StreamExecGroupWindowAggregateBase.scala:54) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan(ExecNode.scala:58) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan$(ExecNode.scala:56) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregateBase.translateToPlan(StreamExecGroupWindowAggregateBase.scala:54) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:54) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:39) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan(ExecNode.scala:58) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan$(ExecNode.scala:56) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.translateToPlan(StreamExecCalcBase.scala:38) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToTransformation(StreamExecSink.scala:184) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:91) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:48) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan(ExecNode.scala:58) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan$(ExecNode.scala:56) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:48) at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:60) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) at scala.collection.Iterator.foreach(Iterator.scala:937) at scala.collection.Iterator.foreach$(Iterator.scala:937) at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) at scala.collection.IterableLike.foreach(IterableLike.scala:70) at scala.collection.IterableLike.foreach$(IterableLike.scala:69) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike.map(TraversableLike.scala:233) at scala.collection.TraversableLike.map$(TraversableLike.scala:226) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:59) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:153) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:682) at org.apache.flink.table.api.internal.TableEnvironmentImpl.insertIntoInternal(TableEnvironmentImpl.java:355) at org.apache.flink.table.api.internal.TableEnvironmentImpl.insertInto(TableEnvironmentImpl.java:334) CASE:SELECT DATE_FORMAT(tumble_end(ROWTIME ,interval '1' hour),'yyyy-MM-dd HH') as stat_time, count(crypto_customer_number) first_phone_numFROM ( SELECT ROWTIME, crypto_customer_number, row_number() over(partition by crypto_customer_number order by ROWTIME ) as rn FROM source_kafka_biz_shuidi_sdb_crm_call_record ) cal where rn =1group by tumble(ROWTIME,interval '1' hour);</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-5-19 01:00:00" id="2050" opendate="2015-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pipelining mechanism for chainable transformers and estimators</summary>
      <description>The key concept of an easy to use ML library is the quick and simple construction of data analysis pipelines. Scikit-learn's approach to define transformers and estimators seems to be a really good solution to this problem. I propose to follow a similar path, because it makes FlinkML flexible in terms of code reuse as well as easy for people coming from Scikit-learn to use the FlinkML.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.recommendation.ALS.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseVector.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery9ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery4ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery3WithUnionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery3ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery10ITCase.java</file>
      <file type="M">flink-tests.src.test.assembly.test-streamingclassloader-assembly.xml</file>
      <file type="M">flink-tests.src.test.assembly.test-kmeans-assembly.xml</file>
      <file type="M">flink-tests.src.test.assembly.test-custominput-assembly.xml</file>
      <file type="M">flink-staging.flink-tez.src.main.java.org.apache.flink.tez.examples.TPCHQuery3.java</file>
      <file type="M">flink-staging.flink-tachyon.src.test.java.org.apache.flink.tachyon.TachyonFileSystemWrapperTest.java</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.runtime.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.plan.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.expressions.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.scala.table.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.java.org.apache.flink.api.table.package-info.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.pom.xml</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.MultipleLinearRegressionITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.recommendation.ALSITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.preprocessing.StandardScalerITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.feature.PolynomialBaseITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.experimental.SciKitPipelineSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.classification.CoCoASuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">docs.libs.ml.als.md</file>
      <file type="M">docs.libs.ml.cocoa.md</file>
      <file type="M">docs.libs.ml.multiple.linear.regression.md</file>
      <file type="M">docs.libs.ml.polynomial.base.feature.mapper.md</file>
      <file type="M">docs.libs.ml.standard.scaler.md</file>
      <file type="M">flink-clients.src.test.assembly.test-assembly.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigurationTest.java</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-examples.flink-java-examples.pom.xml</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.relational.TPCHQuery3.java</file>
      <file type="M">flink-examples.flink-scala-examples.pom.xml</file>
      <file type="M">flink-java8.pom.xml</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.AvgAggregationFunction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.TupleGenerator.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.traversals.package-info.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.Job.java</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.Job.scala</file>
      <file type="M">flink-quickstart.flink-tez-quickstart.src.main.resources.archetype-resources.src.main.java.LocalJob.java</file>
      <file type="M">flink-quickstart.flink-tez-quickstart.src.main.resources.archetype-resources.src.main.java.YarnJob.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.checkpoint.package-info.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.package-info.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.JarFileCreator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.JarFileCreatorTest.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-staging.flink-avro.src.test.assembly.test-assembly.xml</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.HadoopFileOutputCommitter.java</file>
      <file type="M">flink-staging.flink-language-binding.flink-python.src.main.java.org.apache.flink.languagebinding.api.java.python.PythonPlanBinder.java</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.classification.CoCoA.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedLearner.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedTransformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Learner.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.ChainedPredictor.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.ChainedTransformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Estimator.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.KMeans.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Offset.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Predictor.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Scaler.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.feature.PolynomialBase.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Breeze.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.CanCopy.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.package.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.preprocessing.StandardScaler.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-7 01:00:00" id="20508" opendate="2020-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce PythonStreamGroupTableAggregateOperator</summary>
      <description>Adds PythonStreamGroupTableAggregateOperator to support running Python TableAggregateFunction</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-7 01:00:00" id="20521" opendate="2020-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Null result values are being swallowed by RPC system</summary>
      <description>If an RPC method returns a null value, then it seems that the request future won't get completed as reported in FLINK-17921.We should either not allow to return null values as responses or make sure that a null value is properly transmitted to the caller.</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-20 01:00:00" id="2053" opendate="2015-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Preregister ML types for Kryo serialization</summary>
      <description>Currently, FlinkML uses interfaces and abstract types to implement generic algorithms. As a consequence we have to use Kryo to serialize the effective subtypes. In order to speed the data transfer up, it's necessary to preregister these types in order to assign them fixed IDs.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.pipeline.PipelineITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.feature.PolynomialBaseITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.recommendation.ALS.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.preprocessing.StandardScaler.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.pipeline.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.pipeline.Predictor.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.pipeline.Estimator.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.BreezeVectorConverter.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Breeze.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.FlinkTools.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.classification.CoCoA.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.PojoSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-9 01:00:00" id="20543" opendate="2020-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix typo in upsert kafka docs</summary>
      <description/>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.upsert-kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.upsert-kafka.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-9 01:00:00" id="20550" opendate="2020-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong savepoint config in some docs</summary>
      <description>Fix config 'state.savepoint.dir' into 'state.savepoints.dir' in docs</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.flink-operations-playground.zh.md</file>
      <file type="M">docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.ops.state.savepoints.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-10 01:00:00" id="20562" opendate="2020-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support ExplainDetails for EXPLAIN sytnax</summary>
      <description>Currently, EXPLAIN syntax only supports to print the default AST, logical plan, and physical plan. However, it doesn't support to print detailed information such as CHANGELOG_MODE, ESTIMATED_COST, JSON_EXECUTION_PLAN which are defined in ExplainDetail.Allow users to specify the ExplainDetails in statement.EXPLAIN [ExplainDetail[, ExplainDetail]*] &lt;statement&gt;ExplainDetail: { ESTIMATED_COST, CHANGELOG_MODE, JSON_EXECUTION_PLAN}Print the plan for the statement with specified ExplainDetails. ESTIMATED_COSTgenerates cost information on physical node estimated by optimizer, e.g. TableSourceScan(..., cumulative cost = {1.0E8 rows, 1.0E8 cpu, 2.4E9 io, 0.0 network, 0.0 memory})CHANGELOG_MODEgenerates changelog mode for every physical rel node. e.g. GroupAggregate(..., changelogMode=&amp;#91;I,UA,D&amp;#93;)JSON_EXECUTION_PLANgenerates the execution plan in json format of the program.Flink SQL&gt; EXPLAIN ESTIMATED_COST, CHANGELOG SELECT * FROM MyTable;...</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ExplainOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.resources.org.apache.flink.sql.parser.utils.ParserResource.properties</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.utils.ParserResource.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dql.SqlRichExplain.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-10 01:00:00" id="20563" opendate="2020-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support built-in functions for Hive versions prior to 1.2.0</summary>
      <description>Currently Hive built-in functions are supported only for Hive-1.2.0 and later. We should investigate how to lift this limitation.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV120.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-12 01:00:00" id="20582" opendate="2020-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix typos in `CREATE Statements` docs</summary>
      <description/>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-16 01:00:00" id="20615" opendate="2020-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local recovery and sticky scheduling end-to-end test timeout with "IOException: Stream Closed"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10905&amp;view=logs&amp;j=6caf31d6-847a-526e-9624-468e053467d6&amp;t=0b23652f-b18b-5b6e-6eb6-a11070364610It tried to restart many times, and the final error was following:2020-12-15T23:54:00.5067862Z Dec 15 23:53:42 2020-12-15 23:53:41,538 ERROR org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Caught unexpected exception.2020-12-15T23:54:00.5068392Z Dec 15 23:53:42 java.io.IOException: Stream Closed2020-12-15T23:54:00.5068767Z Dec 15 23:53:42 at java.io.FileInputStream.readBytes(Native Method) ~[?:?]2020-12-15T23:54:00.5069223Z Dec 15 23:53:42 at java.io.FileInputStream.read(FileInputStream.java:279) ~[?:?]2020-12-15T23:54:00.5070150Z Dec 15 23:53:42 at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5071217Z Dec 15 23:53:42 at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:61) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5072295Z Dec 15 23:53:42 at org.apache.flink.runtime.util.ForwardingInputStream.read(ForwardingInputStream.java:51) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5072967Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:200) ~[?:?]2020-12-15T23:54:00.5073483Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:170) ~[?:?]2020-12-15T23:54:00.5074535Z Dec 15 23:53:42 at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5075847Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:222) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5077187Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:169) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5078495Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:152) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5079802Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:269) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5081013Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:565) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5082215Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:94) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5083500Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:299) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5084899Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5086342Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5087601Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:316) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5088924Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:155) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5090261Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:248) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5091459Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5092604Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:507) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5093748Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5094866Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5095912Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:531) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5096875Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5097814Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5098373Z Dec 15 23:53:42 at java.lang.Thread.run(Thread.java:834) [?:?]2020-12-15T23:54:00.5099549Z Dec 15 23:53:42 2020-12-15 23:53:41,557 WARN org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Exception while restoring keyed state backend for StreamFlatMap_20ba6b65f97481d5570070de90e4e791_(1/4) from alternative (1/1), will retry while more alternatives are available.2020-12-15T23:54:00.5100556Z Dec 15 23:53:42 org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.2020-12-15T23:54:00.5101480Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:328) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5102669Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:565) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5103763Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:94) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5104723Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:299) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5105700Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5106630Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5107587Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:316) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5108581Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:155) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5109505Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:248) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5110456Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5111316Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:507) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5112175Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5113012Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5113787Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:531) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5114521Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5115209Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5115635Z Dec 15 23:53:42 at java.lang.Thread.run(Thread.java:834) [?:?]2020-12-15T23:54:00.5115949Z Dec 15 23:53:42 Caused by: java.io.IOException: Stream Closed2020-12-15T23:54:00.5116246Z Dec 15 23:53:42 at java.io.FileInputStream.readBytes(Native Method) ~[?:?]2020-12-15T23:54:00.5116589Z Dec 15 23:53:42 at java.io.FileInputStream.read(FileInputStream.java:279) ~[?:?]2020-12-15T23:54:00.5117284Z Dec 15 23:53:42 at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5118080Z Dec 15 23:53:42 at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:61) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5118894Z Dec 15 23:53:42 at org.apache.flink.runtime.util.ForwardingInputStream.read(ForwardingInputStream.java:51) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5119392Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:200) ~[?:?]2020-12-15T23:54:00.5119808Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:170) ~[?:?]2020-12-15T23:54:00.5120605Z Dec 15 23:53:42 at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5121576Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:222) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5122579Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:169) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5123543Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:152) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5124476Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:269) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5124994Z Dec 15 23:53:42 ... 16 more </description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-12-17 01:00:00" id="20639" opendate="2020-12-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use list to optimize the Row used by Python UDAF intermediate results</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-17 01:00:00" id="20648" opendate="2020-12-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to restore job from savepoint when using Kubernetes based HA services</summary>
      <description>When restoring job from savepoint, we always end up with following error:Caused by: org.apache.flink.runtime.client.JobInitializationException: Could not instantiate JobManager. at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$5(Dispatcher.java:463) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1764) ~[?:?] ... 3 moreCaused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Stopped retrying the operation because the error is not retryable. at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) ~[?:?] at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2063) ~[?:?] at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStore.addAndLock(KubernetesStateHandleStore.java:150) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore.addCheckpoint(DefaultCompletedCheckpointStore.java:211) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1479) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.scheduler.SchedulerBase.tryRestoreExecutionGraphFromSavepoint(SchedulerBase.java:325) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:266) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.scheduler.SchedulerBase.&lt;init&gt;(SchedulerBase.java:238) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.scheduler.DefaultScheduler.&lt;init&gt;(DefaultScheduler.java:134) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:108) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:323) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.jobmaster.JobMaster.&lt;init&gt;(JobMaster.java:310) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:96) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:41) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.&lt;init&gt;(JobManagerRunnerImpl.java:141) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:80) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$5(Dispatcher.java:450) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1764) ~[?:?] ... 3 moreCaused by: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Stopped retrying the operation because the error is not retryable. at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperation$1(FutureUtils.java:166) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) ~[?:?] at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) ~[?:?] at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) ~[?:?] ... 3 moreCaused by: java.util.concurrent.CompletionException: org.apache.flink.kubernetes.kubeclient.resources.KubernetesException: Cannot retry checkAndUpdateConfigMap with configMap pipelines-runner-fulltext-6e99e672-4af29f0768624632839835717898b08d-jobmanager-leader because it does not exist. at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$6(Fabric8FlinkKubeClient.java:289) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at java.util.Optional.orElseThrow(Optional.java:401) ~[?:?] at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$7(Fabric8FlinkKubeClient.java:289) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1764) ~[?:?] ... 3 moreCaused by: org.apache.flink.kubernetes.kubeclient.resources.KubernetesException: Cannot retry checkAndUpdateConfigMap with configMap pipelines-runner-fulltext-6e99e672-4af29f0768624632839835717898b08d-jobmanager-leader because it does not exist. at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$6(Fabric8FlinkKubeClient.java:289) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at java.util.Optional.orElseThrow(Optional.java:401) ~[?:?] at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$7(Fabric8FlinkKubeClient.java:289) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1764) ~[?:?]Cause of the issue is following: We construct `jobMasterServices` prior starting `leaderElectionService` (in `JobManagerRunnerImpl`) During `jobMasterServices` initialization `tryRestoreExecutionGraphFromSavepoint` gets called. This calls `KubernetesStateHandleStore.addAndLock` interally. `KubernetesStateHandleStore.addAndLock` expects configmap for JM leadership to be already present, which is wrong, because `leaderElectionService` which is responsible for its creation has not started yetPossible fixes: Start `leaderElectionService` before `jobMasterServices` Fix `KubernetesStateHandleStore`, so it can handle the case, when leader hasn't been elected</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.kubernetes.itcases.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-17 01:00:00" id="20650" opendate="2020-12-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mark "native-k8s" as deprecated in docker-entrypoint.sh</summary>
      <description>When we are publishing the image 1.12 to docker hub, some docker guys raise up a issue for the docker-entrypoint.sh. They want the images to have a certain standard, because they are the official ones. However the proposed native-k8s command is more like an internal bridge. It is only used for native Kubernetes integration. Another suggestion is removing the "bash -c" wrapper and generate it in the flink codes. Refer here&amp;#91;1&amp;#93; for more information. Note: We mark the native-k8s as deprecated and export the environments for all pass-through mode commands, the flink Kubernetes codes should be adjusted accordingly. &amp;#91;1&amp;#93;. https://github.com/docker-library/official-images/pull/9249</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesTaskManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.JavaCmdTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.JavaCmdJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.JavaCmdTaskManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.JavaCmdJobManagerDecorator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-18 01:00:00" id="20666" opendate="2020-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the deserialized Row losing the field_name information in PyFlink</summary>
      <description>Now, the deserialized Row loses the field_name information.@udf(result_type=DataTypes.STRING())def get_string_element(my_list): my_string = 'xxx' for element in my_list: if element.integer_element == 2: # element lost the field_name information my_string = element.string_element return my_stringt = t_env.from_elements( [("1", [Row(3, "flink")]), ("3", [Row(2, "pyflink")]), ("2", [Row(2, "python")])], DataTypes.ROW( [DataTypes.FIELD("Key", DataTypes.STRING()), DataTypes.FIELD("List_element", DataTypes.ARRAY(DataTypes.ROW( [DataTypes.FIELD("integer_element", DataTypes.INT()), DataTypes.FIELD("string_element", DataTypes.STRING())])))]))print(t.select(get_string_element(t.List_element)).to_pandas())element lost the field_name information</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.11.4,1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.fast.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.common.types.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-21 01:00:00" id="2067" opendate="2015-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chained streaming operators should not throw chained exceptions</summary>
      <description>The exceptions that come from chained operators have an non-intuitive chaining structure, that makes the stack traces harder to understand.For every chained task, there is a "Failed to forward record" exception, before the actual exception comes as a cause.In the Batch API, we use a special "ExceptionInChainedStubException" that is recognized and un-nested to make chained operator exceptions surface as root exceptions. We should do the same for the streaming API.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-1-22 01:00:00" id="20717" opendate="2020-12-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create backPressuredTimeMsPerSecond metric</summary>
      <description>Create backPressuredTimeMsPerSecond metric, measured similarly to the existing idleTimeMsPerSecond.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxProcessorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.java</file>
      <file type="M">docs.ops.metrics.zh.md</file>
      <file type="M">docs.ops.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-22 01:00:00" id="20720" opendate="2020-12-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for ProcessFunction in Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.python.datastream.operators.overview.md</file>
      <file type="M">docs.content.docs.dev.datastream.operators.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.python.datastream.operators.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.overview.md</file>
      <file type="M">docs.content.docs.dev.datastream.operators.process.function.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-24 01:00:00" id="20756" opendate="2020-12-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PythonCalcSplitConditionRule is not working as expected</summary>
      <description>Currently if users write such a SQL:`SELECT pyFunc5(f0, f1) FROM (SELECT e.f0, e.f1 FROM (SELECT pyFunc5(a) as e FROM MyTable) where e.f0 is NULL)`It will be optimized to:`FlinkLogicalCalc(select=&amp;#91;pyFunc5(pyFunc5(a)) AS f0&amp;#93;)+- FlinkLogicalCalc(select=&amp;#91;a&amp;#93;, where=&amp;#91;IS NULL(pyFunc5(a).f0)&amp;#93;) +- FlinkLogicalLegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: &amp;#91;TestTableSource(a, b, c, d)&amp;#93;]], fields=&amp;#91;a, b, c, d&amp;#93;)`The optimized plan is not runnable, we need to fix this.</description>
      <version>None</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-25 01:00:00" id="20769" opendate="2020-12-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support minibatch to optimize Python UDAF</summary>
      <description>Support minibatch to optimize Python UDAF</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.udaf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.fast.pxd</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-1-28 01:00:00" id="20783" opendate="2020-12-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate the implementation of BatchExec nodes for Join</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSortMergeJoinRule.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.util.DataFormatTestUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.codegen.SortCodeGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.SortUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.OverAggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecOverAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMatch.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.sort.SortCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.sort.ComparatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.SortSpec.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortMergeJoin.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortLimit.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecRank.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.JoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecHashJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.ExecNodeUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSingleRowJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecNestedLoopJoinRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecNestedLoopJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-4 01:00:00" id="20832" opendate="2021-1-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deliver bootstrap resouces ourselves for website and documentation</summary>
      <description>The typesetting of the official website is abnormal. The typesetting seen by users in China is abnormal.Because user can't load https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js CDN.</description>
      <version>1.12.0</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE</file>
      <file type="M">docs..layouts.base.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-4 01:00:00" id="20835" opendate="2021-1-4 00:00:00" resolution="Done">
    <buginformation>
      <summary>Implement FineGrainedSlotManager</summary>
      <description>Introduce SlotManager plugin for fine-grained resource management. Request slot from TaskExecutor with the actual resource profiles. Use ResourceTracker to bookkeep the resource requirements Introduce TaskExecutorTracker, which bookkeep the total / available resource and slot assignment for registered task executor. Bookkeep task manager total and available resources. Bookkeep slot allocations and assignments. Intorduce PendingTaskManager. Map registered task executors to matching PendingTaskManager.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.WorkerResourceSpecTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.WorkerResourceSpec.java</file>
      <file type="M">tools.ci.test.controller.sh</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServicesConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.SlotReport.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServices.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-4 01:00:00" id="20840" opendate="2021-1-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Projection pushdown doesn&amp;#39;t work in temporal(lookup) join</summary>
      <description>sql 1: |SELECT T.*, D.id|FROM MyTable AS T|JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D|ON T.a = D.idoptmized plan:Calc(select=[a, b, c, PROCTIME_MATERIALIZE(proctime) AS proctime, rowtime, id])+- LookupJoin(table=[default_catalog.default_database.LookupTable], joinType=[InnerJoin], async=[false], lookup=[id=a], select=[a, b, c, proctime, rowtime, id, name, age]) +- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])sql 2:|SELECT T.a, D.id|FROM MyTable AS T|JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D|ON T.a = D.idoptmized plan:LookupJoin(table=[default_catalog.default_database.LookupTable], joinType=[InnerJoin], async=[false], lookup=[id=a], select=[a, id])+- Calc(select=[a]) +- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TemporalTableFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.LegacySinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinReorderTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalRankRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.WindowGroupReorderRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RankNumberColumnRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForRangeEndTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.CalcRankTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testExecuteSqlWithExplainInsert.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-5 01:00:00" id="20854" opendate="2021-1-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce BytesMultiMap to support buffering records</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.aggregate.SumHashAggTestOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.aggregate.BytesHashMapTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.LazyMemorySegmentPool.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashWindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-7 01:00:00" id="20882" opendate="2021-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add better error message for illegal import checks</summary>
      <description>We have a list of imports that we currently do not allow, among others for various classes where the flink-shaded variants should be used instead.A developer running into this is just represent with an IllegalImport error, without additional guidance on what to do.We can split the IllegalImport module and provide error messages specific to each case.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.checkstyle.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-1-10 01:00:00" id="20909" opendate="2021-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MiniBatch Interval derivation does not work well when enable miniBatch optimization in a job which contains deduplicate on row and unbounded aggregate.</summary>
      <description>MiniBatch Interval derivation does not work well when enable miniBatch optimization in a job which contains deduplicate on row and unbounded aggregate.@Testdef testLastRowOnRowtime1(): Unit = { val t = env.fromCollection(rowtimeTestData) .assignTimestampsAndWatermarks(new RowtimeExtractor) .toTable(tEnv, 'a, 'b, 'c, 'rowtime.rowtime()) tEnv.registerTable("T", t) tEnv.executeSql( s""" |CREATE TABLE rowtime_sink ( | cnt BIGINT |) WITH ( | 'connector' = 'values', | 'sink-insert-only' = 'false', | 'changelog-mode' = 'I,UA,D' |) |""".stripMargin) val sql = """ |INSERT INTO rowtime_sink |SELECT COUNT(b) FROM ( | SELECT a, b, c, rowtime | FROM ( | SELECT *, | ROW_NUMBER() OVER (PARTITION BY b ORDER BY rowtime DESC) as rowNum | FROM T | ) | WHERE rowNum = 1 | ) """.stripMargin tEnv.executeSql(sql).await() val rawResult = TestValuesTableFactory.getRawResults("rowtime_sink")}E.g for the above sql, when enable MiniBatch optimization, the optimized plan is as following.Sink(table=[default_catalog.default_database.rowtime_sink], fields=[EXPR$0])+- GlobalGroupAggregate(select=[COUNT_RETRACT(count$0) AS EXPR$0]) +- Exchange(distribution=[single]) +- LocalGroupAggregate(select=[COUNT_RETRACT(b) AS count$0, COUNT_RETRACT(*) AS count1$1]) +- Calc(select=[b]) +- Deduplicate(keep=[LastRow], key=[b], order=[ROWTIME]) +- Exchange(distribution=[hash[b]]) +- Calc(select=[b, rowtime]) +- MiniBatchAssigner(interval=[1000ms], mode=[ProcTime]) +- DataStreamScan(table=[[default_catalog, default_database, T]], fields=[a, b, c, rowtime])A `StreamExecMiniBatchAssigner` will be inserted. The behavior is weird because `Deduplicate` depends on rowTime, however `ProcTimeMiniBatchAssignerOperator` will send watermark every specified interval second depends on process time. For `Deduplicate`, the incoming watermark does not relate to rowTime of incoming record, it cannot indicate rowTime of all following input records are all larger than or equals to the current incoming watermark. </description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DeduplicateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalDeduplicate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-10 01:00:00" id="20914" opendate="2021-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Format the description of &amp;#39;security.ssl.internal.session-cache-size&amp;#39; option</summary>
      <description>The description of ConfigOption 'security.ssl.internal.session-cache-size' has a URL link, as follows:public static final ConfigOption&lt;Integer&gt; SSL_INTERNAL_SESSION_CACHE_SIZE = key("security.ssl.internal.session-cache-size") .intType() .defaultValue(-1) .withDescription( "The size of the cache used for storing SSL session objects. " + "According to https://github.com/netty/netty/issues/832, you should always set " + "this to an appropriate number to not run into a bug with stalling IO threads " + "during garbage collection. (-1 = use system default).") .withDeprecatedKeys("security.ssl.session-cache-size");so the most reasonable way is to use Text Description with Link, as follows:public static final ConfigOption&lt;Integer&gt; SSL_INTERNAL_SESSION_CACHE_SIZE = key("security.ssl.internal.session-cache-size") .intType() .defaultValue(-1) .withDescription( Description.builder() .text( "The size of the cache used for storing SSL session objects. " + "According to %s, you should always set " + "this to an appropriate number to not run into a bug with stalling IO threads " + "during garbage collection. (-1 = use system default).", link( "https://github.com/netty/netty/issues/832", "here")) .build()) .withDeprecatedKeys("security.ssl.session-cache-size");</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">docs..includes.generated.security.configuration.html</file>
      <file type="M">docs..includes.generated.expert.security.ssl.section.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-11 01:00:00" id="20921" opendate="2021-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Date/Time/Timestamp in Python DataStream</summary>
      <description>Currently the Date/Time/Timestamp type doesn't works in Python DataStream.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.streaming.api.utils.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.PythonTypeUtils.java</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-11 01:00:00" id="20925" opendate="2021-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate the implementation of StreamExecLookup and BatchExecLookup</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecLookupJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.common.CommonLookupJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecLookupJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.common.CommonTemporalTableJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnUniqueness.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.utils.LookupJoinUtil.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-1-12 01:00:00" id="20940" opendate="2021-1-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The LOCALTIME/LOCALTIMSTAMP functions should use session time zone</summary>
      <description>LOCALTIMELOCALTIME TIME(0) NOT NULL #session timezone: UTC 08:52:52 #session timezone: UTC+8 08:52:52wall clock: UTC+8:2020-12-29 08:52:52|LOCALTIMESTAMPLOCALTIMESTAMP TIMESTAMP(0) NOT NULL #session timezone: UTC 2020-12-29T08:52:52 #session timezone: UTC + 8 LOCALTIMESTAMP TIMESTAMP(0) NOT NULL 2020-12-29T08:52:52wall clock: UTC+8:2020-12-29 08:52:52|</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-12 01:00:00" id="20942" opendate="2021-1-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Digest of FLOAT literals throws UnsupportedOperationException</summary>
      <description>The recent refactoring of Calcite's digests might have caused a regression for FLOAT literals. org.apache.calcite.rex.RexLiteral#appendAsJava throws a UnsupportedOperationException for the following query:def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val source = env.fromElements( (1.0f, 11.0f, 12.0f), (2.0f, 21.0f, 22.0f), (3.0f, 31.0f, 32.0f), (4.0f, 41.0f, 42.0f), (5.0f, 51.0f, 52.0f) ) val settings = EnvironmentSettings.newInstance() .inStreamingMode() .useBlinkPlanner() .build() val tEnv = StreamTableEnvironment.create(env, settings) tEnv.createTemporaryView("myTable", source, $("id"), $("f1"), $("f2")) val query = """ |select * from myTable where id in (1.0, 2.0, 3.0) |""".stripMargin tEnv.executeSql(query).print()}Stack trace:Exception in thread "main" java.lang.UnsupportedOperationException: class org.apache.calcite.sql.type.SqlTypeName: FLOAT at org.apache.calcite.util.Util.needToImplement(Util.java:1075) at org.apache.calcite.rex.RexLiteral.appendAsJava(RexLiteral.java:703) at org.apache.calcite.rex.RexLiteral.toJavaString(RexLiteral.java:408) at org.apache.calcite.rex.RexLiteral.computeDigest(RexLiteral.java:276) at org.apache.calcite.rex.RexLiteral.&lt;init&gt;(RexLiteral.java:223) at org.apache.calcite.rex.RexLiteral.toLiteral(RexLiteral.java:737) at org.apache.calcite.rex.RexLiteral.lambda$printSarg$4(RexLiteral.java:710) at org.apache.calcite.util.RangeSets$Printer.singleton(RangeSets.java:397) at org.apache.calcite.util.RangeSets.forEach(RangeSets.java:237) at org.apache.calcite.util.Sarg.lambda$printTo$0(Sarg.java:110) at org.apache.calcite.linq4j.Ord.forEach(Ord.java:157) at org.apache.calcite.util.Sarg.printTo(Sarg.java:106) at org.apache.calcite.rex.RexLiteral.printSarg(RexLiteral.java:709) at org.apache.calcite.rex.RexLiteral.lambda$appendAsJava$1(RexLiteral.java:652) at org.apache.calcite.util.Util.asStringBuilder(Util.java:2502) at org.apache.calcite.rex.RexLiteral.appendAsJava(RexLiteral.java:651) at org.apache.calcite.rex.RexLiteral.toJavaString(RexLiteral.java:408) at org.apache.calcite.rex.RexLiteral.computeDigest(RexLiteral.java:276) at org.apache.calcite.rex.RexLiteral.&lt;init&gt;(RexLiteral.java:223) at org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:971) at org.apache.calcite.rex.RexBuilder.makeSearchArgumentLiteral(RexBuilder.java:1066) at org.apache.calcite.rex.RexSimplify$SargCollector.fix(RexSimplify.java:2786) at org.apache.calcite.rex.RexSimplify.lambda$simplifyOrs$6(RexSimplify.java:1843) at java.util.ArrayList.forEach(ArrayList.java:1257) at org.apache.calcite.rex.RexSimplify.simplifyOrs(RexSimplify.java:1843) at org.apache.calcite.rex.RexSimplify.simplifyOr(RexSimplify.java:1817) at org.apache.calcite.rex.RexSimplify.simplify(RexSimplify.java:313) at org.apache.calcite.rex.RexSimplify.simplifyUnknownAs(RexSimplify.java:282) at org.apache.calcite.rex.RexSimplify.simplify(RexSimplify.java:257) at org.apache.flink.table.planner.plan.utils.FlinkRexUtil$.simplify(FlinkRexUtil.scala:213) at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.simplify(SimplifyFilterConditionRule.scala:63) at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.onMatch(SimplifyFilterConditionRule.scala:46) at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333) at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542) at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407) at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243) at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127) at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202) at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.immutable.Range.foreach(Range.scala:160) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:79) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:286) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1329) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:707) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:1107) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:666) at org.apache.flink.table.examples.scala.basics.WordCountTable$.main(WordCountTable.scala:59)</description>
      <version>None</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-13 01:00:00" id="20961" opendate="2021-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink throws NullPointerException for tables created from DataStream with no assigned timestamps and watermarks</summary>
      <description> Given the following program://import org.apache.flink.api.common.eventtime.{ SerializableTimestampAssigner, WatermarkStrategy }import org.apache.flink.streaming.api.functions.source.SourceFunctionimport org.apache.flink.streaming.api.scala.{StreamExecutionEnvironment, _}import org.apache.flink.streaming.api.watermark.Watermarkimport org.apache.flink.table.annotation.{DataTypeHint, FunctionHint}import org.apache.flink.table.api.bridge.scala.StreamTableEnvironmentimport org.apache.flink.table.api.{$, AnyWithOperations}import org.apache.flink.table.functions.{AggregateFunction, ScalarFunction}import java.time.Instantobject BugRepro { def text: String = s""" |{ | "s": "hello", | "i": ${Random.nextInt()} |} |""".stripMargin def main(args: Array[String]): Unit = { val flink = StreamExecutionEnvironment.createLocalEnvironment() val tableEnv = StreamTableEnvironment.create(flink) val dataStream = flink .addSource { new SourceFunction[(Long, String)] { var isRunning = true override def run(ctx: SourceFunction.SourceContext[(Long, String)]): Unit = while (isRunning) { val x = (Instant.now().toEpochMilli, text) ctx.collect(x) ctx.emitWatermark(new Watermark(x._1)) Thread.sleep(300) } override def cancel(): Unit = isRunning = false } }// .assignTimestampsAndWatermarks(// WatermarkStrategy// .forBoundedOutOfOrderness[(Long, String)](Duration.ofSeconds(30))// .withTimestampAssigner {// new SerializableTimestampAssigner[(Long, String)] {// override def extractTimestamp(element: (Long, String), recordTimestamp: Long): Long =// element._1// }// }// )// tableEnv.createTemporaryView("testview", dataStream, $("event_time").rowtime(), $("json_text")) val res = tableEnv.sqlQuery(""" |SELECT json_text |FROM testview |""".stripMargin) val sink = tableEnv.executeSql( """ |CREATE TABLE SINK ( | json_text STRING |) |WITH ( | 'connector' = 'print' |) |""".stripMargin ) res.executeInsert("SINK").await() () } res.executeInsert("SINK").await() Flink will throw a NullPointerException at runtime:Caused by: java.lang.NullPointerExceptionCaused by: java.lang.NullPointerException at SourceConversion$3.processElement(Unknown Source) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30) at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:305) at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:394) at ai.hunters.pipeline.BugRepro$$anon$1.run(BugRepro.scala:78) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:215)This is due to the fact that the DataStream did not assign a timestamp to the underlying source. This is the generated code:public class SourceConversion$3 extends org.apache.flink.table.runtime.operators.AbstractProcessStreamOperator implements org.apache.flink.streaming.api.operators.OneInputStreamOperator { private final Object[] references; private transient org.apache.flink.table.data.util.DataFormatConverters.CaseClassConverter converter$0; org.apache.flink.table.data.GenericRowData out = new org.apache.flink.table.data.GenericRowData(2); private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null); public SourceConversion$3( Object[] references, org.apache.flink.streaming.runtime.tasks.StreamTask task, org.apache.flink.streaming.api.graph.StreamConfig config, org.apache.flink.streaming.api.operators.Output output, org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception { this.references = references; converter$0 = (((org.apache.flink.table.data.util.DataFormatConverters.CaseClassConverter) references[0])); this.setup(task, config, output); if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) { ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this) .setProcessingTimeService(processingTimeService); } } @Override public void open() throws Exception { super.open(); } @Override public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception { org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) (org.apache.flink.table.data.RowData) converter$0.toInternal((scala.Tuple2) element.getValue()); org.apache.flink.table.data.TimestampData result$1; boolean isNull$1; org.apache.flink.table.data.binary.BinaryStringData field$2; boolean isNull$2; isNull$2 = in1.isNullAt(1); field$2 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8; if (!isNull$2) { field$2 = ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(1)); } ctx.element = element; result$1 = org.apache.flink.table.data.TimestampData.fromEpochMillis(ctx.timestamp()); if (result$1 == null) { throw new RuntimeException("Rowtime timestamp is null. Please make sure that a " + "proper TimestampAssigner is defined and the stream environment uses the EventTime " + "time characteristic."); } isNull$1 = false; if (isNull$1) { out.setField(0, null); } else { out.setField(0, result$1); } if (isNull$2) { out.setField(1, null); } else { out.setField(1, field$2); } output.collect(outElement.replace(out)); ctx.element = null; } @Override public void close() throws Exception { super.close(); } }The important line is here:result$1 = org.apache.flink.table.data.TimestampData.fromEpochMillis(ctx.timestamp()); if (result$1 == null) { throw new RuntimeException("Rowtime timestamp is null. Please make sure that a " + "proper TimestampAssigner is defined and the stream environment uses the EventTime " + "time characteristic.");`ctx.timestamp` returns null in case no timestamp assigner was created, and `TimestampData.fromEpochMillis` expects a primitive `long`, so a deference fails. The actual check should be:if (!ctx.hasTimestamp) { throw new RuntimeException("Rowtime timestamp is null. Please make sure that a " + "proper TimestampAssigner is defined and the stream environment uses the EventTime " + "time characteristic.");}result$1 = TimestampData.fromEpochMillis(ctx.timestamp()); </description>
      <version>1.12.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-13 01:00:00" id="20963" opendate="2021-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rewrite the example under &amp;#39;flink-python/pyflink/table/examples&amp;#39;</summary>
      <description>Currently the example under 'flink-python/pyflink/table/examples' still uses the deprecated APIs. We need to rewrite it with the latest recommended API.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.examples.batch.word.count.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-16 01:00:00" id="20997" opendate="2021-1-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>YarnTestBaseTest fails due to NPE</summary>
      <description>YarnTestBase depends on classpaths generated by Maven dependency plugin in `package` phase, but YarnTestBaseTest is a unit test that executed in `test` phase (which is before `package`), so it's unable to find `yarn.classpath` and causes NPE.</description>
      <version>1.11.3,1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-19 01:00:00" id="21017" opendate="2021-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix missing backquote in table connectors docs</summary>
      <description/>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-19 01:00:00" id="21019" opendate="2021-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Netty 4 to 4.1.46</summary>
      <description>Our current Netty version (4.1.44) is vulnerable for at least this CVE:https://nvd.nist.gov/vuln/detail/CVE-2020-11612Bumping to 4.1.46+ should solve it.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-27 01:00:00" id="2102" opendate="2015-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add predict operation for LabeledVector</summary>
      <description>Currently we can only call predict on DataSet&amp;#91;V &lt;: Vector&amp;#93;.A lot of times though we have a DataSet&amp;#91;LabeledVector&amp;#93; that we split into a train and test set.We should be able to make predictions on the test DataSet&amp;#91;LabeledVector&amp;#93; without having to transform it into a DataSet&amp;#91;Vector&amp;#93;</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.MultipleLinearRegressionITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.classification.SVMITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.classification.SVM.scala</file>
      <file type="M">docs.libs.ml.svm.md</file>
      <file type="M">docs.libs.ml.multiple.linear.regression.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-20 01:00:00" id="21054" opendate="2021-1-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement mini-batch optimized slicing window aggregate operator</summary>
      <description>We have supported cumulative windows in FLINK-19605. However, the current cumulative window is not efficient, because the slices are not shared. We leverages the slicing ideas proposed in FLINK-7001 and this design doc &amp;#91;1&amp;#93;. The slicing is an optimized implementation for hopping, cumulative, tumbling windows. Besides of that, we introduced ManagedMemory based mini-batch optimization for the slicing window aggregate operator, this can tremendously reduce the accessing of state and get the higher throughtput without latency loss. &amp;#91;1&amp;#93;: https://docs.google.com/document/d/1ziVsuW_HQnvJr_4a9yKwx_LEnhVkdlde2Z5l6sx5HlY/edit#</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.StateConfigUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.collections.binary.BytesMap.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-21 01:00:00" id="21073" opendate="2021-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mention that RocksDB ignores equals/hashCode because it works on binary data</summary>
      <description>See https://lists.apache.org/thread.html/ra43e2b5d388831290c293b9daf0eee0b0a5d9712543b62c83234a829%40%3Cuser.flink.apache.org%3E</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-25 01:00:00" id="21112" opendate="2021-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ValueState/ListState/MapState and corresponding StateDescriptors for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.common.state.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-25 01:00:00" id="21114" opendate="2021-1-25 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add ReducingState and corresponding StateDescriptor for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.state.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-25 01:00:00" id="21115" opendate="2021-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add AggregatingState and corresponding StateDescriptor for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.state.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-26 01:00:00" id="21142" opendate="2021-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink guava Dependence problem</summary>
      <description>We set up a new Hadoop cluster, and we use the flink1.12.0 compiled by the previous release-1.12.0 branch.If I add hive jar to flink/lib/, it will report errors.Operating environment：     flink1.12.0      Hadoop 3.3.0     hive 3.1.2Flink run official demo shell： /tmp/yjb/buildjar/flink1.12.0/bin/flink run -m yarn-cluster /usr/local/flink1.12.0/examples/streaming/WordCount.jarIf I put one of the jar flink-sql-connector-hive-3.1.2_2.11-1.12.0.jar or hive-exec-3.1.2.jar in the Lib directory and execute the above shell, an error will be reported  java.lang.NoSuchMethodError : com.google.common . base.Preconditions.checkArgument (ZLjava/lang/String;Ljava/lang/Object;)V. We can see that it's the dependency conflict of guava.My cluster guava‘s version: /usr/local/hadoop-3.3.0/share/hadoop/yarn/csi/lib/guava-20.0.jar /usr/local/hadoop-3.3.0/share/hadoop/common/lib/guava-27.0-jre.jar /usr/local/apache-hive-3.1.2-bin/lib/guava-20.0.jar /usr/local/apache-hive-3.1.2-bin/lib/jersey-guava-2.25.1.jar /usr/local/spark-3.0.1-bin-hadoop3.2/jars/guava-14.0.1.jarCan you give me some advice？ Thank you！</description>
      <version>1.12.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-28 01:00:00" id="21174" opendate="2021-1-28 00:00:00" resolution="Done">
    <buginformation>
      <summary>Optimize the performance of ResourceAllocationStrategy</summary>
      <description>In FLINK-20835, we introduce the ResourceAllocationStrategy for fine-grained resource management, which matches resource requirements against available and pending resources and returns the allocation result.We need to optimize the computation logic of it, which is so complicated atm.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceAllocationStrategy.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-2-11 01:00:00" id="21363" opendate="2021-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix baseurl in documentation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.README.md</file>
      <file type="M">docs.layouts.shortcodes.img.html</file>
      <file type="M">docs.layouts.partials.docs.inject.menu-before.html</file>
      <file type="M">docs.layouts.partials.docs.inject.head.html</file>
      <file type="M">docs.config.toml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-24 01:00:00" id="21481" opendate="2021-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move git-commit-id-plugin execution to flink-runtime</summary>
      <description>The properties set by the git-commit-id-plugin are only accessed in flink-runtime (see EnvironmentInformation), so we should use the execution of this plugin into flink-runtime to save some build time.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.PseudoRandomValueSelector.java</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-5 01:00:00" id="21627" opendate="2021-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>When you insert multiple inserts with statementSet, you modify multiple inserts with OPTIONS(&amp;#39;table-name &amp;#39;=&amp;#39; XXX &amp;#39;), but only the first one takes effect</summary>
      <description>//代码占位符StatementSet statementSet = tableEnvironment.createStatementSet();String sql1 = "insert into test select a,b,c from test_a_12342 /*+OPTIONS('table-name'='test_a_1')*/";String sql2 = "insert into test select a,b,c from test_a_12342 /*+OPTIONS('table-name'='test_a_2')*/";statementSet.addInsertSql(sql1);statementSet.addInsertSql(sql2);statementSet.execute();Sql code as above, in the final after the insert is put test_a_1 table data into the two times, and test_a_2 data did not insert, is excuse me this bug</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalLegacySinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LegacySinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LegacySinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalLegacySinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushLimitIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkLogicalRelFactories.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalDataStreamTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalLegacyTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalBoundedStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalLegacyTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalLegacyTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalLegacyTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalBoundedStreamScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalDataStreamScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.hint.OptionsHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.CalcRankTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RankNumberColumnRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LegacyTableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LegacyTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSourceSinks.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.LegacySink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.LogicalLegacySink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.LogicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.Sink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalLegacySink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalLegacySink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalLegacySink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalSink.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-5 01:00:00" id="2175" opendate="2015-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow multiple jobs in single jar file</summary>
      <description>Allow to package multiple jobs into a single jar. extend WebClient to display all available jobs extend WebClient to diplay plan and submit each job</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">docs.apis.web.client.md</file>
      <file type="M">flink-clients.src.main.resources.web-docs.js.program.js</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.web.PactJobJSONServlet.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.web.JobSubmissionServlet.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.web.JobsServlet.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-14 01:00:00" id="21777" opendate="2021-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace the 4M data writing cache of sort-merge shuffle with writev system call</summary>
      <description>Currently, the sort-merge shuffle implementation uses 4M unmanaged direct memory as cache for data writing. It can be replaced by the writev system call which can reduce the unmanaged direct memory usage without any performance loss.</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionSortedBufferTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriteReadTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionSortedBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferReaderWriterUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-14 01:00:00" id="21778" opendate="2021-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use heap memory instead of direct memory as index entry cache for sort-merge shuffle</summary>
      <description>Currently, the sort-merge shuffle implementation uses a piece of direct memory as index entry cache for acceleration. We can use heap memory instead to reduce the usage of direct memory which further reduces the possibility of OutOfMemoryError.</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-3-31 01:00:00" id="22058" opendate="2021-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support FLIP-27 based NumberSequenceSource connector in PyFlink DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-12 01:00:00" id="2209" opendate="2015-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to use TableAPI, Gelly and FlinkML, StreamingConnectors on a cluster</summary>
      <description>Currently the TableAPI, Gelly, FlinkML and StreamingConnectors are not part of the Flink dist module. Therefore they are not included in the binary distribution. As a consequence, if you want to use one of these libraries the corresponding jar and all their dependencies have to be either manually put on the cluster or the user has to include them in the user code jar.Usually a fat jar is built if the one uses the quickstart archetypes. However if one sets the project manually up this ist not necessarily the case. Therefore, it should be well documented how to run programs using one of these libraries.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.libs.table.md</file>
      <file type="M">docs.libs.ml.index.md</file>
      <file type="M">docs.libs.gelly.guide.md</file>
      <file type="M">docs.apis.streaming.guide.md</file>
      <file type="M">docs.apis.cluster.execution.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-5-12 01:00:00" id="22233" opendate="2021-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Spelling error of word "constant" in source code</summary>
      <description>When I write the data to MySQL and forget to add the primary key, I report such an error：' org.apache.flink.table.api.ValidationException: Flink doesn't support ENFORCED mode for PRIMARY KEY constaint. 'As mentioned above, The last word 'constant' is misspelled . And the correct spelling is 'constraint'. </description>
      <version>1.12.0</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-19 01:00:00" id="2248" opendate="2015-6-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow disabling of sdtout logging output</summary>
      <description>Currently when a job is submitted through the CLI we get in stdout all the log output about each stage in the job.It would useful to have an easy way to disable this output when submitting the job, as most of the time we are only interested in the log output if something goes wrong.</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.testjar.WordCount.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">docs.apis.cli.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-14 01:00:00" id="22666" opendate="2021-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make structured type&amp;#39;s fields more lenient during casting</summary>
      <description>While writing documentation in FLINK-22537, I found some issues when using the Scala DataStream API. We should add more tests to identify those.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastAvoidanceTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeCasts.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-17 01:00:00" id="22673" opendate="2021-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document about add jar related commands</summary>
      <description>Including ADD JAR, SHOW JAR, REMOVE JAR.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-21 01:00:00" id="22733" opendate="2021-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type mismatch thrown in DataStream.union if parameter is KeyedStream for Python DataStream API</summary>
      <description>See http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-DataStream-union-type-mismatch-td43855.html for more details.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.13.1,1.12.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-11 01:00:00" id="22971" opendate="2021-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Testcontainers to 1.16.0</summary>
      <description>Testcontainer versions below 1.16.0 are affected by a race condition in the okhttp transport:https://github.com/testcontainers/testcontainers-java/issues/3531The following properties can be observed in many high-frequency test instabilities. They are likely sharing a common root cause. Use the Testcontainers framework Fail due to no outputs in long time A stack similar to the following can be find in the thread dump. Thread is RUNNABLE Testcontainers DockerClient is being created: org.testcontainers.DockerClientFactory.client(DockerClientFactory.java:205) Seems to be downloading something via okhttp "main" #1 prio=5 os_prio=0 tid=0x00007f4fec00b800 nid=0x4f56 runnable [0x00007f4ff536e000] java.lang.Thread.State: RUNNABLE at org.testcontainers.shaded.okio.Buffer.getByte(Buffer.java:312) at org.testcontainers.shaded.okio.Buffer.readUtf8Line(Buffer.java:766) at org.testcontainers.shaded.okio.RealBufferedSource.readUtf8LineStrict(RealBufferedSource.java:231) at org.testcontainers.shaded.okio.RealBufferedSource.readUtf8LineStrict(RealBufferedSource.java:224) at org.testcontainers.shaded.okhttp3.internal.http1.Http1ExchangeCodec$ChunkedSource.readChunkSize(Http1ExchangeCodec.java:489) at org.testcontainers.shaded.okhttp3.internal.http1.Http1ExchangeCodec$ChunkedSource.read(Http1ExchangeCodec.java:471) at org.testcontainers.shaded.okhttp3.internal.Util.skipAll(Util.java:204) at org.testcontainers.shaded.okhttp3.internal.Util.discard(Util.java:186) at org.testcontainers.shaded.okhttp3.internal.http1.Http1ExchangeCodec$ChunkedSource.close(Http1ExchangeCodec.java:511) at org.testcontainers.shaded.okio.ForwardingSource.close(ForwardingSource.java:43) at org.testcontainers.shaded.okhttp3.internal.connection.Exchange$ResponseBodySource.close(Exchange.java:313) at org.testcontainers.shaded.okio.RealBufferedSource.close(RealBufferedSource.java:476) at org.testcontainers.shaded.okhttp3.internal.Util.closeQuietly(Util.java:139) at org.testcontainers.shaded.okhttp3.ResponseBody.close(ResponseBody.java:192) at org.testcontainers.shaded.okhttp3.Response.close(Response.java:290) at org.testcontainers.shaded.com.github.dockerjava.okhttp.OkDockerHttpClient$OkResponse.close(OkDockerHttpClient.java:285) at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.lambda$null$0(DefaultInvocationBuilder.java:272) at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder$$Lambda$96/1409690119.close(Unknown Source) at com.github.dockerjava.api.async.ResultCallbackTemplate.close(ResultCallbackTemplate.java:77) at org.testcontainers.utility.ResourceReaper.start(ResourceReaper.java:202) at org.testcontainers.DockerClientFactory.client(DockerClientFactory.java:205) - locked &lt;0x000000008980f398&gt; (a [Ljava.lang.Object;) at org.testcontainers.LazyDockerClient.getDockerClient(LazyDockerClient.java:14) at org.testcontainers.LazyDockerClient.listImagesCmd(LazyDockerClient.java:12) at org.testcontainers.images.LocalImagesCache.maybeInitCache(LocalImagesCache.java:68) - locked &lt;0x000000008980f3e8&gt; (a org.testcontainers.images.LocalImagesCache) at org.testcontainers.images.LocalImagesCache.get(LocalImagesCache.java:32) at org.testcontainers.images.AbstractImagePullPolicy.shouldPull(AbstractImagePullPolicy.java:18) at org.testcontainers.images.RemoteDockerImage.resolve(RemoteDockerImage.java:66) at org.testcontainers.images.RemoteDockerImage.resolve(RemoteDockerImage.java:27) at org.testcontainers.utility.LazyFuture.getResolvedValue(LazyFuture.java:17) - locked &lt;0x000000008980f478&gt; (a java.util.concurrent.atomic.AtomicReference) at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:39) at org.testcontainers.containers.GenericContainer.getDockerImageName(GenericContainer.java:1284) at org.testcontainers.containers.GenericContainer.logger(GenericContainer.java:615) at org.testcontainers.elasticsearch.ElasticsearchContainer.&lt;init&gt;(ElasticsearchContainer.java:73) at org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSinkITCase.&lt;clinit&gt;(ElasticsearchSinkITCase.java:48) at sun.misc.Unsafe.ensureClassInitialized(Native Method) at sun.reflect.UnsafeFieldAccessorFactory.newFieldAccessor(UnsafeFieldAccessorFactory.java:43) at sun.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:156) at java.lang.reflect.Field.acquireFieldAccessor(Field.java:1088) at java.lang.reflect.Field.getFieldAccessor(Field.java:1069) at java.lang.reflect.Field.get(Field.java:393) at org.junit.runners.model.FrameworkField.get(FrameworkField.java:73) at org.junit.runners.model.TestClass.getAnnotatedFieldValues(TestClass.java:230) at org.junit.runners.ParentRunner.classRules(ParentRunner.java:255) at org.junit.runners.ParentRunner.withClassRules(ParentRunner.java:244) at org.junit.runners.ParentRunner.classBlock(ParentRunner.java:194) at org.junit.runners.ParentRunner.run(ParentRunner.java:362) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)</description>
      <version>1.12.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-6-24 01:00:00" id="23133" opendate="2021-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The dependencies are not handled properly when mixing use of Python Table API and Python DataStream API</summary>
      <description>The reason is that when converting from DataStream to Table, the dependencies should be handled and set correctly for the existing DataStream operators.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">docs.content.docs.dev.python.dependency.management.md</file>
      <file type="M">docs.content.zh.docs.dev.python.dependency.management.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-17 01:00:00" id="23838" opendate="2021-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add FLIP-33 metrics to new KafkaSink</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaWriter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-24 01:00:00" id="23936" opendate="2021-8-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python UDFs instances are reinitialized if there is no input for more than 1 minute</summary>
      <description>We receive this feedback from some PyFlink users. After some investigation, we find out that the root case is that there is a mechanism in Beam that it will released the BundleProcessors which are inactive for more than 1 minute: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/worker/sdk_worker.py#L90</description>
      <version>1.10.0,1.11.0,1.12.0,1.13.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-30 01:00:00" id="24049" opendate="2021-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TupleTypeInfo doesn&amp;#39;t handle correctly for data types need conversion</summary>
      <description/>
      <version>1.12.0,1.13.0,1.14.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.common.typeinfo.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-10-13 01:00:00" id="24535" opendate="2021-10-13 00:00:00" resolution="Done">
    <buginformation>
      <summary>Update with the latest AWS Glue Schema Registry version</summary>
      <description>This change is to update the flink avro and json schema serializer / deserializer with latest AWS Glue Schema Registry latest version v1.1.5.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-27 01:00:00" id="24662" opendate="2021-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink sphinx check failed with "node class &amp;#39;meta&amp;#39; is already registered, its visitors will be overridden"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25481&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=8d78fe4f-d658-5c70-12f8-4921589024c3==========mypy checks... [SUCCESS]===========Oct 26 22:08:34 rm -rf _build/*Oct 26 22:08:34 /__w/1/s/flink-python/dev/.conda/bin/sphinx-build -b html -d _build/doctrees -a -W . _build/htmlOct 26 22:08:34 Running Sphinx v2.4.4Oct 26 22:08:34 Oct 26 22:08:34 Warning, treated as error:Oct 26 22:08:34 node class 'meta' is already registered, its visitors will be overriddenOct 26 22:08:34 Makefile:76: recipe for target 'html' failed</description>
      <version>1.12.0,1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-5 01:00:00" id="24798" opendate="2021-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump commons-cli to v1.5.0</summary>
      <description>Bump commons-cli:commons-cli:1.4 to commons-cli:commons-cli:1.5.0</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-12-23 01:00:00" id="25011" opendate="2021-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce VertexParallelismDecider</summary>
      <description>Introduce VertexParallelismDecider and provide a default implementation.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-29 01:00:00" id="25091" opendate="2021-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Official website document FileSink orc compression attribute reference error</summary>
      <description>I see the following version is like this &amp;#91;1.12、1.13、1.14 。。。&amp;#93; What should be quoted here is writerProperties Shouldn't be is writerProps docUrl</description>
      <version>1.12.0,1.13.0,1.14.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-3-24 01:00:00" id="26847" opendate="2022-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Command line option &amp;#39;-py&amp;#39; doesn&amp;#39;t work in YARN application mode</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonDriverTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonDriverOptionsParserFactory.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonDriver.java</file>
    </fixedFiles>
  </bug>
  
</bugrepository>