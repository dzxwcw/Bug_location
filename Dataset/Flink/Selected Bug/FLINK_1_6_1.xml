<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2018-9-30 01:00:00" id="10263" opendate="2018-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>User-defined function with LITERAL paramters yields CompileException</summary>
      <description>When using a user-defined scalar function only with literal parameters, a CompileException is thrown. For exampleSELECT myFunc(CAST(40.750444 AS FLOAT), CAST(-73.993475 AS FLOAT))public class MyFunc extends ScalarFunction { public int eval(float lon, float lat) { // do something }}results in [ERROR] Could not execute SQL statement. Reason:org.codehaus.commons.compiler.CompileException: Line 5, Column 10: Cannot determine simple type name "com"The problem is probably caused by the expression reducer because it disappears if a regular attribute is added to a parameter expression.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-20 01:00:00" id="10379" opendate="2018-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can not use Table Functions in Java Table API</summary>
      <description>As stated in the documentation this is how table functions should be used in Java Table API:// Register the function.tableEnv.registerFunction("split", new Split("#"));myTable.join("split(a) as (word, length)");However Table.join(String) was removed sometime ago and now it is impossible to use Table Functions in Java Table API.</description>
      <version>1.6.1</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.TableFunction.scala</file>
      <file type="M">docs.dev.table.udfs.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-21 01:00:00" id="10384" opendate="2018-9-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Sinh math function supported in Table API and SQL</summary>
      <description>like FLINK-10340 for adding Cosh math function</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.sql.ScalarSqlFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.mathExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.functions.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-9-25 01:00:00" id="10416" opendate="2018-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add files generated by jepsen tests to rat excludes</summary>
      <description>Currently jepsen generates some files that results in rat plugin failures.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-1 01:00:00" id="10474" opendate="2018-10-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t translate IN with Literals to JOIN with VALUES for streaming queries</summary>
      <description>IN predicates with literals are translated to JOIN with VALUES if the number of elements in the IN clause exceeds a certain threshold. This should not be done, because a streaming join is very heavy and materializes both inputs (which is fine for the VALUES) input but not for the other.There are two ways to solve this: don't translate IN to a JOIN at all translate it to a JOIN but have a special join strategy if one input is bound and final (non-updating)Option 1. should be easy to do, option 2. requires much more effort.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.calcite.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.CalcTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.CalcTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.CalciteConfig.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-11 01:00:00" id="10531" opendate="2018-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>State TTL RocksDb backend end-to-end test failed on Travis</summary>
      <description>The State TTL RocksDb backend end-to-end test end-to-end test failed on Travis.https://travis-ci.org/apache/flink/jobs/438226190https://api.travis-ci.org/v3/job/438226190/log.txt</description>
      <version>1.6.1</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.verify.ValueWithTs.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.verify.TtlVerificationContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.verify.TtlValueStateVerifier.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.verify.TtlUpdateContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.verify.TtlReducingStateVerifier.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.verify.TtlMapStateVerifier.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.verify.TtlListStateVerifier.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.verify.TtlFoldingStateVerifier.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.verify.TtlAggregatingStateVerifier.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.verify.AbstractTtlStateVerifier.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.TtlVerifyUpdateFunction.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.TtlStateUpdateSource.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.TtlStateUpdate.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.DataStreamStateTTLTestProgram.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-1-16 01:00:00" id="10571" opendate="2018-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove support for topologies</summary>
      <description>The topology support is hard-wired to the legacy mode and we have no intention of updating it. It should thus be removed to not block the removal of the legacy mode.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.WrapperSetupInLocalClusterTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.api.TestSpout.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.api.TestBolt.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.api.FlinkTopologyTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.api.FlinkOutputFieldsDeclarerTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.WrapperSetupHelper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SpoutWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.BoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.StormStreamSelector.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.TwoFlinkStreamsMerger.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.StormFlinkStreamMerger.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkTopology.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkSubmitter.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkOutputFieldsDeclarer.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkLocalCluster.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkClient.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.wordcount.WordCountLocalNamedITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.wordcount.WordCountLocalITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.StormUnionITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.StormMetaDataITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.StormFieldsGroupingITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.operators.VerifyMetaDataBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.operators.TaskIdBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.operators.MetaDataSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.operators.MergerBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.operators.FiniteRandomSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitStreamSpoutLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitStreamBoltLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitSpoutTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitBoltTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.join.SingleJoinITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.exclamation.StormExclamationLocalITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountRemoteBySubmitter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountRemoteByClient.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountLocalByName.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.print.PrintSampleStream.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.join.SingleJoinExample.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.ExclamationTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.ExclamationLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
      <file type="M">docs.dev.libs.storm.compatibility.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-19 01:00:00" id="10608" opendate="2018-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add avro files generated by datastream-allround-test to RAT exclusions</summary>
      <description>With the 1.5.5 and 1.6.2 release-candidates it was discovered that files generated during the build are not covered by the apache-rat-plugin license-header check.As a result the first compilation succeeds but a subsequent one may fail.This is a bit surprising considering that the plugin is executed in the verify phase, which is executed after the compilation and execution of tests.This is because the plugin is only run in the flink-parent project before anything is compiled. The plugin-configuration is explicitly disables inheritance.I'm re-purposing this Jira to add the avro classes to the exclusion list.However, in the long term I'd suggest to enable inheritance for the plugin and define the module-specific exclusions in each module respectively. This will allows to run the plugin in the verify phase of each module which would've caught this error.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-22 01:00:00" id="10638" opendate="2018-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid table scan resolution for temporal join queries</summary>
      <description>Registered tables that contain a temporal join are not properly resolved when performing a table scan.A planning error occurs when registering a table with a temporal join and reading from it again.LogicalProject(amount=[*($0, $4)]) LogicalFilter(condition=[=($3, $1)]) LogicalCorrelate(correlation=[$cor0], joinType=[inner], requiredColumns=[{2}]) LogicalTableScan(table=[[_DataStreamTable_0]]) LogicalTableFunctionScan(invocation=[Rates(CAST($cor0.rowtime):TIMESTAMP(3) NOT NULL)], rowType=[RecordType(VARCHAR(65536) currency, BIGINT rate, TIME ATTRIBUTE(ROWTIME) rowtime)], elementType=[class [Ljava.lang.Object;])</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.TemporalTableJoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-22 01:00:00" id="10639" opendate="2018-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix java syntax error in document</summary>
      <description>Due to the  StreamTableSourceFactory  is a trait. So the java example in the document should using "implements" keyword.    </description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.types.serialization.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-24 01:00:00" id="10669" opendate="2018-10-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exceptions &amp; errors are not properly checked in logs in e2e tests</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-26 01:00:00" id="1067" opendate="2014-8-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Stratosphere" occurance in docs FAQ</summary>
      <description>The name Stratosphere occurs in the answer to the fault-tolerance question of the FAQ.</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.faq.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-24 01:00:00" id="10670" opendate="2018-10-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Correlate codegen error</summary>
      <description>TableFunctionCollector should handle reuseInitCode and reuseMemberCode</description>
      <version>None</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CollectorCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-25 01:00:00" id="10681" opendate="2018-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>elasticsearch6.ElasticsearchSinkITCase fails if wrong JNA library installed</summary>
      <description>The elasticsearch6.ElasticsearchSinkITCase fails on systems where a wrong JNA library is installed.There is an incompatible JNA native library installed on this systemExpected: 5.2.0Found: 4.0.0/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib.To resolve this issue you may do one of the following: - remove or uninstall the offending library - set the system property jna.nosys=true - set jna.boot.library.path to include the path to the version of the jnidispatch library included with the JNA jar file you are using at com.sun.jna.Native.&lt;clinit&gt;(Native.java:199) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:264) at org.elasticsearch.bootstrap.Natives.&lt;clinit&gt;(Natives.java:45) at org.elasticsearch.bootstrap.BootstrapInfo.isMemoryLocked(BootstrapInfo.java:50) at org.elasticsearch.monitor.process.ProcessProbe.processInfo(ProcessProbe.java:130) at org.elasticsearch.monitor.process.ProcessService.&lt;init&gt;(ProcessService.java:44) at org.elasticsearch.monitor.MonitorService.&lt;init&gt;(MonitorService.java:48) at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:363) at org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl$PluginNode.&lt;init&gt;(EmbeddedElasticsearchNodeEnvironmentImpl.java:85) at org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl.start(EmbeddedElasticsearchNodeEnvironmentImpl.java:53) at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.prepare(ElasticsearchSinkTestBase.java:73) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:113) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)I propose to solve the problem by setting the system property jna.nosys=true to prefer the bundled JNA library.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-26 01:00:00" id="10692" opendate="2018-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden Confluent schema E2E test</summary>
      <description>The Confluent schema E2E test starts a Confluent schema registry to run against. If the schema registry cannot be started, the test proceeds and simply dead locks. In order to improve the situation I suggest to fail if we cannot start the Confluent schema registry.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-27 01:00:00" id="10702" opendate="2018-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn app is not killed when scala shell is terminated</summary>
      <description>When I quit scala shell in yarn mode, yarn app is not killed.</description>
      <version>1.6.1</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
</bugrepository>