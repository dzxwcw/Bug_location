<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2021-4-20 01:00:00" id="23902" opendate="2021-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 3.1.3</summary>
      <description>Make flink support Hive version 3.1.3 version.</description>
      <version>1.14.0,1.15.0,1.15.1</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV310.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">docs.content.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.overview.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-7-8 01:00:00" id="28463" opendate="2022-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink SQL syntax support CREATE TABLE AS SELECT(CTAS)</summary>
      <description>Support CTAS(CREATE TABLE AS SELECT) syntaxCREATE TABLE [ IF NOT EXISTS ] table_name [ WITH ( table_properties ) ][ AS query_expression ]</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlCreateTableConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.CreateTableLikeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlCreateHiveTable.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-8-11 01:00:00" id="28492" opendate="2022-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support "ANALYZE TABLE" execution</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesCatalog.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.Date.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogTableStatistics.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataString.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataLong.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataDouble.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataDate.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataBoolean.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataBinary.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatistics.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-11 01:00:00" id="28493" opendate="2022-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document to describe "ANALYZE TABLE" syntax</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-11 01:00:00" id="28495" opendate="2022-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix typos or mistakes of Flink CEP Document in the official website</summary>
      <description>1. "how you can migrate your job from an older Flink version to Flink-1.3." -&gt; "how you can migrate your job from an older Flink version to Flink-1.13."2. "Will generate the following matches for an input sequence: C D A1 A2 A3 D A4 B. with combinations enabled: { C A1 B}, {C A1 A2 B}, {C A1 A3 B}, {C A1 A4 B}, {C A1 A2 A3 B}, {C A1 A2 A4 B}, {C A1 A3 A4 B}, {C A1 A2 A3 A4 B}" -&gt; "Will generate the following matches for an input sequence: C D A1 A2 A3 D A4 B. with combinations enabled: {C A1 B}, {C A1 A2 B}, {C A1 A3 B}, {C A1 A4 B}, {C A1 A2 A3 B}, {C A1 A2 A4 B}, {C A1 A3 A4 B}, {C A1 A2 A3 A4 B}, {C A2 B}, {C A2 A3 B}, {C A2 A4 B}, {C A2 A3 A4 B}, {C A3 B}, {C A3 A4 B}, {C A4 B}"3. "For SKIP_TO_FIRST/LAST there are two options how to handle cases when there are no elements mapped to the specified variable." -&gt; "For SKIP_TO_FIRST/LAST there are two options how to handle cases when there are no events mapped to the PatternName."</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.libs.cep.md</file>
      <file type="M">docs.content.zh.docs.libs.cep.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-7-17 01:00:00" id="28577" opendate="2022-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>1.15.1 web ui console report error about checkpoint size</summary>
      <description>1.15.11 start-cluster2 submit job: ./bin/flink run -d ./examples/streaming/TopSpeedWindowing.jar3 trigger savepoint: ./bin/flink savepoint {{{jobId} ./sp0}}4 open web ui for job and change to checkpoint tab, nothing showed.Chrome console log shows some error:{{main.a7e97c2f60a2616e.js:1 ERROR TypeError: Cannot read properties of null (reading 'checkpointed_size')    at q (253.e9e8f2b56b4981f5.js:1:607974)    at Sl (main.a7e97c2f60a2616e.js:1:186068)    at Br (main.a7e97c2f60a2616e.js:1:184696)    at N8 (main.a7e97c2f60a2616e.js:1:185128)    at Br (main.a7e97c2f60a2616e.js:1:185153)    at N8 (main.a7e97c2f60a2616e.js:1:185128)    at Br (main.a7e97c2f60a2616e.js:1:185153)    at N8 (main.a7e97c2f60a2616e.js:1:185128)    at Br (main.a7e97c2f60a2616e.js:1:185153)    at B8 (main.a7e97c2f60a2616e.js:1:191872)}}   </description>
      <version>1.15.1</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-12-21 01:00:00" id="28617" opendate="2022-7-21 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support stop job statement in SqlGatewayService</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestUtils.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.utils.Constants.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-7-21 01:00:00" id="28636" opendate="2022-7-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add utility to test POJO compliance</summary>
      <description>Users should be encouraged to eagerly verify that their POJOs satisfy all the requirements that Flink imposes, however we provide no convenient way to test that.They currently have to resort to something like below, which isn't obvious at all:TypeSerializer&lt;Event&gt; eventSerializer = TypeInformation.of(Event.class).createSerializer(new ExecutionConfig());assertThat(eventSerializer).isInstanceOf(PojoSerializer.class);</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.serialization.types.serialization.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.serialization.types.serialization.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-10-19 01:00:00" id="2872" opendate="2015-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the documentation for Scala part to add readFileOfPrimitives</summary>
      <description>Currently the Scala part of the ExecutionEnvironment missing readFileOfPrimitives to create Dataset from file for primitive types.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-9-29 01:00:00" id="28738" opendate="2022-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[doc] Add a user doc about the correctness for non-deterministic updates</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.concepts.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-1 01:00:00" id="28765" opendate="2022-8-1 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create a doc for protobuf format</summary>
      <description>After this feature has been done https://issues.apache.org/jira/browse/FLINK-18202, we should write a doc to introduce how to use the protobuf format in SQL. </description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.connectors.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-8-4 01:00:00" id="28801" opendate="2022-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade OSS SDK Version</summary>
      <description/>
      <version>1.15.1</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-8-5 01:00:00" id="28841" opendate="2022-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document dynamic property support for startup scripts</summary>
      <description>The support for dynamic properties in startup scripts isn't documented anywhere.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.docker.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-8-7 01:00:00" id="28849" opendate="2022-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add it cases with lookup hint using new lookup function api for lookup join</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AsyncLookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.LookupJoinUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-8-8 01:00:00" id="28865" opendate="2022-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add updated Print sink for new interfaces</summary>
      <description>The built-in print sink still uses the old sink interfaces. Add a new implementation for the new sink interfaces.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.util.PrintSinkOutputWriter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-10-15 01:00:00" id="28960" opendate="2022-8-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar throws java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlElement</summary>
      <description>Unknown HK2 failure detected:MultiException stack 1 of 2java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlElement at org.apache.pulsar.shade.com.fasterxml.jackson.module.jaxb.JaxbAnnotationIntrospector.&lt;init&gt;(JaxbAnnotationIntrospector.java:137) at org.apache.pulsar.shade.com.fasterxml.jackson.module.jaxb.JaxbAnnotationIntrospector.&lt;init&gt;(JaxbAnnotationIntrospector.java:124) at org.apache.pulsar.shade.com.fasterxml.jackson.module.jaxb.JaxbAnnotationIntrospector.&lt;init&gt;(JaxbAnnotationIntrospector.java:116) at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490) at java.base/java.lang.Class.newInstance(Class.java:584)</description>
      <version>1.15.1,1.14.6</version>
      <fixedVersion>1.17.0,1.15.3,1.14.7,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-9-15 01:00:00" id="28971" opendate="2022-8-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adds user documentation for the new LOOKUP hint</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.hints.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.hints.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-15 01:00:00" id="28972" opendate="2022-8-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add methods of StartCursor and StopCursor to align the Java</summary>
      <description>Add fromPublishTime in the StartCursor classAdd afterEventTime and afterPublishTime in the StopCursor class</description>
      <version>1.14.5,1.15.1,1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.connectors.tests.test.pulsar.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.pulsar.py</file>
      <file type="M">docs.content.docs.connectors.datastream.pulsar.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.pulsar.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-12-16 01:00:00" id="28988" opendate="2022-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect result for filter after temporal join</summary>
      <description>The following code can reproduce the case public class TemporalJoinSQLExample1 { public static void main(String[] args) throws Exception { // set up the Java DataStream API final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // set up the Java Table API final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); final DataStreamSource&lt;Tuple3&lt;Integer, String, Instant&gt;&gt; ds = env.fromElements( new Tuple3&lt;&gt;(0, "online", Instant.ofEpochMilli(0)), new Tuple3&lt;&gt;(0, "offline", Instant.ofEpochMilli(10)), new Tuple3&lt;&gt;(0, "online", Instant.ofEpochMilli(20))); final Table table = tableEnv.fromDataStream( ds, Schema.newBuilder() .column("f0", DataTypes.INT()) .column("f1", DataTypes.STRING()) .column("f2", DataTypes.TIMESTAMP_LTZ(3)) .watermark("f2", "f2 - INTERVAL '2' SECONDS") .build()) .as("id", "state", "ts"); tableEnv.createTemporaryView("source_table", table); final Table dedupeTable = tableEnv.sqlQuery( "SELECT * FROM (" + " SELECT *, ROW_NUMBER() OVER (PARTITION BY id ORDER BY ts DESC) AS row_num FROM source_table" + ") WHERE row_num = 1"); tableEnv.createTemporaryView("versioned_table", dedupeTable); DataStreamSource&lt;Tuple2&lt;Integer, Instant&gt;&gt; event = env.fromElements( new Tuple2&lt;&gt;(0, Instant.ofEpochMilli(0)), new Tuple2&lt;&gt;(0, Instant.ofEpochMilli(5)), new Tuple2&lt;&gt;(0, Instant.ofEpochMilli(10)), new Tuple2&lt;&gt;(0, Instant.ofEpochMilli(15)), new Tuple2&lt;&gt;(0, Instant.ofEpochMilli(20)), new Tuple2&lt;&gt;(0, Instant.ofEpochMilli(25))); final Table eventTable = tableEnv.fromDataStream( event, Schema.newBuilder() .column("f0", DataTypes.INT()) .column("f1", DataTypes.TIMESTAMP_LTZ(3)) .watermark("f1", "f1 - INTERVAL '2' SECONDS") .build()) .as("id", "ts"); tableEnv.createTemporaryView("event_table", eventTable); final Table result = tableEnv.sqlQuery( "SELECT * FROM event_table" + " LEFT JOIN versioned_table FOR SYSTEM_TIME AS OF event_table.ts" + " ON event_table.id = versioned_table.id"); result.execute().print(); result.filter($("state").isEqual("online")).execute().print(); }}  The result of temporal join is the following:op         id                     ts        id0                         state                    ts0             row_num+I          01970-01-01 08:00:00.000          0                        online1970-01-01 08:00:00.000                   1+I          01970-01-01 08:00:00.005          0                        online1970-01-01 08:00:00.000                   1+I          01970-01-01 08:00:00.010          0                       offline1970-01-01 08:00:00.010                   1+I          01970-01-01 08:00:00.015          0                       offline1970-01-01 08:00:00.010                   1+I          01970-01-01 08:00:00.020          0                        online1970-01-01 08:00:00.020                   1+I          01970-01-01 08:00:00.025          0                        online1970-01-01 08:00:00.020                   1 After filtering with predicate state = 'online', I expect only the two rows with state offline will be filtered out. But I got the following result:op         id                     ts        id0                         state                    ts0             row_num+I          01970-01-01 08:00:00.020          0                        online1970-01-01 08:00:00.020                   1+I          01970-01-01 08:00:00.025          0                        online1970-01-01 08:00:00.020                   1    </description>
      <version>1.15.1</version>
      <fixedVersion>1.17.0,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.WatermarkAssignerChangelogNormalizeTransposeRuleTest.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-22 01:00:00" id="29062" opendate="2022-8-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Protobuf-plugin should download protoc via maven on flink-protobuf module</summary>
      <description>&amp;#91;ERROR&amp;#93; Failed to execute goal com.github.os72:protoc-jar-maven-plugin:3.11.4:run (default) on project flink-protobuf: Error extracting protoc for version 3.21.2: Unsupported platform: protoc-3.21.2-osx-x86_64.exe -&gt; &amp;#91;Help 1&amp;#93;This issue is similar to FLINK-23661 but on the different module.</description>
      <version>1.15.1</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-protobuf.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-10-5 01:00:00" id="29514" opendate="2022-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Minikdc to v3.2.4</summary>
      <description>Bump Minikdc to v3.2.4 to remove false positive scans on CVEs like CVE-2021-29425 and CVE-2020-15250</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>