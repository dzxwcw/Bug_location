<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2015-4-17 01:00:00" id="1902" opendate="2015-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web interface reports false (the default) jobmanager.rpc.port on YARN</summary>
      <description>Running Flink as YARN session mode I was completely confused by the web interface reporting a false jobmanager.rpc.port (the default).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMaster.scala</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-5-20 01:00:00" id="22373" opendate="2021-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Flink 1.13 release notes</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content..index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-4 01:00:00" id="22563" opendate="2021-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add migration guide for new StateBackend interfaces</summary>
      <description/>
      <version>1.14.0,1.13.1</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-20 01:00:00" id="22722" opendate="2021-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation for Kafka New Source</summary>
      <description>Documentation describing the usage of Kafka FLIP-27 new source is required in Flink documentations.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-7-28 01:00:00" id="22802" opendate="2021-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Fabric8 Kubernetes Client to &gt;= 5.X</summary>
      <description>Hi All, Currently, Flink is using version 4.9.2 of the Fabric8 Kubernetes Client which does not support new versions of the Kubernetes API such as 1.19 or 1.20 as pointed out by their Compatibility Matrix which is support in 5.4.0 or above.As far as I have seen in the Flink documentation, Flink supports `Kubernetes &gt;= 1.9.` but due to this dependency, it might not be the case. Is there a plan to update this dependency?What is the plan moving forwards when new versions of Kubernetes are released?I am raising this because I have been testing Flink HA Session Cluster on Kubernetes 1.19 and 1.20 and I have encountered some frequent errors that force the JM pods to restart or even result in an unrecoverable state.Thanks!</description>
      <version>1.13.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.MixedDispatcher.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClientTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.TestingFlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformerITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPodsWatcherTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionAndRetrievalITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase.java</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.AbstractKubernetesWatcher.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.KubernetesSharedWatcher.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback-session.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback-console.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-session.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-console.properties</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-6-31 01:00:00" id="22815" opendate="2021-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable unaligned checkpoints for broadcast partitioning</summary>
      <description>Broadcast partitioning can not work with unaligned checkpointing. Thereare no guarantees that records are consumed at the same rate in allchannels. This can result in some tasks applying state changescorresponding to a certain broadcasted event while others don't. In turnupon restore, it may lead to an inconsistent state.</description>
      <version>1.13.1,1.12.4</version>
      <fixedVersion>1.11.4,1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.BroadcastPartitioner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.SubtaskStateMapperTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.SubtaskStateMapper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-1 01:00:00" id="22824" opendate="2021-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop usages of legacy planner in Kafka modules</summary>
      <description>Remove references to flink-table-planner in the Kafka modules.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-6-6 01:00:00" id="22886" opendate="2021-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thread leak in RocksDBStateUploader</summary>
      <description>ExecutorService in RocksDBStateUploader is not shut down, which may leak thread when tasks fail.BTW, we should name the thread group in ExecutorService, otherwise what we see in the stack, is a lot of threads named with pool-m-thread-n like this: </description>
      <version>1.11.3,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateDataTransfer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-6 01:00:00" id="22890" opendate="2021-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Few tests fail in HiveTableSinkITCase</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18692&amp;view=logs&amp;j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&amp;t=420bd9ec-164e-562e-8947-0dacde3cec91&amp;l=23189Jun 05 01:22:13 [ERROR] Errors: Jun 05 01:22:13 [ERROR] HiveTableSinkITCase.testBatchAppend:138 » Validation Could not execute CREATE ...Jun 05 01:22:13 [ERROR] HiveTableSinkITCase.testDefaultSerPartStreamingWrite:156-&gt;testStreamingWrite:494 » ValidationJun 05 01:22:13 [ERROR] HiveTableSinkITCase.testHiveTableSinkWithParallelismInStreaming:100-&gt;testHiveTableSinkWithParallelismBase:108 » ValidationJun 05 01:22:13 [ERROR] HiveTableSinkITCase.testPartStreamingMrWrite:179-&gt;testStreamingWrite:423 » ValidationJun 05 01:22:13 [ERROR] HiveTableSinkITCase.testStreamingSinkWithTimestampLtzWatermark:360-&gt;fetchRows:384 » TestTimedOut</description>
      <version>1.14.0,1.13.1</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReaderITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-7 01:00:00" id="22906" opendate="2021-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add build time to Flink documentation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content..index.md</file>
      <file type="M">docs.content.zh..index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-8 01:00:00" id="22931" opendate="2021-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate to flink-shaded-force-shading</summary>
      <description>Migrate to flink-shaded-force-shading, allowing us to drop force-shading removing an annoying bit of our build setup.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.update.branch.version.sh</file>
      <file type="M">tools.releasing.create.release.branch.sh</file>
      <file type="M">tools.force-shading.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-9 01:00:00" id="22934" opendate="2021-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add instructions for using the " &amp;#39; " escape syntax of SQL client</summary>
      <description>FLINK-22921</description>
      <version>1.13.0,1.13.1</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-10 01:00:00" id="22957" opendate="2021-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rank TTL should use enableTimeToLive of state instead of timer</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.TopNFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyFirstNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyFirstNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AbstractTopNFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-10 01:00:00" id="22963" opendate="2021-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The description of taskmanager.memory.task.heap.size in the official document is incorrect</summary>
      <description>When I studied the memory model of TaskManager, I found that there is a problem in the official document, which is the description of taskmanager.memory.task.heap.size is incorrect.According to the official memory model, I think the correct description should be that task Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for tasks. If not specified, it will be derived as Total Flink Memory minus Framework Heap Memory, Framework Off-Heap Heap Memory, Task Off-Heap Memory, Managed Memory and Network Memory.However, in the official document, the Framework Off-Heap Heap Memory should be subtracted.</description>
      <version>1.14.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.memory.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.memory.section.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-10 01:00:00" id="22964" opendate="2021-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Connector-base exposes dependency to flink-core.</summary>
      <description>Connectors get shaded into the user jar and as such should contain no unnecessary dependencies to flink. However, connector-base is exposing `flink-core` which then by default gets shaded into the user jar. Except for 6MB of extra size, the dependency also causes class loading issues, when `classloader.parent-first-patterns` does not include `o.a.f`.Fix is to make `flink-core` provided in `connector-base`.</description>
      <version>1.14.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-7-15 01:00:00" id="22985" opendate="2021-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NullPointerException when cast string literal to date or time</summary>
      <description>sql:CREATE TABLE source_table( id INT, score INT, address STRING, create_time TIME, create_date DATE, create_timestamp TIMESTAMP) WITH ( 'connector' = 'datagen' );CREATE TABLE console_table( create_time TIME, create_date DATE, create_timestamp TIMESTAMP) WITH ( 'connector' = 'print' );INSERT INTO console_tableSELECT CASE WHEN A.create_time IS NULL OR A.create_time = '' THEN CURRENT_TIME ELSE A.create_time END AS create_time, CASE WHEN A.create_date IS NULL OR A.create_date = '' THEN CURRENT_DATE ELSE A.create_date END AS create_date, CASE WHEN A.create_timestamp IS NULL OR A.create_timestamp = '' THEN CURRENT_TIMESTAMP ELSE A.create_timestamp END AS create_timestampFROM source_table A;exception:java.lang.RuntimeException: Could not instantiate generated class 'StreamExecCalc$23'java.lang.RuntimeException: Could not instantiate generated class 'StreamExecCalc$23' at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:66) at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40) at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:80) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:652) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:626) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:566) at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:181) at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:548) at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:647) at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:537) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.GeneratedConstructorAccessor27.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:64) ... 12 moreCaused by: java.lang.NullPointerException at StreamExecCalc$23.&lt;init&gt;(Unknown Source) ... 16 more</description>
      <version>1.13.1</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.validation.ScalarOperatorsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.utils.ScalarOperatorsTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-16 01:00:00" id="23004" opendate="2021-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix misleading log</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-7-22 01:00:00" id="23074" opendate="2021-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>There is a class conflict between flink-connector-hive and flink-parquet</summary>
      <description>flink-connector-hive 2.3.6 include parquet-hadoop 1.8.1 version but flink-parquet include 1.11.1.org.apache.parquet.hadoop.example.GroupWriteSupport is different.</description>
      <version>1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-6-23 01:00:00" id="23119" opendate="2021-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the issue that the exception that General Python UDAF is unsupported is not thrown in Compile Stage.</summary>
      <description/>
      <version>1.13.1,1.12.4</version>
      <fixedVersion>1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.PythonOverWindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.PythonGroupWindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalOverAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalPythonWindowAggregateRule.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  <bug fixdate="2021-7-5 01:00:00" id="23256" opendate="2021-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain string shows output of legacy planner</summary>
      <description>The output on Concepts &amp; Common API page documentation page of:table.explain()is showing the result of the legacy planner. </description>
      <version>1.13.1</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-7 01:00:00" id="23297" opendate="2021-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Protobuf to 3.17.3</summary>
      <description>In order to support compilation with ARM (e.g. Apple M1 chip), we need to bump our Protobuf dependency to version 3.17.3.</description>
      <version>1.14.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-7-8 01:00:00" id="23312" opendate="2021-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use -Dfast for building e2e tests on AZP</summary>
      <description>The "e2e" builder in Azure pipelines builds Flink again on top of what the "compile" builder is already doing. This unnecessary duplicates a couple of checks that are enough to execute once and can be skipped via providing -Dfast.On my local machine with 32GB RAM, 8 physical cores and a fast NVMe SSD, the difference is pretty big:time mvn clean install -Dscala-2.12 -DskipTests -pl flink-dist -am# -&gt; 6:40 mintime mvn clean install -Dscala-2.12 -DskipTests -Dfast -pl flink-dist -am# -&gt; 5:40 minTherefore, I'm proposing to add this parameter to the "e2e" builder's compile step.</description>
      <version>1.13.1</version>
      <fixedVersion>1.11.4,1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-13 01:00:00" id="23368" opendate="2021-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the wrong mapping of state cache in PyFlink</summary>
      <description>The details and demo are discussed inhttps://lists.apache.org/x/thread.html/r69780c67e0f1f8522df15f4420997842e9f4faceedf019d99901b1ef@%3Cuser.flink.apache.org%3E</description>
      <version>1.13.1</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-13 01:00:00" id="23369" opendate="2021-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use enums for connector options</summary>
      <description>For some connector options we should replace string usages with enum usages now that the options themselves become exposed API.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.stream.PartitionCommitTrigger.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.stream.PartitionCommitPredicate.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.FileSystemConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.read.HivePartitionFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HivePartitionFetcherContextBase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.ConsumeOrder.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaSinkSemantic.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaConnectorOptionsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.config.StartupMode.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-9 01:00:00" id="2337" opendate="2015-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multiple SLF4J bindings using Storm compatibility layer</summary>
      <description>Storm depends on logback as slf4j implemenation but Flink uses log4j. The log shows the following conflict:SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in &amp;#91;jar:file:/home/cicero/.m2/repository/ch/qos/logback/logback-classic/1.0.13/logback-classic-1.0.13.jar!/org/slf4j/impl/StaticLoggerBinder.class&amp;#93;SLF4J: Found binding in &amp;#91;jar:file:/home/cicero/.m2/repository/org/slf4j/slf4j-log4j12/1.7.7/slf4j-log4j12-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class&amp;#93;SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.Need to exclude logback from storm dependencies to fix this.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-13 01:00:00" id="23370" opendate="2021-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Propagate Boundedness of SourceFunctionProvider to Transformation</summary>
      <description>SourceFunctionProvider does currently not propagate the boundedness for StreamGraphGenerator.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-7-13 01:00:00" id="23372" opendate="2021-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable AllVerticesInSameSlotSharingGroupByDefault in DataStream batch mode</summary>
      <description>In order to unify the behavior of DataStream API and Table API batch mode, we should disable AllVerticesInSameSlotSharingGroupByDefault also in DataStream API.FLINK-20001 reverted setting this flag but without concrete arguments and the following comment: reconsider actually setting this flag in the futureAfter a offline chat with zhuzh, we should introduce this again for consistency:The goal to assign different regions to different slot sharing groups by default is to reduce waste of resources. In batch jobs, there can be one region which has data dependency on another region. And the resource computation for slots and managed memory will be affected: 1. If these regions are in the same slot sharing group, the group will require a large slot which can host tasks from both the regions. 2. In managed memory fraction computing, tasks from both regions will be considered to compete for managed memory, so each task will be assigned with a smaller managed memory fraction (FLIP-53).However, those regions will not run at the same time and results in a waste of resources. For streaming jobs, all tasks will run at the same time. So assigning them to the same slot sharing group will not result resource waste.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorExecutionModeDetectionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-14 01:00:00" id="23374" opendate="2021-7-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up logs produced by code splitter</summary>
      <description>Currently logs are full of errors like2021-07-12T20:44:24.0059271Z line 221:17 missing ';' at '('2021-07-12T20:44:24.0063922Z line 221:59 mismatched input ',' expecting ')'2021-07-12T20:44:24.0070238Z line 221:80 mismatched input ',' expecting ';'2021-07-12T20:44:24.0076791Z line 223:69 mismatched input ',' expecting ';'2021-07-12T20:44:24.0079044Z line 224:33 mismatched input ',' expecting ';'2021-07-12T20:44:24.0081798Z line 225:65 mismatched input ',' expecting ';'2021-07-12T20:44:24.0088060Z line 226:16 mismatched input ',' expecting ';'2021-07-12T20:44:24.0089917Z line 227:76 extraneous input ')' expecting ';'2021-07-12T20:44:24.0792855Z line 385:17 missing ';' at '('2021-07-12T20:44:24.0796518Z line 385:59 mismatched input ',' expecting ')'2021-07-12T20:44:24.0800736Z line 385:80 mismatched input ',' expecting ';'2021-07-12T20:44:24.0805884Z line 387:69 mismatched input ',' expecting ';'2021-07-12T20:44:24.0808360Z line 388:33 mismatched input ',' expecting ';'2021-07-12T20:44:24.0810762Z line 389:65 mismatched input ',' expecting ';'2021-07-12T20:44:24.0817301Z line 390:15 mismatched input ',' expecting ';'2021-07-12T20:44:24.0818725Z line 391:75 extraneous input ')' expecting ';'2021-07-12T20:44:24.1364946Z line 339:17 missing ';' at '('2021-07-12T20:44:24.1366054Z line 339:59 mismatched input ',' expecting ')'2021-07-12T20:44:24.1373495Z line 339:80 mismatched input ',' expecting ';'2021-07-12T20:44:24.1378822Z line 341:69 mismatched input ',' expecting ';'2021-07-12T20:44:24.1380984Z line 342:33 mismatched input ',' expecting ';'2021-07-12T20:44:24.1385438Z line 343:65 mismatched input ',' expecting ';'2021-07-12T20:44:24.1388520Z line 344:16 mismatched input ',' expecting ';'2021-07-12T20:44:24.1392687Z line 345:76 extraneous input ')' expecting ';'2021-07-12T20:44:24.1633870Z line 221:17 missing ';' at '('2021-07-12T20:44:24.1640923Z line 221:59 mismatched input ',' expecting ')'2021-07-12T20:44:24.1647307Z line 221:80 mismatched input ',' expecting ';'2021-07-12T20:44:24.1654857Z line 223:69 mismatched input ',' expecting ';'2021-07-12T20:44:24.1659580Z line 224:33 mismatched input ',' expecting ';'2021-07-12T20:44:24.1664576Z line 225:65 mismatched input ',' expecting ';'2021-07-12T20:44:24.1669345Z line 226:16 mismatched input ',' expecting ';'2021-07-12T20:44:24.1672873Z line 227:75 extraneous input ')' expecting ';'2021-07-12T20:44:24.1842542Z line 325:17 missing ';' at '('2021-07-12T20:44:24.1846374Z line 325:59 mismatched input ',' expecting ')'2021-07-12T20:44:24.1852461Z line 325:80 mismatched input ',' expecting ';'2021-07-12T20:44:24.1867352Z line 327:69 mismatched input ',' expecting ';'2021-07-12T20:44:24.1872101Z line 328:33 mismatched input ',' expecting ';'2021-07-12T20:44:24.1876855Z line 329:65 mismatched input ',' expecting ';'2021-07-12T20:44:24.1888644Z line 330:15 mismatched input ',' expecting ';'2021-07-12T20:44:24.1892271Z line 331:75 extraneous input ')' expecting ';'This is caused by the code splitter logic and should be cleaned up.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.WindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.generated.GeneratedResultFutureWrapper.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.generated.GeneratedFunctionWrapper.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.generated.GeneratedCollectorWrapper.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CodeSplitITCase.scala</file>
      <file type="M">flink-table.flink-table-code-splitter.src.main.antlr4.JavaParser.g4</file>
      <file type="M">flink-table.flink-table-code-splitter.src.main.antlr4.JavaLexer.g4</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-14 01:00:00" id="23375" opendate="2021-7-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink connector jdbc tests jar is almost empty</summary>
      <description>All the test files are missing in flink-connector-jdbc_2.12-1.13.1-tests.jarThis is contest of archive:Archive: /mnt/c/Users/maverick/.m2/repository/org/apache/flink/flink-connector-jdbc_2.12/1.13.1/flink-connector-jdbc_2.12-1.13.1-tests.jar Length Method Size Cmpr Date Time CRC-32 Name -------- ------ ------- ---- ---------- ----- -------- ---- 0 Defl:N 2 0% 2021-05-25 13:24 00000000 META-INF/services/org.apache.flink.table.factories.TableFactory 0 Defl:N 2 0% 2021-05-25 13:24 00000000 META-INF/services/org.apache.flink.table.factories.Factory 0 Defl:N 2 0% 2021-05-25 13:24 00000000 META-INF/NOTICE -------- ------- --- ------- 0 6 0% 3 files  </description>
      <version>1.13.1</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-7-15 01:00:00" id="23395" opendate="2021-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Okhttp to 3.14.9</summary>
      <description>We currently use 3 different version of Okhttp, which are partially lagging behind the last 3.X version by quite a bit.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-metrics.flink-metrics-datadog.pom.xml</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-20 01:00:00" id="23438" opendate="2021-7-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump httpclient from 4.5.3 and 4.5.9 to 4.5.13</summary>
      <description>Flink is still using org.apache.httpcomponents.httpclient:4.5.3 and org.apache.httpcomponents.httpclient:4.5.9. Those versions are impacted by CVE-2020-13956 (though Flink is not impacted by it). The latest available version is 4.5.13</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-test.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-twitter.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-23 01:00:00" id="23487" opendate="2021-7-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>IRSA doesn&amp;#39;t work with S3</summary>
      <description>Using IRSA (IAM Role for Service Account) currently doesn't work with s3 (both presto and hadoop) and fails with Access Denied. This has been brought up previously in FLINK-18676, but seems to be broken still or again.We have tested a patch which updates the version for AWS and presto-hive in Flink, and doing so we successfully verified that it works with s3-presto. However, this didn't yet fix s3-hadoop. The patch will be posted in the comments.Another curious finding was FLINK-17859 which states that presto-hive had been updated. However, this seems to be the case neither in the posted Flink 1.8 link nor current master.</description>
      <version>1.13.1</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.licenses.LICENSE-aopalliance</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-26 01:00:00" id="23493" opendate="2021-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>python tests hang on Azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20898&amp;view=logs&amp;j=821b528f-1eed-5598-a3b4-7f748b13f261&amp;t=4fad9527-b9a5-5015-1b70-8356e5c91490&amp;l=22829</description>
      <version>1.14.0,1.13.1,1.12.4,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.boot.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-26 01:00:00" id="23496" opendate="2021-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maven snapshot build does not use MAVEN_GLOBAL_OPTIONS</summary>
      <description>On CI we have a collection of useful settings for maven that we generally use everywhere, but the deployment of maven snapshot artifacts currently doesn't.This leads to some duplication and noisy logs.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.maven-utils.sh</file>
      <file type="M">tools.azure-pipelines.build-nightly-dist.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-27 01:00:00" id="23517" opendate="2021-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add messages to bannedDependencies enforcer rules</summary>
      <description>Add messages to all bannedDependencies rules to properly inform developers about the problem and ways to mitigate it.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-30 01:00:00" id="23561" opendate="2021-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Detail the container completed message</summary>
      <description>Use the ContainerStatus to detailed the container completed reason, and thus users can explicitly know why the container completed.</description>
      <version>1.13.1</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManagerDriver.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerDriverTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-30 01:00:00" id="23562" opendate="2021-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update CI docker image to latest java version (1.8.0_292)</summary>
      <description>The java version we are using on our CI is outdated (1.8.0_282 vs 1.8.0_292). The latest java version has TLSv1 disabled, which makes the KubernetesClusterDescriptorTest fail.This will be fixed by FLINK-22802.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-4 01:00:00" id="23625" opendate="2021-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate all TaskManagerJobMG instantiations to factory method</summary>
      <description>Modify all existing usages of the TaskManagerJobMG constructor to use runtime Apis, for consistency and to make constructor changes easier.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.MetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.InternalOperatorGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskManagerJobMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.test.java.org.apache.flink.metrics.prometheus.PrometheusReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.test.java.org.apache.flink.metrics.prometheus.PrometheusReporterTaskScopeTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-4 01:00:00" id="23626" opendate="2021-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate all TaskMG instantiations to factory method</summary>
      <description>Modify all existing usages of the TaskMG constructor to use runtime Apis, for consistency and to make constructor changes easier.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainedOperatorsMetricTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.OperatorGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.MetricGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.test.java.org.apache.flink.metrics.prometheus.PrometheusReporterTaskScopeTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-5 01:00:00" id="23639" opendate="2021-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate Table API to new KafkaSink</summary>
      <description>With the KafkaSink ported to FLIP-143 we should also adapt the Table API to leverage the new KafkaSink</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaConnectorOptionsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.BufferedUpsertSinkFunction.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-9 01:00:00" id="23692" opendate="2021-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify text on "Cancel Job" confirmation buttons</summary>
      <description>When prompting a user to confirm a job cancellation currently we present the following popup (see screenshots). These text descriptions are the default in the html layer, however it happens to be misleading when the action to be confirmed is cancel.Multiple users reported that this behavior is confusing and as it is only a display issue its scope is very limited. </description>
      <version>1.13.1,1.12.5</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-12-9 01:00:00" id="24232" opendate="2021-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Archiving of suspended jobs prevents breaks subsequent archive attempts</summary>
      <description>To archive a job we write a file that uses the job ID as the name. Since suspended jobs are handled like other terminal jobs they are also being archived.When that job then later resumes any attempt to archive the job on termination will fail because an archive already exists.The simplest option is to add a suffix if an archive already exists, like "_1".</description>
      <version>1.14.0,1.13.1,1.12.5</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.AbstractDispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-31 01:00:00" id="2454" opendate="2015-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Travis file to run build using Java7</summary>
      <description>Update Travis file to run build using Java7</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-3 01:00:00" id="24755" opendate="2021-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document solution for &amp;#39;sun.misc doesn&amp;#39;t exist&amp;#39; error with JDK 11</summary>
      <description>After FLINK-24634 , the default target is jdk8. When I running tests from IDE, some compiler error come up. I revert the change and test again, It can work now. It's also ok to build from command.I can not quite figure it out , can you give some inputs chesnay</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.flinkDev.ide.setup.md</file>
      <file type="M">docs.content.zh.docs.flinkDev.ide.setup.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-4 01:00:00" id="24762" opendate="2021-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove agg function: INCR_SUM</summary>
      <description>The removal of INCR_SUM should be completed in FLINK-13529, but that PR only removes function definition of INCR_SUM.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.IncrSumWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.IncrSumAggFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-23 01:00:00" id="2565" opendate="2015-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support primitive arrays as keys</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.util.CollectionDataSets.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.GroupReduceITCase.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.GroupingTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-3 01:00:00" id="26460" opendate="2022-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Unsupported type when convertTypeToSpec: MAP</summary>
      <description>CREATE TABLE zm_test ( `a` BIGINT, `m` MAP&lt;STRING,BIGINT&gt;);if we insert into zm_test useINSERT INTO zm_test(`a`) SELECT `a` FROM MyTable;then will throw ExceptionUnsupported type when convertTypeToSpec: MAPwe must useINSERT INTO zm_test SELECT `a`, cast(null AS MAP&lt;STRING,BIGINT&gt;) FROM MyTable;</description>
      <version>1.13.1,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-5-19 01:00:00" id="27297" opendate="2022-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the StreamExecutionEnvironment#getExecutionEnvironment(Configuration) method in PyFlink</summary>
      <description>StreamExecutionEnvironment#getExecutionEnvironment(Configuration) method has been added in Java side since release-1.12, we need to add this method in Python too</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.common.tests.test.execution.config.py</file>
      <file type="M">docs.content.docs.dev.python.python.config.md</file>
      <file type="M">docs.content.zh.docs.dev.python.python.config.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-4-19 01:00:00" id="27308" opendate="2022-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the Hadoop implementation for filesystems to 3.3.2</summary>
      <description>Flink currently uses Hadoop version 3.2.2 for the Flink filesystem implementations. Upgrading this to version 3.3.2 would provide users the features listed in HADOOP-17566</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.java.org.apache.flink.fs.s3hadoop.HadoopS3AccessHelper.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.pom.xml</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-25 01:00:00" id="27386" opendate="2022-4-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>throw NPE if multi MAPJOIN hint union all</summary>
      <description>We can reproduce through a UTAdd test case in HiveDialectITCase@Testpublic void testHiveMultiMapJoinUnionAll() { tableEnv.executeSql("create table t1 (id bigint, name string)"); tableEnv.executeSql("create table t2 (id bigint, name string)"); tableEnv.executeSql("select /*+ mapjoin(t2) */ t1.id from t1 join t2 on t1.id = t2.id union all select /*+ mapjoin(t2) */ t1.id from t1 join t2 on t1.name = t2.name");}</description>
      <version>1.13.1,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveASTParseUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-7-25 01:00:00" id="27387" opendate="2022-4-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Insert Multi-Table</summary>
      <description>We can reproduce through a UTAdd test case in HiveDialectITCase@Testpublic void testInsertMultiTable() { tableEnv.executeSql("create table t1 (id bigint, name string)"); tableEnv.executeSql("create table t2 (id bigint, name string)"); tableEnv.executeSql("create table t3 (id bigint, name string, age int)"); tableEnv.executeSql("from (select id, name, age from t3) t " + "insert overwrite table t1 select id, name where age &lt; 20 " + "insert overwrite table t2 select id, name where age &gt; 20 ");} This is a very common case for batch.</description>
      <version>1.13.1,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
    </fixedFiles>
  </bug>
  
  
</bugrepository>