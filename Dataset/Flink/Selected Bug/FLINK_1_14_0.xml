<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2015-3-13 01:00:00" id="1696" opendate="2015-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add multiple linear regression to ML library</summary>
      <description>Add multiple linear regression to ML library.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">docs..layouts.default.html</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Matrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
      <file type="M">flink-examples.flink-scala-examples.src.main.scala.org.apache.flink.examples.scala.ml.LinearRegression.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-17 01:00:00" id="20188" opendate="2020-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation for new File Source</summary>
      <description/>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.execution.mode.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.content.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.text.files.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.parquet.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.azure.table.storage.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.execution.mode.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.file.sink.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-30 01:00:00" id="20423" opendate="2020-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove usage of {{site.baseurl}} from markdown files</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.execution.configuration.zh.md</file>
      <file type="M">docs.dev.execution.configuration.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.sql.gettingStarted.zh.md</file>
      <file type="M">docs.dev.table.sql.gettingStarted.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-3-19 01:00:00" id="21885" opendate="2021-3-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minor Typo Fix in JDBC Connector Documentation</summary>
      <description>There currently exists a minor typo within the JDBC Datastream documentation:A JDBC batch is executed as soon as one of the following condition is true: the configured batch interval time is elapsed the maximum batch size is reached a Flink checkpoint has startedSince it's plural, condition should be changed to conditions.Â </description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.jdbc.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-20 01:00:00" id="21887" opendate="2021-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add show views test in CliClientITCase</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.catalog.database.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutorBuilder.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-20 01:00:00" id="21888" opendate="2021-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maintain our own ASTNode class</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.SelectClauseASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.IdentifiersASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserStorageFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserBaseSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTParseUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTParseDriver.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTLexer.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTHintParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.FromClauseASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserContext.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserAuthorizationParseUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserASTBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.desc.HiveParserCreateViewDesc.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.desc.CreateTableASDesc.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-3-31 01:00:00" id="22070" opendate="2021-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support FileSink in PyFlink DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-python.pyflink.common.serialization.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-11 01:00:00" id="22198" opendate="2021-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaTableITCase hang.</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16287&amp;view=logs&amp;j=c5f0071e-1851-543e-9a45-9ac140befc32&amp;t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&amp;l=6625There is no any artifacts.</description>
      <version>1.14.0,1.12.4</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-12 01:00:00" id="22243" opendate="2021-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reactive Mode parallelism changes are not shown in the job graph visualization in the UI</summary>
      <description>As reported here FLINK-22134, the parallelism in the visual job graph on top of the detail page is not in sync with the parallelism listed in the task list below, when reactive mode causes a parallelism change.</description>
      <version>1.13.0,1.14.0</version>
      <fixedVersion>1.16.0,1.17.0,1.15.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.ReactiveModeITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateTrackingMockExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.CreatingExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.jsonplan.JsonPlanGenerator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-5-22 01:00:00" id="22407" opendate="2021-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump log4j to 2.14.1</summary>
      <description>Flink is currently relying on log4j 2.12.1 .Unfortunately, this vesrion has a bug related to json layout that prevents a user from adding additional log fields, as reported here https://issues.apache.org/jira/browse/LOG4J2-2652Â as well as here: https://stackoverflow.com/questions/57003440/why-is-log4j2-jsonlayout-keyvaluepair-printing-empty-logevent-messagesThe problem is fixed in Log4j 2.13.1.Is there a good reason to keep Log4j 2.12.1, or can we upgrade?As an illustration, the presence ofÂ  additional1 in the snippet below: rootLogger.level = INFOrootLogger.appenderRef.console.ref = LogConsole appender.console.name = LogConsoleappender.console.type = CONSOLEappender.console.layout.type = JsonLayoutappender.console.layout.complete = falseappender.console.layout.compact = trueappender.console.layout.eventEol = trueappender.console.layout.properties = trueappender.console.layout.includeStacktrace=trueappender.console.layout.stacktraceAsString=true appender.console.layout.additional1.type=KeyValuePairappender.console.layout.additional1.key=timestampappender.console.layout.additional1.value=$${date:yyyy-MM-dd'T'HH:mm:ss.SSSZ}Â  leads to missing fields in the resulting logs, e.g.:{"logEvent":"Recover all persisted job graphs.","timestamp":"2021-04-21T16:50:31.722+0000"}{"logEvent":"Successfully recovered 0 persisted job graphs.","timestamp":"2021-04-21T16:50:31.723+0000"}{"logEvent":"Starting the SlotManager.","timestamp":"2021-04-21T16:50:31.732+0000"}{"logEvent":"Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .","timestamp":"2021-04-21T16:50:31.822+0000"}Â  Removing the additional1 resolves the issue and yield json logs containing all expected fields:{"thread":"cluster-io-thread-1","level":"INFO","loggerName":"org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess","message":"Successfully recovered 0 persisted job graphs.","endOfBatch":false,"loggerFqcn":"org.apache.logging.slf4j.Log4jLogger","instant":{"epochSecond":1619080838,"nanoOfSecond":216868000},"threadId":48,"contextMap":{},"threadPriority":5}{"thread":"flink-akka.actor.default-dispatcher-3","level":"INFO","loggerName":"org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl","message":"Starting the SlotManager.","endOfBatch":false,"loggerFqcn":"org.apache.logging.slf4j.Log4jLogger","instant":{"epochSecond":1619080838,"nanoOfSecond":313130000},"threadId":19,"contextMap":{},"threadPriority":5}{"thread":"cluster-io-thread-1","level":"INFO","loggerName":"org.apache.flink.runtime.rpc.akka.AkkaRpcService","message":"Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .","endOfBatch":false,"loggerFqcn":"org.apache.logging.slf4j.Log4jLogger","instant":{"epochSecond":1619080838,"nanoOfSecond":408714000},"threadId":48,"contextMap":{},"threadPriority":5}Â Â Â </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-27 01:00:00" id="22487" opendate="2021-4-27 00:00:00" resolution="Done">
    <buginformation>
      <summary>Support `print` to print logs in PyFlink</summary>
      <description>Currently, if users want to print logs, they need to use logging module.@udf(result_type=DataTypes.BIGINT())def add(i, j): import logging logging.info("debug") return i + jIt will be more convenient to use `print` to print logs.Unable to find source-code formatter for language: python. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yaml@udf(result_type=DataTypes.BIGINT())def add(i, j): print("debug") return i + j</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
      <file type="M">docs.content.docs.dev.python.debugging.md</file>
      <file type="M">docs.content.zh.docs.dev.python.debugging.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-4 01:00:00" id="22563" opendate="2021-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add migration guide for new StateBackend interfaces</summary>
      <description/>
      <version>1.14.0,1.13.1</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2021-5-18 01:00:00" id="22694" opendate="2021-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor TPCH end to end tests</summary>
      <description>Currently the TPCH test use yaml to init the environment. However, it's suggested to use sql file to init right now.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpch.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q9.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q8.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q7.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q6.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q5.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q4.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q3.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q22.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q21.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q20.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q2.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q19.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q18.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q17.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q16.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q15.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q14.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q13.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q12.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q11.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q10.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q1.yaml</file>
      <file type="M">flink-end-to-end-tests.flink-tpch-test.src.main.java.org.apache.flink.table.tpch.TpchResultComparator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-18 01:00:00" id="22696" opendate="2021-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable Confluent Schema Registry e2e Test on jdk 11</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-18 01:00:00" id="22697" opendate="2021-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up examples to not use legacy planner anymore</summary>
      <description>Clean up the `flnk-examples-table` module to not reference the legacy planner anymore.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.basics.StreamSQLExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.basics.StreamSQLExample.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.basics.TPCHQuery3Table.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.basics.WordCountTable.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.basics.WordCountTable.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.basics.WordCountSQL.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.basics.WordCountSQL.java</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-20 01:00:00" id="22722" opendate="2021-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation for Kafka New Source</summary>
      <description>Documentation describing the usage of Kafka FLIP-27 new source is required in Flink documentations.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-21 01:00:00" id="22745" opendate="2021-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MesosWorkerStore is started with an illegal namespace</summary>
      <description>The MesosWorkerStore is started with an illegal namespace because of FLINK-22636.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperUtilityFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-26 01:00:00" id="22782" opendate="2021-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy planner from Chinese docs</summary>
      <description>FLINK-22740 should also be applied to Chinese docs.It should remove: Remove reference ofÂ useBlink/LegacyPlanner RemoveÂ DataSet Remove legacy planner mentioning</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.docs.dev.python.python.config.md</file>
      <file type="M">docs.content.zh.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.explain.md</file>
      <file type="M">docs.content.zh.docs.dev.table.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.modules.md</file>
      <file type="M">docs.content.zh.docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.catalogs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.python.table.api.connectors.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.operations.row.based.operations.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.intro.to.table.api.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.api.tutorial.md</file>
      <file type="M">docs.content.zh.docs.dev.python.python.config.md</file>
      <file type="M">docs.content.zh.docs.dev.python.dependency.management.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.jdbc.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.dialect.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-26 01:00:00" id="22784" opendate="2021-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen tests broken due to change in zNode layout</summary>
      <description/>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.client.clj</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-28 01:00:00" id="22794" opendate="2021-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade JUnit Vintage for flink-sql-parser and flink-sql-hive-parser</summary>
      <description>In FLINK-22778,set junit.version:4.13.2,it cause flink-sql-parser test work failed.and now junit-jupiter-engine:5.5.2;junit-vintage-engine:5.5.2.org.junit.vintage.engine.JUnit4VersionCheck#parseVersion cannot identify the version number correctly,e.g 4.13.2.so we can update sql-parser and sql-hive-parser to 5.7.0 at least.below is release note:JUnit VintageBug Fixes The Vintage engine no longer fails when resolving aÂ MethodSelectorÂ for methods of test classes that cannot be found via reflection. This allows selecting Spock feature methods by their source code name even though they have a generated method name in the bytecode.New Features and Improvements The internalÂ JUnit4VersionCheckÂ classâââwhich verifies that a supported version of JUnit 4 is on the classpathââânow implements a lenient version ID parsing algorithm in order to support custom version ID formats such asÂ 4.12.0,Â 4.12-patch_1, etc.Â Â </description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser-hive.pom.xml</file>
      <file type="M">tools.ci.stage.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-6-1 01:00:00" id="22822" opendate="2021-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop usages of legacy planner in JDBC module</summary>
      <description>Remove references to flink-table-planner in the JDBC module.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-2 01:00:00" id="22844" opendate="2021-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add doc to introduce ExplainDetails for EXPLAIN sytnax</summary>
      <description>Link toÂ FLINK-20562,add doc to introduct this new sytax.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.explain.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.explain.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-6-2 01:00:00" id="22856" opendate="2021-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move our Azure pipelines away from Ubuntu 16.04 by September</summary>
      <description>Azure won't support Ubuntu 16.04 starting from October, hence we need to migrate to a newer ubuntu version.We should do this at a time when the builds are relatively stable to be able to clearly identify issues relating to the version upgrade. Also, we shouldn't do this before a feature freeze</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-python-wheels.yml</file>
      <file type="M">tools.azure-pipelines.build-nightly-dist.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.kubernetes.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.docker.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-6-4 01:00:00" id="22878" opendate="2021-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow prefix syntax for ConfigOption.mapType</summary>
      <description>The current factory design does not allow placeholder options in EncodingFormatFactory or DecodingFormatFactory.The past has shown that placeholder options are used at a couple of locations.See FLINK-22475 or KafkaOptions#PROPERTIES_PREFIX.We should think about adding an additional functionality to ReadableConfig or a special ConfigOption type to finally solve this problem. This could also be useful for FLIP-129. And would solve the current shortcomings for Confluent Avro registry.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigurationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigurationUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.Configuration.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-7-6 01:00:00" id="22890" opendate="2021-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Few tests fail in HiveTableSinkITCase</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18692&amp;view=logs&amp;j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&amp;t=420bd9ec-164e-562e-8947-0dacde3cec91&amp;l=23189Jun 05 01:22:13 [ERROR] Errors: Jun 05 01:22:13 [ERROR] HiveTableSinkITCase.testBatchAppend:138 Â» Validation Could not execute CREATE ...Jun 05 01:22:13 [ERROR] HiveTableSinkITCase.testDefaultSerPartStreamingWrite:156-&gt;testStreamingWrite:494 Â» ValidationJun 05 01:22:13 [ERROR] HiveTableSinkITCase.testHiveTableSinkWithParallelismInStreaming:100-&gt;testHiveTableSinkWithParallelismBase:108 Â» ValidationJun 05 01:22:13 [ERROR] HiveTableSinkITCase.testPartStreamingMrWrite:179-&gt;testStreamingWrite:423 Â» ValidationJun 05 01:22:13 [ERROR] HiveTableSinkITCase.testStreamingSinkWithTimestampLtzWatermark:360-&gt;fetchRows:384 Â» TestTimedOut</description>
      <version>1.14.0,1.13.1</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReaderITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-6 01:00:00" id="22893" opendate="2021-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Leader retrieval fails with NoNodeException</summary>
      <description>The NodeCache used by the LeaderElection-/-RetrievalDrivers ensures that parents to the observed node exists by regularly issuing mkdir calls. This operation can fail if concurrently the HA data is being cleaned up, which results in curator throwing an unhandled exception which crashes the TM.https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18700&amp;view=logs&amp;j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&amp;t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc&amp;l=4382</description>
      <version>1.11.1,1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-8 01:00:00" id="22920" opendate="2021-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Guava version conflict in flink-format module</summary>
      <description>In the Flink-ORC and Flink-Parquet modules, The hadoop-common dependency contains the 11.0.2 version of guava, which conflicts with the 29.0-jre version required by the flink-table-planner-blink module. We should exclude guava from the hadoop-common dependency. Otherwise, running the unit test through the IDE throws a NoClassDefFoundError</description>
      <version>1.14.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-10 01:00:00" id="22963" opendate="2021-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The description of taskmanager.memory.task.heap.size in the official document is incorrect</summary>
      <description>When I studied the memory model of TaskManager, I found that there is a problem in the official document, which is the description of taskmanager.memory.task.heap.size is incorrect.According to the official memory model, I think the correct description should be that task Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for tasks. If not specified, it will be derived as Total Flink Memory minus Framework Heap Memory, Framework Off-Heap Heap Memory, Task Off-Heap Memory, Managed Memory and Network Memory.However, in the official document, the Framework Off-Heap Heap Memory should be subtracted.</description>
      <version>1.14.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.memory.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.memory.section.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-10 01:00:00" id="22964" opendate="2021-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Connector-base exposes dependency to flink-core.</summary>
      <description>Connectors get shaded into the user jar and as such should contain no unnecessary dependencies to flink. However, connector-base is exposing `flink-core` which then by default gets shaded into the user jar. Except for 6MB of extra size, the dependency also causes class loading issues, when `classloader.parent-first-patterns` does not include `o.a.f`.Fix is to make `flink-core` provided in `connector-base`.</description>
      <version>1.14.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-11 01:00:00" id="22970" opendate="2021-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The documentation for `TO_TIMESTAMP` UDF has an incorrect description</summary>
      <description>According to this ML discussion http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/confused-about-TO-TIMESTAMP-document-description-td44352.htmlThe description for `TO_TIMESTAMP` udf is not correct. It will use UTC+0 timezone instead of session timezone. We should fix this documentation.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-16 01:00:00" id="23004" opendate="2021-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix misleading log</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-16 01:00:00" id="23011" opendate="2021-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>FLIP-27 sources are generating non-deterministic results when using event time</summary>
      <description>FLIP-27 sources currently start in the StreamStatus.IDLE state and they switch to ACTIVE only after emitting first Watermark. Until this happens, downstream operators are ignoring IDLE inputs from calculating the input (min) watermark. An extreme example to what problem this leads to, are completely bogus results if for example one FLIP-27 source subtask is slower than others for some reason:env.getConfig().setAutoWatermarkInterval(2000);env.setParallelism(2);env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 10));DataStream&lt;Long&gt; eventStream = env.fromSource( new NumberSequenceSource(0, Long.MAX_VALUE), WatermarkStrategy.&lt;Long&gt;forMonotonousTimestamps() .withTimestampAssigner(new LongTimestampAssigner()), "NumberSequenceSource") .map( new RichMapFunction&lt;Long, Long&gt;() { @Override public Long map(Long value) throws Exception { if (getRuntimeContext().getIndexOfThisSubtask() == 0) { Thread.sleep(1); } return 1L; } });eventStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(1))).sum(0).print();(...)private static class LongTimestampAssigner implements SerializableTimestampAssigner&lt;Long&gt; { private long counter = 0; @Override public long extractTimestamp(Long record, long recordTimeStamp) { return counter++; }}In such case, after 2 seconds (setAutoWatermarkInterval) the not throttled subtask (subTaskId == 1) generates very high watermarks. The other source subtask (subTaskId == 0) emits very low watermarks. If the non throttled watermark reaches the downstream WindowOperator first, while the other input channel is still idle, it will take those high watermarks as combined input watermark for the the whole WindowOperator. When the input channel from the throttled source subtask finally receives it's ACTIVE status and a much lower watermark, that's already too late.Actual output of the example program:15962000100010001000100010001000(...)while the expected output should be always "2000" (2000 records fitting in every 1 second global window)2000200020002000(...).</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.eventtime.WatermarkOutputMultiplexerTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.eventtime.CombinedWatermarkStatus.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-11-17 01:00:00" id="23015" opendate="2021-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement streaming window Deduplicate operator</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionHelper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-17 01:00:00" id="23023" opendate="2021-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support offset in window TVF</summary>
      <description>Window offset is an optional parameter which could be used to change the alignment of windows.There are something we need clarify about window offset:(1) In SQL,Â window offset is an optionalÂ parameter, if it is specified, it is the last parameter of the window.for Tumble windowTUMBLE(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE, INTERVAL '5' MINUTE)for Hop WindowHOP(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '1' MINUTE, INTERVAL '15' MINUTE,INTERVAL '5' MINUTE)for Cumulate WindowCUMULATE(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '1' MINUTE, INTERVAL '15' MINUTE,Â INTERVAL '5' MINUTE)(2) Window offset could be positive duration and negative duration.(3) Window offset is used to change the alignment of Windows. The same record may be assigned to the different window after set window offset. But it always apply a rule, timestamp &gt;= window_start &amp;&amp; timestamp &lt; window_end.Give a demo, for a tumble window, window size is 10 MINUTE, which window would be assigned to for a record with timestamp 2021-06-30 00:00:04? offset is '-16 MINUTE',Â Â the record assigns to window [2021-06-29 23:54:00, 2021-06-30 00:04:00) offset is '-6 MINUTE', the record assigns to window [2021-06-29 23:54:00, 2021-06-30 00:04:00) offset is '-4 MINUTE', the record assigns to windowÂ [2021-06-29 23:56:00, 2021-06-30 00:06:00) offset is '0', the record assigns to window [2021-06-30 00:00:00, 2021-06-30 00:10:00) offset is '4 MINUTE',Â the record assigns to window [2021-06-29 23:54:00, 2021-06-30 00:04:00) offset is '6 MINUTE,Â the record assigns to window [2021-06-29 23:56:00, 2021-06-30 00:06:00) offset is '16 MINUTE',Â the record assigns to window [2021-06-29 23:56:00, 2021-06-30 00:06:00)(4)Â We could find that, some window offset parameters may have same effect on the alignment of windows, in the above case,Â Â '-16 MINUTE' /'-6 MINUTE'/'4 MINUTE' have same effect on a tumble window with '10 MINUTE'Â  size.(5) Window offset is only used to change the alignment of Windows, it has no effect on watermark.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.WindowTableFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowTableFunctionTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowAggregateBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.logical.TumblingWindowSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.logical.HoppingWindowSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.logical.CumulativeWindowSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.SqlTumbleTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.SqlHopTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.SqlCumulateTableFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-1 01:00:00" id="2305" opendate="2015-7-1 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add documenation about Storm compatibility layer</summary>
      <description>Storm compatibility layer is currently no documented at the project web site.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.StormBoltFileSink.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.SimpleOutputFormatter.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.excamation.ExclamationTopology.java</file>
      <file type="M">docs..includes.navbar.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-21 01:00:00" id="23055" opendate="2021-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for Window TVF offset</summary>
      <description/>
      <version>1.14.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-tvf.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-21 01:00:00" id="23073" opendate="2021-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix space handling in Row CSV timestamp parser</summary>
      <description>FLINK-21947 Added support for TIMESTAMP_LTZ in the CSV format by replacing java.sql.Timestamp.valueOf with java.time.LocalDateTime.parse. Timestamp.valueOf internally calls `trim()` on the string before parsing while LocalDateTime.parse does not. This caused a breaking change where the CSV format can no longer parse timestamps of CSV's with spaces after the delimiter. We should manually re-add the call to trim to revert the behavior.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDeSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvToRowDataConverters.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-22 01:00:00" id="23075" opendate="2021-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python API for enabling ChangelogStateBackend</summary>
      <description>After FLINK-22678, two APIs ```enableChangelogStateBackend``` and ```isChangelogStateBackendEnabled``` have been added. The corresponding interfaces should be added to python API.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-6-23 01:00:00" id="23104" opendate="2021-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-statebackend-changelog does not build with scala 2.12</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19335&amp;view=logs&amp;j=ed6509f5-1153-558c-557a-5ee0afbcdf24&amp;t=241b1e5e-1a8e-5e6a-469a-a9b8cad87065&amp;l=4868</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-changelog.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-23 01:00:00" id="23107" opendate="2021-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate deduplicate rank from rank functions</summary>
      <description>SELECT * FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY d ORDER BY e DESC) AS rownum from T) WHERE rownum=1Actually above sql is a deduplicate rank instead of a normal rank. We should separate the implementation for optimize the deduplicate rank and reduce bugs.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RankUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-23 01:00:00" id="23129" opendate="2021-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>When cancelling any running job of multiple jobs in an application cluster, JobManager shuts down</summary>
      <description>I have a jar with two jobs, both executeAsync() from the same main method. I execute the main method in an Application Mode cluster. When I cancel one of the two jobs, both jobs will stop executing.I would expect that the JobManager shuts down once all jobs submitted from an application are finished.If this is a known limitation, we should document it.2021-06-23 21:29:53,123 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job first job (18181be02da272387354d093519b2359) switched from state RUNNING to CANCELLING.2021-06-23 21:29:53,124 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -&gt; Sink: Unnamed (1/1) (5a69b1c19f8da23975f6961898ab50a2) switched from RUNNING to CANCELING.2021-06-23 21:29:53,141 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -&gt; Sink: Unnamed (1/1) (5a69b1c19f8da23975f6961898ab50a2) switched from CANCELING to CANCELED.2021-06-23 21:29:53,144 INFO org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 18181be02da272387354d093519b23592021-06-23 21:29:53,145 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job first job (18181be02da272387354d093519b2359) switched from state CANCELLING to CANCELED.2021-06-23 21:29:53,145 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Stopping checkpoint coordinator for job 18181be02da272387354d093519b2359.2021-06-23 21:29:53,147 INFO org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down2021-06-23 21:29:53,150 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Job 18181be02da272387354d093519b2359 reached terminal state CANCELED.2021-06-23 21:29:53,152 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Stopping the JobMaster for job first job(18181be02da272387354d093519b2359).2021-06-23 21:29:53,155 INFO org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [c35b64879d6b02d383c825ea735ebba0].2021-06-23 21:29:53,159 INFO org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 18181be02da272387354d093519b23592021-06-23 21:29:53,159 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Close ResourceManager connection 281b3fcf7ad0a6f7763fa90b8a5b9adb: Stopping JobMaster for job first job(18181be02da272387354d093519b2359)..2021-06-23 21:29:53,160 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@localhost:6123/user/rpc/jobmanager_2 for job 18181be02da272387354d093519b2359 from the resource manager.2021-06-23 21:29:53,225 INFO org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap [] - Application CANCELED:java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: CANCELED at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$unwrapJobResultException$4(ApplicationDispatcherBootstrap.java:304) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616) ~[?:1.8.0_252] at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) ~[?:1.8.0_252] at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252] at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) ~[?:1.8.0_252] at org.apache.flink.client.deployment.application.JobStatusPollingUtils.lambda$null$2(JobStatusPollingUtils.java:101) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_252] at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_252] at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252] at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) ~[?:1.8.0_252] at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:237) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) [?:1.8.0_252] at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) [?:1.8.0_252] at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) [?:1.8.0_252] at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) [?:1.8.0_252] at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1081) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.OnComplete.internal(Future.scala:264) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.OnComplete.internal(Future.scala:261) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]Caused by: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: CANCELED at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:71) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] ... 42 moreCaused by: org.apache.flink.runtime.client.JobCancellationException: Job was cancelled. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:60) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] ... 42 more2021-06-23 21:29:53,238 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Shutting StandaloneApplicationClusterEntryPoint down with application status CANCELED. Diagnostics null.2021-06-23 21:29:53,239 INFO org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shutting down rest endpoint.2021-06-23 21:29:53,257 INFO org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Removing cache directory /var/folders/js/yfk_y2450q7559kygttykwk00000gn/T/flink-web-a0d034d2-da2b-4d72-9ece-ec00c9ae032b/flink-web-ui2021-06-23 21:29:53,307 INFO org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - http://localhost:8081 lost leadership2021-06-23 21:29:53,307 INFO org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shut down complete.2021-06-23 21:29:53,307 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Shut down cluster because application is in CANCELED, diagnostics null.2021-06-23 21:29:53,307 INFO org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent [] - Closing components.2021-06-23 21:29:53,308 INFO org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Stopping SessionDispatcherLeaderProcess.2021-06-23 21:29:53,308 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Stopping dispatcher akka.tcp://flink@localhost:6123/user/rpc/dispatcher_0.2021-06-23 21:29:53,308 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Stopping all currently running jobs of dispatcher akka.tcp://flink@localhost:6123/user/rpc/dispatcher_0.2021-06-23 21:29:53,308 INFO org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Stopping resource manager service.2021-06-23 21:29:53,308 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Stopping the JobMaster for job second job(e4ff65c30754648cf114232c07ef903e).2021-06-23 21:29:53,309 INFO org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Closing the slot manager.2021-06-23 21:29:53,309 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Job e4ff65c30754648cf114232c07ef903e reached terminal state SUSPENDED.2021-06-23 21:29:53,309 INFO org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Suspending the slot manager.2021-06-23 21:29:53,309 INFO org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is not running. Ignore revoking leadership.2021-06-23 21:29:53,309 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job second job (e4ff65c30754648cf114232c07ef903e) switched from state RUNNING to SUSPENDED.org.apache.flink.util.FlinkException: Scheduler is being stopped. at org.apache.flink.runtime.scheduler.SchedulerBase.closeAsync(SchedulerBase.java:604) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.jobmaster.JobMaster.stopScheduling(JobMaster.java:962) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.jobmaster.JobMaster.stopJobExecution(JobMaster.java:926) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:398) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2021-06-23 21:29:53,311 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -&gt; Sink: Unnamed (1/1) (b08fac5184817c72f73a0b3fff0afbd3) switched from RUNNING to CANCELING.2021-06-23 21:29:53,312 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -&gt; Sink: Unnamed (1/1) (b08fac5184817c72f73a0b3fff0afbd3) switched from CANCELING to CANCELED.2021-06-23 21:29:53,313 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Discarding the results produced by task execution b08fac5184817c72f73a0b3fff0afbd3.2021-06-23 21:29:53,314 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Stopping checkpoint coordinator for job e4ff65c30754648cf114232c07ef903e.2021-06-23 21:29:53,314 INFO org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down2021-06-23 21:29:53,314 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job e4ff65c30754648cf114232c07ef903e has been suspended.2021-06-23 21:29:53,314 INFO org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [30b64fc00bc2c8e83e80567e4f984ae9].2021-06-23 21:29:53,315 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Close ResourceManager connection 281b3fcf7ad0a6f7763fa90b8a5b9adb: Stopping JobMaster for job second job(e4ff65c30754648cf114232c07ef903e)..2021-06-23 21:29:53,318 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Stopped dispatcher akka.tcp://flink@localhost:6123/user/rpc/dispatcher_0.2021-06-23 21:29:53,323 INFO org.apache.flink.runtime.blob.BlobServer [] - Stopped BLOB server at 0.0.0.0:614982021-06-23 21:29:53,323 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopping Akka RPC service.2021-06-23 21:29:53,326 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopping Akka RPC service.2021-06-23 21:29:53,331 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.2021-06-23 21:29:53,331 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.2021-06-23 21:29:53,332 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.2021-06-23 21:29:53,332 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.2021-06-23 21:29:53,348 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Remoting shut down.2021-06-23 21:29:53,348 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Remoting shut down.2021-06-23 21:29:53,359 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopped Akka RPC service.2021-06-23 21:29:53,366 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopped Akka RPC service.2021-06-23 21:29:53,366 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Terminating cluster entrypoint process StandaloneApplicationClusterEntryPoint with exit code 0.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-23 01:00:00" id="23131" opendate="2021-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove scala from plugin parent-first patterns</summary>
      <description>In order to load akka and it's scala version through a separate classloader we need to remove scala from the parent-first patterns for plugins.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-7-29 01:00:00" id="23184" opendate="2021-6-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CompileException Assignment conversion not possible from type "int" to type "short"</summary>
      <description>CREATE TABLE MySink ( `a` SMALLINT) WITH ( 'connector' = 'filesystem', 'format' = 'testcsv', 'path' = '$resultPath')CREATE TABLE database8_t0 ( `c0` SMALLINT) WITH ( 'connector' = 'filesystem', 'format' = 'testcsv', 'path' = '$resultPath11')CREATE TABLE database8_t1 ( `c0` SMALLINT, `c1` TINYINT) WITH ( 'connector' = 'filesystem', 'format' = 'testcsv', 'path' = '$resultPath22')INSERT OVERWRITE database8_t0(c0) VALUES(cast(22424 as SMALLINT))INSERT OVERWRITE database8_t1(c0, c1) VALUES(cast(-17443 as SMALLINT), cast(97 as TINYINT))insert into MySinkSELECT database8_t0.c0 AS ref0 FROM database8_t0, database8_t1 WHERE CAST ((- (database8_t0.c0)) AS BOOLEAN)After running that , you will get the errors:2021-06-29 19:39:27org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:207) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:197) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:188) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:677) at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:440) at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: Could not instantiate generated class 'BatchExecCalc$4536' at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:66) at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:43) at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:80) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:626) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:600) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:540) at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:171) at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:547) at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:646) at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:536) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) at java.lang.Thread.run(Thread.java:834)Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:78) at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:77) at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:64) ... 12 moreCaused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76) ... 14 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:106) at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:76) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ... 17 moreCaused by: org.codehaus.commons.compiler.CompileException: Line 53, Column 26: Assignment conversion not possible from type "int" to type "short" at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211) at org.codehaus.janino.UnitCompiler.assignmentConversion(UnitCompiler.java:11062) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3790) at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754) at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734) at org.codehaus.janino.Java$Assignment.accept(Java.java:4477) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2779) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:103) ... 23 more</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-30 01:00:00" id="23188" opendate="2021-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unsupported function definition: IFNULL. Only user defined functions are supported as inline functions</summary>
      <description>CREATE TABLE database0_t0(c0 FLOAT) WITH (Â  'connector' = 'filesystem',Â  'path' = 'hdfs:///tmp/database0_t0.csv',Â  'format' = 'csv');INSERT OVERWRITE database0_t0(c0) VALUES(0.40445197);SELECT database0_t0.c0 AS ref0 FROM database0_t0 WHEREÂ ((IFNULL(database0_t0.c1, database0_t0.c1)) IS NULL);The errors:"&lt;ExceptionÂ onÂ serverÂ side: org.apache.flink.table.api.TableException:Â UnsupportedÂ functionÂ definition:Â IFNULL.Â OnlyÂ userÂ definedÂ functionsÂ areÂ supportedÂ asÂ inlineÂ functions. Â atÂ org.apache.flink.table.planner.functions.bridging.BridgingUtils.lambda$createInlineFunctionName$0(BridgingUtils.java:81) Â atÂ java.util.Optional.orElseThrow(Optional.java:290) Â atÂ org.apache.flink.table.planner.functions.bridging.BridgingUtils.createInlineFunctionName(BridgingUtils.java:78) Â atÂ org.apache.flink.table.planner.functions.bridging.BridgingUtils.createName(BridgingUtils.java:58) Â atÂ org.apache.flink.table.planner.functions.bridging.BridgingSqlFunction.&lt;init&gt;(BridgingSqlFunction.java:76) Â atÂ org.apache.flink.table.planner.functions.bridging.BridgingSqlFunction.of(BridgingSqlFunction.java:116) Â atÂ org.apache.flink.table.planner.expressions.converter.FunctionDefinitionConvertRule.convert(FunctionDefinitionConvertRule.java:65) Â atÂ org.apache.flink.table.planner.expressions.converter.ExpressionConverter.visit(ExpressionConverter.java:97) Â atÂ org.apache.flink.table.planner.expressions.converter.ExpressionConverter.visit(ExpressionConverter.java:71) Â atÂ org.apache.flink.table.expressions.CallExpression.accept(CallExpression.java:134) Â atÂ org.apache.flink.table.planner.expressions.converter.ExpressionConverter$1.toRexNode(ExpressionConverter.java:247) Â atÂ java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) Â atÂ java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374) Â atÂ java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) Â atÂ java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) Â atÂ java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) Â atÂ java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) Â atÂ java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) Â atÂ org.apache.flink.table.planner.expressions.converter.ExpressionConverter.toRexNodes(ExpressionConverter.java:240) Â atÂ org.apache.flink.table.planner.expressions.converter.DirectConvertRule.lambda$convert$0(DirectConvertRule.java:220) Â atÂ java.util.Optional.map(Optional.java:215) Â atÂ org.apache.flink.table.planner.expressions.converter.DirectConvertRule.convert(DirectConvertRule.java:217) Â atÂ org.apache.flink.table.planner.expressions.converter.ExpressionConverter.visit(ExpressionConverter.java:97) Â atÂ org.apache.flink.table.planner.expressions.converter.ExpressionConverter.visit(ExpressionConverter.java:71) Â atÂ org.apache.flink.table.expressions.CallExpression.accept(CallExpression.java:134) Â atÂ org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoSourceScanRuleBase.lambda$convertExpressionToRexNode$0(PushFilterIntoSourceScanRuleBase.java:73) Â atÂ java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) Â atÂ java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374) Â atÂ java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) Â atÂ java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) Â atÂ java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) Â atÂ java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) Â atÂ java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) Â atÂ org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoSourceScanRuleBase.convertExpressionToRexNode(PushFilterIntoSourceScanRuleBase.java:73) Â atÂ org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoSourceScanRuleBase.resolveFiltersAndCreateTableSourceTable(PushFilterIntoSourceScanRuleBase.java:116) Â atÂ org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.pushFilterIntoScan(PushFilterIntoTableSourceScanRule.java:95) Â atÂ org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.onMatch(PushFilterIntoTableSourceScanRule.java:70) Â atÂ org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333) Â atÂ org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542) Â atÂ org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407) Â atÂ org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243) Â atÂ org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127) Â atÂ org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202) Â atÂ org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60) Â atÂ scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) Â atÂ scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) Â atÂ scala.collection.Iterator$class.foreach(Iterator.scala:891) Â atÂ scala.collection.AbstractIterator.foreach(Iterator.scala:1334) Â atÂ scala.collection.IterableLike$class.foreach(IterableLike.scala:72) Â atÂ scala.collection.AbstractIterable.foreach(Iterable.scala:54) Â atÂ scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) Â atÂ scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55) Â atÂ scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) Â atÂ scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) Â atÂ scala.collection.immutable.Range.foreach(Range.scala:160) Â atÂ scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) Â atÂ scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) Â atÂ scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) Â atÂ scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) Â atÂ scala.collection.Iterator$class.foreach(Iterator.scala:891) Â atÂ scala.collection.AbstractIterator.foreach(Iterator.scala:1334) Â atÂ scala.collection.IterableLike$class.foreach(IterableLike.scala:72) Â atÂ scala.collection.AbstractIterable.foreach(Iterable.scala:54) Â atÂ scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) Â atÂ scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) Â atÂ org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87) Â atÂ org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58) Â atÂ org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46) Â atÂ org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46) Â atÂ scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) Â atÂ scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) Â atÂ org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46) Â atÂ org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:93) Â atÂ org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:310) Â atÂ org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:172) Â atÂ com.ververica.flink.table.gateway.operation.SelectOperation.lambda$executeQueryInternal$0(SelectOperation.java:183) Â atÂ com.ververica.flink.table.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:130) Â atÂ com.ververica.flink.table.gateway.operation.SelectOperation.executeQueryInternal(SelectOperation.java:182) Â atÂ com.ververica.flink.table.gateway.operation.SelectOperation.execute(SelectOperation.java:82) Â atÂ com.ververica.flink.table.gateway.operation.executor.OneByOneOperationExecutor.execute(OneByOneOperationExecutor.java:57) Â atÂ com.ververica.flink.table.gateway.rest.session.Session.lambda$runStatement$1(Session.java:115) Â atÂ com.ververica.flink.table.gateway.utils.EnvironmentUtil.lambda$wrapWithHadoopUsernameIfNeeded$0(EnvironmentUtil.java:57) Â atÂ com.ververica.flink.table.gateway.utils.EnvironmentUtil.wrapWithHadoopUsernameIfNeeded(EnvironmentUtil.java:65) Â atÂ com.ververica.flink.table.gateway.utils.EnvironmentUtil.wrapWithHadoopUsernameIfNeeded(EnvironmentUtil.java:56) Â atÂ com.ververica.flink.table.gateway.rest.session.Session.runStatement(Session.java:114) Â atÂ com.ververica.flink.table.gateway.rest.handler.StatementExecuteHandler.handleRequest(StatementExecuteHandler.java:83) Â atÂ com.ververica.flink.table.gateway.rest.handler.AbstractRestHandler.respondToRequest(AbstractRestHandler.java:85) Â atÂ com.ververica.flink.table.gateway.rest.handler.AbstractHandler.channelRead0(AbstractHandler.java:184) Â atÂ com.ververica.flink.table.gateway.rest.handler.AbstractHandler.channelRead0(AbstractHandler.java:76) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) Â atÂ org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:115) Â atÂ org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:94) Â atÂ org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:55) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) Â atÂ org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) Â atÂ org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:208) Â atÂ org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:69) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436) Â atÂ org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) Â atÂ org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) Â atÂ org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) Â atÂ org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) Â atÂ java.lang.Thread.run(Thread.java:834) EndÂ ofÂ exceptionÂ onÂ serverÂ side&gt;"</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonFunctionInWhereClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testComplexCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractor.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-8-2 01:00:00" id="23214" opendate="2021-7-2 00:00:00" resolution="Done">
    <buginformation>
      <summary>Make ShuffleMaster a cluster level shared service</summary>
      <description>This ticket tries to make ShuffleMaster a cluster level shared service which makes it consistent with the ShuffleEnvironment at the TM side.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.TestingJobManagerSharedServicesBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerSharedServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.JobMasterServiceLeadershipRunnerFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-5 01:00:00" id="23232" opendate="2021-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink tox check fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19855&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=8d78fe4f-d658-5c70-12f8-4921589024c3&amp;l=23069Jul 03 00:07:56 ============tox checks... [FAILED]============Jul 03 00:07:56 Process exited with EXIT CODE: 1.Jul 03 00:07:56 Trying to KILL watchdog (3140)./__w/2/s/tools/ci/watchdog.sh: line 100: 3140 Terminated watchdogJul 03 00:07:56 Searching for .dump, .dumpstream and related files in '/__w/2/s'The STDIO streams did not close within 10 seconds of the exit event from process '/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.datastream.operations.py</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-8-5 01:00:00" id="23255" opendate="2021-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add JUnit 5 jupiter and vintage engine</summary>
      <description>Add dependencies for JUnit 5 jupiter for supporting JUnit 5 tests, and vintage engine for supporting test cases in JUnit 4 style</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser-hive.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hbase.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-8-7 01:00:00" id="23287" opendate="2021-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create user document for Window Join in SQL</summary>
      <description>Create user document for Window Join in SQL</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-tvf.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-7 01:00:00" id="23290" opendate="2021-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>cast &amp;#39;(LZ *3&amp;#39; as boolean, get a null</summary>
      <description>CREATE TABLE database5_t0(c0 VARCHAR , c1 BIGINT ) WITH ( 'connector' = 'filesystem', 'path' = 'hdfs:///tmp/database5_t0.csv', 'format' = 'csv');INSERT OVERWRITE database5_t0(c0, c1) VALUES('(LZ *3', 2135917226)SELECT database5_t0.c0 AS ref0 FROM database5_t0 WHERE CAST (database5_t0.c0 AS BOOLEAN)After excuting that, you will get the error:Caused by: java.lang.NullPointerException at BatchExecCalc$20.processElement(Unknown Source) at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:101) at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:82) at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:319) at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:414) at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:104) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:62) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:269)</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-7 01:00:00" id="23297" opendate="2021-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Protobuf to 3.17.3</summary>
      <description>In order to support compilation with ARM (e.g. Apple M1 chip), we need to bump our Protobuf dependency to version 3.17.3.</description>
      <version>1.14.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-8 01:00:00" id="23308" opendate="2021-7-8 00:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>Performance regression on 06.07</summary>
      <description>http://codespeed.dak8s.net:8000/timeline/#/?exe=1,3&amp;ben=twoInputMapSink&amp;env=2&amp;revs=200&amp;equid=off&amp;quarts=on&amp;extr=onhttp://codespeed.dak8s.net:8000/timeline/?ben=readFileSplit&amp;env=2</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSourceContexts.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-11 01:00:00" id="23345" opendate="2021-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate to the next version of Python `requests` when released</summary>
      <description>Hello Maintainers,Â I am a PMC member of Apache Airflow, and I wanted to give you a bit of heads-up with rather important migration to the upcoming version of `requests` library in your Python release.Â Since you are using `requests` library in your project (at least indirectly via apache-beam), you are affected.As discussed at length in https://issues.apache.org/jira/browse/LEGAL-572Â we found out that the 'chardet` library used by `requests` library was a mandatory dependency to requests and since it has LGPL licence, we should not release any Apache Software with it.Â Since then (and since in Airflow we rely on requests heavily) we have been working with the requests maintainers and "charset-normalizer" maintainer to make it possible to replace `chardet` with MIT-licensed `charset-normalizer` instead so that requests library can be used in Python releases by Apache projects.This was a bumpy road but finally the PR by ashÂ has been merged: https://github.com/psf/requests/pull/5797Â and we hope soon a new version of requests library will be released.Â This is just a heads-up. I will let you know when it is released, but I have a kind requests as well - I might ask the maintainers to release a release candidate of requests and maybe you could help to test it before it is released, that would be some re-assurance for the maintainers of requests who are very concerned about stability of their releases.Let me know if you need any more information and whether you would like to help in testing the candidate when it is out.</description>
      <version>1.14.0</version>
      <fixedVersion>1.10.4,1.14.0,1.13.3,1.11.7,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-7-15 01:00:00" id="23395" opendate="2021-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Okhttp to 3.14.9</summary>
      <description>We currently use 3 different version of Okhttp, which are partially lagging behind the last 3.X version by quite a bit.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-metrics.flink-metrics-datadog.pom.xml</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-15 01:00:00" id="23399" opendate="2021-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a performance benchmark for statebackend rescaling</summary>
      <description>We notice that rescaling is not covered in the current state benchmark, so we'd like to introduce a benchmark to test performance of state backend restore durign rescaling in flink-benchmark.The benchmark process is:(1) generate some states,(2) change the parallelism of the operator, and restore from these states generate before.The implementation of this benchmark is based on RocksIncrementalCheckpointRescalingTest, and AverageTime is used to measure the rescaling performance on each subtask.Â And this benchmark does not conflict with `RocksIncrementalCheckpointRescalingBenchmarkTest` inÂ  PR(#14893).Â Compare with `RocksIncrementalCheckpointRescalingBenchmarkTest`, this benchmark supports testing rescaling on different state backends, and has a finer granularity.</description>
      <version>1.14.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-8-16 01:00:00" id="23409" opendate="2021-7-16 00:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>CrossITCase fails with "NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20548&amp;view=logs&amp;j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&amp;t=5360d54c-8d94-5d85-304e-a89267eb785a&amp;l=10074Jul 16 09:21:37 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)Jul 16 09:21:37 at akka.actor.Actor$class.aroundReceive(Actor.scala:517)Jul 16 09:21:37 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)Jul 16 09:21:37 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)Jul 16 09:21:37 at akka.actor.ActorCell.invoke(ActorCell.scala:561)Jul 16 09:21:37 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)Jul 16 09:21:37 at akka.dispatch.Mailbox.run(Mailbox.scala:225)Jul 16 09:21:37 at akka.dispatch.Mailbox.exec(Mailbox.scala:235)Jul 16 09:21:37 ... 4 moreJul 16 09:21:37 Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeoutJul 16 09:21:37 at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)Jul 16 09:21:37 at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)Jul 16 09:21:37 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)Jul 16 09:21:37 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)Jul 16 09:21:37 ... 31 moreJul 16 09:21:37 Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeoutJul 16 09:21:37 at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.lambda$schedulePendingRequestBulkWithTimestampCheck$0(PhysicalSlotRequestBulkCheckerImpl.java:86)Jul 16 09:21:37 ... 24 moreJul 16 09:21:37 Caused by: java.util.concurrent.TimeoutException: Timeout has occurred: 300000 msJul 16 09:21:37 ... 25 more</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-21 01:00:00" id="23446" opendate="2021-7-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor SQL Client end to end tests</summary>
      <description>Remove useage of the YAML in SQL Client end to end tests</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka.sql.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-21 01:00:00" id="23447" opendate="2021-7-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump lz4-java to 1.8</summary>
      <description>Bump lz4 to the latest version for bug fixes and performance improvements.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-test.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-21 01:00:00" id="23462" opendate="2021-7-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the abfs documentation to chinese</summary>
      <description>Translate the documentation changes that were made in this PR to chinese https://github.com/apache/flink/pull/16559/</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.deployment.filesystems.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.azure.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-22 01:00:00" id="23463" opendate="2021-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace &lt;div&gt; tags with ShortCodes</summary>
      <description>In FLINK-22922, we migrate Flink website to hugo. At the moment, most of the div tag in user doc is no long take effect. We need to replace them with the ShortCodes.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.savepoints.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.queryable.state.md</file>
      <file type="M">docs.content.docs.dev.datastream.event-time.generating.watermarks.md</file>
      <file type="M">docs.content.docs.dev.dataset.iterations.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.tuning.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.migration.md</file>
      <file type="M">docs.content.docs.deployment.config.md</file>
      <file type="M">docs.content.docs.deployment.advanced.external.resources.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kinesis.md</file>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.zh.docs.ops.state.savepoints.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpoints.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.serialization.custom.serialization.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.queryable.state.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.event-time.generating.watermarks.md</file>
      <file type="M">docs.content.zh.docs.dev.dataset.iterations.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.tuning.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.migration.md</file>
      <file type="M">docs.content.zh.docs.deployment.config.md</file>
      <file type="M">docs.content.zh.docs.deployment.advanced.external.resources.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-22 01:00:00" id="23476" opendate="2021-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Snapshot deployments are broken</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20855&amp;view=logs&amp;j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&amp;t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7[ERROR] 'dependencies.dependency.version' for com.typesafe.akka:akka-testkit_2.11:jar must be a valid version but is '${akka.version}'. @ line 1212, column 15</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-12-26 01:00:00" id="23493" opendate="2021-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>python tests hang on Azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20898&amp;view=logs&amp;j=821b528f-1eed-5598-a3b4-7f748b13f261&amp;t=4fad9527-b9a5-5015-1b70-8356e5c91490&amp;l=22829</description>
      <version>1.14.0,1.13.1,1.12.4,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.boot.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-26 01:00:00" id="23497" opendate="2021-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-table-planner does not compile on scala 2.12</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20961&amp;view=logs&amp;j=b9f58bb2-ed4a-500b-bef9-cc3cf2248e69&amp;t=e6d8efc2-861e-5470-71ae-bbaad6c133d32021-07-26T09:10:26.7655809Z [INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ flink-table-planner_2.12 ---2021-07-26T09:10:26.8567767Z [INFO] /__w/1/s/flink-table/flink-table-planner/src/main/java:-1: info: compiling2021-07-26T09:10:26.8568668Z [INFO] /__w/1/s/flink-table/flink-table-planner/src/main/scala:-1: info: compiling2021-07-26T09:10:26.8571934Z [INFO] Compiling 903 source files to /__w/1/s/flink-table/flink-table-planner/target/classes at 16272906268562021-07-26T09:10:31.0803565Z [ERROR] /__w/1/s/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/expressions/PlannerExpressionParserImpl.scala:32: error: object parsing is not a member of package util2021-07-26T09:10:31.0804449Z [ERROR] import _root_.scala.util.parsing.combinator.{JavaTokenParsers, PackratParsers}2021-07-26T09:10:31.0804904Z [ERROR] ^2021-07-26T09:10:31.1130420Z [ERROR] /__w/1/s/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/expressions/PlannerExpressionParserImpl.scala:59: error: not found: type JavaTokenParsers2021-07-26T09:10:31.1131245Z [ERROR] object PlannerExpressionParserImpl extends JavaTokenParsers2...</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.table.shaded.dependencies.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-7-29 01:00:00" id="23534" opendate="2021-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Banned dependencies in flink-statebackend-changelog</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21129&amp;view=logs&amp;j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&amp;t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7&amp;l=34482[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-statebackend-changelog ---[WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:Found Banned Dependency: org.apache.flink:flink-streaming-java_2.11:jar:1.14-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-streaming-java_2.11:test-jar:tests:1.14-SNAPSHOTFound Banned Dependency: com.twitter:chill_2.11:jar:0.7.6Found Banned Dependency: org.apache.flink:flink-scala_2.11:jar:1.14-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-dstl-dfs_2.11:jar:1.14-SNAPSHOTUse 'mvn dependency:tree' to locate the source of the banned dependencies.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-changelog.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-7-29 01:00:00" id="23546" opendate="2021-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>stop-cluster.sh produces warning on macOS 11.4</summary>
      <description>Since FLINK-17470, we are stopping daemons with a timeout, to SIGKILL them if they are not gracefully stopping.I noticed that this mechanism causes warnings on macOS:â°robertâ/tmp/flink-1.14-SNAPSHOTâ±ââ» ./bin/start-cluster.shStarting cluster.Starting standalonesession daemon on host MacBook-Pro-2.localdomain.Starting taskexecutor daemon on host MacBook-Pro-2.localdomain.â°robertâ/tmp/flink-1.14-SNAPSHOTâ±ââ» ./bin/stop-cluster.shStopping taskexecutor daemon (pid: 50044) on host MacBook-Pro-2.localdomain.tail: illegal option -- -usage: tail [-F | -f | -r] [-q] [-b # | -c # | -n #] [file ...]Stopping standalonesession daemon (pid: 49812) on host MacBook-Pro-2.localdomain.tail: illegal option -- -usage: tail [-F | -f | -r] [-q] [-b # | -c # | -n #] [file ...]</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-daemon.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-30 01:00:00" id="23559" opendate="2021-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Randomize periodic materialisation interval in tests</summary>
      <description>FLINK-21448 adds the capability of test randomization.It's already used for the changelog backend (FLINK-23279).Â FLINK-21357 adds periodic materialization; the default interval is 10m which is likely too high for tests (so materialization isn't triggered).This interval should be randomized/reduced;Â Depends on FLINK-23170.Â </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.TestStreamEnvironment.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-1 01:00:00" id="23570" opendate="2021-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation lists incorrect scala suffixes</summary>
      <description>Some of the maven dependencies in the documentation seem to have some problems and cannot be used directly.Page: DataStreamÂ Connectors -&gt; File Sink/Streaming FIle SinkÂ </description>
      <version>1.13.0,1.14.0</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.layouts.shortcodes.artifact.html</file>
      <file type="M">docs.content.docs.dev.datastream.testing.md</file>
      <file type="M">docs.content.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.testing.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.streamfile.sink.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-1 01:00:00" id="23571" opendate="2021-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The internal query-start options missed when convert exec graph to transformation</summary>
      <description>The internal query-start configuration options is missed when convert exec graph to transformation, please see:// org.apache.flink.table.planner.delegation.PlannerBase translateToPlan(execGraph: ExecNodeGraph)</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-3 01:00:00" id="23594" opendate="2021-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unstable test_from_and_to_data_stream_event_time failed</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21337&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-4 01:00:00" id="23611" opendate="2021-8-4 00:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>YARNSessionCapacitySchedulerITCase.testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots hangs on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21439&amp;view=logs&amp;j=245e1f2e-ba5b-5570-d689-25ae21e5302f&amp;t=e7f339b2-a7c3-57d9-00af-3712d4b15354&amp;l=28959</description>
      <version>1.14.0,1.12.5</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.log4j.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-4 01:00:00" id="23614" opendate="2021-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The resulting scale of TRUNCATE(DECIMAL, ...) is not correct</summary>
      <description>Run the following SQLSELECT TRUNCATE(123.456, 2), TRUNCATE(123.456, 0), TRUNCATE(123.456, -2), TRUNCATE(CAST(123.456 AS DOUBLE), 2), TRUNCATE(CAST(123.456 AS DOUBLE), 0), TRUNCATE(CAST(123.456 AS DOUBLE), -2)The result is123.450123.000100.000123.45123.0100.0It seems that the resulting scale of TRUNCATE(DECIMAL, ...) is the same as that of the input decimal.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.MathFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-4 01:00:00" id="23615" opendate="2021-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation of the TRUNCATE SQL function is not correct</summary>
      <description>The documentation of the TRUNCATE SQL function states that "E.g. 42.324.truncate(2) to 42.34", which should be "42.32".Some period in the document also lacks trailing spaces.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-5 01:00:00" id="23644" opendate="2021-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resolve maven warnings for duplicate dependencies/plugins</summary>
      <description/>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-8-6 01:00:00" id="23658" opendate="2021-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup JMX reporter</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.metrics.jmx.JMXReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.main.java.org.apache.flink.metrics.jmx.JMXReporter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-6 01:00:00" id="23659" opendate="2021-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup Prometheus reporter</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.test.java.org.apache.flink.metrics.prometheus.PrometheusReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.test.java.org.apache.flink.metrics.prometheus.PrometheusReporterTaskScopeTest.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.test.java.org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.main.java.org.apache.flink.metrics.prometheus.AbstractPrometheusReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-6 01:00:00" id="23663" opendate="2021-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce state size in ChangelogNormalize through filter push down</summary>
      <description>ChangelogNormalize is an expensive stateful operation as it stores data for each key. Filters are generally not pushed through a ChangelogNormalize node which means that users have no possibility to at least limit the key space. Pushing filters like a &lt; 10 into a source like upsert-kafka that is emitting +I&amp;#91;key1, a=9&amp;#93; and -D&amp;#91;key1, a=10&amp;#93;, is problematic as the deletion will be filtered and leads to wrong results. But limiting the filter push down to key space should be safe.Furthermore, it seems the current implementation is also wrong as it pushes any kind of filter through ChangelogNormalize but only if the source implements filter push down.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-9 01:00:00" id="23686" opendate="2021-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaSource metric "commitsSucceeded" should count per-commit instead of per-partition</summary>
      <description>Currently if a successful offset commit includes multiple topic partition (let's say 4), the counter will increase by 4 instead of 1</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.metrics.KafkaSourceReaderMetricsTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.metrics.KafkaSourceReaderMetrics.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2021-8-11 01:00:00" id="23715" opendate="2021-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for reading fields that do not exist in Parquet files</summary>
      <description>In a production environment, it is often encountered that users add fields to the hive table, but do not refresh the data of the historical partition. Therefore, if the new field is not in the historical partition file, an error will be reported when reading the historical partition.General users would expect that if there is no such field, then fill in null and return.The current flink Parquet format does not support this function.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetColumnarRowInputFormatTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.utils.ParquetSchemaConverter.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetVectorizedInputFormat.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-8-13 01:00:00" id="23742" opendate="2021-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>test_keyed_co_process test failed in py36 and py37</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22020&amp;view=logs&amp;j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&amp;t=b4612f28-e3b5-5853-8a8b-610ae894217a</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-13 01:00:00" id="23750" opendate="2021-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Window Top-N after Windowing TVF</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-topn.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-topn.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-13 01:00:00" id="23753" opendate="2021-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about Python DataStream API chaining optimization</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.python.datastream.operators.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.python.datastream.operators.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-13 01:00:00" id="23755" opendate="2021-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify the default value of table.dynamic-table-options.enabled to true</summary>
      <description>The below SQL will throw an exception:"SELECT * FROM kafka_table /*+ OPTIONS('group.id'='new_id') */".Before, we introduced the feature of dynamic table options, but in order to avoid user abuse, we turned it off by default, so the above SQL will throw exceptions.When modifying the value of table.dynamic-table-options.enabled to true, the SQL can work.After so many versions, we think this feature can be provided to users, and it is also relied on by many users, so we open it by default.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LegacyTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.hint.OptionsHintTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LegacyTableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LegacySinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.catalog.CatalogViewITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.hints.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.hints.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-13 01:00:00" id="23757" opendate="2021-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support JSON_EXISTS / JSON_VALUE methods in pyFlink</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.expression.completeness.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-17 01:00:00" id="2376" opendate="2015-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>testFindConnectableAddress sometimes fails on Windows because of the time limit</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.net.NetUtilsTest.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-12-16 01:00:00" id="23791" opendate="2021-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable RocksDB log again</summary>
      <description>FLINK-15068 disabled the RocksDB's local LOG due to previous RocksDB cannot limit the local log files.After we upgraded to newer RocksDB version, we can then enable RocksDB log again.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-16 01:00:00" id="23798" opendate="2021-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid using reflection to get filter when partition filter is enabled</summary>
      <description>FLINK-20496 introduce partitioned index &amp; filter to Flink. However, RocksDB only support new full format of filter in this feature, and we need to replace previous filter if user enabled. Previous implementation use reflection to get the filter and we could use API to get that after upgrading to newer version.</description>
      <version>None</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainerTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-16 01:00:00" id="23809" opendate="2021-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Respect the finished flag when extracting operator states due to skip in-flight data</summary>
      <description>Currently when using a flag to skip in-flight data will cause task to loose it's finished on restore state.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.FullyFinishedOperatorState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-17 01:00:00" id="23827" opendate="2021-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ModifiedMonotonicity inference for some node</summary>
      <description>ModifiedMonotonicity handler do not handle some node properly, such asÂ IntermediateTableScan, Deduplicate and LookupJoin.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicity.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-17 01:00:00" id="23832" opendate="2021-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for batch mode in StreamTableEnvironment</summary>
      <description>The DataStream API Integration page needs an update.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.zh.docs.dev.table.data.stream.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-18 01:00:00" id="23845" opendate="2021-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify metric deletion guarantees for Prometheus PushGateway reporter on shutdown</summary>
      <description>see https://issues.apache.org/jira/browse/FLINK-20691Â .Â Â whatever the problem has always existed, we should avoid other guys met it</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.main.java.org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.prometheus.push.gateway.reporter.configuration.html</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-8-19 01:00:00" id="23871" opendate="2021-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dispatcher should handle finishing job exception when recover</summary>
      <description>The exception during run recovery job will trigger fatal error which is introduced in https://issues.apache.org/jira/browse/FLINK-9097.Â  If a job have reached a finished status. But crash at clean up phase or any other post phase. When recover job, it may recover a job in RunningJobsRegistry.JobSchedulingStatus.DONE status, this may lead to the dispatcher fatal again.Â I think we should deal with theÂ  RunningJobsRegistry.JobSchedulingStatus.DONE with special exception like JobFinishingException, which represents the job/master crashed in job finishing phase. And only do the clean up work for this exception</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.JobMasterBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcessTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.OnCompletionActions.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-4-20 01:00:00" id="23902" opendate="2021-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 3.1.3</summary>
      <description>Make flink support Hive version 3.1.3 version.</description>
      <version>1.14.0,1.15.0,1.15.1</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV310.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">docs.content.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-21 01:00:00" id="23906" opendate="2021-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase akka.ask.timeout for tests using the MiniCluster</summary>
      <description>We have seen over the last couple of weeks/months an increased number of test failures because of TimeoutException that were triggered because the akka.ask.timeout was exceeded. The reason for this was that on our CI infrastructure it can happen that there are pauses of more than 10s (not sure about the exact reason) or our infrastructure simply being slow. In order to harden all tests relying on the MiniCluster I propose to increase the akka.ask.timeout to 5 minutes if nothing else has been configured.</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-23 01:00:00" id="23912" opendate="2021-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary "Clearing resource requirements of job"</summary>
      <description>The ResourceManager will print the log each time it receives an empty resource declaration. For deduplication, we need to: At JM side, skip decreasing empty resources. At SlotManager side, does not log if it is already empty.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.ResourceTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-23 01:00:00" id="23914" opendate="2021-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make connector testing framework more verbose on test failure</summary>
      <description>Currently testing framework doesn't provide enough debugging if test case fails. We need to add more logs on test failure to reveal more information for debugging.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.testsuites.SourceTestSuiteBase.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.test.java.org.apache.flink.connectors.test.common.utils.TestDataMatchersTest.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.test.java.org.apache.flink.connectors.test.common.utils.IteratorWithCurrentTest.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.utils.TestDataMatchers.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-24 01:00:00" id="23929" opendate="2021-8-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chaining optimization doesn&amp;#39;t handle properly for transformations with multiple outputs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizerTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-25 01:00:00" id="23962" opendate="2021-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UpdateKind trait is not propagated properly in changeLog inference for DAG optimizing</summary>
      <description>For sql jobs with multi-sinks, the plan is divided into relNode blocks, changeLog mode should be also inferred among blocks. Currently, updateKind trait is not propagated properly from parent block to child blocks for the following pattern.Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â -&gt; block3 block0 -&gt; block1 -&gt; block4 Â  Â  Â  Â  Â  Â  -&gt; block2Â In the above example, if block3 requires UB and block2,Â block4 do not require UB, block1 only contains Calc node.For Agg in block0, UB should be emitted, but the updateKind for block0 is inferred asÂ ONLY_UPDATE_AFTER.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-25 01:00:00" id="23965" opendate="2021-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>E2E do not execute locally on MacOS</summary>
      <description>After FLINK-21346, the e2e tests are no longer executing locally on MacOS. The problem seems to be that the e2e configure a log directory that does not exist and this fails starting a Flink cluster.I suggest to change the directory to the old directory FLINK_DIR/log instead of FLINK_DIR/logs.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-single-test.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2021-10-27 01:00:00" id="24019" opendate="2021-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separately package Scala-reliant modules</summary>
      <description>Bundle all Scala-reliant modules (flink-scala, flink-streaming-scala, flink-scala-shell) into a separate jar, containing Scala-exclusive dependencies (e.g., Scala itself, Scala extension of Chill). This jar will be added to lib/ by default, but can be removed by users to get a Scala-free experience.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.licenses.LICENSE.scala</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-27 01:00:00" id="24020" opendate="2021-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aggregate HTTP requests before custom netty handers are getting the data</summary>
      <description>Custom netty handlers can do authentication (amongst other possibilities).This requires that the handlers are getting the whole HttpRequest content and not just partial data.At the moment it's not implemented this way which ends-up in flaky behaviour.Namely sometimes for example History server responds properly (when the request fits into one netty chunk) but sometimes authentication fails (when the request split into multiple netty chunks).</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-28 01:00:00" id="24036" opendate="2021-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSL cannot be installed on CI</summary>
      <description># install libssl1.0.0 for netty tcnativewget http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.debsudo apt install ./libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.deb--2021-08-27 20:48:49-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 91.189.91.38, 2001:67c:1562::15, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2021-08-27 20:48:49 ERROR 404: Not Found.</description>
      <version>1.14.0,1.12.5,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-30 01:00:00" id="24049" opendate="2021-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TupleTypeInfo doesn&amp;#39;t handle correctly for data types need conversion</summary>
      <description/>
      <version>1.12.0,1.13.0,1.14.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.common.typeinfo.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-9-31 01:00:00" id="24083" opendate="2021-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The result isn&amp;#39;t as expected when the result type is generator of string for Python UDTF</summary>
      <description/>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.utils.operation.utils.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-31 01:00:00" id="24085" opendate="2021-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The `versioned table` and `Timezone` pages missed the first class subject</summary>
      <description/>
      <version>1.14.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.concepts.versioned.tables.md</file>
      <file type="M">docs.content.docs.dev.table.concepts.timezone.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.timezone.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-1 01:00:00" id="24097" opendate="2021-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the streaming check in StreamTableEnvironment in PyFlink</summary>
      <description>Since it has supported to DataStream batch mode in StreamTableEnvironment in FLINK-20897, it should also work in PyFlink. Currently there are a few checks in the Python code and we should remove them.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-2 01:00:00" id="24120" opendate="2021-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document MALLOC_ARENA_MAX as workaround for glibc memory leak</summary>
      <description>My task will do a savepoint every hour, so every hour will do a savepoint. From the memory monitoring, it can be seen that the memory of each hour will soar up, although the memory will drop a little later, but from every hour From the point of view of the memory peak on the whole point, the memory continues to rise little by little, and eventually rises to the limited memory, which will lead to being killed by k8SÂ Â </description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.trouble.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.trouble.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-2 01:00:00" id="24126" opendate="2021-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use increment of bytes-consumed-total for updating numBytesIn in KafkaSource</summary>
      <description/>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.metrics.KafkaSourceReaderMetrics.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaWriter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-11-3 01:00:00" id="24138" opendate="2021-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automated architectural tests</summary>
      <description>See ML thread:Â https://lists.apache.org/thread.html/r35b679f0b0d83be8a4912dcd2155e28b316f476547ae5dab601bda65%40%3Cdev.flink.apache.org%3E</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-3 01:00:00" id="24155" opendate="2021-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate documentation for how to configure the CheckpointFailureManager</summary>
      <description>Documentation added in FLINK-23916 should be translated to it's Chinese counterpart. Note that this applies to three separate commits:merged to master as cd01d4c0279merged to release-1.14 as 2e769746bf2merged to release-1.13 as e1a71219454</description>
      <version>1.14.0,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-9-6 01:00:00" id="24170" opendate="2021-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Building a fresh clone with latest Maven fails</summary>
      <description>Building with the latest Maven fails during the highest-dir goal:Â {{Cannot find a single highest directory for this project set. First two candidates directories don't share a common root. }}Â As suggested on this SO answer https://stackoverflow.com/questions/3084629/finding-the-root-directory-of-a-multi-module-maven-reactor-project, the directory-of goal works better</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-6 01:00:00" id="24172" opendate="2021-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API join documentation for Java has missing end quote after table name</summary>
      <description>The table api join documentation has missing ending quote after table name:Â Table left = tableEnv.from("MyTable).select($("a"), $("b"), $("c"));Â </description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.zh.docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-9-7 01:00:00" id="24199" opendate="2021-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose StreamExecutionEnvironment#configure in Python API</summary>
      <description>There are certain parameters that can be configured only through the underlying configuration of StreamExecutionEnvironment e.g. (execution.checkpointing.checkpoints-after-tasks-finish.enabled).We should be able to set those in the Python API.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-8 01:00:00" id="24212" opendate="2021-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>kerberos krb5.conf file is mounted as empty directory, not the expected file</summary>
      <description>From FLINK-18971ï¼we can mount kerberos krb5 conf file to pod with path /etc/krb5.confï¼however if the krb5 conf file is not named krb5.conf (e.g named mykrb5.conf)ï¼the mount path /etc/krb5.conf in pod will be an empty directory, not a file that we expect.root@mykrb5-conf-test-6dd5c76f87-vfwh5:/# ls /etc/krb5.conf/ -latotal 8drwxrwxrwx 2 root root 4096 Sep 8 10:42 .drwxr-xr-x 1 root root 4096 Sep 8 10:42 ..Â Â The reason is that, the codeÂ  in KerberosMountDecrator#decroateFlinkPod, we create the deployment like this:Â ... volumeMounts: - mountPath: /etc/krb5.conf name: my-krb5conf-volume subPath: krb5.conf ... volumes: - configMap: defaultMode: 420 items: - key: mykrb5.conf path: mykrb5.conf name: my-krb5conf name: my-krb5conf-volumepath value should be set to const value "krb5.conf", not the file name that user provide (path: mykrb5.conf).Â we can use the yaml description file attachment to reproduce the problem.Â Â mykrb5conf.yamlÂ </description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.KerberosMountDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.KerberosMountDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-8 01:00:00" id="24213" opendate="2021-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deadlock in QueryableState Client</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23750&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&amp;l=15476 Found one Java-level deadlock:Sep 08 11:12:50 =============================Sep 08 11:12:50 "Flink Test Client Event Loop Thread 0":Sep 08 11:12:50 waiting to lock monitor 0x00007f4e380309c8 (object 0x0000000086b2cd50, a java.lang.Object),Sep 08 11:12:50 which is held by "main"Sep 08 11:12:50 "main":Sep 08 11:12:50 waiting to lock monitor 0x00007f4ea4004068 (object 0x0000000086b2cf50, a java.lang.Object),Sep 08 11:12:50 which is held by "Flink Test Client Event Loop Thread 0"</description>
      <version>1.14.0,1.12.6,1.13.3</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.ServerConnection.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-28 01:00:00" id="2422" opendate="2015-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web client is showing a blank page if "Meta refresh" is disabled in browser</summary>
      <description>A user reported via the Flink IRC channel that Firefox was showing only a blank page instead of the web client.We should add a link to that page as well, so that users can click it if the redirect doesn't work.Workaround: browse to launch.html directly.</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.resources.web-docs.index.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-9 01:00:00" id="24232" opendate="2021-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Archiving of suspended jobs prevents breaks subsequent archive attempts</summary>
      <description>To archive a job we write a file that uses the job ID as the name. Since suspended jobs are handled like other terminal jobs they are also being archived.When that job then later resumes any attempt to archive the job on termination will fail because an archive already exists.The simplest option is to add a suffix if an archive already exists, like "_1".</description>
      <version>1.14.0,1.13.1,1.12.5</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.AbstractDispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-3-13 01:00:00" id="24274" opendate="2021-9-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong parameter order in documentation of State Processor API</summary>
      <description>Wrong order of parameters path and stateBackend in example code ofÂ State Processor Api # modifying-savepointsÂ Â </description>
      <version>None</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.libs.state.processor.api.md</file>
      <file type="M">docs.content.zh.docs.libs.state.processor.api.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-14 01:00:00" id="24276" opendate="2021-9-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary info in Loopback mode</summary>
      <description>If the job runs in loopback mode, it will print unnecessary info `apache_beam.typehints.native_type_compatibility - INFO - Using Any for unsupported type: typing.SequenceT` in the console. We need to remove this confusing info.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.worker.pool.service.py</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2021-9-16 01:00:00" id="24305" opendate="2021-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BatchPandasUDAFITTests.test_over_window_aggregate_function fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24170&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901&amp;l=23011Sep 15 20:40:43 cls = &lt;class 'pyflink.table.tests.test_pandas_udaf.BatchPandasUDAFITTests'&gt;Sep 15 20:40:43 actual = JavaObject id=o8666Sep 15 20:40:43 expected = ['+I[1, 4.3333335, 13, 5.5, 3.0, 3.0, 4.3333335, 8.0, 5.0, 5.0]', '+I[1, 4.3333335, 5, 4.3333335, 3.0, 3.0, 2.5, 4.333....0, 4.0, 2.0]', '+I[2, 2.0, 9, 2.0, 4.0, 4.0, 2.0, 2.0, 4.0, 4.0]', '+I[3, 2.0, 3, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0]']Sep 15 20:40:43 Sep 15 20:40:43 @classmethodSep 15 20:40:43 def assert_equals(cls, actual, expected):Sep 15 20:40:43 if isinstance(actual, JavaObject):Sep 15 20:40:43 actual_py_list = cls.to_py_list(actual)Sep 15 20:40:43 else:Sep 15 20:40:43 actual_py_list = actualSep 15 20:40:43 actual_py_list.sort()Sep 15 20:40:43 expected.sort()Sep 15 20:40:43 assert len(actual_py_list) == len(expected)Sep 15 20:40:43 &gt; assert all(x == y for x, y in zip(actual_py_list, expected))Sep 15 20:40:43 E AssertionError: assert FalseSep 15 20:40:43 E + where False = all(&lt;generator object PyFlinkTestCase.assert_equals.&lt;locals&gt;.&lt;genexpr&gt; at 0x7f792d98b900&gt;)</description>
      <version>1.14.0,1.12.5,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-16 01:00:00" id="24308" opendate="2021-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate KafkaSink docs to chinese</summary>
      <description>With https://issues.apache.org/jira/browse/FLINK-23664 only the English documentation was updated. We also have to update the Chinese docs.</description>
      <version>1.14.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-16 01:00:00" id="24310" opendate="2021-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>A bug in the BufferingSink example in the doc</summary>
      <description>The following line in the BufferingSink onÂ this pageÂ has a bug:if (bufferedElements.size() == threshold) {It should be &gt;=Â instead ofÂ ==Â , because when restoring from a checkpoint during downscaling, the task may get more elements than the threshold.Â </description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.state.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.state.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-17 01:00:00" id="24315" opendate="2021-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot rebuild watcher thread while the K8S API server is unavailable</summary>
      <description>In native k8s integration, Flink will try to rebuild the watcher thread if the API server is temporarily unavailable. However, if the jitter is longer than the web socket timeout, the rebuilding of the watcher will timeout and Flink cannot handle the pod event correctly.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.13.3,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-17 01:00:00" id="24317" opendate="2021-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the test implementation in test_flat_aggregate</summary>
      <description/>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.row.based.operation.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-17 01:00:00" id="24318" opendate="2021-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Casting a number to boolean has different results between &amp;#39;select&amp;#39; fields and &amp;#39;where&amp;#39; condition</summary>
      <description>The same cast in the following two sql:// SQL 1SELECT cast(0.1 as boolean)// SQL 2SELECT * from test2 where cast(0.1 as boolean)has different results.The cast result in SQL 1 is true and the cast in SQL 2 is false.</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinDependentConditionDerivationRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionTypeCoerceRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionEqualityTransferRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceScanRule.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-9-18 01:00:00" id="24336" opendate="2021-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink TableEnvironment executes the SQL randomly MalformedURLException with the configuration for &amp;#39;pipeline.classpaths&amp;#39;</summary>
      <description>When I run flink client to submit a python based workflow, I got the MalformedURLException like this:https://gist.github.com/is/faabafc7f8750f3f3161fbb6517ed6ffAfter some debug work, I found the problem is related with TableEvneriontment.execute_sql. The root cause is TableEvenriontment._add_jars_to_j_env_config in pyflink/table/TableEnverionment.py.```if j_configuration.containsKey(config_key): for url in j_configuration.getString(config_key, "").split(";"): jar_urls_set.add(url)```In our case, pipeline.classpaths was set by empty list valuefrom FromProgramOption, so the upper code block willintroduce a empty string ("") into pipeline.classpaths, for example"a.jar;b.jar;;c.jar", and it will cause the according exception.Another problem, the order of string set in python is notdeterminate, so ";".join(jar_urls_set) does NOT keep theclasspaths order. The list is more suiteable in this case.</description>
      <version>1.14.0,1.13.2,1.14.1</version>
      <fixedVersion>1.13.3,1.12.8,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-11-23 01:00:00" id="24366" opendate="2021-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary/misleading error message about failing restores when tasks are already canceled.</summary>
      <description>The following line is logged in all cases where the restore operation fails. The check whether the task is canceled comes only after that line.The fix would be to move the log line to after the check.Exception while restoring my-stateful-task from alternative (1/1), will retry while more alternatives are available.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.BackendRestorerProcedure.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-26 01:00:00" id="24376" opendate="2021-9-26 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Operator name in OperatorCoordinator should not use chained name</summary>
      <description>Currently the operator name passed to CoordinatedOperatorFactory#getCoordinatorProvider is a chained operator name (e.g. Source -&gt; Map) instead of the name of coordinating operator, which might be misleading.Â </description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.7,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-27 01:00:00" id="24380" opendate="2021-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink should handle the state transition of the pod from Pending to Failed</summary>
      <description>In K8s, there is five phases in pod's lifecycle: Pending, Running, Secceeded, Failed and Unknown. Currently, Flink does not handle the state transition of the pod from Pending to Failed. If a pod failed from Pending by `OutOfCPU` or `OutOfMem`, it will never be released and Flink keep waiting for it.To fix this issue, Flink should terminate the pod in Failed phase proactively.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.13.3,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPod.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-10-28 01:00:00" id="24393" opendate="2021-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for all currently supported cast combinations</summary>
      <description>Currently there are only tests to check feasibility of casting between various types but not actual tests verifying the conversion. It would be nice to have them first before addressing the various bugs and missing functionality of CAST.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.BuiltInFunctionTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-28 01:00:00" id="24394" opendate="2021-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor scalar function testing infrastructure to allow testing multiple columns</summary>
      <description>With the casting tests, to verify already supported conversion coming up soon, executing a table pipeline for each one of them is expensive. Allow the execution of multiple test cases as multiple columns of a table in a single execution to speed up the process.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.StringFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.MiscFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.MathFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.JsonFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.GreatestLeastFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.ConstructedAccessFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CoalesceFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.BuiltInFunctionTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-30 01:00:00" id="24407" opendate="2021-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar connector chinese document link to Pulsar document location incorrectly.</summary>
      <description>Pulsar connector chinese document link to Pulsar document location incorrectly.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.connectors.datastream.pulsar.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-11-30 01:00:00" id="2441" opendate="2015-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Introduce an OpInfo object on the python side</summary>
      <description>All information required to construct on operation are currently saved in a plain dictionary on the python side, whose fields are generally accessed using a variety of string constants.so right now you find lines like: op_info[_Fields.KEYS] = keysThe following shortcomings exist in the current system: There is no central place to define default values. This is done all over the place, to some extent in a redundant way, It produces fairly long code, is surprisingly cumbersome to write.Instead i would like to add a separate OperationInfo object. This code be a special dictionary with preset values for each field, but due to points 2 and 3, Id prefer having an attribute for every field. the resulting code would look like this:op_info.keys = keysisn't that lovely.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.type.deduction.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.DataSet.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Constants.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-30 01:00:00" id="24410" opendate="2021-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Confluent Platform OSS version in end-to-end tests</summary>
      <description>Flink uses Confluent Platform OSS/community edition 5.0.0, which doesn't exist in a Scala 2.12 version. We should bump the used version to at least 5.2.x.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.confluent.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-30 01:00:00" id="24431" opendate="2021-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Kinesis][EFO] EAGER registration strategy does not work when job fails over</summary>
      <description>BackgroundThe EFO Kinesis connector will register and de-register stream consumers based on the configured registration strategy. When EAGER is used, the client (usually job manager) will register the consumer and then the task managers will de-register the consumer when job stops/fails. If the job is configured to restart on fail, then the consumer will not exist and the job will continuously fail over.SolutionThe proposal is to not deregister the stream consumer when EAGER is used. The documentation should be updated to reflect this.</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.13.3,1.12.8,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.StreamConsumerRegistrarUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.StreamConsumerRegistrarUtil.java</file>
      <file type="M">docs.content.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kinesis.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-10-4 01:00:00" id="24445" opendate="2021-10-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move RPC System packaging to package phase</summary>
      <description>mvn compile/test currently fails because the copying of the flink-rpc-akka jar is done in the generate-sources phase.We should move this copying to the packaging phase.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka-loader.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-10-6 01:00:00" id="24462" opendate="2021-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor casting rules in a similar fashion to DataStructureConverter</summary>
      <description>The goal of this issue is to reorganize CAST logic in rules, similarly to DataStructureConverter. This makes the casting rules easier to debug and extend, allows us to reuse some of the rules for https://issues.apache.org/jira/browse/FLINK-21456 without duplicating any code and simplifies/cleanups the code generator code base. These rules can be reused in the context of https://issues.apache.org/jira/browse/FLINK-24385 as well.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.generated.CompileUtilsTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedClass.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.CompileUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LongHashJoinGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.JsonGenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.EqualiserCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-7 01:00:00" id="24467" opendate="2021-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set min and max buffer size even if the difference less than threshold</summary>
      <description>Right now, we apply a new buffer size only if it differs from the old buffer size more than the configured threshold but if the old buffer size is close to the max or min value less than this threshold we are always stuck on this value. For example, if we have the old buffer size 22k and our threshold is 50% then the value which we can apply should 33k but this is impossible because the max value is 32k so once we calculate the buffer size to 22k it is impossible to increase it.The suggestion is to apply the changes every time when we calculate the new value to min or max size and the old value was different.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.bufferdebloat.BufferDebloaterTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.bufferdebloat.BufferDebloater.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-7 01:00:00" id="24476" opendate="2021-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename all ElasticSearch to Elasticsearch (without camel case)</summary>
      <description>Elasticsearch is a trademark and service mark. It's incorrect to use CamelCase: it's not two words, nor is the internal capital S part of the brand.Where possible, we should use the single word without an internal capital S, especially in user documentation.(Luckily, I don't believe there are any user-facing APIs with incorrect capitalization.)</description>
      <version>1.14.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.release-notes.flink-1.6.md</file>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.release-notes.flink-1.6.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-31 01:00:00" id="2448" opendate="2015-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>registerCacheFile fails with MultipleProgramsTestbase</summary>
      <description>When trying to register a file using a constant name an expection is thrown saying the file was already cached.This is probably because the same environment is reused, and the cacheFile entries are not cleared between runs.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestEnvironment.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.CollectionTestEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-8 01:00:00" id="24481" opendate="2021-10-8 00:00:00" resolution="Done">
    <buginformation>
      <summary>Translate buffer debloat documenation to chinese</summary>
      <description>It needs to translate the documentation of the buffer debloat to chinese. The original documentation was introduced here - https://issues.apache.org/jira/browse/FLINK-23458</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.deployment.memory.network.mem.tuning.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.setup.tm.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.setup.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-12 01:00:00" id="24516" opendate="2021-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modernize Maven Archetype</summary>
      <description>The maven archetypes used by many to start their first Flink application do not reflect the project's current state.Â Issues: They still bundle the DataSet API and recommend it for batch processing The JavaDoc recommends deprecated APIsÂ </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.StreamingJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.BatchJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.StreamingJob.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.BatchJob.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-31 01:00:00" id="2453" opendate="2015-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update POM to use Java7 as the source and target version</summary>
      <description>This sub task is created to track effort to update POM files to move to Java7</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.md</file>
      <file type="M">docs.setup.cluster.setup.md</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-staging.flink-avro.pom.xml</file>
      <file type="M">flink-quickstart.flink-tez-quickstart.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-10-14 01:00:00" id="24541" opendate="2021-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ConfigurationUtils#assembleDynamicConfigsStr should consider special characters</summary>
      <description>Without quoting, some special characters will be misunderstood by shell, e.g. ';' used in list type options.</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-1 01:00:00" id="2460" opendate="2015-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ReduceOnNeighborsWithExceptionITCase failure</summary>
      <description>I noticed a build error due to failure on this case. It was on a branch of my fork, which didn't actually have anything to do with the failed test or the runtime system at all.Here's the error log: https://s3.amazonaws.com/archive.travis-ci.org/jobs/73695554/log.txt</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SubpartitionTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionViewSyncIO.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionViewAsyncIO.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-20 01:00:00" id="24600" opendate="2021-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Duplicate 99th percentile displayed in checkpoint summary</summary>
      <description>Â flink checkpoints page has two p99 which is duplicated</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-11-21 01:00:00" id="24608" opendate="2021-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sinks built with the unified sink framework do not receive timestamps when used in Table API</summary>
      <description>All sinks built with the unified sink framework extract the timestamp from the internal StreamRecord. The Table API does not facilitate the timestamp field in the StreamRecord but extracts the timestamp from the actual data. We either have to use a dedicated operator before all the sinks to simulate the behavior or allow a customizable timestamp extraction during the sink translation.</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.match.RowtimeProcessFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-21 01:00:00" id="24612" opendate="2021-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka test container creates a large amount of logs</summary>
      <description>When we use a testcontainer setup we try to forward all container STDOUT logs to the surrounding test logger. Unfortunately, Kafka loggers are by default writing a large number of logs because some of the internal loggers are defaulting to TRACE logging.A good example is this test job https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25084&amp;view=logs&amp;j=32a18cd8-d404-5807-996d-abcee436b891where one of the test was stuck and the generated artifact is ~25GB. This makes debugging very hard because the file is hard to parse.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaTransactionLogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.FlinkKafkaInternalProducerITCase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-11-25 01:00:00" id="24627" opendate="2021-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port JUnit 4 rules to JUnit5 extensions</summary>
      <description>We have to use junit5 extensions to replace the existed junit4 rules in order to change tests to junit5 in flink. There are some generic rules that should be provided in advance.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskCancelAsyncProducerConsumerITCase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-10-25 01:00:00" id="24634" opendate="2021-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Java 11 profile should target JDK 8</summary>
      <description>Thee java11 profile currently targets Java 11. This was useful because we saw that doing so reveals additional issues that are not detected when building for Java 8. The end goal was to ensure a smooth transition once we switch.However, this has adverse effects on developer productivity.If you happen to switch between Java versions (for example, because of separate environments, or because certain features require Java 8), then you can easily run into UnsupportedVersionErrors when attempting to use Java 8 with Java 11 bytecode.IntelliJ also picks up on this and automatically sets the language level to 11, which means that it will readily allow you to use Java 11 exclusive APIs that will fail on CI later on.To remedy this I propose to split the profile.The java11 profile will pretty much stay as is, except that it is targeting java 8. The value proposition of this profile is being able to build Flink for Java 8 with Java 11.A new explicitly-opt-in java11-target profile then sets the target version to Java 11, which we will use on CI. This profile will ensure that we can readily switch to Java 11 as the target in the future.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-11-25 01:00:00" id="24639" opendate="2021-10-25 00:00:00" resolution="Done">
    <buginformation>
      <summary>Improve assignment of Kinesis shards to subtasks</summary>
      <description>The default assigner of Kinesis shards to Flink subtasks simply takes the hashCode() of the StreamShardHandle (an integer), which is then interpreted modulo the number of subtasks. This basically does random-ish but deterministic assignment of shards to subtasks.However, this can lead to some subtasks getting several times the number of shards as others. To prevent those unlucky subtasks from being overloaded, the overall Flink cluster must be over-provisioned, so that each subtask has more headroom to handle any over-assignment of shards.We can do better here, at least if Kinesis is being used in a common way. Each record sent to a Kinesis stream has a particular hash key in the range [0, 2^128), which is used to determine which shard gets used; each shard has an assigned range of hash keys. By default Kinesis assigns each shard equal fractions of the hash-key space. And when you scale up or down using UpdateShardCount, it tries to maintain equal fractions to the extent possible. Also, a shard's hash key range is fixed at creation; it can only be replaced by new shards, which split it, or merge it.Given the above, one way to assign shards to subtasks is to do a linear mapping from hash-keys in range [0, 2^128) to subtask indices in [0, nSubtasks). For the 'coordinate' of each shard we pick the middle of the shard's range, to ensure neither subtask 0 nor subtask (n-1) is assigned too many.However this will probably not be helpful for Kinesis users that don't randomly assign partition or hash keys to Kinesis records. The existing assigner is probably better for them.I ran a simulation of the default shard assigner versus some alternatives, using shards taken from one of our Kinesis streams; results attached. The measure I used I call 'overload' and it measures how many times more shards the most heavily-loaded subtask has than is necessary. (DEFAULT is the default assigner, Sha256 is similar to the default but with a stronger hashing function, ShardId extracts the shard number from the shardId and uses that, and HashKey is the one I describe above.)Patch is at: https://github.com/apache/flink/compare/master...john-karp:uniform-shard-assigner?expand=1</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">docs.content.docs.connectors.datastream.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kinesis.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-11-26 01:00:00" id="24658" opendate="2021-10-26 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Debug logs for buffer size calculation</summary>
      <description>Since the buffer debloater recalculates buffer size based on several different parameters. It makes sense to add debug logging to print all of them in case of necessary debugging.</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.throughput.BufferDebloaterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.throughput.BufferDebloater.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-27 01:00:00" id="24662" opendate="2021-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink sphinx check failed with "node class &amp;#39;meta&amp;#39; is already registered, its visitors will be overridden"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25481&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=8d78fe4f-d658-5c70-12f8-4921589024c3==========mypy checks... [SUCCESS]===========Oct 26 22:08:34 rm -rf _build/*Oct 26 22:08:34 /__w/1/s/flink-python/dev/.conda/bin/sphinx-build -b html -d _build/doctrees -a -W . _build/htmlOct 26 22:08:34 Running Sphinx v2.4.4Oct 26 22:08:34 Oct 26 22:08:34 Warning, treated as error:Oct 26 22:08:34 node class 'meta' is already registered, its visitors will be overriddenOct 26 22:08:34 Makefile:76: recipe for target 'html' failed</description>
      <version>1.12.0,1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-3 01:00:00" id="2467" opendate="2015-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Example WordCountStorm.jar is not packaged correctly</summary>
      <description>After moving storm compatibility to flink-contrib, WordCountStorm example is not package correctly any more and the jar is not usable.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.assembly.word-count-storm.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-28 01:00:00" id="24676" opendate="2021-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema does not match if explain insert statement with partial column</summary>
      <description>create table MyTable (a int, b int) with ('connector' = 'datagen');create table MySink (c int, d int) with ('connector' = 'print');explain plan for insert into MySink(d) select a from MyTable where a &gt; 10;If execute the above statement, we will get the following exceptionorg.apache.flink.table.api.ValidationException: Column types of query result and sink for registered table 'default_catalog.default_database.MySink' do not match.Cause: Different number of columns.Query schema: &amp;#91;a: BIGINT&amp;#93;Sink schema: &amp;#91;d: BIGINT, e: INT&amp;#93;</description>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-28 01:00:00" id="24686" opendate="2021-10-28 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Make doc clear on AsyncFunction::timeout() overriding</summary>
      <description>Sometimes, a user overrides AsyncFunction::timeout() with an empty method or with only logging code. This causes the timeout does not signal back to the framework and job stuck especially when using orderedWait(). Opening this Jira to make the doc clear on this.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.operators.asyncio.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.asyncio.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-10-29 01:00:00" id="24695" opendate="2021-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update how to configure unaligned checkpoints in the documentation</summary>
      <description>It looks like we don't have a code example how to enabled unaligned checkpoints anywhere in the docs. That should be fixed.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.checkpointing.under.backpressure.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpointing.under.backpressure.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-11-29 01:00:00" id="24699" opendate="2021-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move scalastyle execution to validation phase</summary>
      <description>For some reason the scalstyle plugin by default runs in the verify phase.I propose to move it to the validate phase where other source QA plugins are run, which also makes it easier to run it locally.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-1 01:00:00" id="24714" opendate="2021-11-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Validate partition columns for ResolvedCatalogTable</summary>
      <description>Currently, partition columns are not validated and might not exist in the schema.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.CatalogBaseTableResolutionTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-1 01:00:00" id="24715" opendate="2021-11-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update multiple Jackson dependencies to v2.13.0</summary>
      <description>Flink uses multiple com.fasterxml.jackson components in different Flink modules. We should update these to the latest versionExample (one or more used in ElasticSearch connector, Kinesis, FS Hadoop/Presto, AVRO, Python, Table API etc) com.fasterxml.jackson.core:jackson-core:2.12.1 com.fasterxml.jackson.core:jackson-databind:2.12.1 com.fasterxml.jackson.core:jackson-annotations:2.12.1 com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:2.12.1 com.fasterxml.jackson.dataformat:jackson-dataformat-smile:2.12.1 com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.12.1Could all be updated to: com.fasterxml.jackson.core:jackson-core:2.13.0 com.fasterxml.jackson.core:jackson-databind:2.13.0 com.fasterxml.jackson.core:jackson-annotations:2.13.0 com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:2.13.0 com.fasterxml.jackson.dataformat:jackson-dataformat-smile:2.13.0 com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.13.0</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-11-1 01:00:00" id="24719" opendate="2021-11-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Postgres test dependencies to 0.13.4</summary>
      <description>Update Postgress and Postgress test dependencies to the latest version</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-1 01:00:00" id="24720" opendate="2021-11-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove flink-runtime-web dependency from flink-tests</summary>
      <description>For some reason flink-tests has a test dependency on flink-runtime-web and it's test-jar.From what I can tell however these are entirely unused, and removing said dependency neither fails any test nor results in any additional test to be skipped.I propose to remove these dependencies to simplify the dependency tree. This means that developers only have to worry about the UI being built when they build flink-dist/flink-docs. It also removes a bottleneck for parallel builds.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-1 01:00:00" id="24724" opendate="2021-11-1 00:00:00" resolution="Later">
    <buginformation>
      <summary>Update japicmp jaxb dependencies</summary>
      <description>Update com.sun.xml.bind:jaxb-impl and com.sun.xml.bind:jaxb-core to the latest available minor version. This means an upgrade from the 2.3.0 version (released August 2017) to 3.0.2 (released August 2021)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-1 01:00:00" id="24725" opendate="2021-11-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Cython to the latest version</summary>
      <description>Update Cython from 0.29.16 to 0.29.24</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-2 01:00:00" id="24733" opendate="2021-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data loss in pulsar source when using shared mode</summary>
      <description>Descriptionï¼ Noticed that when using Flink-connector-pulsar with Shared mode (default mode), some messages are lost. Detailsï¼ When a topic partition does not receive message for more than 10s (pulsar.source.maxFetchTime), the next coming message will be dropped by source.In stream applications, this situation which there is no data for more than 10s is very common. So this bug will cause data loss.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-2 01:00:00" id="24739" opendate="2021-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>State requirements for Flink&amp;#39;s application mode in the documentation</summary>
      <description>If I am not mistaken, then Flink won't ship jars when using the application mode because it assumes the jars to be on the classpath. If this is true, then we should make this requirement a bit more prominent in the deployment documentation because currently it is only subtly hinted at.Alternatively, we could enable Flink to ship the user code jars to its components also when using the application mode.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-2 01:00:00" id="24740" opendate="2021-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update testcontainers dependency to v1.16.2</summary>
      <description>We should update our testcontainers dependency to the latest version, which is 1.16.2Main benefits (based on https://github.com/testcontainers/testcontainers-java/releases) Better startup performance for all containers Faster Cassandra startup Host port access for containers (make hosts ports accessible to containers, even after the container has started) New Azure Cosmos DB module</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-2 01:00:00" id="24742" opendate="2021-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL client add info about key strokes to docs</summary>
      <description>SQL client supports key strokes from jline.Unfortunately there is no docs about that in jline however there is source from which it could be found &amp;#91;1&amp;#93;here it is a list of most useful key strokes which are already supported by all existing Flink SQL client Key-Stroke Description `alt-b` Backward word `alt-f` Forward word `alt-c` Capitalize word `alt-l` Lowercase word `alt-u` Uppercase word `alt-d` Kill word `alt-n` History search forward `alt-p` History search backward `alt-t` Transpose words `ctrl-a` To the beginning of line `ctrl-e` To the end of line `ctrl-b` Backward char `ctrl-f` Forward char `ctrl-d` Delete char `ctrl-h` Backward delete char `ctrl-t` Transpose chars `ctrl-i` Invoke completion `ctrl-j` Submit a query `ctrl-m` Submit a query `ctrl-k` Kill the line to the right from the cursor `ctrl-w` Kill the line to the left from the cursor `ctrl-u` Kill the whole line `ctrl-l` Clear screen `ctrl-n` Down line from history `ctrl-p` Up line from history `ctrl-r` History incremental search backward `ctrl-s` History incremental search forward &amp;#91;1&amp;#93; https://github.com/jline/jline3/blob/997496e6a6338ca5d82c7dec26f32cf089dd2838/reader/src/main/java/org/jline/reader/impl/LineReaderImpl.java#L5907</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-4 01:00:00" id="24766" opendate="2021-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ceiling/flooring dates to day return wrong results</summary>
      <description>Query to reproduceselect ceil(date '2021-11-04' to day) as `ceil`, floor(date '2021-11-04' to day) as `floor`;givesceil floor8525-03-02 1970-01-01expectedceil floor2021-11-05 2021-11-05</description>
      <version>1.14.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.TimeFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.FloorCeilCallGen.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.DateTimeUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-4 01:00:00" id="24777" opendate="2021-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Processed (persisted) in-flight data description miss on Monitoring Checkpointing page</summary>
      <description>Processed (persisted) in-flight data description is missed, we need to merge fromÂ Processed (persisted) in-flight data and Persisted) in-flight dataÂ </description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.monitoring.checkpoint.monitoring.md</file>
      <file type="M">docs.content.zh.docs.ops.monitoring.checkpoint.monitoring.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-12-5 01:00:00" id="24785" opendate="2021-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Relocate RocksDB&amp;#39;s log under flink log directory by default</summary>
      <description>Previously, RocksDB's log locates at its own DB folder, which makes the debuging RocksDB not so easy. We could let RocksDB's log stay in Flink's log directory by default.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-5 01:00:00" id="24786" opendate="2021-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce RocksDB&amp;#39;s statistics related options</summary>
      <description>Currently, Flink only support RocksDB's native metrics of property related. However, such metrics cannot help on performance tunning, we could introduce statistics related metrics to help debug performance related problem.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResource.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBNativeMetricMonitorTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBNativeMetricOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBNativeMetricMonitor.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBHandle.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.native.metric.configuration.html</file>
      <file type="M">docs.content.docs.deployment.config.md</file>
      <file type="M">docs.content.zh.docs.deployment.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-5 01:00:00" id="24787" opendate="2021-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more details of state latency tracking documentation</summary>
      <description>Current documentation only tells how to enable or configure state latency tracking related options. We could add more details of state specific descriptions.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-11-5 01:00:00" id="24798" opendate="2021-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump commons-cli to v1.5.0</summary>
      <description>Bump commons-cli:commons-cli:1.4 to commons-cli:commons-cli:1.5.0</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-8 01:00:00" id="24816" opendate="2021-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump com.rabbitmq:amqp-client to v5.13.1</summary>
      <description>Bump com.rabbitmq:amqp-client:5.9.0 to com.rabbitmq:amqp-client:5.13.1List of fixes for the RabbitMQ client: https://github.com/rabbitmq/rabbitmq-java-client/releases</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-rabbitmq.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-8 01:00:00" id="24820" opendate="2021-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Examples in documentation for value1 IS DISTINCT FROM value2 are wrong</summary>
      <description>Currently it is stated in docs for value1 IS DISTINCT FROM value2E.g., 1 IS NOT DISTINCT FROM NULL returns TRUE; NULL IS NOT DISTINCT FROM NULL returns FALSE.In fact they return opposite values.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-9 01:00:00" id="24848" opendate="2021-11-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error codes for savepoint handlers</summary>
      <description>We currently return a 500 Server Error in pretty much all error scenarios. We can improve this a fair bit, like returning 404 if no operation is known for a given key, or 409 Conflict when attempting to trigger an operation that has already failed.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.UnknownOperationKeyException.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-11-10 01:00:00" id="24861" opendate="2021-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to disable caching missing key for lookup cache</summary>
      <description>Ideally, in case of cache miss for a key, or with null value fetch for key, key shouldn't be cached</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunctionTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcLookupTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.options.JdbcLookupOptions.java</file>
      <file type="M">docs.content.docs.connectors.table.jdbc.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-11 01:00:00" id="24869" opendate="2021-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-core should be provided in flink-file-sink-common</summary>
      <description>As example flink-connector-files brings flink-core with compile scope via flink-file-sink-common.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-file-sink-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-11 01:00:00" id="24875" opendate="2021-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Distributed e2e tests across 2 profiles</summary>
      <description>It's that time of the year again :/</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-11 01:00:00" id="24876" opendate="2021-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-12 01:00:00" id="24880" opendate="2021-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error messages "OverflowError: timeout value is too large" shown when executing PyFlink jobs</summary>
      <description>The following exception shown when executing PyFlink jobs according to the demo show in https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/python/datastream/intro_to_datastream_api/ Â  Â Â Common Structure of Python DataStream API ProgramsÂ Â Exception in thread Thread-15:Traceback (most recent call last):Â  File "D:\soft\py\lib\threading.py", line 932, in _bootstrap_innerÂ  Â  self.run()Â  File "C:\Users\wangdonglin\AppData\Roaming\Python\Python38\site-packages\apache_beam\runners\worker\data_plane.py", line 218, in runÂ  Â  while not self._finished.wait(next_call - time.time()):Â  File "D:\soft\py\lib\threading.py", line 558, in waitÂ  Â  signaled = self._cond.wait(timeout)Â  File "D:\soft\py\lib\threading.py", line 306, in waitÂ  Â  gotit = waiter.acquire(True, timeout)OverflowError: timeout value is too large</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.stream.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.stream.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.utils.operation.utils.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-12 01:00:00" id="24884" opendate="2021-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink flame graph webui bug</summary>
      <description>i can not compile success when i port the flame graph feature to our low version of flink.but it is success in the high version of flink Â </description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.@types.d3-flame-graph.index.d.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-12 01:00:00" id="24889" opendate="2021-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink SQL Client should print corrently multisets</summary>
      <description>Probably the easiest way to reproduce is CREATE TABLE flink_multiset_example ( m multiset&lt;BIGINT&gt; ) WITH ( 'connector' = 'datagen' );select * from flink_multiset_example;I think it relates to https://issues.apache.org/jira/browse/FLINK-21456</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.MapToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleProvider.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-16 01:00:00" id="24928" opendate="2021-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve type-safety in Flink UI</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.test.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.humanize-watermark.pipe.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.humanize-duration.pipe.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.humanize-date.pipe.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.checkpoint-badge.checkpoint-badge.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.checkpoint-badge.checkpoint-badge.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.backpressure-badge.backpressure-badge.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.backpressure-badge.backpressure-badge.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.resize.resize.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.svg-container.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.svg-container.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.safe-any.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.public-api.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.interceptor.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.@types.d3-flame-graph.index.d.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.task-badge.task-badge.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-list.job-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.flame-graph.flame-graph.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.graph.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.dagre.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.task-manager.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.plan.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.overview.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-vertex-task-manager.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-timeline.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-subtask.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-overview.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-manager.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-flamegraph.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-exception.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-detail.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-config.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-backpressure.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-accumulators.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.jar.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.configuration.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-badge.job-badge.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.config.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.package.json</file>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.status.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.overview.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.metrics.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.jar.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.thread-dump.task-manager-thread-dump.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.stdout.task-manager-stdout.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-checkpoint.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.list.job-overview-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.list.job-overview-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.job-overview-drawer-taskmanagers.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.job-overview-drawer-taskmanagers.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.list.task-manager-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.list.task-manager-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.utils.deepFind.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.utils.public-api.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.humanize-bytes.pipe.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.utils.isNil.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.configuration.job-manager-configuration.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.log-detail.job-manager-log-detail.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.log-list.job-manager-log-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.logs.job-manager-logs.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.metrics.job-manager-metrics.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.stdout.job-manager-stdout.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.configuration.job-configuration.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.job.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.accumulators.job-overview-drawer-accumulators.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.accumulators.job-overview-drawer-accumulators.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.chart.job-overview-drawer-chart.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.detail.job-overview-drawer-detail.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.drawer.job-overview-drawer.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.timeline.job-timeline.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.overview.overview.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.overview.statistic.overview-statistic.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.submit.submit.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.submit.submit.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-detail.task-manager-log-detail.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-list.task-manager-log-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.logs.task-manager-logs.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.ts</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-11-19 01:00:00" id="24958" opendate="2021-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>correct the example and link for temporal table function documentation</summary>
      <description>correct the example and link for temporal table function documentation</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.joins.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.joins.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-11-19 01:00:00" id="24964" opendate="2021-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade frontend-maven-plugin to 1.11.0</summary>
      <description>In order to support compilation with ARM (e.g. Apple M1 chip), we need to bump our frontend-maven-plugin to 1.11.0.</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-22 01:00:00" id="24998" opendate="2021-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SIGSEGV in Kryo / C2 CompilerThread on Java 17</summary>
      <description>While running our tests on CI with Java 17 they failed infrequently with a SIGSEGV error.All occurrences were related to Kryo and happened in the C2 CompilerThread.Current thread (0x00007f1394165c00): JavaThread "C2 CompilerThread0" daemon [_thread_in_native, id=470077, stack(0x00007f1374361000,0x00007f1374462000)]Current CompileTask:C2: 14251 6333 4 com.esotericsoftware.kryo.io.Input::readString (127 bytes)The full error is attached to the ticket. I can also provide the core dump if needed.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-23 01:00:00" id="25016" opendate="2021-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump netty to 4.1.70</summary>
      <description>But Netty to the latest version for general improvements and a proper for FLINK-24197.</description>
      <version>None</version>
      <fixedVersion>shaded-15.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-23 01:00:00" id="25022" opendate="2021-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassLoader leak with ThreadLocals on the JM when submitting a job through the REST API</summary>
      <description>If a job is submitted using the REST API's /jars/:jarid/run endpoint, user code has to be executed on the JobManager and it is doing this in a couple of (pooled) dispatcher threads like Flink-DispatcherRestEndpoint-thread-*.If the user code is using thread locals (and not cleaning them up), they may remain in the thread with references to the ChildFirstClassloader of the job and thus leaking that.We saw this for the jsoniter scala library at the JM which creates ThreadLocal instances but doesn't remove them, but it can actually happen with any user code or (worse) library used in user code.Â There are a few workarounds a user can use, e.g. putting the library in Flink's lib/ folder or submitting via the Flink CLI, but these may actually not be possible to use, depending on the circumstances.Â A proper fix should happen in Flink by guarding against any of these things in the dispatcher threads. We could, for example, spawn a separate thread for executing the user's main() method and once the job is submitted exit that thread and destroy all thread locals along with it.</description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebSubmissionExtension.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarMessageParameters.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-25 01:00:00" id="25051" opendate="2021-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port raw &lt;-&gt; binary logic to CastRule</summary>
      <description>More details on the parent task</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-25 01:00:00" id="25052" opendate="2021-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port row to row cast logic to CastRule</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RowToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleProvider.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.AbstractCodeGeneratorCastRule.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.TableAssertions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-25 01:00:00" id="25053" opendate="2021-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to use the usrlib to load code in the user code class loader</summary>
      <description>With FLINK-13993 we introduced the usrlib directory that can be used to load code in the user code class loader. This functionality has not been properly documented so that it is very hard to use. I would suggest to change this so that our users can benefit from this cool feature.</description>
      <version>1.14.0,1.12.5,1.13.3,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-26 01:00:00" id="25079" opendate="2021-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Globally introduce assertj assertions for tests</summary>
      <description>As discussed on the dev@ mailing list, we advertise assertj instead of Hamcrest in the future.https://lists.apache.org/thread/33t7hz8w873p1bc5msppk65792z08rgt</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.ValueDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.utils.DataTypeUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.utils.DataTypeFactoryMock.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.TypeTestingUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.TypeInfoDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.logical.utils.LogicalTypeMergingTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.logical.utils.LogicalTypeChecksTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeParserTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeDuplicatorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastAvoidanceTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalCommonTypeTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeStrategiesTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.extraction.TypeInferenceExtractorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.extraction.DataTypeExtractorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.DataTypeTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.DataTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.ClassDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.data.utils.ProjectedRowDataTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.data.utils.JoinedRowDataTest.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.FlinkMatchers.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-12-29 01:00:00" id="25091" opendate="2021-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Official website document FileSink orc compression attribute reference error</summary>
      <description>I see the following version is like this &amp;#91;1.12ã1.13ã1.14 ããã&amp;#93; What should be quoted here is writerProperties Shouldn't be is writerProps docUrl</description>
      <version>1.12.0,1.13.0,1.14.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-29 01:00:00" id="25096" opendate="2021-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make errors happened during JobMaster initialization accessible through the exception history</summary>
      <description>Currently we are using flink version 1.13.2 and as per the flink documentation we should get all exceptions through exceptions api in exceptionHistory tag. While running few scenarios we observed that the below two exceptions are not coming in exceptionHistory tag but are coming in root-exception tag.Exception 1 - caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: java.io.FileNotFoundException: Cannot find checkpoint or savepoint file/directory 'C:\Users\abc\Documents\checkpoints\a737088e21206281db87f6492bcba074' on file system 'file'.Exception 2 - Caused by: java.lang.IllegalStateException: Failed to rollback to checkpoint/savepoint file:/mnt/c/Users/abc/Documents/checkpoints/a737088e21206281db87f6492bcba074/chk-144.Please find the attachment for the logs of above exceptions.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcessTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionGraphInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.exceptionhistory.RootExceptionHistoryEntry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-30 01:00:00" id="25109" opendate="2021-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update jline3 to 3.21.0</summary>
      <description>There is a number improvements in a new jline which could be used in this FLIPe.g. rgb support in style (could be used for prompts and highlighting)line numbers support in prompt continuationautopairing, display hints during completion</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-1 01:00:00" id="25123" opendate="2021-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve expression description in SQL operator</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TemporalTableFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.ColumnFunctionsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowDeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.PartitionableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MatchRecognizeTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.FilterableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalSortAggRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalHashAggRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RemoveUnreachableCoalesceArgumentsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RemoveSingleAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RankNumberColumnRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForRangeEndTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateExpandDistinctAggregatesRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ExpressionReductionRulesTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.DecomposeGroupingSetsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.CorrelateSortToRankRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.AggregateReduceGroupingRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ValuesJsonPlanTest.jsonplan.testValues.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testPartitionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testFilterPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testPartitioning.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupAggregateJsonPlanTest.jsonplan.tesPythonAggCallsWithGroupBy.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoinOverrideParameters.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testComplexCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.common.ViewsExpandingTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.common.PartialInsertTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.stringexpr.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.stringexpr.CorrelateStringExpressionTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.FlinkRelNode.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalCorrelateBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalCorrelateBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalIntervalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalMatch.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.connector.file.table.FileSystemTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SingleRowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.PartitionableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.ValuesTest.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-1 01:00:00" id="25128" opendate="2021-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reorganize table modules and introduce flink-table-planner-loader</summary>
      <description>For more details, see https://docs.google.com/document/d/12yDUCnvcwU2mODBKTHQ1xhfOq1ujYUrXltiN_rbhT34/edit?usp=sharing</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunctionWithOrderTest.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteMinusAllRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteIntersectAllRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.common.PartialInsertTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.TimestampToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-architecture-tests.pom.xml</file>
      <file type="M">flink-architecture-tests.violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
      <file type="M">flink-architecture-tests.violations.7602816f-5c01-4b7a-9e3e-235dfedec245</file>
      <file type="M">flink-architecture-tests.violations.e5126cae-f3fe-48aa-b6fb-60ae6cc3fcd5</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-stream-sql-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-tpcds-test.pom.xml</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
      <file type="M">flink-python.apache-flink-libraries.setup.py</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.licenses.LICENSE.icu4j</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.licenses.LICENSE.janino</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-runtime.pom.xml</file>
      <file type="M">flink-table.flink-table-runtime.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-uber.pom.xml</file>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">tools.ci.java-ci-tools.src.main.java.org.apache.flink.tools.ci.suffixcheck.ScalaSuffixChecker.java</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.CollectAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.JsonArrayAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.JsonObjectAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LagAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.tablefunctions.ReplicateRows.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.SetOpRewriteUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LagAggFunctionTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-1 01:00:00" id="25129" opendate="2021-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update docs to use flink-table-planner-loader instead of flink-table-planner</summary>
      <description>For more details https://docs.google.com/document/d/12yDUCnvcwU2mODBKTHQ1xhfOq1ujYUrXltiN_rbhT34/edit?usp=sharing</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.README.md</file>
      <file type="M">docs.content.docs.dev.configuration.testing.md</file>
      <file type="M">docs.content.docs.dev.configuration.overview.md</file>
      <file type="M">docs.content.docs.dev.configuration.maven.md</file>
      <file type="M">docs.content.docs.dev.configuration.connector.md</file>
      <file type="M">docs.content.docs.dev.configuration.advanced.md</file>
      <file type="M">docs.layouts.shortcodes.artifact.html</file>
      <file type="M">docs.content.docs.libs.gelly.overview.md</file>
      <file type="M">docs.content.docs.libs.cep.md</file>
      <file type="M">docs.content.docs.flinkDev.ide.setup.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.match.recognize.md</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.docs.dev.table.sourcesSinks.md</file>
      <file type="M">docs.content.docs.dev.table.overview.md</file>
      <file type="M">docs.content.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.docs.dev.datastream..index.md</file>
      <file type="M">docs.content.docs.dev.datastream.testing.md</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.queryable.state.md</file>
      <file type="M">docs.content.docs.connectors.table.upsert-kafka.md</file>
      <file type="M">docs.content.docs.connectors.table.overview.md</file>
      <file type="M">docs.content.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.docs.connectors.table.jdbc.md</file>
      <file type="M">docs.content.docs.connectors.table.hbase.md</file>
      <file type="M">docs.content.docs.connectors.table.elasticsearch.md</file>
      <file type="M">docs.content.docs.connectors.datastream.twitter.md</file>
      <file type="M">docs.content.docs.connectors.datastream.rabbitmq.md</file>
      <file type="M">docs.content.docs.connectors.datastream.pulsar.md</file>
      <file type="M">docs.content.docs.connectors.datastream.pubsub.md</file>
      <file type="M">docs.content.docs.connectors.datastream.nifi.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.docs.connectors.datastream.jdbc.md</file>
      <file type="M">docs.content.docs.connectors.datastream.elasticsearch.md</file>
      <file type="M">docs.content.docs.connectors.datastream.cassandra.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-2 01:00:00" id="25143" opendate="2021-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ITCase for Generalized incremental checkpoints</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.util.TestUtils.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointFormatITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.TestStreamEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-3 01:00:00" id="25160" opendate="2021-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make doc clear: tolerable-failed-checkpoints counts consecutive failures</summary>
      <description>According to the code, tolerable-failed-checkpoints counts the consecutive failures. We should make this clear in the doc configÂ </description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.checkpointing.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-3 01:00:00" id="25161" opendate="2021-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update dependency for japicmp-maven-plugin</summary>
      <description>compiliation with jdk 17 fails like belowupdate of jaxb-impl to 2.3.1 helpsjava.security.PrivilegedActionException: java.lang.NoSuchMethodException: sun.misc.Unsafe.defineClass(java.lang.String,[B,int,int,java.lang.ClassLoader,java.security.ProtectionDomain) at java.base/java.security.AccessController.doPrivileged(AccessController.java:573) at com.sun.xml.bind.v2.runtime.reflect.opt.Injector.&lt;clinit&gt;(Injector.java:197) at com.sun.xml.bind.v2.runtime.reflect.opt.AccessorInjector.prepare(AccessorInjector.java:81) at com.sun.xml.bind.v2.runtime.reflect.opt.OptimizedAccessorFactory.get(OptimizedAccessorFactory.java:125) at com.sun.xml.bind.v2.runtime.reflect.Accessor$GetterSetterReflection.optimize(Accessor.java:402) at com.sun.xml.bind.v2.runtime.reflect.TransducedAccessor$CompositeTransducedAccessorImpl.&lt;init&gt;(TransducedAccessor.java:235) at com.sun.xml.bind.v2.runtime.reflect.TransducedAccessor.get(TransducedAccessor.java:175) at com.sun.xml.bind.v2.runtime.property.AttributeProperty.&lt;init&gt;(AttributeProperty.java:91) at com.sun.xml.bind.v2.runtime.property.PropertyFactory.create(PropertyFactory.java:108) at com.sun.xml.bind.v2.runtime.ClassBeanInfoImpl.&lt;init&gt;(ClassBeanInfoImpl.java:181) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.getOrCreate(JAXBContextImpl.java:514) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.&lt;init&gt;(JAXBContextImpl.java:331) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.&lt;init&gt;(JAXBContextImpl.java:139) at com.sun.xml.bind.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1156) at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:165) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:568) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:297) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:286) at javax.xml.bind.ContextFinder.find(ContextFinder.java:409) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:721) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:662) at japicmp.output.xml.XmlOutputGenerator.createXmlDocumentAndSchema(XmlOutputGenerator.java:119) at japicmp.output.xml.XmlOutputGenerator.generate(XmlOutputGenerator.java:70) at japicmp.maven.JApiCmpMojo.generateXmlOutput(JApiCmpMojo.java:866) at japicmp.maven.JApiCmpMojo.executeWithParameters(JApiCmpMojo.java:149) at japicmp.maven.JApiCmpMojo.execute(JApiCmpMojo.java:125)</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-12-7 01:00:00" id="25204" opendate="2021-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Spell one more as in the DataStream interval join docs</summary>
      <description>Spell one more as in the DataStream interval join documentBoth the lower and upper bound can be either negative or positive as long as as the lower bound is always smaller or equal to the upper bound.=&gt;Both the lower and upper bound can be either negative or positive as long as the lower bound is always smaller or equal to the upper bound.</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.operators.joining.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.joining.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-8-10 01:00:00" id="25252" opendate="2021-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable Kafka E2E tests on Java 11</summary>
      <description>The Java Kafka E2E tests are currently not run on Java 11. We should check what the actual issue is and whether it can be resolved (e.g., by a Kafka server version bump):</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SmokeKafkaITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-13 01:00:00" id="25274" opendate="2021-12-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use ResolvedSchema in DataGen instead of TableSchema</summary>
      <description>TableSchema is deprecated It is recommended to use ResolvedSchema and Schema in TableSchema javadoc</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.connector.datagen.table.types.RowDataGenerator.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.connector.datagen.table.RandomGeneratorVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.connector.datagen.table.DataGenTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.connector.datagen.table.DataGenTableSource.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-2-14 01:00:00" id="25288" opendate="2021-12-14 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add savepoint and metric cases in DataStream source suite of connector testing framework</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.environment.MiniClusterTestEnvironment.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkContainerTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.testutils.KafkaSourceExternalContext.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.utils.ConnectorTestConstants.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.CommonTestUtils.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.common.FlinkContainerWithPulsarEnvironment.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-12-14 01:00:00" id="25295" opendate="2021-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Log4j to 2.16.0</summary>
      <description>Log4j 2.16.0 has been released https://lists.apache.org/thread/d6v4r6nosxysyq9rvnr779336yf0woz4This version removes message lookups and disables JNDI by default and results in a hardening of the default behaviour and configuration. Just to be clear, this dependency upgrade is not required to fix CVE-2021-44228. That has already been covered by https://issues.apache.org/jira/browse/FLINK-25240</description>
      <version>None</version>
      <fixedVersion>1.11.6,1.12.7,1.13.5,1.14.2,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  
  
  
  <bug fixdate="2022-1-13 01:00:00" id="25638" opendate="2022-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase the default write buffer size of sort-shuffle to 16M</summary>
      <description>As discussed in https://lists.apache.org/thread/pt2b1f17x2l5rlvggwxs6m265lo4ly7p, this ticket aims to increase the default write buffer size of sort-shuffle to 16M.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.BlockingShuffleITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.network.section.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-13 01:00:00" id="25639" opendate="2022-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase the default read buffer size of sort-shuffle to 64M</summary>
      <description>As discussed in https://lists.apache.org/thread/pt2b1f17x2l5rlvggwxs6m265lo4ly7p, this ticket aims to increase the default read buffer size of sort-shuffle to 64M.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.memory.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.memory.section.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-13 01:00:00" id="25645" opendate="2022-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UnsupportedOperationException would thrown out when hash shuffle by a field with array type</summary>
      <description>Currently array type is not supported as hash shuffle key because CodeGen does not support it yet. An unsupportedOperationException would thrown out when hash shuffle by a field with array type,</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.HashFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.HashCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.HashCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-13 01:00:00" id="25646" opendate="2022-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document buffer debloating issues with high parallelism</summary>
      <description>According to last benchmarks, there are some problems with buffer debloat when job has high parallelism. The high parallelism means the different value from job to job but in general it is more than 200. So it makes sense to document that problem and propose the solution - increasing the number of buffers.</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.memory.network.mem.tuning.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.network.mem.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-13 01:00:00" id="25650" opendate="2022-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document unaligned checkpoints performance limitations (larger records/flat map/timers/...)</summary>
      <description>The unaligned checkpoint can be delayed if the current record is consumed too long(because it is too large or it is the flat map etc.). Which can be pretty confused. So it makes sense to document this limitation to give the user understanding of this situation.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.checkpointing.under.backpressure.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpointing.under.backpressure.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-3-7 01:00:00" id="26506" opendate="2022-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support StreamExecutionEnvironment.registerCachedFile in Python DataStream API</summary>
      <description>This API is missed in Python DataStream API.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-11 01:00:00" id="26607" opendate="2022-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>There are multiple MAX_LONG_VALUE value errors in pyflink code</summary>
      <description>There are multiple MAX_LONG_VALUE values sys. In pyflink code maxsizeMAX_LONG_VALUE = sys.maxsizemaxsizeÂ attribute of theÂ sysÂ module fetches the largest value aÂ variable of data typeÂ Py_ssize_tÂ **Â can store. It is the Python platformâs pointerÂ thatÂ dictates the maximum size of lists and strings in Python. The size value returned by maxsize depends on the platform architecture: 32-bit:Â the value will be 2^31 â 1, i.e.Â 2147483647 64-bit:Â the value will be 2^63 â 1, i.e.Â 9223372036854775807</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.functions.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.process.function.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.context.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.window.window.operator.py</file>
      <file type="M">flink-python.pyflink.datastream.window.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-24 01:00:00" id="26842" opendate="2022-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove flink-python Scala dependency</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-python.apache-flink-libraries.setup.py</file>
      <file type="M">flink-end-to-end-tests.flink-python-test.pom.xml</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-python.src.test.scala.org.apache.flink.table.legacyutils.TestCollectionTableFactory.scala</file>
      <file type="M">flink-python.src.test.scala.org.apache.flink.table.legacyutils.legacyTestingSinks.scala</file>
      <file type="M">flink-python.src.test.scala.org.apache.flink.table.legacyutils.legacyTestingFunctions.scala</file>
      <file type="M">flink-python.src.test.scala.org.apache.flink.table.legacyutils.legacyTestingDescriptors.scala</file>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-2 01:00:00" id="27026" opendate="2022-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade checkstyle plugin</summary>
      <description>Newer versions of the checkstyle plugin allow running checkstyle:check without requiring dependency resolution. This allows it to be used in a fresh environment.</description>
      <version>None</version>
      <fixedVersion>1.16.0,elasticsearch-3.0.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-2 01:00:00" id="27027" opendate="2022-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get rid of oddly named mvn-${sys mvn.forkNumber}.log files</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-7 01:00:00" id="27108" opendate="2022-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>State cache clean up doesn&amp;#39;t work as expected</summary>
      <description>The test case test_session_window_late_merge failed when working on FLINK-26190. After digging into this problem, I found that the reason should be that the logic to determine whether a key &amp; namespace exists in state cache is wrong is wrong. It causes the state cache isn't clean up when it becomes invalidate.</description>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-13 01:00:00" id="27223" opendate="2022-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>State access doesn&amp;#39;t work as expected when cache size is set to 0</summary>
      <description>For the following job:import jsonimport loggingimport sysfrom pyflink.common import Types, Configurationfrom pyflink.datastream import StreamExecutionEnvironmentfrom pyflink.util.java_utils import get_j_env_configurationif __name__ == '__main__': logging.basicConfig(stream=sys.stdout, level=logging.INFO, format="%(message)s") env = StreamExecutionEnvironment.get_execution_environment() config = Configuration( j_configuration=get_j_env_configuration(env._j_stream_execution_environment)) config.set_integer("python.state.cache-size", 0) env.set_parallelism(1) # define the source ds = env.from_collection( collection=[ (1, '{"name": "Flink", "tel": 123, "addr": {"country": "Germany", "city": "Berlin"}}'), (2, '{"name": "hello", "tel": 135, "addr": {"country": "China", "city": "Shanghai"}}'), (3, '{"name": "world", "tel": 124, "addr": {"country": "USA", "city": "NewYork"}}'), (4, '{"name": "PyFlink", "tel": 32, "addr": {"country": "China", "city": "Hangzhou"}}') ], type_info=Types.ROW_NAMED(["id", "info"], [Types.INT(), Types.STRING()]) ) # key by ds = ds.map(lambda data: (json.loads(data.info)['addr']['country'], json.loads(data.info)['tel'])) \ .key_by(lambda data: data[0]).sum(1) ds.print() env.execute()The expected result should be:('Germany', 123)('China', 135)('USA', 124)('China', 167)However, the actual result is:('Germany', 123)('China', 135)('USA', 124)('China', 32)</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.5,1.15.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-13 01:00:00" id="27224" opendate="2022-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop redundant flink.forkCountTestPackage property</summary>
      <description>This property has been identical to flink.forkCount for as long as I can remember. We should be able to get rid of it.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.test.controller.sh</file>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-13 01:00:00" id="27225" opendate="2022-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove redundant reuseForks settings</summary>
      <description>A number of modules set reuseForks without it having any effect because of it being enabled by default and/or the module not containing unit tests.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-4-14 01:00:00" id="27252" opendate="2022-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove surefire fork options from connector-hive</summary>
      <description>Cleanup of unnecessary settings, that will also slightly speed up testing.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-14 01:00:00" id="27253" opendate="2022-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove custom surefire config from connector-cassandra</summary>
      <description>With the recent improvements around the cassandra test stability we can clean up some technical debt.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-6-9 01:00:00" id="27544" opendate="2022-5-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Example code in &amp;#39;Structure of Table API and SQL Programs&amp;#39; is out of date and cannot run</summary>
      <description>The example code in Structure of Table API and SQL Programs of 'Concepts &amp; Common API' is out of date and when user run this piece of code, they will get the following result:Exception in thread "main" org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.SinkTable'.Table options are:'connector'='blackhole''rows-per-second'='1' at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:262) at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:421) at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:222) at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:178) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) at scala.collection.Iterator.foreach(Iterator.scala:937) at scala.collection.Iterator.foreach$(Iterator.scala:937) at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) at scala.collection.IterableLike.foreach(IterableLike.scala:70) at scala.collection.IterableLike.foreach$(IterableLike.scala:69) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike.map(TraversableLike.scala:233) at scala.collection.TraversableLike.map$(TraversableLike.scala:226) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:178) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:782) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:861) at org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:56) at com.yck.TestTableAPI.main(TestTableAPI.java:43)Caused by: org.apache.flink.table.api.ValidationException: Unsupported options found for 'blackhole'.Unsupported options:rows-per-secondSupported options:connectorproperty-version at org.apache.flink.table.factories.FactoryUtil.validateUnconsumedKeys(FactoryUtil.java:624) at org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validate(FactoryUtil.java:914) at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.validate(FactoryUtil.java:978) at org.apache.flink.connector.blackhole.table.BlackHoleTableSinkFactory.createDynamicTableSink(BlackHoleTableSinkFactory.java:64) at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:259) ... 19 moreI think this mistake would drive users crazy when they first fry Table API &amp; Flink SQL since this is the very first code they see.Overall this code is outdated in two places:1. The Query creating temporary table should be CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable (EXCLUDING OPTIONS) instead of CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable which missed (EXCLUDING OPTIONS) sql_like_pattern2. The part creating a source table should be tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 1L) .build());instead of tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenOptions.ROWS_PER_SECOND, 100) .build());since the class DataGenOptions was replaced by class DataGenConnectorOptions in this commitThe test code is in my github Repository(version 1.15) and version 1.14The affected versions are 1.15 and 1.14.</description>
      <version>1.14.0,1.14.2,1.14.3,1.14.4,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-29 01:00:00" id="2779" opendate="2015-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation to reflect new Stream/Window API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.internals.general.arch.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.apis.streaming.guide.md</file>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-1-21 01:00:00" id="31170" opendate="2023-2-21 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>The spelling error of the document word causes sql to fail to execute</summary>
      <description>The spelling error of the document word causes sql to fail to execute</description>
      <version>1.14.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
    </fixedFiles>
  </bug>
</bugrepository>