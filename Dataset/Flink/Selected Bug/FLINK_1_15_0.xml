<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2015-3-13 01:00:00" id="1696" opendate="2015-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add multiple linear regression to ML library</summary>
      <description>Add multiple linear regression to ML library.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">docs..layouts.default.html</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Matrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
      <file type="M">flink-examples.flink-scala-examples.src.main.scala.org.apache.flink.examples.scala.ml.LinearRegression.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-3 01:00:00" id="18082" opendate="2020-6-3 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>UnsignedTypeConversionITCase stalls in ch.vorburger.mariadb4j.DB.stop</summary>
      <description>CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2582&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=03dca39c-73e8-5aaf-601d-328ae5c35f202020-06-02T19:01:31.8486456Z ==============================================================================2020-06-02T19:01:31.8487052Z Printing stack trace of Java process 86532020-06-02T19:01:31.8487424Z ==============================================================================2020-06-02T19:01:31.8541169Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError2020-06-02T19:01:32.1665740Z 2020-06-02 19:01:322020-06-02T19:01:32.1666470Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):2020-06-02T19:01:32.1666735Z 2020-06-02T19:01:32.1667614Z "Attach Listener" #537 daemon prio=9 os_prio=0 tid=0x00007f61f8001000 nid=0x3b9f waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1668130Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1668311Z 2020-06-02T19:01:32.1668958Z "flink-akka.actor.default-dispatcher-193" #535 prio=5 os_prio=0 tid=0x00007f6034001000 nid=0x3af7 waiting on condition [0x00007f61a25b8000]2020-06-02T19:01:32.1669418Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1669730Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1670301Z - parking to wait for &lt;0x0000000080c51528&gt; (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)2020-06-02T19:01:32.1670791Z at akka.dispatch.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)2020-06-02T19:01:32.1671329Z at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)2020-06-02T19:01:32.1671763Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2020-06-02T19:01:32.1672211Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2020-06-02T19:01:32.1672491Z 2020-06-02T19:01:32.1673104Z "flink-akka.actor.default-dispatcher-191" #533 prio=5 os_prio=0 tid=0x00007f619801e000 nid=0x3ae1 waiting on condition [0x00007f60770f1000]2020-06-02T19:01:32.1673564Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1673839Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1674422Z - parking to wait for &lt;0x0000000080c51528&gt; (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)2020-06-02T19:01:32.1674865Z at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)2020-06-02T19:01:32.1675305Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2020-06-02T19:01:32.1675751Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2020-06-02T19:01:32.1676046Z 2020-06-02T19:01:32.1676669Z "jobmanager-future-thread-2" #466 daemon prio=5 os_prio=0 tid=0x00007f6124001000 nid=0x3795 waiting on condition [0x00007f61a23b6000]2020-06-02T19:01:32.1677316Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1677617Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1678220Z - parking to wait for &lt;0x00000000816a4c90&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1678702Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1679209Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1679822Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1680422Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1680962Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1681424Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1682062Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1682445Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1682656Z 2020-06-02T19:01:32.1683271Z "Flink-DispatcherRestEndpoint-thread-4" #349 daemon prio=5 os_prio=0 tid=0x00007f618c00a000 nid=0x29a4 waiting on condition [0x00007f61a029f000]2020-06-02T19:01:32.1683750Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1684057Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1684648Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1685145Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1685673Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-06-02T19:01:32.1686400Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-06-02T19:01:32.1687200Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1687724Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1688211Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1688679Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1689076Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1689266Z 2020-06-02T19:01:32.1689923Z "FlinkCompletableFutureDelayScheduler-thread-1" #123 daemon prio=5 os_prio=0 tid=0x00007f60e801d000 nid=0x277b waiting on condition [0x00007f61104dd000]2020-06-02T19:01:32.1690403Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1690698Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1691291Z - parking to wait for &lt;0x00000000879019c0&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1691779Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1692314Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1692905Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)2020-06-02T19:01:32.1693506Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1694040Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1694500Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1694988Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1695372Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1695583Z 2020-06-02T19:01:32.1696176Z "Flink-DispatcherRestEndpoint-thread-3" #84 daemon prio=5 os_prio=0 tid=0x00007f614c003800 nid=0x26ca waiting on condition [0x00007f6113bfa000]2020-06-02T19:01:32.1696685Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1697117Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1697737Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1698205Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1698749Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1699339Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1699942Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1700582Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1701042Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1701528Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1701926Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1702114Z 2020-06-02T19:01:32.1702714Z "mysql-cj-abandoned-connection-cleanup" #83 daemon prio=5 os_prio=0 tid=0x00007f61625c9000 nid=0x26c7 in Object.wait() [0x00007f6113dfc000]2020-06-02T19:01:32.1703201Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1703530Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1703857Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)2020-06-02T19:01:32.1704416Z - locked &lt;0x00000000814a4b00&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-06-02T19:01:32.1704939Z at com.mysql.cj.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:85)2020-06-02T19:01:32.1705464Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2020-06-02T19:01:32.1705950Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1706329Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1706566Z 2020-06-02T19:01:32.1707083Z "Exec Stream Pumper" #76 daemon prio=5 os_prio=0 tid=0x00007f6118001000 nid=0x269c runnable [0x00007f6113ffe000]2020-06-02T19:01:32.1707485Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1707773Z at java.io.FileInputStream.readBytes(Native Method)2020-06-02T19:01:32.1708117Z at java.io.FileInputStream.read(FileInputStream.java:255)2020-06-02T19:01:32.1708522Z at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)2020-06-02T19:01:32.1708944Z at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)2020-06-02T19:01:32.1709357Z at java.io.BufferedInputStream.read(BufferedInputStream.java:345)2020-06-02T19:01:32.1709977Z - locked &lt;0x0000000081555688&gt; (a java.lang.UNIXProcess$ProcessPipeInputStream)2020-06-02T19:01:32.1710386Z at java.io.FilterInputStream.read(FilterInputStream.java:107)2020-06-02T19:01:32.1710779Z at org.apache.commons.exec.StreamPumper.run(StreamPumper.java:107)2020-06-02T19:01:32.1711147Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1711643Z 2020-06-02T19:01:32.1711987Z "Exec Default Executor" #75 prio=5 os_prio=0 tid=0x00007f63009ee800 nid=0x269a in Object.wait() [0x00007f61a019e000]2020-06-02T19:01:32.1712409Z java.lang.Thread.State: WAITING (on object monitor)2020-06-02T19:01:32.1712729Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1713189Z - waiting on &lt;0x00000000816a4ab0&gt; (a java.lang.UNIXProcess)2020-06-02T19:01:32.1713521Z at java.lang.Object.wait(Object.java:502)2020-06-02T19:01:32.1713849Z at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)2020-06-02T19:01:32.1714370Z - locked &lt;0x00000000816a4ab0&gt; (a java.lang.UNIXProcess)2020-06-02T19:01:32.1714778Z at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:364)2020-06-02T19:01:32.1715244Z at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)2020-06-02T19:01:32.1715710Z at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)2020-06-02T19:01:32.1716078Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1716292Z 2020-06-02T19:01:32.1716642Z "process reaper" #71 daemon prio=10 os_prio=0 tid=0x00007f61a805f000 nid=0x2654 runnable [0x00007f61a3441000]2020-06-02T19:01:32.1717218Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1717519Z at java.lang.UNIXProcess.waitForProcessExit(Native Method)2020-06-02T19:01:32.1717906Z at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:289)2020-06-02T19:01:32.1718285Z at java.lang.UNIXProcess$$Lambda$7/861659238.run(Unknown Source)2020-06-02T19:01:32.1718721Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2020-06-02T19:01:32.1719318Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1719700Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1719908Z 2020-06-02T19:01:32.1720537Z "Flink-DispatcherRestEndpoint-thread-2" #66 daemon prio=5 os_prio=0 tid=0x00007f614c002800 nid=0x2506 waiting on condition [0x00007f61a2bbc000]2020-06-02T19:01:32.1721013Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1721288Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1721899Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1722366Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1722891Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1723582Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1724166Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1724705Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1725166Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1725653Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1726050Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1726238Z 2020-06-02T19:01:32.1727051Z "mini-cluster-io-thread-10" #64 daemon prio=5 os_prio=0 tid=0x00007f613c00b000 nid=0x236e waiting on condition [0x00007f61a06a1000]2020-06-02T19:01:32.1727500Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1727795Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1728406Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1728888Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1729397Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1729941Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1730413Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1730874Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1731360Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1731740Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1731947Z 2020-06-02T19:01:32.1732513Z "mini-cluster-io-thread-9" #63 daemon prio=5 os_prio=0 tid=0x00007f613c006800 nid=0x236d waiting on condition [0x00007f61a07a2000]2020-06-02T19:01:32.1732963Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1733237Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1733844Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1734329Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1734836Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1735379Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1735833Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1736314Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1737055Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1737598Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1737787Z 2020-06-02T19:01:32.1738405Z "jobmanager-future-thread-1" #62 daemon prio=5 os_prio=0 tid=0x00007f613c01c000 nid=0x236c waiting on condition [0x00007f61a08a3000]2020-06-02T19:01:32.1738863Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1739148Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1739755Z - parking to wait for &lt;0x00000000816a4c90&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1740227Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1740771Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-06-02T19:01:32.1741390Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-06-02T19:01:32.1742053Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1742594Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1743076Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1743544Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1743942Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1744129Z 2020-06-02T19:01:32.1744722Z "mini-cluster-io-thread-8" #59 daemon prio=5 os_prio=0 tid=0x00007f6190005000 nid=0x2369 waiting on condition [0x00007f61a0ba6000]2020-06-02T19:01:32.1745147Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1745440Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1746027Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1746553Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1747227Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1747753Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1748230Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1748691Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1749186Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1749585Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1749771Z 2020-06-02T19:01:32.1750347Z "mini-cluster-io-thread-7" #58 daemon prio=5 os_prio=0 tid=0x00007f6190003000 nid=0x2368 waiting on condition [0x00007f61a0ca7000]2020-06-02T19:01:32.1750793Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1751092Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1751686Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1752170Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1752679Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1753226Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1753703Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1754167Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1754652Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1755031Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1755240Z 2020-06-02T19:01:32.1755806Z "mini-cluster-io-thread-6" #57 daemon prio=5 os_prio=0 tid=0x00007f6190001800 nid=0x2367 waiting on condition [0x00007f61a0da8000]2020-06-02T19:01:32.1756340Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1756674Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1757414Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1757914Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1758437Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1758963Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1759434Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1759895Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1760468Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1760864Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1761052Z 2020-06-02T19:01:32.1761621Z "mini-cluster-io-thread-5" #56 daemon prio=5 os_prio=0 tid=0x00007f6150008000 nid=0x2366 waiting on condition [0x00007f61a0ea9000]2020-06-02T19:01:32.1762063Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1762352Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1762938Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1763417Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1763926Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1764464Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1764944Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1765409Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1765886Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1766284Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1766511Z 2020-06-02T19:01:32.1767253Z "mini-cluster-io-thread-4" #55 daemon prio=5 os_prio=0 tid=0x00007f6300311000 nid=0x2365 waiting on condition [0x00007f61a0faa000]2020-06-02T19:01:32.1767692Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1767965Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1768575Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1769051Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1769558Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1770111Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1770563Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1771042Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1771526Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1771906Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1772094Z 2020-06-02T19:01:32.1772676Z "mini-cluster-io-thread-3" #54 daemon prio=5 os_prio=0 tid=0x00007f6198013800 nid=0x2364 waiting on condition [0x00007f61a10ab000]2020-06-02T19:01:32.1773113Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1773526Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1774138Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1774697Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1775231Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1775770Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1776225Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1776747Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1777357Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1777752Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1777943Z 2020-06-02T19:01:32.1778554Z "mini-cluster-io-thread-2" #53 daemon prio=5 os_prio=0 tid=0x00007f630030c800 nid=0x2363 waiting on condition [0x00007f61a11ac000]2020-06-02T19:01:32.1779065Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1779372Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1779968Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1780447Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1780980Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1781503Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1781977Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1782451Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1782920Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1783316Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1783510Z 2020-06-02T19:01:32.1784119Z "Flink-DispatcherRestEndpoint-thread-1" #52 daemon prio=5 os_prio=0 tid=0x00007f6161fad000 nid=0x2362 waiting on condition [0x00007f61a12ad000]2020-06-02T19:01:32.1784565Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1784856Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1785445Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1785925Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1786475Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1787228Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1787826Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1788354Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1788830Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1789300Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1789691Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1789880Z 2020-06-02T19:01:32.1790488Z "mini-cluster-io-thread-1" #51 daemon prio=5 os_prio=0 tid=0x00007f6161faa800 nid=0x2361 waiting on condition [0x00007f61a13ae000]2020-06-02T19:01:32.1790911Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1791202Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1791786Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1792265Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1792794Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1793408Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1793880Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1794359Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1794829Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1795224Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1795411Z 2020-06-02T19:01:32.1796007Z "flink-rest-server-netty-boss-thread-1" #50 daemon prio=5 os_prio=0 tid=0x00007f6161fa5000 nid=0x2360 runnable [0x00007f61a14af000]2020-06-02T19:01:32.1796466Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1796778Z at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)2020-06-02T19:01:32.1797363Z at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)2020-06-02T19:01:32.1797790Z at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)2020-06-02T19:01:32.1798206Z at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)2020-06-02T19:01:32.1798866Z - locked &lt;0x0000000081f004e8&gt; (a org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet)2020-06-02T19:01:32.1799476Z - locked &lt;0x0000000081f00500&gt; (a java.util.Collections$UnmodifiableSet)2020-06-02T19:01:32.1799991Z - locked &lt;0x0000000081f004a0&gt; (a sun.nio.ch.EPollSelectorImpl)2020-06-02T19:01:32.1800359Z at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)2020-06-02T19:01:32.1800875Z at org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)2020-06-02T19:01:32.1801498Z at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:806)2020-06-02T19:01:32.1802036Z at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:454)2020-06-02T19:01:32.1802624Z at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)2020-06-02T19:01:32.1803260Z at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)2020-06-02T19:01:32.1803690Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1803895Z 2020-06-02T19:01:32.1804241Z "IOManager reader thread #1" #45 daemon prio=5 os_prio=0 tid=0x00007f63014fa000 nid=0x235d waiting on condition [0x00007f61a1db0000]2020-06-02T19:01:32.1804685Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1804981Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1805571Z - parking to wait for &lt;0x0000000081901368&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1806053Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1806604Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1807373Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1807892Z at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:354)2020-06-02T19:01:32.1808204Z 2020-06-02T19:01:32.1808550Z "IOManager writer thread #1" #44 daemon prio=5 os_prio=0 tid=0x00007f63014f9000 nid=0x235c waiting on condition [0x00007f61a1eb1000]2020-06-02T19:01:32.1808996Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1809285Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1809897Z - parking to wait for &lt;0x0000000081977a50&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1810382Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1810893Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1811530Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1812047Z at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:460)2020-06-02T19:01:32.1812361Z 2020-06-02T19:01:32.1812888Z "Timer-2" #42 daemon prio=5 os_prio=0 tid=0x00007f63014c0000 nid=0x235b in Object.wait() [0x00007f61a1fb2000]2020-06-02T19:01:32.1813326Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1813641Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1814113Z - waiting on &lt;0x0000000081901c00&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1814450Z at java.util.TimerThread.mainLoop(Timer.java:552)2020-06-02T19:01:32.1814940Z - locked &lt;0x0000000081901c00&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1815270Z at java.util.TimerThread.run(Timer.java:505)2020-06-02T19:01:32.1815469Z 2020-06-02T19:01:32.1816092Z "Timer-1" #40 daemon prio=5 os_prio=0 tid=0x00007f63014be000 nid=0x235a in Object.wait() [0x00007f61a20b3000]2020-06-02T19:01:32.1816569Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1817028Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1817513Z - waiting on &lt;0x0000000081901570&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1817846Z at java.util.TimerThread.mainLoop(Timer.java:552)2020-06-02T19:01:32.1818339Z - locked &lt;0x0000000081901570&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1818650Z at java.util.TimerThread.run(Timer.java:505)2020-06-02T19:01:32.1818862Z 2020-06-02T19:01:32.1819196Z "BLOB Server listener at 39424" #36 daemon prio=5 os_prio=0 tid=0x00007f63014bb000 nid=0x2359 runnable [0x00007f61a21b4000]2020-06-02T19:01:32.1819610Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1819903Z at java.net.PlainSocketImpl.socketAccept(Native Method)2020-06-02T19:01:32.1820304Z at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)2020-06-02T19:01:32.1820726Z at java.net.ServerSocket.implAccept(ServerSocket.java:560)2020-06-02T19:01:32.1821107Z at java.net.ServerSocket.accept(ServerSocket.java:528)2020-06-02T19:01:32.1821511Z at org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:262)2020-06-02T19:01:32.1821756Z 2020-06-02T19:01:32.1822269Z "Timer-0" #37 daemon prio=5 os_prio=0 tid=0x00007f63014a5800 nid=0x2358 in Object.wait() [0x00007f61a22b5000]2020-06-02T19:01:32.1822709Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1823030Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1823478Z - waiting on &lt;0x0000000081901740&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1823816Z at java.util.TimerThread.mainLoop(Timer.java:552)2020-06-02T19:01:32.1824304Z - locked &lt;0x0000000081901740&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1824629Z at java.util.TimerThread.run(Timer.java:505)2020-06-02T19:01:32.1824823Z 2020-06-02T19:01:32.1825397Z "flink-metrics-scheduler-1" #32 prio=5 os_prio=0 tid=0x00007f6301468000 nid=0x2354 waiting on condition [0x00007f61a26b9000]2020-06-02T19:01:32.1825831Z java.lang.Thread.State: TIMED_WAITING (sleeping)2020-06-02T19:01:32.1826134Z at java.lang.Thread.sleep(Native Method)2020-06-02T19:01:32.1826559Z at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)2020-06-02T19:01:32.1827253Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)2020-06-02T19:01:32.1827792Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2020-06-02T19:01:32.1828191Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1828392Z 2020-06-02T19:01:32.1829014Z "flink-scheduler-1" #27 prio=5 os_prio=0 tid=0x00007f6300ec1800 nid=0x22f5 waiting on condition [0x00007f61a2fbe000]2020-06-02T19:01:32.1829625Z java.lang.Thread.State: TIMED_WAITING (sleeping)2020-06-02T19:01:32.1830040Z at java.lang.Thread.sleep(Native Method)2020-06-02T19:01:32.1830784Z at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)2020-06-02T19:01:32.1831508Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)2020-06-02T19:01:32.1832253Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2020-06-02T19:01:32.1832842Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1833195Z 2020-06-02T19:01:32.1833740Z "process reaper" #24 daemon prio=10 os_prio=0 tid=0x00007f61a8048000 nid=0x2222 waiting on condition [0x00007f61a3a81000]2020-06-02T19:01:32.1834332Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1834709Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1835502Z - parking to wait for &lt;0x0000000080ba0518&gt; (a java.util.concurrent.SynchronousQueue$TransferStack)2020-06-02T19:01:32.1836129Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1837204Z at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)2020-06-02T19:01:32.1837935Z at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)2020-06-02T19:01:32.1838607Z at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)2020-06-02T19:01:32.1839184Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)2020-06-02T19:01:32.1839801Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1840433Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1840957Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1841215Z 2020-06-02T19:01:32.1842084Z "surefire-forkedjvm-ping-30s" #23 daemon prio=5 os_prio=0 tid=0x00007f63003c3000 nid=0x221f waiting on condition [0x00007f61b07c6000]2020-06-02T19:01:32.1842680Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1843116Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1843930Z - parking to wait for &lt;0x0000000080b92410&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1844585Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1845348Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-06-02T19:01:32.1846221Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-06-02T19:01:32.1847278Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1848018Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1848653Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1849341Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1849850Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1850130Z 2020-06-02T19:01:32.1850983Z "surefire-forkedjvm-command-thread" #22 daemon prio=5 os_prio=0 tid=0x00007f63003ac000 nid=0x221c runnable [0x00007f61b0ad1000]2020-06-02T19:01:32.1851572Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1851989Z at java.io.FileInputStream.readBytes(Native Method)2020-06-02T19:01:32.1852454Z at java.io.FileInputStream.read(FileInputStream.java:255)2020-06-02T19:01:32.1852980Z at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)2020-06-02T19:01:32.1853504Z at java.io.BufferedInputStream.read(BufferedInputStream.java:265)2020-06-02T19:01:32.1854228Z - locked &lt;0x0000000080c53790&gt; (a java.io.BufferedInputStream)2020-06-02T19:01:32.1854701Z at java.io.DataInputStream.readInt(DataInputStream.java:387)2020-06-02T19:01:32.1855323Z at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)2020-06-02T19:01:32.1856252Z at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)2020-06-02T19:01:32.1857036Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1857304Z 2020-06-02T19:01:32.1857755Z "Service Thread" #21 daemon prio=9 os_prio=0 tid=0x00007f63002d5000 nid=0x221a runnable [0x0000000000000000]2020-06-02T19:01:32.1858280Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1858532Z 2020-06-02T19:01:32.1858991Z "C1 CompilerThread14" #20 daemon prio=9 os_prio=0 tid=0x00007f63002c8000 nid=0x2219 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1859548Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1859786Z 2020-06-02T19:01:32.1860247Z "C1 CompilerThread13" #19 daemon prio=9 os_prio=0 tid=0x00007f63002c6000 nid=0x2218 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1860802Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1861042Z 2020-06-02T19:01:32.1861653Z "C1 CompilerThread12" #18 daemon prio=9 os_prio=0 tid=0x00007f63002c4000 nid=0x2217 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1862227Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1862465Z 2020-06-02T19:01:32.1862931Z "C1 CompilerThread11" #17 daemon prio=9 os_prio=0 tid=0x00007f63002c2000 nid=0x2216 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1863458Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1863711Z 2020-06-02T19:01:32.1864153Z "C1 CompilerThread10" #16 daemon prio=9 os_prio=0 tid=0x00007f63002c0000 nid=0x2215 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1864696Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1864932Z 2020-06-02T19:01:32.1865389Z "C2 CompilerThread9" #15 daemon prio=9 os_prio=0 tid=0x00007f63002bd000 nid=0x2214 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1865921Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1866175Z 2020-06-02T19:01:32.1866632Z "C2 CompilerThread8" #14 daemon prio=9 os_prio=0 tid=0x00007f63002bb800 nid=0x2213 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1867418Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1867660Z 2020-06-02T19:01:32.1868123Z "C2 CompilerThread7" #13 daemon prio=9 os_prio=0 tid=0x00007f63002b9000 nid=0x2212 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1868652Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1868908Z 2020-06-02T19:01:32.1869440Z "C2 CompilerThread6" #12 daemon prio=9 os_prio=0 tid=0x00007f63002b7800 nid=0x2211 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1869969Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1870199Z 2020-06-02T19:01:32.1870646Z "C2 CompilerThread5" #11 daemon prio=9 os_prio=0 tid=0x00007f63002b5000 nid=0x2210 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1871166Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1871396Z 2020-06-02T19:01:32.1871860Z "C2 CompilerThread4" #10 daemon prio=9 os_prio=0 tid=0x00007f63002b3000 nid=0x220f waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1872387Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1872635Z 2020-06-02T19:01:32.1873060Z "C2 CompilerThread3" #9 daemon prio=9 os_prio=0 tid=0x00007f63002a9000 nid=0x220e waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1873590Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1873815Z 2020-06-02T19:01:32.1874267Z "C2 CompilerThread2" #8 daemon prio=9 os_prio=0 tid=0x00007f63002a6800 nid=0x220d waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1874805Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1875035Z 2020-06-02T19:01:32.1875466Z "C2 CompilerThread1" #7 daemon prio=9 os_prio=0 tid=0x00007f63002a4800 nid=0x220c waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1875992Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1876218Z 2020-06-02T19:01:32.1876676Z "C2 CompilerThread0" #6 daemon prio=9 os_prio=0 tid=0x00007f63002a2800 nid=0x220b waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1877580Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1877815Z 2020-06-02T19:01:32.1878222Z "Signal Dispatcher" #5 daemon prio=9 os_prio=0 tid=0x00007f63002a0800 nid=0x220a runnable [0x0000000000000000]2020-06-02T19:01:32.1878748Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1878977Z 2020-06-02T19:01:32.1879472Z "Surrogate Locker Thread (Concurrent GC)" #4 daemon prio=9 os_prio=0 tid=0x00007f630029f000 nid=0x2209 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1880026Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1880278Z 2020-06-02T19:01:32.1880699Z "Finalizer" #3 daemon prio=8 os_prio=0 tid=0x00007f630026e800 nid=0x2208 in Object.wait() [0x00007f620958e000]2020-06-02T19:01:32.1881124Z java.lang.Thread.State: WAITING (on object monitor)2020-06-02T19:01:32.1881408Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1882065Z - waiting on &lt;0x0000000080c53a60&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-06-02T19:01:32.1882609Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)2020-06-02T19:01:32.1883144Z - locked &lt;0x0000000080c53a60&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-06-02T19:01:32.1883522Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)2020-06-02T19:01:32.1883906Z at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)2020-06-02T19:01:32.1884151Z 2020-06-02T19:01:32.1884675Z "Reference Handler" #2 daemon prio=10 os_prio=0 tid=0x00007f630026a000 nid=0x2207 in Object.wait() [0x00007f620968f000]2020-06-02T19:01:32.1885086Z java.lang.Thread.State: WAITING (on object monitor)2020-06-02T19:01:32.1885390Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1885872Z - waiting on &lt;0x0000000080c53a50&gt; (a java.lang.ref.Reference$Lock)2020-06-02T19:01:32.1886186Z at java.lang.Object.wait(Object.java:502)2020-06-02T19:01:32.1886587Z at java.lang.ref.Reference.tryHandlePending(Reference.java:191)2020-06-02T19:01:32.1887443Z - locked &lt;0x0000000080c53a50&gt; (a java.lang.ref.Reference$Lock)2020-06-02T19:01:32.1887838Z at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)2020-06-02T19:01:32.1888077Z 2020-06-02T19:01:32.1888380Z "main" #1 prio=5 os_prio=0 tid=0x00007f630000b800 nid=0x21cf waiting on condition [0x00007f6306d43000]2020-06-02T19:01:32.1888757Z java.lang.Thread.State: TIMED_WAITING (sleeping)2020-06-02T19:01:32.1889050Z at java.lang.Thread.sleep(Native Method)2020-06-02T19:01:32.1889437Z at org.apache.commons.exec.DefaultExecuteResultHandler.waitFor(DefaultExecuteResultHandler.java:121)2020-06-02T19:01:32.1889917Z at ch.vorburger.exec.ManagedProcess.destroy(ManagedProcess.java:344)2020-06-02T19:01:32.1890289Z at ch.vorburger.mariadb4j.DB.stop(DB.java:327)2020-06-02T19:01:32.1890764Z - locked &lt;0x00000000816a5170&gt; (a ch.vorburger.mariadb4j.DB)2020-06-02T19:01:32.1891151Z at ch.vorburger.mariadb4j.junit.MariaDB4jRule.after(MariaDB4jRule.java:64)2020-06-02T19:01:32.1891571Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:50)2020-06-02T19:01:32.1892001Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-06-02T19:01:32.1892425Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-06-02T19:01:32.1892802Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-06-02T19:01:32.1893180Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-06-02T19:01:32.1893594Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-06-02T19:01:32.1894085Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-06-02T19:01:32.1894575Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-06-02T19:01:32.1895059Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-06-02T19:01:32.1895567Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-06-02T19:01:32.1896211Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-06-02T19:01:32.1896744Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-06-02T19:01:32.1897423Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)</description>
      <version>1.12.0,1.15.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-17 01:00:00" id="20188" opendate="2020-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation for new File Source</summary>
      <description/>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.execution.mode.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.content.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.text.files.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.parquet.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.azure.table.storage.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.execution.mode.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.file.sink.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-31 01:00:00" id="22070" opendate="2021-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support FileSink in PyFlink DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-python.pyflink.common.serialization.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-7-15 01:00:00" id="23395" opendate="2021-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Okhttp to 3.14.9</summary>
      <description>We currently use 3 different version of Okhttp, which are partially lagging behind the last 3.X version by quite a bit.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-metrics.flink-metrics-datadog.pom.xml</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-26 01:00:00" id="23493" opendate="2021-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>python tests hang on Azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20898&amp;view=logs&amp;j=821b528f-1eed-5598-a3b4-7f748b13f261&amp;t=4fad9527-b9a5-5015-1b70-8356e5c91490&amp;l=22829</description>
      <version>1.14.0,1.13.1,1.12.4,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.boot.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-16 01:00:00" id="23798" opendate="2021-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid using reflection to get filter when partition filter is enabled</summary>
      <description>FLINK-20496 introduce partitioned index &amp; filter to Flink. However, RocksDB only support new full format of filter in this feature, and we need to replace previous filter if user enabled. Previous implementation use reflection to get the filter and we could use API to get that after upgrading to newer version.</description>
      <version>None</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainerTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-4-20 01:00:00" id="23902" opendate="2021-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 3.1.3</summary>
      <description>Make flink support Hive version 3.1.3 version.</description>
      <version>1.14.0,1.15.0,1.15.1</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV310.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">docs.content.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.overview.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-27 01:00:00" id="24020" opendate="2021-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aggregate HTTP requests before custom netty handers are getting the data</summary>
      <description>Custom netty handlers can do authentication (amongst other possibilities).This requires that the handlers are getting the whole HttpRequest content and not just partial data.At the moment it's not implemented this way which ends-up in flaky behaviour.Namely sometimes for example History server responds properly (when the request fits into one netty chunk) but sometimes authentication fails (when the request split into multiple netty chunks).</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-28 01:00:00" id="24036" opendate="2021-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSL cannot be installed on CI</summary>
      <description># install libssl1.0.0 for netty tcnativewget http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.debsudo apt install ./libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.deb--2021-08-27 20:48:49-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 91.189.91.38, 2001:67c:1562::15, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2021-08-27 20:48:49 ERROR 404: Not Found.</description>
      <version>1.14.0,1.12.5,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-30 01:00:00" id="24043" opendate="2021-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reuse the code of &amp;#39;check savepoint preconditions&amp;#39;.</summary>
      <description>here</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.stopwithsavepoint.StopWithSavepointTerminationManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.Executing.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.SavepointRestoreSettings.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-3 01:00:00" id="24155" opendate="2021-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate documentation for how to configure the CheckpointFailureManager</summary>
      <description>Documentation added in FLINK-23916 should be translated to it's Chinese counterpart. Note that this applies to three separate commits:merged to master as cd01d4c0279merged to release-1.14 as 2e769746bf2merged to release-1.13 as e1a71219454</description>
      <version>1.14.0,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-7 01:00:00" id="24181" opendate="2021-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update org.jsoup.jsoup to 1.14.2</summary>
      <description>Update org.jsoup.jsoup to at least 1.14.2 to address CVEGHSA-m72m-mhq2-9p6cFlink itself isn't directly affected, but it's still good to update the dependency to avoid any scanners reporting vulnerabilities in Flink</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-8 01:00:00" id="24208" opendate="2021-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow idempotent savepoint triggering</summary>
      <description>As a user of Flink, I want to be able to trigger a savepoint from an external system in a way that I can detect if I have requested this savepoint already.By passing a custom ID to the savepoint request, I can check (in case of an error of the original request, or the external system) if the request has been made already.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.savepoints.SavepointTriggerRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">docs.content.docs.ops.rest.api.md</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.SavepointHandlerRequestBodyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.savepoints.StopWithSavepointHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.savepoints.stop.StopWithSavepointRequestBody.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.savepoints.SavepointTriggerRequestBody.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientSavepointTriggerTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-8 01:00:00" id="24212" opendate="2021-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>kerberos krb5.conf file is mounted as empty directory, not the expected file</summary>
      <description>From FLINK-18971we can mount kerberos krb5 conf file to pod with path /etc/krb5.confhowever if the krb5 conf file is not named krb5.conf (e.g named mykrb5.conf)the mount path /etc/krb5.conf in pod will be an empty directory, not a file that we expect.root@mykrb5-conf-test-6dd5c76f87-vfwh5:/# ls /etc/krb5.conf/ -latotal 8drwxrwxrwx 2 root root 4096 Sep 8 10:42 .drwxr-xr-x 1 root root 4096 Sep 8 10:42 ..The reason is that, the code in KerberosMountDecrator#decroateFlinkPod, we create the deployment like this:... volumeMounts: - mountPath: /etc/krb5.conf name: my-krb5conf-volume subPath: krb5.conf ... volumes: - configMap: defaultMode: 420 items: - key: mykrb5.conf path: mykrb5.conf name: my-krb5conf name: my-krb5conf-volumepath value should be set to const value "krb5.conf", not the file name that user provide (path: mykrb5.conf).we can use the yaml description file attachment to reproduce the problem.mykrb5conf.yaml</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.KerberosMountDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.KerberosMountDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-28 01:00:00" id="2422" opendate="2015-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web client is showing a blank page if "Meta refresh" is disabled in browser</summary>
      <description>A user reported via the Flink IRC channel that Firefox was showing only a blank page instead of the web client.We should add a link to that page as well, so that users can click it if the redirect doesn't work.Workaround: browse to launch.html directly.</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.resources.web-docs.index.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-10 01:00:00" id="24244" opendate="2021-9-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add logging about whether it&amp;#39;s executed in loopback mode</summary>
      <description>Currently, it's unclear whether a job is running in loopback mode or process mode, it would be great to add some logging to make it clear. This would be helpful for debugging. It makes it clear whether a failed test is running in loopback mode or process mode.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.worker.pool.service.py</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2021-9-16 01:00:00" id="24305" opendate="2021-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BatchPandasUDAFITTests.test_over_window_aggregate_function fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24170&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901&amp;l=23011Sep 15 20:40:43 cls = &lt;class 'pyflink.table.tests.test_pandas_udaf.BatchPandasUDAFITTests'&gt;Sep 15 20:40:43 actual = JavaObject id=o8666Sep 15 20:40:43 expected = ['+I[1, 4.3333335, 13, 5.5, 3.0, 3.0, 4.3333335, 8.0, 5.0, 5.0]', '+I[1, 4.3333335, 5, 4.3333335, 3.0, 3.0, 2.5, 4.333....0, 4.0, 2.0]', '+I[2, 2.0, 9, 2.0, 4.0, 4.0, 2.0, 2.0, 4.0, 4.0]', '+I[3, 2.0, 3, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0]']Sep 15 20:40:43 Sep 15 20:40:43 @classmethodSep 15 20:40:43 def assert_equals(cls, actual, expected):Sep 15 20:40:43 if isinstance(actual, JavaObject):Sep 15 20:40:43 actual_py_list = cls.to_py_list(actual)Sep 15 20:40:43 else:Sep 15 20:40:43 actual_py_list = actualSep 15 20:40:43 actual_py_list.sort()Sep 15 20:40:43 expected.sort()Sep 15 20:40:43 assert len(actual_py_list) == len(expected)Sep 15 20:40:43 &gt; assert all(x == y for x, y in zip(actual_py_list, expected))Sep 15 20:40:43 E AssertionError: assert FalseSep 15 20:40:43 E + where False = all(&lt;generator object PyFlinkTestCase.assert_equals.&lt;locals&gt;.&lt;genexpr&gt; at 0x7f792d98b900&gt;)</description>
      <version>1.14.0,1.12.5,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-16 01:00:00" id="24310" opendate="2021-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>A bug in the BufferingSink example in the doc</summary>
      <description>The following line in the BufferingSink onthis pagehas a bug:if (bufferedElements.size() == threshold) {It should be &gt;=instead of==, because when restoring from a checkpoint during downscaling, the task may get more elements than the threshold.</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.state.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.state.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-17 01:00:00" id="24317" opendate="2021-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the test implementation in test_flat_aggregate</summary>
      <description/>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.row.based.operation.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-17 01:00:00" id="24318" opendate="2021-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Casting a number to boolean has different results between &amp;#39;select&amp;#39; fields and &amp;#39;where&amp;#39; condition</summary>
      <description>The same cast in the following two sql:// SQL 1SELECT cast(0.1 as boolean)// SQL 2SELECT * from test2 where cast(0.1 as boolean)has different results.The cast result in SQL 1 is true and the cast in SQL 2 is false.</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinDependentConditionDerivationRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionTypeCoerceRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionEqualityTransferRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceScanRule.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-17 01:00:00" id="24324" opendate="2021-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create ElasticSearch 7 Sink</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-17 01:00:00" id="24325" opendate="2021-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create ElasticSearch 6.8 Sink</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.PreElasticsearch6BulkProcessorIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.KeyExtractorTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.KeyExtractor.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch6SinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch6SinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch6SinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilderBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBaseITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchSinkBuilderSupplier.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilderBase.java</file>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch5-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch5SinkExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch5-test.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.licenses.LICENSE.webbit</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.licenses.LICENSE.jzlib</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.licenses.LICENSE.jsr166y</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.licenses.LICENSE.joptsimple</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.licenses.LICENSE.hdrhistogram</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.licenses.LICENSE.base64</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.Elasticsearch5ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGeneratorFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.index.IndexGeneratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.util.ElasticsearchUtils.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGeneratorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGenerator.java</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch7-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch7SinkExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-quickstart-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-quickstart-test.src.main.java.org.apache.flink.quickstarts.test.Elasticsearch5SinkExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-quickstart-test.src.main.scala.org.apache.flink.quickstarts.test.Elasticsearch5SinkExample.scala</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
      <file type="M">tools.ci.java-ci-tools.src.main.resources.modules-skipping-deployment.modulelist</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchValidationUtils.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.RequestFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.RowElasticsearchSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6Configuration.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7Configuration.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7ConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.RowElasticsearchEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.BulkProcessorConfig.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.FlushBackoffType.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.NetworkClientConfig.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.RequestIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.TestClient.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.TestEmitter.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.index.AbstractTimeIndexGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.index.IndexGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.index.IndexGeneratorBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.index.IndexGeneratorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.index.StaticIndexGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-17 01:00:00" id="24326" opendate="2021-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Docs with new Sinks</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.elasticsearch.md</file>
      <file type="M">docs.content.docs.connectors.datastream.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-1-20 01:00:00" id="24339" opendate="2021-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove guard against CRLF split across chunks</summary>
      <description>Revert FLINK-24197 once Netty was upgraded to a version that includes a fix for https://github.com/netty/netty/issues/11668.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-30 01:00:00" id="2434" opendate="2015-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>org.apache.hadoop:hadoop-yarn-common:jar with value &amp;#39;jersey-test-framework-grizzly2+&amp;#39; does not match a valid id pattern</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-shaded-hadoop.flink-shaded-include-yarn.pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop1.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-23 01:00:00" id="24358" opendate="2021-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-avro-glue-schema-registry fails compiling with scala 2.12 due to dependency convergence</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24405&amp;view=logs&amp;j=ed6509f5-1153-558c-557a-5ee0afbcdf24&amp;t=241b1e5e-1a8e-5e6a-469a-a9b8cad87065&amp;l=13506[WARNING] Dependency convergence error for io.netty:netty-handler:4.1.63.Final paths to dependency are:+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-common:1.1.2 +-software.amazon.awssdk:glue:2.16.92 +-software.amazon.awssdk:netty-nio-client:2.16.92 +-io.netty:netty-codec-http:4.1.63.Final +-io.netty:netty-handler:4.1.63.Finaland+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-common:1.1.2 +-software.amazon.awssdk:glue:2.16.92 +-software.amazon.awssdk:netty-nio-client:2.16.92 +-io.netty:netty-codec-http2:4.1.63.Final +-io.netty:netty-handler:4.1.63.Finaland+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-common:1.1.2 +-software.amazon.awssdk:glue:2.16.92 +-software.amazon.awssdk:netty-nio-client:2.16.92 +-io.netty:netty-handler:4.1.63.Finaland+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-common:1.1.2 +-software.amazon.awssdk:glue:2.16.92 +-software.amazon.awssdk:netty-nio-client:2.16.92 +-com.typesafe.netty:netty-reactive-streams-http:2.0.5 +-com.typesafe.netty:netty-reactive-streams:2.0.5 +-io.netty:netty-handler:4.1.52.Final[WARNING] Dependency convergence error for org.jetbrains.kotlin:kotlin-stdlib:1.3.50 paths to dependency are:+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-serde:1.1.2 +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39 +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-compiler-impl-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-common:1.3.50 +-org.jetbrains.kotlin:kotlin-reflect:1.3.50 +-org.jetbrains.kotlin:kotlin-stdlib:1.3.50and+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-serde:1.1.2 +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39 +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-compiler-impl-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-common:1.3.50 +-org.jetbrains.kotlin:kotlin-stdlib:1.3.50and+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-serde:1.1.2 +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39 +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-compiler-impl-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-jvm:1.3.50 +-org.jetbrains.kotlin:kotlin-stdlib:1.3.50and+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-serde:1.1.2 +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39 +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-compiler-impl-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-stdlib:1.3.50and+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-serde:1.1.2 +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39 +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-compiler-impl-embeddable:1.3.50 +-org.jetbrains.kotlinx:kotlinx-coroutines-core:1.1.1 +-org.jetbrains.kotlin:kotlin-stdlib:1.3.20and+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-serde:1.1.2 +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39 +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-stdlib:1.3.50[WARNING] Dependency convergence error for io.netty:netty-codec-http:4.1.63.Final paths to dependency are:+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-common:1.1.2 +-software.amazon.awssdk:glue:2.16.92 +-software.amazon.awssdk:netty-nio-client:2.16.92 +-io.netty:netty-codec-http:4.1.63.Finaland+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-common:1.1.2 +-software.amazon.awssdk:glue:2.16.92 +-software.amazon.awssdk:netty-nio-client:2.16.92 +-io.netty:netty-codec-http2:4.1.63.Final +-io.netty:netty-codec-http:4.1.63.Finaland+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-common:1.1.2 +-software.amazon.awssdk:glue:2.16.92 +-software.amazon.awssdk:netty-nio-client:2.16.92 +-com.typesafe.netty:netty-reactive-streams-http:2.0.5 +-io.netty:netty-codec-http:4.1.52.Final[WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:Failed while enforcing releasability. See above detailed error message.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.pom.xml</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-23 01:00:00" id="24360" opendate="2021-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop Scala Shell</summary>
      <description>The community has discussed and voted to drop the Scala Shell from FlinkDiscussion thread: https://lists.apache.org/thread.html/rf7a7f935c43d3e98f94193be81b69f1c0d6e60b6fa09570531c3fa67%40%3Cdev.flink.apache.org%3EVote thread: https://lists.apache.org/thread.html/r5f605db6be4788af58ffa6bcd366028e0cd9bf9df1dfeaf054cec86b%40%3Cdev.flink.apache.org%3E</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-scala-shell.start-script.start-scala-shell.sh</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellLocalStartupITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.jar.TestingData.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.jar.package.scala</file>
      <file type="M">flink-scala-shell.src.test.resources.flink-conf.yaml</file>
      <file type="M">flink-scala-shell.src.test.java.org.apache.flink.api.java.FlinkILoopTest.java</file>
      <file type="M">flink-scala-shell.src.test.assembly.test-scalashell-customjar-assembly.xml</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellStreamEnvironment.java</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellEnvironment.java</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.JarHelper.java</file>
      <file type="M">flink-scala-shell.pom.xml</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.RemoteEnvironmentConfigUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.LocalEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.licenses.LICENSE.scopt</file>
      <file type="M">flink-dist.src.main.assemblies.bin-scala2.11.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">docs.content.docs.deployment.repls.scala.shell.md</file>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.docs.connectors.table.hive.hive.catalog.md</file>
      <file type="M">docs.content.zh.docs.deployment.repls.scala.shell.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.catalog.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-24 01:00:00" id="24372" opendate="2021-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate Elasticsearch Sinkfunctions</summary>
      <description>Once all other tickets of FLINK-24323 are resolved we can mark the Elasticsearch sinks implementing sinkfunction as deprecated.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch7.RestClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.RestClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.util.RetryRejectedExecutionFailureHandler.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ActionRequestFailureHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-24 01:00:00" id="24373" opendate="2021-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove BETA Tag from FLIP-27 Source Docs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.sources.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.sources.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-11-30 01:00:00" id="2441" opendate="2015-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Introduce an OpInfo object on the python side</summary>
      <description>All information required to construct on operation are currently saved in a plain dictionary on the python side, whose fields are generally accessed using a variety of string constants.so right now you find lines like: op_info[_Fields.KEYS] = keysThe following shortcomings exist in the current system: There is no central place to define default values. This is done all over the place, to some extent in a redundant way, It produces fairly long code, is surprisingly cumbersome to write.Instead i would like to add a separate OperationInfo object. This code be a special dictionary with preset values for each field, but due to points 2 and 3, Id prefer having an attribute for every field. the resulting code would look like this:op_info.keys = keysisn't that lovely.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.type.deduction.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.DataSet.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Constants.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-30 01:00:00" id="24410" opendate="2021-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Confluent Platform OSS version in end-to-end tests</summary>
      <description>Flink uses Confluent Platform OSS/community edition 5.0.0, which doesn't exist in a Scala 2.12 version. We should bump the used version to at least 5.2.x.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.confluent.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-10-9 01:00:00" id="24492" opendate="2021-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect implicit type conversion between numeric and (var)char</summary>
      <description>The result of the sql "select 1 = '1'" is false. This is caused by the CodeGen. CodeGen incorrectly transform this "=" to "BinaryStringData.equals (int 1)". And "&lt;&gt;" has the same wrong result.In my opinion, "=" should have the same behavior with "&gt;" and "&lt;", which have the correct results. So before calcite solves this bug or flink supports this kind of implicit type conversion, we'd better temporarily forbidding this implicit type conversion in "=" and "&lt;&gt;".</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionTypeCoerceRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-11 01:00:00" id="24495" opendate="2021-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python installdeps hangs</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24922&amp;view=logs&amp;j=821b528f-1eed-5598-a3b4-7f748b13f261&amp;t=6bb545dd-772d-5d8c-f258-f5085fba3295&amp;l=23587Oct 10 02:30:01 py38-cython create: /__w/1/s/flink-python/.tox/py38-cythonOct 10 02:30:04 py38-cython installdeps: pytest, apache-beam==2.27.0, cython==0.29.16, grpcio&gt;=1.29.0,&lt;2, grpcio-tools&gt;=1.3.5,&lt;=1.14.2, apache-flink-librariesOct 10 02:45:22 ==============================================================================Oct 10 02:45:22 Process produced no output for 900 seconds.Oct 10 02:45:22 ==============================================================================</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-10-15 01:00:00" id="24559" opendate="2021-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-rpc-akka-loader does not bundle flink-rpc-akka</summary>
      <description>Plugins are executed in the wrong order, causing packaging to occur before the dependency has been copied.</description>
      <version>1.14.1,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-15 01:00:00" id="24565" opendate="2021-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port Avro FileSystemFormatFactory to BulkFormat</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroFilesystemITCase.java</file>
      <file type="M">flink-formats.flink-avro.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-8-1 01:00:00" id="2460" opendate="2015-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ReduceOnNeighborsWithExceptionITCase failure</summary>
      <description>I noticed a build error due to failure on this case. It was on a branch of my fork, which didn't actually have anything to do with the failed test or the runtime system at all.Here's the error log: https://s3.amazonaws.com/archive.travis-ci.org/jobs/73695554/log.txt</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SubpartitionTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionViewSyncIO.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionViewAsyncIO.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-21 01:00:00" id="24608" opendate="2021-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sinks built with the unified sink framework do not receive timestamps when used in Table API</summary>
      <description>All sinks built with the unified sink framework extract the timestamp from the internal StreamRecord. The Table API does not facilitate the timestamp field in the StreamRecord but extracts the timestamp from the actual data. We either have to use a dedicated operator before all the sinks to simulate the behavior or allow a customizable timestamp extraction during the sink translation.</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.match.RowtimeProcessFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-21 01:00:00" id="24612" opendate="2021-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka test container creates a large amount of logs</summary>
      <description>When we use a testcontainer setup we try to forward all container STDOUT logs to the surrounding test logger. Unfortunately, Kafka loggers are by default writing a large number of logs because some of the internal loggers are defaulting to TRACE logging.A good example is this test job https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25084&amp;view=logs&amp;j=32a18cd8-d404-5807-996d-abcee436b891where one of the test was stuck and the generated artifact is ~25GB. This makes debugging very hard because the file is hard to parse.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaTransactionLogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.FlinkKafkaInternalProducerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-27 01:00:00" id="24662" opendate="2021-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink sphinx check failed with "node class &amp;#39;meta&amp;#39; is already registered, its visitors will be overridden"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25481&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=8d78fe4f-d658-5c70-12f8-4921589024c3==========mypy checks... [SUCCESS]===========Oct 26 22:08:34 rm -rf _build/*Oct 26 22:08:34 /__w/1/s/flink-python/dev/.conda/bin/sphinx-build -b html -d _build/doctrees -a -W . _build/htmlOct 26 22:08:34 Running Sphinx v2.4.4Oct 26 22:08:34 Oct 26 22:08:34 Warning, treated as error:Oct 26 22:08:34 node class 'meta' is already registered, its visitors will be overriddenOct 26 22:08:34 Makefile:76: recipe for target 'html' failed</description>
      <version>1.12.0,1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-28 01:00:00" id="24676" opendate="2021-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema does not match if explain insert statement with partial column</summary>
      <description>create table MyTable (a int, b int) with ('connector' = 'datagen');create table MySink (c int, d int) with ('connector' = 'print');explain plan for insert into MySink(d) select a from MyTable where a &gt; 10;If execute the above statement, we will get the following exceptionorg.apache.flink.table.api.ValidationException: Column types of query result and sink for registered table 'default_catalog.default_database.MySink' do not match.Cause: Different number of columns.Query schema: &amp;#91;a: BIGINT&amp;#93;Sink schema: &amp;#91;d: BIGINT, e: INT&amp;#93;</description>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-10-29 01:00:00" id="24695" opendate="2021-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update how to configure unaligned checkpoints in the documentation</summary>
      <description>It looks like we don't have a code example how to enabled unaligned checkpoints anywhere in the docs. That should be fixed.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.checkpointing.under.backpressure.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpointing.under.backpressure.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-11-29 01:00:00" id="24699" opendate="2021-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move scalastyle execution to validation phase</summary>
      <description>For some reason the scalstyle plugin by default runs in the verify phase.I propose to move it to the validate phase where other source QA plugins are run, which also makes it easier to run it locally.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-29 01:00:00" id="24703" opendate="2021-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CSV format support for filesystem based on StreamFormat and BulkWriter interfaces.</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.utils.JsonPlanTestBase.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.DataStreamCsvITCase.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.RowDataToCsvConverters.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFilesystemBatchITCase.java</file>
      <file type="M">flink-formats.flink-csv.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvToRowDataConverters.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileSystemFormatFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-2 01:00:00" id="24740" opendate="2021-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update testcontainers dependency to v1.16.2</summary>
      <description>We should update our testcontainers dependency to the latest version, which is 1.16.2Main benefits (based on https://github.com/testcontainers/testcontainers-java/releases) Better startup performance for all containers Faster Cassandra startup Host port access for containers (make hosts ports accessible to containers, even after the container has started) New Azure Cosmos DB module</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-2 01:00:00" id="24742" opendate="2021-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL client add info about key strokes to docs</summary>
      <description>SQL client supports key strokes from jline.Unfortunately there is no docs about that in jline however there is source from which it could be found &amp;#91;1&amp;#93;here it is a list of most useful key strokes which are already supported by all existing Flink SQL client Key-Stroke Description `alt-b` Backward word `alt-f` Forward word `alt-c` Capitalize word `alt-l` Lowercase word `alt-u` Uppercase word `alt-d` Kill word `alt-n` History search forward `alt-p` History search backward `alt-t` Transpose words `ctrl-a` To the beginning of line `ctrl-e` To the end of line `ctrl-b` Backward char `ctrl-f` Forward char `ctrl-d` Delete char `ctrl-h` Backward delete char `ctrl-t` Transpose chars `ctrl-i` Invoke completion `ctrl-j` Submit a query `ctrl-m` Submit a query `ctrl-k` Kill the line to the right from the cursor `ctrl-w` Kill the line to the left from the cursor `ctrl-u` Kill the whole line `ctrl-l` Clear screen `ctrl-n` Down line from history `ctrl-p` Up line from history `ctrl-r` History incremental search backward `ctrl-s` History incremental search forward &amp;#91;1&amp;#93; https://github.com/jline/jline3/blob/997496e6a6338ca5d82c7dec26f32cf089dd2838/reader/src/main/java/org/jline/reader/impl/LineReaderImpl.java#L5907</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-3 01:00:00" id="24746" opendate="2021-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alibaba maven mirror is unstable</summary>
      <description>Our Maven mirror setup for CI alicloud-mvn-mirror is currently incredibly unstable.</description>
      <version>None</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.maven-utils.sh</file>
      <file type="M">tools.ci.alibaba-mirror-settings.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-3 01:00:00" id="24749" opendate="2021-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reuse CheckpointStatsTracker across rescaling</summary>
      <description>We can solve the collision of checkpointing metrics by using the same CheckpointStatsTracker instance.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.TestingDefaultExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-3 01:00:00" id="2475" opendate="2015-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename Flink Client log file</summary>
      <description>Currently, JoManager and TaskManager log/out files are names as follows: flink-mjsax-jobmanager-....log flink-mjsax-jobmanager-....out flink-mjsax-taskmanager-....log flink-mjsax-taskmanager-....outHowever, CLI log file is named differently: flink-mjsax-flink-client-....logThis should be "client" only and not "flink-client" for consistency.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-4 01:00:00" id="24760" opendate="2021-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update user document for batch window tvf</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-topn.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-join.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-agg.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-topn.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-join.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-agg.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-4 01:00:00" id="24765" opendate="2021-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Kafka dependency</summary>
      <description>We rely on a very old Kafka version 2.4.1 which was the last version supporting scala 2.11. Since we dropped Scala 2.11 we can now update to a more recent one.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.KafkaSourceE2ECase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-state-machine.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.KafkaEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricMutableWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.FlinkKafkaInternalProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.FlinkKafkaInternalProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-4 01:00:00" id="24772" opendate="2021-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update user document for individual window table-valued function</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-tvf.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-4-5 01:00:00" id="24804" opendate="2021-11-5 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Upgrade oshi-core from v3.4.0 to v.6.1.5</summary>
      <description>Flink still uses com.github.oshi:oshi-core:3.4.0 (released Feb 2017) while com.github.oshi:oshi-core:5.8.3 (released Oct 2021) is also available.The license for 3.4.0 is EPL 1.0 which has also changed with the newer version, which is now MIT. Upgrading to the newer version would allow us to remove a "weak copyleft" license per https://www.apache.org/legal/resolved.html</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.metrics.SystemResourcesMetricsITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.utils.SystemResourcesCounterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.util.SystemResourcesMetricsInitializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.util.SystemResourcesCounter.java</file>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-8 01:00:00" id="24822" opendate="2021-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve CatalogTableSpecBase and its subclass method parameter</summary>
      <description>Currently, CatalogTableSpecBase and its subclass related method use PlannerBase as parameter, we can improve it use FlinkContext enough.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSinkSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.TemporalTableSourceSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSourceSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSinkSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-8 01:00:00" id="24827" opendate="2021-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump maven-dependency-plugin to 3.2.0</summary>
      <description>Flink currently uses version 3.1.1 of org.apache.maven.plugins:maven-dependency-plugin. We should update to 3.2.0 (currently the latest version)</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.pyflink.pyflink.gateway.server.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-8 01:00:00" id="24828" opendate="2021-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Powermock to v2.0.9</summary>
      <description>We should update Powermock to the latest available version, which is currently v2.0.9</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-11-8 01:00:00" id="24832" opendate="2021-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update JUnit5 to v5.8.1</summary>
      <description>We should update to the latest version of JUnit5, v5.8.1</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-8 01:00:00" id="24833" opendate="2021-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent use of deprecated APIs in flink-examples</summary>
      <description>We should be able to setup java compiler for examplesto fail on any usage of deprecated APIs.Something along the lines of:&lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;...&lt;/version&gt; &lt;executions&gt;  &lt;execution&gt;   &lt;id&gt;compile&lt;/id&gt;   &lt;phase&gt;process-sources&lt;/phase&gt;   &lt;goals&gt;    &lt;goal&gt;compile&lt;/goal&gt;   &lt;/goals&gt;   &lt;configuration&gt;    &lt;compilerArgument&gt;-Xlint:deprecation&lt;/compilerArgument&gt;    &lt;failOnWarning&gt;true&lt;/failOnWarning&gt;   &lt;/configuration&gt;  &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-examples.flink-examples-batch.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-6-10 01:00:00" id="24865" opendate="2021-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support MATCH_RECOGNIZE in Batch mode</summary>
      <description>Currently MATCH_RECOGNIZE only works in Streaming mode. It should also be supported in Batch mode</description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.match.PatternTranslatorTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.MatchRecognizeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.operator.BatchOperatorNameTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.operator.BatchOperatorNameTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalMatchRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalMatch.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-11 01:00:00" id="24869" opendate="2021-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-core should be provided in flink-file-sink-common</summary>
      <description>As example flink-connector-files brings flink-core with compile scope via flink-file-sink-common.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-file-sink-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-12 01:00:00" id="24887" opendate="2021-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retrying savepoints may cause early cluster shutdown</summary>
      <description>If an operation is retried we potentially access the result of a previous attempt to see if it has already failed and eagerly fail the trigger request. If that attempt is already complete then this may lead to an unexpected shutdown of the cluster.Beyond this issue, the eager checking of previous attempts makes error handling more complicated, because you have to cover all cases for both the trigger and status-retrieval operations.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherCachedOperationsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherCachedOperationsHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.async.CompletedOperationCacheTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.async.CompletedOperationCache.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-15 01:00:00" id="24904" opendate="2021-11-15 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add documentation for KDS Async Sink</summary>
      <description>MotivationFLINK-24227 introduces a new sink for Kinesis Data Streams that supersedes the existing one based on KPL.Scope: Deprecate the current section in the docs for the Kinesis KPL sink and write documentation and usage guide for the new sink.ReferencesMore details to be foundhttps://cwiki.apache.org/confluence/display/FLINK/FLIP-171%3A+Async+Sink</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kinesis.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-1-17 01:00:00" id="24932" opendate="2021-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Frocksdb cannot run on Apple M1</summary>
      <description>After we bump up RocksDB version to 6.20.3, we support to run RocksDB on linux arm cluster. However, according to the feedback from Robert, Apple M1 machines cannot run FRocksDB yet:java.lang.Exception: Exception while creating StreamOperatorStateContext. at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:255) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-runtime-1.14.0.jar:1.14.0] at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-runtime-1.14.0.jar:1.14.0] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-runtime-1.14.0.jar:1.14.0] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-runtime-1.14.0.jar:1.14.0] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_312]Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for StreamFlatMap_c21234bcbf1e8eb4c61f1927190efebd_(1/1) from any of the 1 provided restore options. at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] ... 11 moreCaused by: java.io.IOException: Could not load the native RocksDB library at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.ensureRocksDBIsLoaded(EmbeddedRocksDBStateBackend.java:882) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:402) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:345) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:87) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:329) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] ... 11 moreCaused by: java.lang.UnsatisfiedLinkError: /private/var/folders/js/yfk_y2450q7559kygttykwk00000gn/T/rocksdb-lib-5783c058ce68d31d371327abc9b51cac/librocksdbjni-osx.jnilib: dlopen(/private/var/folders/js/yfk_y2450q7559kygttykwk00000gn/T/rocksdb-lib-5783c058ce68d31d371327abc9b51cac/librocksdbjni-osx.jnilib, 0x0001): tried: '/private/var/folders/js/yfk_y2450q7559kygttykwk00000gn/T/rocksdb-lib-5783c058ce68d31d371327abc9b51cac/librocksdbjni-osx.jnilib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/usr/lib/librocksdbjni-osx.jnilib' (no such file) at java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[?:1.8.0_312] at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1950) ~[?:1.8.0_312] at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1832) ~[?:1.8.0_312] at java.lang.Runtime.load0(Runtime.java:811) ~[?:1.8.0_312] at java.lang.System.load(System.java:1088) ~[?:1.8.0_312] at org.rocksdb.NativeLibraryLoader.loadLibraryFromJar(NativeLibraryLoader.java:79) ~[frocksdbjni-6.20.3-ververica-1.0.jar:?] at org.rocksdb.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:57) ~[frocksdbjni-6.20.3-ververica-1.0.jar:?] at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.ensureRocksDBIsLoaded(EmbeddedRocksDBStateBackend.java:856) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:402) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:345) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:87) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:329) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] ... 11 moreThis issue is tracked by RocksDB community: rocksdb/issues/7720</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-1-22 01:00:00" id="24978" opendate="2021-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade ASM to 9.2</summary>
      <description>As per usual we need to bump flink-shaded-asm.</description>
      <version>None</version>
      <fixedVersion>shaded-15.0,1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.ExtractionUtils.java</file>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ClosureCleaner.scala</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.ClassRelocator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractionUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.ClosureCleaner.java</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-22 01:00:00" id="24979" opendate="2021-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove MaxPermSize configuration in HBase surefire config</summary>
      <description>The MaxPermSize parameter has no effect since JDK 8, and is actively rejected in Java 17. Given that we for years it worked just fine in without it, we can just remove it.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-22 01:00:00" id="24983" opendate="2021-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade surefire to 3.0.0-M5</summary>
      <description>Surefire 3.0.0-M5 comes with a new TCP/IP communication channel between surefire and JVM forks.This will allow us to resolve "Corrupted STDOUT" issues when the JVM is printing warnings due to unsafe accesses.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-22 01:00:00" id="24988" opendate="2021-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade lombok to 1.18.22</summary>
      <description>Our current Lombok version fails on Java 17 due to illegal accesses to JDK internals. Newer versions of Lombok do some hacks at runtime to resolve the issue...</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-22 01:00:00" id="24989" opendate="2021-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade shade-plugin to 3.2.4</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-24 01:00:00" id="25039" opendate="2021-11-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable shading of test jars by default</summary>
      <description>The AZP build fails with a license check:21:26:40,233 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - Missing META-INF/LICENSE in /tmp/flink-validation-deployment/org/apache/flink/flink-sql-parquet_2.12/1.15-SNAPSHOT/flink-sql-parquet_2.12-1.15-20211123.212027-1-tests.jar21:26:40,738 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - The notice file in /tmp/flink-validation-deployment/org/apache/flink/flink-connector-cassandra_2.12/1.15-SNAPSHOT/flink-connector-cassandra_2.12-1.15-20211123.211736-1-tests.jar does not contain the expected entries.21:26:40,739 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - Missing META-INF/LICENSE in /tmp/flink-validation-deployment/org/apache/flink/flink-connector-cassandra_2.12/1.15-SNAPSHOT/flink-connector-cassandra_2.12-1.15-20211123.211736-1-tests.jar21:26:41,673 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - The notice file in /tmp/flink-validation-deployment/org/apache/flink/flink-kubernetes/1.15-SNAPSHOT/flink-kubernetes-1.15-20211123.212114-1-tests.jar does not contain the expected entries.21:26:41,675 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - Missing META-INF/LICENSE in /tmp/flink-validation-deployment/org/apache/flink/flink-kubernetes/1.15-SNAPSHOT/flink-kubernetes-1.15-20211123.212114-1-tests.jar21:28:00,582 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Found a total of 5 severe license issues==============================================================================License Check failed. See previous output for details.==============================================================================##[error]Bash exited with code '1'.https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26967&amp;view=logs&amp;j=946871de-358d-5815-3994-8175615bc253&amp;t=e0240c62-4570-5d1c-51af-dd63d2093da1&amp;l=30668https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26967&amp;view=logs&amp;j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&amp;t=a7382ec4-87d2-5a9d-7c53-a2f93e317458&amp;l=31863https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26967&amp;view=logs&amp;j=6e8542d7-de38-5a33-4aca-458d6c87066d&amp;t=dffc2faa-5b48-5b4e-0797-dec1b1f74872&amp;l=31863</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-code-splitter.pom.xml</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.pom.xml</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro.pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-twitter.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-state-machine.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-gcp-pubsub.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-rabbitmq.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-24 01:00:00" id="25042" opendate="2021-11-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow calls to @VisibleForTesting from inner classes</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.violations.e5126cae-f3fe-48aa-b6fb-60ae6cc3fcd5</file>
      <file type="M">flink-architecture-tests.src.test.java.org.apache.flink.architecture.rules.ApiAnnotationRules.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-24 01:00:00" id="25045" opendate="2021-11-24 00:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce AdaptiveBatchScheduler</summary>
      <description>Introduce AdaptiveBatchScheduler</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SsgNetworkMemoryCalculationUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerComponentsFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateTrackingMockExecutionGraph.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.IntermediateResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertexTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocatorFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionVertexOperations.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultSchedulerComponents.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.BlockingResultInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.topology.DefaultLogicalTopology.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobEdge.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.IntermediateResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.SchedulingPipelinedRegionComputeUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.PipelinedRegionComputeUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.LogicalPipelinedRegionComputeUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-2-25 01:00:00" id="25053" opendate="2021-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to use the usrlib to load code in the user code class loader</summary>
      <description>With FLINK-13993 we introduced the usrlib directory that can be used to load code in the user code class loader. This functionality has not been properly documented so that it is very hard to use. I would suggest to change this so that our users can benefit from this cool feature.</description>
      <version>1.14.0,1.12.5,1.13.3,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-26 01:00:00" id="25067" opendate="2021-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the description of RocksDB&amp;#39;s background threads</summary>
      <description>RocksDB actually has changed the maximum number of concurrent background flush and compaction jobs to 2 for long time, we should fix the related documentation.</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-26 01:00:00" id="25072" opendate="2021-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce description for operator</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.DataStreamTest.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.DataStreamTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.SimpleTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.JSONGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSink.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.dag.Transformation.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-26 01:00:00" id="25073" opendate="2021-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce Tree Mode description for job vertex</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.detail.job-overview-drawer-detail.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.detail.job-overview-drawer-detail.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.detail.job-overview-drawer-detail.component.html</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.PipelineOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pipeline.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-12-26 01:00:00" id="25076" opendate="2021-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify name of SQL operators</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.testStreamTableEnvironmentExecutionExplain.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.testStatementSetExecutionExplain.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.testExplainJsonPlan.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.testExecuteSqlWithExplainDetailsSelect.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.testExecuteSqlWithExplainDetailsInsert.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.testExecuteSqlWithExplainDetailsAndUnion.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.filesystem.testFileSystemTableSinkWithParallelismInStreamingSql1.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.filesystem.testFileSystemTableSinkWithParallelismInStreamingSql0.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.filesystem.testFileSystemTableSinkWithParallelismInBatch.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.api.internal.TableEnvironmentInternalTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.connector.file.table.FileSystemTableSinkTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.ScanUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.ExecNodeUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowDeduplicate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWatermarkAssigner.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalSort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupTableAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMiniBatchAssigner.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLegacyTableSourceScan.java</file>
      <file type="M">docs.layouts.shortcodes.generated.optimizer.config.configuration.html</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.explain.testHiveTableSinkWithParallelismInBatch.out</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.explain.testHiveTableSinkWithParallelismInStreaming.out</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.ExternalDynamicSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.ExternalDynamicSource.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecBoundedStreamScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecLegacyTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecLimit.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecMultipleInput.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecNestedLoopJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortLimit.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortMergeJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecExpand.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLegacySink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCalc.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecValues.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecWindowTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecChangelogNormalize.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDataStreamScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDeduplicate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDropUpdateBefore.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupTableAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIncrementalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIntervalJoin.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-12-1 01:00:00" id="25128" opendate="2021-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reorganize table modules and introduce flink-table-planner-loader</summary>
      <description>For more details, see https://docs.google.com/document/d/12yDUCnvcwU2mODBKTHQ1xhfOq1ujYUrXltiN_rbhT34/edit?usp=sharing</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunctionWithOrderTest.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteMinusAllRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteIntersectAllRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.common.PartialInsertTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.TimestampToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-architecture-tests.pom.xml</file>
      <file type="M">flink-architecture-tests.violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
      <file type="M">flink-architecture-tests.violations.7602816f-5c01-4b7a-9e3e-235dfedec245</file>
      <file type="M">flink-architecture-tests.violations.e5126cae-f3fe-48aa-b6fb-60ae6cc3fcd5</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-stream-sql-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-tpcds-test.pom.xml</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
      <file type="M">flink-python.apache-flink-libraries.setup.py</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.licenses.LICENSE.icu4j</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.licenses.LICENSE.janino</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-runtime.pom.xml</file>
      <file type="M">flink-table.flink-table-runtime.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-uber.pom.xml</file>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">tools.ci.java-ci-tools.src.main.java.org.apache.flink.tools.ci.suffixcheck.ScalaSuffixChecker.java</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.CollectAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.JsonArrayAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.JsonObjectAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LagAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.tablefunctions.ReplicateRows.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.SetOpRewriteUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LagAggFunctionTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-1 01:00:00" id="25129" opendate="2021-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update docs to use flink-table-planner-loader instead of flink-table-planner</summary>
      <description>For more details https://docs.google.com/document/d/12yDUCnvcwU2mODBKTHQ1xhfOq1ujYUrXltiN_rbhT34/edit?usp=sharing</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.README.md</file>
      <file type="M">docs.content.docs.dev.configuration.testing.md</file>
      <file type="M">docs.content.docs.dev.configuration.overview.md</file>
      <file type="M">docs.content.docs.dev.configuration.maven.md</file>
      <file type="M">docs.content.docs.dev.configuration.connector.md</file>
      <file type="M">docs.content.docs.dev.configuration.advanced.md</file>
      <file type="M">docs.layouts.shortcodes.artifact.html</file>
      <file type="M">docs.content.docs.libs.gelly.overview.md</file>
      <file type="M">docs.content.docs.libs.cep.md</file>
      <file type="M">docs.content.docs.flinkDev.ide.setup.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.match.recognize.md</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.docs.dev.table.sourcesSinks.md</file>
      <file type="M">docs.content.docs.dev.table.overview.md</file>
      <file type="M">docs.content.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.docs.dev.datastream..index.md</file>
      <file type="M">docs.content.docs.dev.datastream.testing.md</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.queryable.state.md</file>
      <file type="M">docs.content.docs.connectors.table.upsert-kafka.md</file>
      <file type="M">docs.content.docs.connectors.table.overview.md</file>
      <file type="M">docs.content.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.docs.connectors.table.jdbc.md</file>
      <file type="M">docs.content.docs.connectors.table.hbase.md</file>
      <file type="M">docs.content.docs.connectors.table.elasticsearch.md</file>
      <file type="M">docs.content.docs.connectors.datastream.twitter.md</file>
      <file type="M">docs.content.docs.connectors.datastream.rabbitmq.md</file>
      <file type="M">docs.content.docs.connectors.datastream.pulsar.md</file>
      <file type="M">docs.content.docs.connectors.datastream.pubsub.md</file>
      <file type="M">docs.content.docs.connectors.datastream.nifi.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.docs.connectors.datastream.jdbc.md</file>
      <file type="M">docs.content.docs.connectors.datastream.elasticsearch.md</file>
      <file type="M">docs.content.docs.connectors.datastream.cassandra.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-2 01:00:00" id="25145" opendate="2021-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for Zookeeper 3.6</summary>
      <description/>
      <version>None</version>
      <fixedVersion>shaded-15.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.container.FlinkContainersBuilder.java</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.ha.datastream.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.ha.per.job.cluster.datastream.sh</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFilters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.DefaultLastStateConnectionStateListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.LastStateConnectionStateListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointRecoveryFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.AbstractZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.CuratorFrameworkWithUnhandledErrorListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperMultipleComponentLeaderElectionHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.CoLocationGroupImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStoreWatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriverFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriverFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperSharedCount.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperSharedValue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperVersionedValue.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphsStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStoreWatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsTreeCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperTestEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-3 01:00:00" id="25153" opendate="2021-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inappropriate variable naming</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.ClosingFSDataOutputStream.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-3 01:00:00" id="25155" opendate="2021-12-3 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement claim snapshot mode</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointMetadataLoadingTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.SavepointRestoreSettings.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.SavepointConfigOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.Checkpoints.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">docs.layouts.shortcodes.generated.savepoint.config.configuration.html</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-12-6 01:00:00" id="25180" opendate="2021-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen test fails while setting up libzip4</summary>
      <description>The Jepsen tests fail from time to time while trying to set up libzip4. java.util.concurrent.ExecutionException: clojure.lang.ExceptionInfo: throw+: {:type :jepsen.control/nonzero-exit, :cmd "sudo -S -u root bash -c \"cd /; env DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes libzip4\"", :exit -1, :out "Reading package lists...Building dependency tree...Reading state information...The following NEW packages will be installed: libzip40 upgraded, 1 newly installed, 0 to remove and 120 not upgraded.Need to get 40.6 kB of archives.After this operation, 103 kB of additional disk space will be used.Get:1 http://cdn-aws.deb.debian.org/debian stretch/main amd64 libzip4 amd64 1.1.2-1.1+b1 [40.6 kB]Fetched 40.6 kB in 0s (0 B/s)Selecting previously unselected package libzip4:amd64. (Reading database ... (Reading database ... 5% (Reading database ... 10% (Reading database ... 15% (Reading database ... 20% (Reading database ... 25% (Reading database ... 30% (Reading database ... 35% (Reading database ... 40% (Reading database ... 45% (Reading database ... 50% (Reading database ... 55% (Reading database ... 60% (Reading database ... 65% (Reading database ... 70% (Reading database ... 75% (Reading database ... 80% (Reading database ... 85% (Reading database ... 90% (Reading database ... 95% (Reading database ... 100% (Reading database ... 49065 files and directories currently installed.) Preparing to unpack .../libzip4_1.1.2-1.1+b1_amd64.deb ... Unpacking libzip4:amd64 (1.1.2-1.1+b1) ... Setting up libzip4:amd64 (1.1.2-1.1+b1) ... ", :err "", :host "172.31.4.8", :action {:cmd "sudo -S -u root bash -c \"cd /; env DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes libzip4\"", :in "root"}} {:type :jepsen.control/nonzero-exit, :cmd "sudo -S -u root bash -c \"cd /; env DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes libzip4\"", :exit -1, :out "Reading package lists...Building dependency tree...Reading state information...The following NEW packages will be installed: libzip40 upgraded, 1 newly installed, 0 to remove and 120 not upgraded.Need to get 40.6 kB of archives.After this operation, 103 kB of additional disk space will be used.Get:1 http://cdn-aws.deb.debian.org/debian stretch/main amd64 libzip4 amd64 1.1.2-1.1+b1 [40.6 kB]Fetched 40.6 kB in 0s (0 B/s)Selecting previously unselected package libzip4:amd64. (Reading database ... (Reading database ... 5% (Reading database ... 10% (Reading database ... 15% (Reading database ... 20% (Reading database ... 25% (Reading database ... 30% (Reading database ... 35% (Reading database ... 40% (Reading database ... 45% (Reading database ... 50% (Reading database ... 55% (Reading database ... 60% (Reading database ... 65% (Reading database ... 70% (Reading database ... 75% (Reading database ... 80% (Reading database ... 85% (Reading database ... 90% (Reading database ... 95% (Reading database ... 100% (Reading database ... 49065 files and directories currently installed.) Preparing to unpack .../libzip4_1.1.2-1.1+b1_amd64.deb ... Unpacking libzip4:amd64 (1.1.2-1.1+b1) ... Setting up libzip4:amd64 (1.1.2-1.1+b1) ... ", :err "", :host "172.31.4.8", :action {:cmd "sudo -S -u root bash -c \"cd /; env DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes libzip4\"", :in "root"}}https://app.travis-ci.com/github/dataArtisans/flink-jepsen-ci/jobs/550915650#L1300</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
      <file type="M">flink-jepsen.project.clj</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-12-6 01:00:00" id="25187" opendate="2021-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Apply padding for BINARY(&lt;precision&gt;)</summary>
      <description>When the resulting byte array that is generated for a CAST(XXX AS BINARY(&lt;precision&gt;) has length &lt;precision, then it should be padded with0 to the right, to end up with a byte array ofprecision length, similarly to padding with spaces forCHAR(&lt;precision&gt;).</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionMiscITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.StringToBinaryCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RawToBinaryCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.BinaryToBinaryCastRule.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-2-9 01:00:00" id="25225" opendate="2021-12-9 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add e2e TPCDS tests to run against the AdaptiveBatchScheduler</summary>
      <description>To automatically and continuously verify the AdaptiveBatchScheduler, we should add a new e2e test which runs TPCDS against the AdaptiveBatchScheduler.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-tpcds-test.src.main.java.org.apache.flink.table.tpcds.TpcdsTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-9 01:00:00" id="25226" opendate="2021-12-9 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add documentation about the AdaptiveBatchScheduler</summary>
      <description>Documentation is needed to explain to users how to enable the AdaptiveBatchScheduler and properly configuring it.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-10 01:00:00" id="25244" opendate="2021-12-10 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Deprecate Java 8 support</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2021-1-16 01:00:00" id="25348" opendate="2021-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update release guide to reset japicmp exceptions for every release</summary>
      <description>I propose to clean up the japicmp maven plugin exclusion for every minor release for @Public and the exclusions for @PublicEvolving with every minor release. Currently, we dont do this and thats why we have accumulated quite some list of exclusions that a) might shadow other problems and b) nobody really knows why they are still relevant. I would propose to make this part of the release guide. The result should be that we minimize our set of exclusions.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-16 01:00:00" id="25351" opendate="2021-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add FlinkVersion</summary>
      <description>In order to check when a method needs to graduate we need a FlinkVersion enum that can represent the different versions. Moreover, we should add it to the release guide that this enum needs to be extended for every Flink version.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.typeutils.LinkedListSerializerUpgradeTest.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.migration.StatefulJobWBroadcastStateMigrationITCase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.migration.StatefulJobSavepointMigrationITCase.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainUnionTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainOrderTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainLengthStatelessDecreaseTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainLengthIncreaseTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainLengthDecreaseTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainBreakTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.AbstractNonKeyedOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.keyed.KeyedComplexChainTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.keyed.AbstractKeyedOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.migration.TypeSerializerSnapshotMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.StatefulJobWBroadcastStateMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.StatefulJobSavepointMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.LegacyStatefulJobSavepointMigrationITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcXaSinkMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerMigrationOperatorTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaMigrationTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaSerializerUpgradeTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.test.java.org.apache.flink.api.java.typeutils.runtime.WritableSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.array.PrimitiveArraySerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.array.PrimitiveArraySerializerUpgradeTestSpecifications.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.BasicTypeSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.BasicTypeSerializerUpgradeTestSpecifications.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.EnumSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.ListSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.MapSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.CompositeTypeSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.TypeSerializerUpgradeTestBase.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.CopyableSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.NullableSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.PojoSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.PojoSerializerUpgradeTestSpecifications.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.RowSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.ValueSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.testutils.migration.MigrationVersion.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.AvroSerializerUpgradeTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingMigrationTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.NFASerializerUpgradeTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.sharedbuffer.LockableTypeSerializerUpgradeTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPMigrationTest.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.test.java.org.apache.flink.graph.drivers.transform.LongValueWithProperHashCodeSerializerUpgradeTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.types.valuearray.ValueArraySerializerUpgradeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ArrayListSerializerUpgradeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.JavaSerializerUpgradeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlSerializerUpgradeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.VoidNamespaceSerializerUpgradeTest.java</file>
      <file type="M">flink-scala.src.test.java.org.apache.flink.api.scala.typeutils.OptionSerializerUpgradeTest.java</file>
      <file type="M">flink-scala.src.test.java.org.apache.flink.api.scala.typeutils.ScalaEitherSerializerUpgradeTest.java</file>
      <file type="M">flink-scala.src.test.java.org.apache.flink.api.scala.typeutils.ScalaTrySerializerUpgradeTest.java</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.EnumValueSerializerUpgradeTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.ScalaCaseClassSerializerUpgradeTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.TraversableSerializerUpgradeTest.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.datastream.UnionSerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkStateSerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.co.BufferEntrySerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.TimerSerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorMigrationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowSerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializerUpgradeTest.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-12-18 01:00:00" id="25372" opendate="2021-12-18 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add thread dump feature for jobmanager</summary>
      <description>Add thread dump feature for jobmanager in addition to the previous work on TM side: FLINK-14816. It is useful for debugging job submission and scheduling issues especially in OLAP scenarios.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager-routing.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-manager.ts</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingDispatcherGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.RestfulGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.ThreadDumpInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.ThreadDumpInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.TestingResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGatewayDecoratorBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.ThreadDumpInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerThreadDumpHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  
  <bug fixdate="2021-1-27 01:00:00" id="25460" opendate="2021-12-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update slf4j-api dependency to 1.7.32</summary>
      <description>Flink is using slf4j-api version 1.7.15 (from February 2016), while version 1.7.32 (Jul 2021) has been released. We should upgrade to the latest dependency.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-27 01:00:00" id="25461" opendate="2021-12-27 00:00:00" resolution="Done">
    <buginformation>
      <summary>Update net.sf.py4j:py4j dependency to 0.10.8.1</summary>
      <description>Flink uses net.sf.py4j:py4j version 0.10.8.1, while version 0.10.9.3 with multiple improvements has been released. We should upgrade this dependency.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.README.md</file>
      <file type="M">flink-python.lib.py4j-0.10.8.1-src.zip</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-29 01:00:00" id="25472" opendate="2021-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update to Log4j 2.17.1</summary>
      <description>We should update from Log4j 2.17.0 to 2.17.1 to address CVE-2021-44832: Apache Log4j2 vulnerable to RCE via JDBC Appender when attacker controls configuration.</description>
      <version>1.12.8,1.13.6,1.14.3,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-29 01:00:00" id="25474" opendate="2021-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Idea Scala plugin can not compile RexExplainUtil</summary>
      <description>Idea version: 2021.2.3Scala version: 2.11.12There are some errors in RexExplainUtil and many classes which use the methods in RexExplainUtil. NOTES: those class can be compiled and executed successfully</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-29 01:00:00" id="25477" opendate="2021-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The directory structure of the State Backends document is not standardized</summary>
      <description>The State Backends document uses multiple first-level headings. It may cause the directory structure displayed incorrectly.Just as the picture shows, the two titles are not in the table of contents on the right.</description>
      <version>None</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-2-30 01:00:00" id="25490" opendate="2021-12-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the Chinese document related to final checkpoint</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
      <file type="M">docs.content.zh.docs.internals.task.lifecycle.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-3 01:00:00" id="25504" opendate="2022-1-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update and synchronise used versions of Kafka Client and Confluent Platform</summary>
      <description>The Flink codebases uses Kafka Client and Confluent Platform in multiple places: AVRO Confluent Schema Registry Flink end-to-end tests (Bash e2e tests) Flink end-to-end tests common (Java e2e tests) SQL AVRO Confluent Schema Registry Flink Test Utils Flink TestsThe used versions are currently not in sync, which could result in unexpected results. For context, these are the currently used versions:Kafka Client: 2.2.0, 2.2.2, 2.4.1, 2.6.0Confluent Platform: 5.2.6, 5.5.2, 6.0.4, 6.2.1Given https://docs.confluent.io/platform/current/installation/versions-interoperability.html it probably makes sense to update to Kafka Client 2.8.1 and Confluent Platform 6.2.2.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.confluent.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-1-5 01:00:00" id="25525" opendate="2022-1-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-examples-table is not runnable in the IDE</summary>
      <description>The following exception is thrown:Exception in thread "main" org.apache.flink.table.api.ValidationException: Could not find any factories that implement 'org.apache.flink.table.delegation.ExecutorFactory' in the classpath.  at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:453)  at org.apache.flink.table.api.internal.TableEnvironmentImpl.create(TableEnvironmentImpl.java:295)  at org.apache.flink.table.api.internal.TableEnvironmentImpl.create(TableEnvironmentImpl.java:266)  at org.apache.flink.table.api.TableEnvironment.create(TableEnvironment.java:95)  at org.apache.flink.table.examples.scala.basics.GettingStartedExample$.main(GettingStartedExample.scala:55)  at org.apache.flink.table.examples.scala.basics.GettingStartedExample.main(GettingStartedExample.scala)</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-20 01:00:00" id="2553" opendate="2015-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Example Jars not build correctly</summary>
      <description>Due to new examples that introduced some package restructuring, example jar files are not build correctly. Furthermore, package/class structure got messed up to some extend. Additionally, documentation need to be extended to point out the correct example jar files.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.wordcount.WordCountTopology.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.wordcount.StormWordCountRemoteBySubmitter.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.wordcount.StormWordCountRemoteByClient.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.wordcount.SpoutSourceWordCount.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.StormWordCountInMemorySpout.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.StormWordCountFileSpout.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.StormInMemorySpout.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.StormFileSpout.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.FiniteStormInMemorySpout.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.FiniteStormFileSpout.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.singlejoin.stormoperators.GenderSpout.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.assembly.word-count-storm.xml</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.README.md</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-core.src.test.java.org.apache.flink.stormcompatibility.wrappers.StormOutputFieldsDeclarerTest.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-core.src.main.java.org.apache.flink.stormcompatibility.api.FlinkSubmitter.java</file>
      <file type="M">docs.apis.storm.compatibility.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-5 01:00:00" id="25532" opendate="2022-1-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide Flink SQL CLI as Docker image</summary>
      <description>Flink is currently available via as Docker images. However, the Flink SQL CLI isn't available as a Docker image. We should also provide this.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-6 01:00:00" id="25553" opendate="2022-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove MapR filesystem</summary>
      <description>Pending a positive outcome in the Dev mailing list https://lists.apache.org/thread/od2137fk5j1gq034sopj5n2th2w719w4 we can remove the MapR filesystem</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.ci.shade.sh</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.test.java.org.apache.flink.runtime.fs.maprfs.MapRNotInClassPathTest.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.test.java.org.apache.flink.runtime.fs.maprfs.MapRFsFactoryTest.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.test.java.com.mapr.fs.MapRFileSystem.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.main.resources.META-INF.services.org.apache.flink.core.fs.FileSystemFactory</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.main.java.org.apache.flink.runtime.fs.maprfs.MapRFsFactory.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileSystem.java</file>
      <file type="M">docs.content.docs.internals.filesystems.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.plugins.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.overview.md</file>
      <file type="M">docs.content.zh.docs.internals.filesystems.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.plugins.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.overview.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-1-7 01:00:00" id="25570" opendate="2022-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce Sink V2 extension interfaces</summary>
      <description>This task introduces the interfaces needed to implement the custom operations before/after the writer and committer.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.function.SerializableSupplier.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.completeness.TypeSerializerTestCoverageTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-7 01:00:00" id="25571" opendate="2022-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Elasticsearch Sink to use decomposed interfaces</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-quickstart-test.src.main.scala.org.apache.flink.quickstarts.test.Elasticsearch7SinkExample.scala</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkFactoryBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkBaseITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.TestEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.RowElasticsearchEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.RequestIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchEmitter.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-7 01:00:00" id="25572" opendate="2022-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update File Sink to use decomposed interfaces</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestFileFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaCommitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterBucketStateSerializerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.committer.FileCommitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.writer.FileWriterBucket.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.writer.FileWriter.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.FileSink.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.committer.FileCommitter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-7 01:00:00" id="25573" opendate="2022-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Kafka Sink to use decomposed interfaces</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.sink.SinkV2Provider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SinkV1Adapter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.sink.InitContextInitializationContextAdapter.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.ReducingUpsertWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaCommitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.ReducingUpsertWriter.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.ReducingUpsertSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaWriter.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaCommitter.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.7602816f-5c01-4b7a-9e3e-235dfedec245</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSinkITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSink.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-7 01:00:00" id="25574" opendate="2022-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Async Sink to use decomposed interfaces</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.sink.writer.TestSinkInitContext.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.sink.writer.AsyncSinkWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.sink.ArrayListAsyncSink.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.sink.writer.ElementConverter.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.sink.AsyncSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.table.KinesisFirehoseDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.table.KinesisFirehoseDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkElementConverter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSink.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.test.java.org.apache.flink.connector.kinesis.table.KinesisDynamicTableSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.table.KinesisDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSinkElementConverter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSink.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-1-7 01:00:00" id="25576" opendate="2022-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update com.h2database:h2 to 2.0.206</summary>
      <description>Flink uses com.h2database:h2 version 1.4.200, we should update this to 2.0.206</description>
      <version>1.13.5,1.14.2,1.15.0</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-1-11 01:00:00" id="25613" opendate="2022-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove excessive surefire-plugin versions</summary>
      <description>Various modules overwrite the default surefire version. We should remove that unless there is a good reason to do so.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-13 01:00:00" id="25638" opendate="2022-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase the default write buffer size of sort-shuffle to 16M</summary>
      <description>As discussed in https://lists.apache.org/thread/pt2b1f17x2l5rlvggwxs6m265lo4ly7p, this ticket aims to increase the default write buffer size of sort-shuffle to 16M.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.BlockingShuffleITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.network.section.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-13 01:00:00" id="25639" opendate="2022-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase the default read buffer size of sort-shuffle to 64M</summary>
      <description>As discussed in https://lists.apache.org/thread/pt2b1f17x2l5rlvggwxs6m265lo4ly7p, this ticket aims to increase the default read buffer size of sort-shuffle to 64M.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.memory.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.memory.section.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-13 01:00:00" id="25645" opendate="2022-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UnsupportedOperationException would thrown out when hash shuffle by a field with array type</summary>
      <description>Currently array type is not supported as hash shuffle key because CodeGen does not support it yet. An unsupportedOperationException would thrown out when hash shuffle by a field with array type,</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.HashFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.HashCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.HashCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-2-19 01:00:00" id="25701" opendate="2022-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add API annotation to some Kafka connector core classes and interface</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.split.KafkaPartitionSplitState.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.split.KafkaPartitionSplitSerializer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.split.KafkaPartitionSplit.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.fetcher.KafkaSourceFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.deserializer.KafkaRecordDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.metrics.KafkaSourceReaderMetrics.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.KafkaSourceOptions.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.KafkaSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.KafkaSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.enumerator.subscriber.KafkaSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumStateSerializer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumState.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchemaBuilder.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-1-20 01:00:00" id="25719" opendate="2022-1-20 00:00:00" resolution="Done">
    <buginformation>
      <summary>Support General Python UDF in Thread Mode</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PassThroughPythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCalc.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PythonTestUtils.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamTableAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamGroupWindowAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupTableAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">docs.layouts.shortcodes.generated.python.configuration.html</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.runtime.context.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.timerservice.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.window.window.operator.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.state.data.view.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.context.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.process.function.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.process.mode.boot.py</file>
      <file type="M">flink-python.pyflink.fn.execution.utils.operation.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.beam.ProcessPythonEnvironmentManager.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.ProcessPythonEnvironment.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.PythonDependencyInfo.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.PythonEnvironmentManager.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonConfig.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonEnvironmentManagerUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractDataStreamPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamDataStreamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractOneInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.env.beam.ProcessPythonEnvironmentManagerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonProcTimeBoundedRangeOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonProcTimeBoundedRowsOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonRowTimeBoundedRangeOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonRowTimeBoundedRowsOperatorTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-1-20 01:00:00" id="25739" opendate="2022-1-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include changelog jars into distribution</summary>
      <description>Add changelog jars to dist/opt folder: flink-dstl-dfs - so users can add it to plugins/ easily (plugin, cluster level) flink-statebackend-changelog - so that it can be added to lib/ if needed (not plugin, cluster or job-level)Update docs if done after FLINK-25024.cc: chesnay</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-1-22 01:00:00" id="25758" opendate="2022-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>GCS Filesystem implementation fails on Java 11 tests due to licensing issues</summary>
      <description>00:33:45,410 DEBUG org.apache.flink.tools.ci.licensecheck.NoticeFileChecker [] - Dependency io.netty:netty-common:4.1.51.Final is mentioned in NOTICE file /__w/2/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency00:33:45,411 ERROR org.apache.flink.tools.ci.licensecheck.NoticeFileChecker [] - Could not find dependency javax.annotation:javax.annotation-api:1.3.2 in NOTICE file /__w/2/s/flink-filesystems/flink-gs-fs-hadoop/src/main/resources/META-INF/NOTICE00:33:45,536 INFO org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - Checking directory /tmp/flink-validation-deployment with a total of 197 jar files.00:34:18,554 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - File '/javax/annotation/security/package.html' in jar '/tmp/flink-validation-deployment/org/apache/flink/flink-gs-fs-hadoop/1.15-SNAPSHOT/flink-gs-fs-hadoop-1.15-20220122.001944-1.jar' contains match with forbidden regex 'gnu ?\R?[\s/#]*general ?\R?[\s/#]*public ?\R?[\s/#]*license'.00:34:18,555 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - File '/javax/annotation/package.html' in jar '/tmp/flink-validation-deployment/org/apache/flink/flink-gs-fs-hadoop/1.15-SNAPSHOT/flink-gs-fs-hadoop-1.15-20220122.001944-1.jar' contains match with forbidden regex 'gnu ?\R?[\s/#]*general ?\R?[\s/#]*public ?\R?[\s/#]*license'.00:35:46,612 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Found a total of 3 severe license issueshttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29932&amp;view=logs&amp;j=946871de-358d-5815-3994-8175615bc253&amp;t=e0240c62-4570-5d1c-51af-dd63d2093da1</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-24 01:00:00" id="25773" opendate="2022-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to flink-shaded 15.0</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-2-24 01:00:00" id="25785" opendate="2022-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update com.h2database:h2 to 2.0.210</summary>
      <description>Two security vulnerabilities in H2 Console (CVE-2022-23221 and possible DNS rebinding attack) are fixed in 2.0.120. Flink is currently on 2.0.206 since https://issues.apache.org/jira/browse/FLINK-25576Note: Flink is using this dependency only for testing, so it's not directly impacted by the CVE. We just want to be good citizens and update our dependencies</description>
      <version>1.13.5,1.14.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0,1.13.7</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-1-24 01:00:00" id="25790" opendate="2022-1-24 00:00:00" resolution="Done">
    <buginformation>
      <summary>Support authentication via core-site.xml in GCS FileSystem plugin</summary>
      <description>Add support for authentication via core-site.xml to the new GCS FileSystem connector, recently added via FLINK-11838 Create RecoverableWriter for GCS - ASF JIRA (apache.org).Specifically, make the RecoverableWriter use explicit credentials supplied in core-site.xml in the "google.cloud.auth.service.account.json.keyfile" property. Otherwise, it should use implicit credentials, as it already does.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.src.test.java.org.apache.flink.fs.gs.TestUtils.java</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.src.main.java.org.apache.flink.fs.gs.GSFileSystemFactory.java</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.src.main.java.org.apache.flink.fs.gs.GSFileSystem.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-1-25 01:00:00" id="25808" opendate="2022-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JsonTypeInfo property should be valid java identifier</summary>
      <description>Some REST classes use the JsonTypeInfo with the property being named @class. This causes invalid java code to be generated by swagger.Rename it to clazz.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.SubtaskCheckpointStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointStatistics.java</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-2-26 01:00:00" id="25818" opendate="2022-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add explanation how Kafka Source deals with idleness when parallelism is higher then the number of partitions</summary>
      <description>Add a section to the Kafka Source documentation to explain what happens with the Kafka Source with regards to idleness when parallelism is higher then the number of partitions</description>
      <version>1.15.0</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-26 01:00:00" id="25826" opendate="2022-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle symbols at a central place with serializable format</summary>
      <description>Symbols are quite messy in the code base. We should unify all locations and define a serializable format for the JSON plan.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.JsonGenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.JsonValueCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.JsonObjectCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.JsonArrayCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.ExpressionConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.converters.JsonQueryConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.converters.JsonExistsConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.converters.JsonConverterUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-27 01:00:00" id="25836" opendate="2022-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>test_keyed_process_function_with_state of BatchModeDataStreamTests faild in PyFlink</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30264&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-2-27 01:00:00" id="25856" opendate="2022-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix use of UserDefinedType in from_elements</summary>
      <description>If we define a new UserDefinedType, and use it in `from_elements`, it will failed.class VectorUDT(UserDefinedType): @classmethod def sql_type(cls): return DataTypes.ROW( [ DataTypes.FIELD("type", DataTypes.TINYINT()), DataTypes.FIELD("size", DataTypes.INT()), DataTypes.FIELD("indices", DataTypes.ARRAY(DataTypes.INT())), DataTypes.FIELD("values", DataTypes.ARRAY(DataTypes.DOUBLE())), ] ) @classmethod def module(cls): return "pyflink.ml.core.linalg" def serialize(self, obj): if isinstance(obj, SparseVector): indices = [int(i) for i in obj._indices] values = [float(v) for v in obj._values] return 0, obj.size(), indices, values elif isinstance(obj, DenseVector): values = [float(v) for v in obj._values] return 1, None, None, values else: raise TypeError("Cannot serialize %r of type %r".format(obj, type(obj)))self.t_env.from_elements([ (Vectors.dense([1, 2, 3, 4]), 0., 1.), (Vectors.dense([2, 2, 3, 4]), 0., 2.), (Vectors.dense([3, 2, 3, 4]), 0., 3.), (Vectors.dense([4, 2, 3, 4]), 0., 4.), (Vectors.dense([5, 2, 3, 4]), 0., 5.), (Vectors.dense([11, 2, 3, 4]), 1., 1.), (Vectors.dense([12, 2, 3, 4]), 1., 2.), (Vectors.dense([13, 2, 3, 4]), 1., 3.), (Vectors.dense([14, 2, 3, 4]), 1., 4.), (Vectors.dense([15, 2, 3, 4]), 1., 5.), ], DataTypes.ROW([ DataTypes.FIELD("features", VectorUDT()), DataTypes.FIELD("label", DataTypes.DOUBLE()), DataTypes.FIELD("weight", DataTypes.DOUBLE())]))</description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.utils.python.PythonTableUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.api.common.python.PythonBridgeUtils.java</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-1-29 01:00:00" id="25874" opendate="2022-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink package dependencies conflict</summary>
      <description>I need to install PyFlink with some other libraries in a project, and I found there's dependency conflict with `great-expectations` because PyFlink has pined dependency `python-dateutil==2.8.0`.There are incompatible versions in the resolved dependencies:python-dateutil==2.8.0 (from apache-flink==1.14.3-&gt;-r requirements.in (line 4))python-dateutil&gt;=2.8.1 (from great-expectations==0.14.4-&gt;-r requirements.in (line 5)) I have to use newer version of great-expectations (&gt;=0.13.1) for some features, so this is blocking me.I found `python-dateutil` v2.8.0 is released in 2019-02-05, see https://github.com/dateutil/dateutil/releases/tag/2.8.0, is there any way to loose the dependency, e.g. &gt;=2.8.0 ?(BTW, `cloudpickle==1.2.2` is also old which released in 2019-09-10, see https://github.com/cloudpipe/cloudpickle/releases/tag/v1.2.2 )</description>
      <version>None</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.README.md</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-31 01:00:00" id="25885" opendate="2022-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClusterEntrypointTest.testWorkingDirectoryIsDeletedIfApplicationCompletes failed on azure</summary>
      <description>2022-01-31T05:00:07.3113870Z Jan 31 05:00:07 java.util.concurrent.CompletionException: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Discard message, because the rpc endpoint akka.tcp://flink@127.0.0.1:6123/user/rpc/resourcemanager_2 has not been started yet.2022-01-31T05:00:07.3115008Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)2022-01-31T05:00:07.3115778Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)2022-01-31T05:00:07.3116527Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)2022-01-31T05:00:07.3117267Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)2022-01-31T05:00:07.3118011Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2022-01-31T05:00:07.3118770Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)2022-01-31T05:00:07.3119608Z Jan 31 05:00:07 at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:251)2022-01-31T05:00:07.3120425Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2022-01-31T05:00:07.3121199Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2022-01-31T05:00:07.3121957Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2022-01-31T05:00:07.3122716Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)2022-01-31T05:00:07.3123457Z Jan 31 05:00:07 at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1387)2022-01-31T05:00:07.3124241Z Jan 31 05:00:07 at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)2022-01-31T05:00:07.3125106Z Jan 31 05:00:07 at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)2022-01-31T05:00:07.3126063Z Jan 31 05:00:07 at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)2022-01-31T05:00:07.3127207Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2022-01-31T05:00:07.3127982Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2022-01-31T05:00:07.3128741Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2022-01-31T05:00:07.3129497Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)2022-01-31T05:00:07.3130385Z Jan 31 05:00:07 at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:45)2022-01-31T05:00:07.3131092Z Jan 31 05:00:07 at akka.dispatch.OnComplete.internal(Future.scala:299)2022-01-31T05:00:07.3131695Z Jan 31 05:00:07 at akka.dispatch.OnComplete.internal(Future.scala:297)2022-01-31T05:00:07.3132310Z Jan 31 05:00:07 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)2022-01-31T05:00:07.3132943Z Jan 31 05:00:07 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)2022-01-31T05:00:07.3133577Z Jan 31 05:00:07 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)2022-01-31T05:00:07.3134340Z Jan 31 05:00:07 at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)2022-01-31T05:00:07.3135149Z Jan 31 05:00:07 at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)2022-01-31T05:00:07.3135898Z Jan 31 05:00:07 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)2022-01-31T05:00:07.3136692Z Jan 31 05:00:07 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)2022-01-31T05:00:07.3137454Z Jan 31 05:00:07 at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)2022-01-31T05:00:07.3138127Z Jan 31 05:00:07 at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)2022-01-31T05:00:07.3138726Z Jan 31 05:00:07 at akka.actor.ActorRef.tell(ActorRef.scala:131)2022-01-31T05:00:07.3139391Z Jan 31 05:00:07 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.sendErrorIfSender(AkkaRpcActor.java:501)2022-01-31T05:00:07.3140173Z Jan 31 05:00:07 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:173)2022-01-31T05:00:07.3140882Z Jan 31 05:00:07 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)2022-01-31T05:00:07.3141535Z Jan 31 05:00:07 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)2022-01-31T05:00:07.3142177Z Jan 31 05:00:07 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)2022-01-31T05:00:07.3142822Z Jan 31 05:00:07 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)2022-01-31T05:00:07.3143467Z Jan 31 05:00:07 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)2022-01-31T05:00:07.3144145Z Jan 31 05:00:07 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2022-01-31T05:00:07.3145019Z Jan 31 05:00:07 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)2022-01-31T05:00:07.3145744Z Jan 31 05:00:07 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)2022-01-31T05:00:07.3146421Z Jan 31 05:00:07 at akka.actor.Actor.aroundReceive(Actor.scala:537)2022-01-31T05:00:07.3147053Z Jan 31 05:00:07 at akka.actor.Actor.aroundReceive$(Actor.scala:535)2022-01-31T05:00:07.3147714Z Jan 31 05:00:07 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)2022-01-31T05:00:07.3148417Z Jan 31 05:00:07 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)2022-01-31T05:00:07.3149072Z Jan 31 05:00:07 at akka.actor.ActorCell.invoke(ActorCell.scala:548)2022-01-31T05:00:07.3149725Z Jan 31 05:00:07 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)2022-01-31T05:00:07.3231566Z Jan 31 05:00:07 at akka.dispatch.Mailbox.run(Mailbox.scala:231)2022-01-31T05:00:07.3232417Z Jan 31 05:00:07 at akka.dispatch.Mailbox.exec(Mailbox.scala:243)2022-01-31T05:00:07.3233367Z Jan 31 05:00:07 at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)2022-01-31T05:00:07.3234208Z Jan 31 05:00:07 at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)2022-01-31T05:00:07.3234909Z Jan 31 05:00:07 at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)2022-01-31T05:00:07.3235609Z Jan 31 05:00:07 at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)2022-01-31T05:00:07.3237023Z Jan 31 05:00:07 Caused by: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Discard message, because the rpc endpoint akka.tcp://flink@127.0.0.1:6123/user/rpc/resourcemanager_2 has not been started yet.2022-01-31T05:00:07.3238316Z Jan 31 05:00:07 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:175)2022-01-31T05:00:07.3238886Z Jan 31 05:00:07 ... 20 more2022-01-31T05:00:07.3239220Z Jan 31 05:00:07 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30491&amp;view=logs&amp;j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&amp;t=7b25afdf-cc6c-566f-5459-359dc2585798&amp;l=12987</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-31 01:00:00" id="25892" opendate="2022-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Develop ArchUnit test for connectors</summary>
      <description>ArchUnit test should be developed for connector submodules after the ArchUnit infra for test code has been built.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.pom.xml</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-31 01:00:00" id="25894" opendate="2022-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explicitly configure japicmp oldVersion for flink-streaming-java</summary>
      <description>Since flink-streaming-java lost its scala suffix japicmp can't find the artifact to compare it against, as the 1.14 artifact still has the suffix.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-31 01:00:00" id="25897" opendate="2022-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update project configuration gradle doc to 7.x version</summary>
      <description>Update the gradle build script and its doc page to 7.x</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.configuration.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-1 01:00:00" id="25907" opendate="2022-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pluggable delegation token manager</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerDisconnectOnShutdownITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.MockResourceManagerRuntimeServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManagerService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.TestingMiniCluster.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.ExecutionGraphInfoStoreTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.modules.HadoopModuleFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerProcessContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-4-2 01:00:00" id="25926" opendate="2022-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update org.postgresql:postgresql to 42.3.3</summary>
      <description>Security vulnerability CVE-2022-21724 is fixed in 42.2.25. Flink is currently on 42.2.10.Note: Flink uses this dependency in a Provided scope only.</description>
      <version>1.13.5,1.14.3,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.converter.PostgresRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.dialect.psql.PostgresTypeMapper.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-2-3 01:00:00" id="25947" opendate="2022-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>License checker doesn&amp;#39;t cover flink-table-planner</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-2-7 01:00:00" id="25976" opendate="2022-2-7 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Update the KDS and KDF Sink&amp;#39;s defaults &amp; update the docs</summary>
      <description>Update:DEFAULT_MAX_IN_FLIGHT_REQUESTS=50to match with the default threads in the AWS SDK v2 HTTP Client default.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSinkBuilder.java</file>
      <file type="M">docs.content.docs.connectors.datastream.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kinesis.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-2-7 01:00:00" id="25984" opendate="2022-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated API usages in ConfigOptions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowEmitStrategy.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.IncrementalAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalSortRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalSortMergeJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalJoinRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinDeriveNullFilterRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.RelNodeBlock.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBNativeMetricOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.ShuffleServiceOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.FileJobGraphRetriever.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.main.java.org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterOptions.java</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.java.org.apache.flink.metrics.influxdb.InfluxdbReporterOptions.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-stateful-job-upgrade-test.src.main.java.org.apache.flink.streaming.tests.StatefulStreamJobUpgradeTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.TtlTestConfig.java</file>
      <file type="M">flink-end-to-end-tests.flink-netty-shuffle-memory-control-test.src.main.java.org.apache.flink.streaming.tests.NettyShuffleMemoryControlTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-heavy-deployment-stress-test.src.main.java.org.apache.flink.deployment.HeavyDeploymentStressTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigurationTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigOptionTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.WebOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestartStrategyOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.QueryableStateOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.OptimizerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JMXServerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HistoryServerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HeartbeatManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CheckpointingOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.BlobServerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.AlgorithmOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.AkkaOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOptions.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-2-8 01:00:00" id="25999" opendate="2022-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate Per-Job Mode</summary>
      <description>As discussed in &amp;#91;1&amp;#93; and voted on in &amp;#91;2&amp;#93;, the community as decided to deprecate per-job mode.&amp;#91;1&amp;#93; https://lists.apache.org/thread/b8g76cqgtr2c515rd1bs41vy285f317n&amp;#91;2&amp;#93; https://lists.apache.org/thread/v6oz92dfp95qcox45l0f8393089oyjv4</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.executors.YarnJobClusterExecutorFactory.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.executors.YarnJobClusterExecutor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnDeploymentTarget.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.JobClusterEntrypoint.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.DeploymentOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.executors.AbstractJobClusterExecutor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.ClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.GenericCLI.java</file>
      <file type="M">docs.layouts.shortcodes.generated.deployment.configuration.html</file>
      <file type="M">docs.content.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.docs.deployment.cli.md</file>
      <file type="M">docs.content.docs.concepts.glossary.md</file>
      <file type="M">docs.content.docs.concepts.flink-architecture.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-1 01:00:00" id="2600" opendate="2015-9-1 00:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>Failing ElasticsearchSinkITCase.testNodeClient test case</summary>
      <description>I observed that the ElasticsearchSinkITCase.testNodeClient test case fails on Travis. The stack trace isorg.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1.applyOrElse(JobManager.scala:414) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.testingUtils.TestingJobManager$$anonfun$handleTestingMessage$1.applyOrElse(TestingJobManager.scala:285) at scala.PartialFunction$OrElse.apply(PartialFunction.scala:162) at org.apache.flink.runtime.LeaderSessionMessageFilter$$anonfun$receive$1.applyOrElse(LeaderSessionMessageFilter.scala:36) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:33) at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:28) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.LogMessages$$anon$1.applyOrElse(LogMessages.scala:28) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:104) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: An error occured in ElasticsearchSink. at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink.close(ElasticsearchSink.java:307) at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:40) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:75) at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:243) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:185) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:581) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: IndexMissingException[[my-index] missing] at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink$1.afterBulk(ElasticsearchSink.java:240) at org.elasticsearch.action.bulk.BulkProcessor.execute(BulkProcessor.java:316) at org.elasticsearch.action.bulk.BulkProcessor.executeIfNeeded(BulkProcessor.java:299) at org.elasticsearch.action.bulk.BulkProcessor.internalAdd(BulkProcessor.java:281) at org.elasticsearch.action.bulk.BulkProcessor.add(BulkProcessor.java:264) at org.elasticsearch.action.bulk.BulkProcessor.add(BulkProcessor.java:260) at org.elasticsearch.action.bulk.BulkProcessor.add(BulkProcessor.java:246) at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink.invoke(ElasticsearchSink.java:286) at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:37) at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:163) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:56) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:172) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:581) at java.lang.Thread.run(Thread.java:745)Resources:&amp;#91;1&amp;#93; https://s3.amazonaws.com/archive.travis-ci.org/jobs/78055773/log.txt</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-elasticsearch.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-2-8 01:00:00" id="26004" opendate="2022-2-8 00:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce ForwardForConsecutiveHashPartitioner</summary>
      <description>If there are multiple consecutive and the same hash shuffles, SQL planner will change them except the first one to use forward partitioner, so that these operators can be chained to reduce unnecessary shuffles.However, sometimes the consecutive hash operators are not chained (e.g. multiple inputs), and this kind of forward partitioners will turn into forward job edges. These forward edges still have the consecutive hash assumption, so that they cannot be changed into rescale/rebalance edges, otherwise it can lead to incorrect results. This prevents the adaptive batch scheduler from determining parallelism for other forward edge downstream job vertices (see FLINK-25046).To solve it, I propose to introduce a new ForwardForConsecutiveHashPartitioner. When SQL planner optimizes the case of multiple consecutive the same groupBy, it should use the proposed partitioner, so that the runtime framework can further decide whether the partitioner can be changed to hash or not.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-8 01:00:00" id="26014" opendate="2022-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to use the working directory for faster local recoveries</summary>
      <description>After having implemented FLIP-198 and FLIP-201, users can now use faster TaskManager failover when using local recovery with persisted volumes. I suggest to add documentation for explaining how to configure Flink to make use of it.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2022-2-9 01:00:00" id="26035" opendate="2022-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework loader-bundle into separate module</summary>
      <description>The flink-table-planner currently creates 2 artifacts. 1 jar containing the planner and various dependencies for the cases where the planner is used directly, and another jar that additionally bundles scala for cases where the loader is used.The latter artifact is purely an intermediate build artifact, and as such we usually wouldn't want to publish it. This is particularly important because this jar doesn't have a correct NOTICE, and having different NOTICE files for different artifacts is surprisingly tricky.We should just rework this into a separate module.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.README.md</file>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-loader.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-9 01:00:00" id="26036" opendate="2022-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory timeout on azure</summary>
      <description>022-02-09T02:18:17.1827314Z Feb 09 02:18:14 [ERROR] org.apache.flink.test.recovery.LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory Time elapsed: 62.252 s &lt;&lt;&lt; ERROR!2022-02-09T02:18:17.1827940Z Feb 09 02:18:14 java.util.concurrent.TimeoutException2022-02-09T02:18:17.1828450Z Feb 09 02:18:14 at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784)2022-02-09T02:18:17.1829040Z Feb 09 02:18:14 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)2022-02-09T02:18:17.1829752Z Feb 09 02:18:14 at org.apache.flink.test.recovery.LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory(LocalRecoveryITCase.java:115)2022-02-09T02:18:17.1830407Z Feb 09 02:18:14 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2022-02-09T02:18:17.1830954Z Feb 09 02:18:14 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2022-02-09T02:18:17.1831582Z Feb 09 02:18:14 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2022-02-09T02:18:17.1832135Z Feb 09 02:18:14 at java.lang.reflect.Method.invoke(Method.java:498)2022-02-09T02:18:17.1832697Z Feb 09 02:18:14 at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)2022-02-09T02:18:17.1833566Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)2022-02-09T02:18:17.1834394Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)2022-02-09T02:18:17.1835125Z Feb 09 02:18:14 at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)2022-02-09T02:18:17.1835875Z Feb 09 02:18:14 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)2022-02-09T02:18:17.1836565Z Feb 09 02:18:14 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)2022-02-09T02:18:17.1837294Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)2022-02-09T02:18:17.1838007Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)2022-02-09T02:18:17.1838743Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)2022-02-09T02:18:17.1839499Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)2022-02-09T02:18:17.1840224Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)2022-02-09T02:18:17.1840952Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)2022-02-09T02:18:17.1841616Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)2022-02-09T02:18:17.1842257Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)2022-02-09T02:18:17.1842951Z Feb 09 02:18:14 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)2022-02-09T02:18:17.1843681Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-02-09T02:18:17.1844782Z Feb 09 02:18:14 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)2022-02-09T02:18:17.1845603Z Feb 09 02:18:14 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)2022-02-09T02:18:17.1846375Z Feb 09 02:18:14 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)2022-02-09T02:18:17.1847084Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)2022-02-09T02:18:17.1847785Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-02-09T02:18:17.1848490Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)2022-02-09T02:18:17.1849138Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2022-02-09T02:18:17.1849797Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)2022-02-09T02:18:17.1850500Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-02-09T02:18:17.1851169Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)2022-02-09T02:18:17.1851834Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)2022-02-09T02:18:17.1852396Z Feb 09 02:18:14 at java.util.ArrayList.forEach(ArrayList.java:1259)2022-02-09T02:18:17.1853086Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)2022-02-09T02:18:17.1853876Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)2022-02-09T02:18:17.1854746Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-02-09T02:18:17.1855633Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)2022-02-09T02:18:17.1856371Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2022-02-09T02:18:17.1857033Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)2022-02-09T02:18:17.1857722Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-02-09T02:18:17.1858400Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)2022-02-09T02:18:17.1859068Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)2022-02-09T02:18:17.1859632Z Feb 09 02:18:14 at java.util.ArrayList.forEach(ArrayList.java:1259)2022-02-09T02:18:17.1860318Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)2022-02-09T02:18:17.1861122Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)2022-02-09T02:18:17.1861818Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-02-09T02:18:17.1862519Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)2022-02-09T02:18:17.1863169Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2022-02-09T02:18:17.1863920Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)2022-02-09T02:18:17.1864685Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-02-09T02:18:17.1865773Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)2022-02-09T02:18:17.1866640Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)2022-02-09T02:18:17.1867395Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)2022-02-09T02:18:17.1868198Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)2022-02-09T02:18:17.1868928Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)2022-02-09T02:18:17.1869645Z Feb 09 02:18:14 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)2022-02-09T02:18:17.1870359Z Feb 09 02:18:14 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)2022-02-09T02:18:17.1871067Z Feb 09 02:18:14 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)2022-02-09T02:18:17.1871823Z Feb 09 02:18:14 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)2022-02-09T02:18:17.1872551Z Feb 09 02:18:14 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)2022-02-09T02:18:17.1873208Z Feb 09 02:18:14 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)2022-02-09T02:18:17.1873826Z Feb 09 02:18:14 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)2022-02-09T02:18:17.1874572Z Feb 09 02:18:14 at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)2022-02-09T02:18:17.1875289Z Feb 09 02:18:14 at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)2022-02-09T02:18:17.1876343Z Feb 09 02:18:14 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)2022-02-09T02:18:17.1877233Z Feb 09 02:18:14 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)2022-02-09T02:18:17.1877928Z Feb 09 02:18:14 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)2022-02-09T02:18:17.1878584Z Feb 09 02:18:14 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)2022-02-09T02:18:17.1879206Z Feb 09 02:18:14 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)2022-02-09T02:18:17.1879793Z Feb 09 02:18:14 at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)2022-02-09T02:18:17.1880381Z Feb 09 02:18:14 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30956&amp;view=logs&amp;j=5c8e7682-d68f-54d1-16a2-a09310218a49&amp;t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&amp;l=23106</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2022-2-10 01:00:00" id="26072" opendate="2022-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mark NiFi connector as deprecated</summary>
      <description>The Flink community has voted to deprecate the NiFi connector in Flink 1.15 and remove it in the version after that one. Details can be found in https://lists.apache.org/thread/gldw588pbdf8hww9jtgfdv5y60v5mt6w</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-nifi.src.main.java.org.apache.flink.streaming.connectors.nifi.NiFiSource.java</file>
      <file type="M">flink-connectors.flink-connector-nifi.src.main.java.org.apache.flink.streaming.connectors.nifi.NiFiSink.java</file>
      <file type="M">docs.content.docs.connectors.datastream.nifi.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.nifi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-10 01:00:00" id="26073" opendate="2022-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Twitter connector</summary>
      <description>The Flink community has voted and agreed to remove the Twitter connector from the Flink repository. Details can be found in https://lists.apache.org/thread/b9mdwqwdyfyq38j6z86rn0d8b26k96c2</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.ci.java-ci-tools.src.main.resources.modules-skipping-deployment.modulelist</file>
      <file type="M">flink-examples.flink-examples-streaming.src.test.scala.org.apache.flink.streaming.scala.examples.StreamingExamplesITCase.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.test.java.org.apache.flink.streaming.test.StreamingExamplesITCase.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.twitter.TwitterExample.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.twitter.util.TwitterExampleData.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.twitter.TwitterExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-twitter.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-twitter.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-twitter.src.main.java.org.apache.flink.streaming.connectors.twitter.TwitterSource.java</file>
      <file type="M">flink-connectors.flink-connector-twitter.pom.xml</file>
      <file type="M">docs.content.docs.connectors.datastream.twitter.md</file>
      <file type="M">docs.content.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.docs.connectors.datastream.guarantees.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.twitter.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.guarantees.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-5-10 01:00:00" id="26074" opendate="2022-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve FlameGraphs scalability for high parallelism jobs</summary>
      <description>The FlameGraph feature added in FLINK-13550 issues 1 RPC call per subtask. This may cause performance problems for jobs with high paralleism and a lot of subtask running on the same TaskManager. It should be possible to improve this by grouping thread sampling requests usingThreadMXBean.getThreadInfo(long[] ids, int maxDepth)instead of the currently used individual callsThreadMXBean.getThreadInfo(long id, int maxDepth)</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.ThreadInfoSampleServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexFlameGraphFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.stats.TaskStatsRequestCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.JvmUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.ThreadInfoSampleService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorThreadInfoGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGatewayDecoratorBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.ThreadInfoSample.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.TaskThreadInfoResponse.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-10 01:00:00" id="26076" opendate="2022-2-10 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Fix ArchUnit violations in Source(Sink)MetricsITCase</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SinkMetricsITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.InMemoryReporter.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.SourceMetricsITCase.java</file>
      <file type="M">flink-connectors.flink-connector-base.archunit-violations.8bad5118-af5d-4976-ac57-382ed16f7f7e</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2022-2-15 01:00:00" id="26145" opendate="2022-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>k8s docs jobmanager-pod-template artifacts-fetcher:latest image is not exist, we can use busybox to replace it</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-15 01:00:00" id="26148" opendate="2022-2-15 00:00:00" resolution="Done">
    <buginformation>
      <summary>Change the format of adaptive batch scheduler config option to "jobmanager.adaptive-batch-scheduler.XXX"</summary>
      <description>Change the format of adaptive batch scheduler config option to jobmanager.adaptive-batch-scheduler.XXX to align the format with the existing scheduler option (For example, jobmanager.adaptive-scheduler.min-parallelism-increase).</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-3-15 01:00:00" id="26164" opendate="2022-2-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document watermark alignment</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.event-time.generating.watermarks.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.event-time.generating.watermarks.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-16 01:00:00" id="26180" opendate="2022-2-16 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Update docs to introduce the compaction for FileSink</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.filesystem.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.filesystem.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-2-16 01:00:00" id="26186" opendate="2022-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update the API_ANNOTATIONS rule</summary>
      <description>Issue 1:It seems that the excluding shaded classes filter does not work in this case since the check is against a valid class whose return type is a shaded external class which somehow was not handled by the filter. For example, please refer to FLINK-26174.Since there is no way to add Flink API annotation to the 3rd party classes, the rule should be updated.the main ideas is to extend the rule allow external shaded classes without Flink API annotation.Issue 2:It should be fine to let caller method annotated with @VisibleForTesting call target method annotated @VisibleForTesting.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.src.test.java.org.apache.flink.architecture.rules.ApiAnnotationRules.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.e5126cae-f3fe-48aa-b6fb-60ae6cc3fcd5</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.7602816f-5c01-4b7a-9e3e-235dfedec245</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.18509c9e-3250-4c52-91b9-11ccefc85db1</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-2-16 01:00:00" id="26192" opendate="2022-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PulsarOrderedSourceReaderTest fails with exit code 255</summary>
      <description>https://dev.azure.com/wysakowiczdawid/Flink/_build/results?buildId=1367&amp;view=logs&amp;j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&amp;t=2d3cd81e-1c37-5c31-0ee4-f5d5cdb9324d&amp;l=26787Feb 16 13:49:46 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-connector-pulsar: There are test failures.Feb 16 13:49:46 [ERROR] Feb 16 13:49:46 [ERROR] Please refer to /__w/1/s/flink-connectors/flink-connector-pulsar/target/surefire-reports for the individual test results.Feb 16 13:49:46 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.Feb 16 13:49:46 [ERROR] The forked VM terminated without properly saying goodbye. VM crash or System.exit called?Feb 16 13:49:46 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-connectors/flink-connector-pulsar &amp;&amp; /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:-UseGCOverheadLimit -Duser.country=US -Duser.language=en -jar /__w/1/s/flink-connectors/flink-connector-pulsar/target/surefire/surefirebooter3139517882560779643.jar /__w/1/s/flink-connectors/flink-connector-pulsar/target/surefire 2022-02-16T13-48-34_435-jvmRun1 surefire3358354372075396323tmp surefire_08509996975514960300tmpFeb 16 13:49:46 [ERROR] Error occurred in starting fork, check output in logFeb 16 13:49:46 [ERROR] Process Exit Code: 255Feb 16 13:49:46 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?Feb 16 13:49:46 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-connectors/flink-connector-pulsar &amp;&amp; /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:-UseGCOverheadLimit -Duser.country=US -Duser.language=en -jar /__w/1/s/flink-connectors/flink-connector-pulsar/target/surefire/surefirebooter3139517882560779643.jar /__w/1/s/flink-connectors/flink-connector-pulsar/target/surefire 2022-02-16T13-48-34_435-jvmRun1 surefire3358354372075396323tmp surefire_08509996975514960300tmpFeb 16 13:49:46 [ERROR] Error occurred in starting fork, check output in logFeb 16 13:49:46 [ERROR] Process Exit Code: 255Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:305)Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:265)Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:932)Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)Feb 16 13:49:46 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)Feb 16 13:49:46 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)Feb 16 13:49:46 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)Feb 16 13:49:46 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)Feb 16 13:49:46 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)Feb 16 13:49:46 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Feb 16 13:49:46 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Feb 16 13:49:46 [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Feb 16 13:49:46 [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)Feb 16 13:49:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)Feb 16 13:49:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)Feb 16 13:49:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)Feb 16 13:49:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)Feb 16 13:49:46 [ERROR] -&gt; [Help 1]Feb 16 13:49:46 [ERROR] Feb 16 13:49:46 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.Feb 16 13:49:46 [ERROR] Re-run Maven using the -X switch to enable full debug logging.Feb 16 13:49:46 [ERROR] Feb 16 13:49:46 [ERROR] For more information about the errors and possible solutions, please read the following articles:Feb 16 13:49:46 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionExceptionFeb 16 13:49:46 [ERROR] Feb 16 13:49:46 [ERROR] After correcting the problems, you can resume the build with the commandFeb 16 13:49:46 [ERROR] mvn &lt;goals&gt; -rf :flink-connector-pulsar</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.embedded.PulsarEmbeddedRuntime.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-16 01:00:00" id="26195" opendate="2022-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka connector tests are mixing JUnit4 and JUnit5</summary>
      <description>In the tests for the Kafka connector there are multiple occurrences of mixing JUnit 4 and JUnit 5. This prevents proper logging from e.g. TestLoggerExtension. There are also tests that run on JUnit 4 but use Assertions or Annotations from JUnit 5</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.FlinkKafkaInternalProducerITCase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-2-17 01:00:00" id="26218" opendate="2022-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable JUnit 5 Automatic Extension Detection</summary>
      <description>This allows us to declare the TestLoggerExtension for all tests in a module via the service entry.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-3-17 01:00:00" id="26223" opendate="2022-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Making ZK-related logs available in tests</summary>
      <description>Recently, we had a few incidents where it appears that ZooKeeper wasn't behaving as expected. It might help to have to the ZooKeeper logs available in these cases.We have multiple options: Introduce an extension to change the ZK log level for specific tests Lower the ZK log level again and make the logs being written to the standard log files Lower the ZK log level again and move the ZK logs into a dedicated file to avoid spoiling the Flink logs</description>
      <version>1.13.6,1.14.3,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.log4j.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  
  
  <bug fixdate="2022-3-21 01:00:00" id="26280" opendate="2022-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a flag to disable uid generation</summary>
      <description>We should add a flag to disable uid generation for back-compat. See the discussion here: https://issues.apache.org/jira/browse/FLINK-25932</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.TransformationsTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIntervalJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-2-21 01:00:00" id="26283" opendate="2022-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden AggregateCall serialization in JSON plan</summary>
      <description>Similar to FLINK-25385, we also need to revisit AggregateCall serialization. It should not contain Java serialization. It should support all kinds of agg functions. It should not support legacy stacks.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowJoinJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoinWithPk.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregateWithSumCountDistinctAndRetraction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ExpandJsonPlanTest.jsonplan.testExpand.out</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.AggregateCallJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.AggregateCallJsonDeserializer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-21 01:00:00" id="26289" opendate="2022-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adaptive Scheduler: Exception history can not be queried by the REST API</summary>
      <description>In FLINK-21439, we've started collecting a history of exceptions in the Adaptive Scheduler. We have a good coverage that this part works properly, but we've missed the part that exposes the history via REST API.The problematic part is that execution graph attached with the `ExecutionGraphInfo` does no longer contain a failure info.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfoWithHistory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-3-22 01:00:00" id="26296" opendate="2022-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing documentation</summary>
      <description>It appears that the documentation update under Deployment / HA / Overview didn't make it to master. We should mention the JobResultStore and the retryable cleanup here.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.docs.deployment.ha.overview.md</file>
      <file type="M">docs.content.docs.concepts.glossary.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.ha.overview.md</file>
      <file type="M">docs.content.zh.docs.concepts.glossary.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-22 01:00:00" id="26298" opendate="2022-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-rpc-core</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-core.src.test.java.org.apache.flink.runtime.concurrent.ScheduledFutureAdapterTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-22 01:00:00" id="26302" opendate="2022-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade rat-plugin to 1.13</summary>
      <description>Reduces verbosity of the plugin</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-22 01:00:00" id="26303" opendate="2022-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print rat-plugin violations to the console</summary>
      <description>Violations found by the rat plugin are currently only written to the rat.txt file. If we'd also print them to console then one wouldn't have to re-run things locally to figure out what actually failed.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-3-24 01:00:00" id="26347" opendate="2022-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should use Flink system Classloader (AppClassloader) when deserializing RPC message</summary>
      <description>FLINK-25742 removed the redundant serialization of RPC invocation at Flink side. However, by accident, it changes the class loading behavior. Before FLINK-25742, Flink system Classloader is used to load RPC message class, but after FLINK-25742, the RpcSystem Classloader (its parent Classloader is not Flink system Classloader) is used which can cause ClassNotFoundException. I encountered this exception when trying to run flink-remote-shuffle on the latest Flink 1.15-SNAPSHOT, the remote shuffle class (shuffle descriptor class) can not be found even when the corresponding jar file is in Flink lib/ directory.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-24 01:00:00" id="26353" opendate="2022-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"flink stop --help" does not list "--type" option</summary>
      <description>./bin/flink stop --helpAction "stop" stops a running program with a savepoint (streaming jobs only). Syntax: stop [OPTIONS] &lt;Job ID&gt; "stop" action options:  -d,--drain              Send MAX_WATERMARK before taking the                     savepoint and stopping the pipelne.  -p,--savepointPath &lt;savepointPath&gt;  Path to the savepoint (for example                     hdfs:///flink/savepoint-1537). If no                     directory is specified, the configured                     default will be used                     ("state.savepoints.dir"). Options for Generic CLI mode:  -D &lt;property=value&gt;  Allows specifying multiple generic configuration             options. The available options can be found at             https://nightlies.apache.org/flink/flink-docs-stable/             ops/config.html  -e,--executor &lt;arg&gt;  DEPRECATED: Please use the -t option instead which is             also available with the "Application Mode".             The name of the executor to be used for executing the             given job, which is equivalent to the             "execution.target" config option. The currently             available executors are: "remote", "local",             "kubernetes-session", "yarn-per-job" (deprecated),             "yarn-session".  -t,--target &lt;arg&gt;   The deployment target for the given application,             which is equivalent to the "execution.target" config             option. For the "run" action the currently available             targets are: "remote", "local", "kubernetes-session",             "yarn-per-job" (deprecated), "yarn-session". For the             "run-application" action the currently available             targets are: "kubernetes-application". Options for yarn-cluster mode:  -m,--jobmanager &lt;arg&gt;      Set to yarn-cluster to use YARN execution                   mode.  -yid,--yarnapplicationId &lt;arg&gt;  Attach to running YARN session  -z,--zookeeperNamespace &lt;arg&gt;  Namespace to create the Zookeeper                   sub-paths for high availability mode Options for default mode:  -D &lt;property=value&gt;       Allows specifying multiple generic                  configuration options. The available                  options can be found at                  https://nightlies.apache.org/flink/flink-do                  cs-stable/ops/config.html  -m,--jobmanager &lt;arg&gt;      Address of the JobManager to which to                  connect. Use this flag to connect to a                  different JobManager than the one specified                  in the configuration. Attention: This                  option is respected only if the                  high-availability configuration is NONE.  -z,--zookeeperNamespace &lt;arg&gt;  Namespace to create the Zookeeper sub-paths                  for high availability mode</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendStopWithSavepointTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendSavepointTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-24 01:00:00" id="26354" opendate="2022-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"-restoreMode" should be "--restoreMode" and should have a shorthand</summary>
      <description>-restoreMode &lt;arg&gt; Defines how should we restore from the given savepoint. Supported options: [claim - claim ownership of the savepoint and delete once it is subsumed, no_claim (default) - do not claim ownership, the first checkpoint will not reuse any files from the restored one, legacy - the old behaviour, do not assume ownership of the savepoint files, but can reuse some shared files.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-25 01:00:00" id="26374" opendate="2022-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JSON_OBJECT may throw NullPointerException on nullable column</summary>
      <description>From ML:Using the latest SNAPSHOT BUILD.If I have a column definition as.column(        "events",        DataTypes.ARRAY(          DataTypes.ROW(            DataTypes.FIELD("status", DataTypes.STRING().notNull()),            DataTypes.FIELD("timestamp", DataTypes.STRING().notNull()),            DataTypes.FIELD("increment_identifier", DataTypes.STRING().nullable()))))And a query asJSON_OBJECT('events' VALUE events) event_jsonWill generate JSON correctly ONLY if increment_identifier is NOT NULL but will throw a NullPointerException on the first record that has that column as null.Exception is not helpful.Exception in thread "main" org.apache.flink.runtime.client.JobExecutionException: Job execution failed.at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:259)at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)at akka.dispatch.OnComplete.internal(Future.scala:300)at akka.dispatch.OnComplete.internal(Future.scala:297)at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategyat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)at java.base/java.lang.reflect.Method.invoke(Method.java:566)at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)at akka.actor.Actor.aroundReceive(Actor.scala:537)at akka.actor.Actor.aroundReceive$(Actor.scala:535)at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)at akka.actor.ActorCell.invoke(ActorCell.scala:548)at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)at akka.dispatch.Mailbox.run(Mailbox.scala:231)at akka.dispatch.Mailbox.exec(Mailbox.scala:243)... 5 moreCaused by: java.lang.NullPointerExceptionat StreamExecCalc$422.convertRow$317$(Unknown Source)at StreamExecCalc$422.convertArray$316$(Unknown Source)at StreamExecCalc$422.processElement_split71(Unknown Source)at StreamExecCalc$422.processElement(Unknown Source)at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)at org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:64)at org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.outputCompletedElement(AsyncWaitOperator.java:302)at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.access$100(AsyncWaitOperator.java:79)at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.processResults(AsyncWaitOperator.java:381)at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.lambda$processInMailbox$0(AsyncWaitOperator.java:362)at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)at java.base/java.lang.Thread.run(Thread.java:829)</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.JsonFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.JsonGenerateUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-28 01:00:00" id="26388" opendate="2022-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Release Testing: Repeatable Cleanup (FLINK-25433)</summary>
      <description>Repeatable cleanup got introduced with FLIP-194 but should be considered as an independent feature of the JobResultStore (JRS) from a user's point of view.Repeatable cleanup can be triggered by running into an error while cleaning up. This can be achieved by disabling access to S3 after the job finished, e.g.: Setting a reasonable enough checkpointing time (checkpointing should be enabled to allow cleanup of s3) Disable s3 (removing permissions or shutting down the s3 server) Stop job with savepointStopping the job should work but the logs should show failure with repeating retries. Enabling S3 again should fix the issue.Keep in mind that if testing this in with HA, you should use a different bucket for the file-based JRS artifacts only change permissions for the bucket that holds JRS-unrelated artifacts. Flink would fail fatally if the JRS is not able to access it's backend storage.Documentation and configuration is still in the process of being updated in FLINK-26296 and FLINK-26331</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-3-28 01:00:00" id="26396" opendate="2022-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Changelog] Upload is not failed even if all attempts timeout</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.resources.log4j2.properties</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.RetryingExecutorTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.RetryingExecutor.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.BatchingStateChangeUploader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-1 01:00:00" id="26418" opendate="2022-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests on flink-table-planner produce tmp_XXX dirs which are not cleaned up</summary>
      <description>Running tests in flink-table-plannerproduces a bunch oftmp_XXXXXX directories in theflink-table-plannerroot module dir, (not inside the build dirs).As a result, if you don't change the globalgitignore show as new dirs/files to commit, and they are not cleaned up when one runsmvn clean. On top, if you try to build the whole flink project you get:[ERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.12:check (default) on project flink-parent: Too many files with unapproved license: 6 See RAT report in: /home/matriv/ververica/flink/target/rat.txt -&gt; [Help 1]and you need to manually remove those dirs.It would be great to keep these directories under thebuild and maybe also automatically remove each one, once the test producing it finishes successfully.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.TestingTaskManagerRuntimeInfo.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-9 01:00:00" id="2642" opendate="2015-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala Table API crashes when executing word count example</summary>
      <description>I tried to run the examples provided in the documentation of Flink's Table API. Unfortunately, the Scala word count example provided in the documentation doesn't work and does not give a meaningful exception.(Other examples work fine)Here my code:package org.apache.flink.examples.scalaimport org.apache.flink.api.scala._import org.apache.flink.api.scala.table._object WordCount { def main(args: Array[String]): Unit = { // set up execution environment val env = ExecutionEnvironment.getExecutionEnvironment case class WC(word: String, count: Int) val input = env.fromElements(WC("hello", 1), WC("hello", 1), WC("ciao", 1)) val expr = input.toTable val result = expr.groupBy('word).select('word, 'count.sum as 'count).toDataSet[WC] result.print() }}Here the thrown exception:Exception in thread "main" org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1.applyOrElse(JobManager.scala:414) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LeaderSessionMessageFilter$$anonfun$receive$1.applyOrElse(LeaderSessionMessageFilter.scala:36) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:33) at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:28) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.LogMessages$$anon$1.applyOrElse(LogMessages.scala:28) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:104) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.Exception: The user defined 'open(Configuration)' method in class org.apache.flink.api.table.runtime.ExpressionSelectFunction caused an exception: null at org.apache.flink.runtime.operators.RegularPactTask.openUserCode(RegularPactTask.java:1368) at org.apache.flink.runtime.operators.chaining.ChainedMapDriver.openTask(ChainedMapDriver.java:47) at org.apache.flink.runtime.operators.RegularPactTask.openChainedTasks(RegularPactTask.java:1408) at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:142) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:581) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.NullPointerException at org.apache.flink.api.table.codegen.IndentStringContext$$anonfun$j$2.apply(Indenter.scala:30) at org.apache.flink.api.table.codegen.IndentStringContext$$anonfun$j$2.apply(Indenter.scala:23) at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771) at org.apache.flink.api.table.codegen.IndentStringContext.j(Indenter.scala:23) at org.apache.flink.api.table.codegen.GenerateSelect.generateInternal(GenerateSelect.scala:55) at org.apache.flink.api.table.codegen.GenerateSelect.generateInternal(GenerateSelect.scala:32) at org.apache.flink.api.table.codegen.ExpressionCodeGenerator.generate(ExpressionCodeGenerator.scala:66) at org.apache.flink.api.table.runtime.ExpressionSelectFunction.open(ExpressionSelectFunction.scala:46) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33) at org.apache.flink.runtime.operators.RegularPactTask.openUserCode(RegularPactTask.java:1366) ... 5 more</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.plan.PlanTranslator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-1 01:00:00" id="26422" opendate="2022-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Chinese documentation with the new TablePipeline docs</summary>
      <description>Chinese docs needs to be updated with the content of this commit: https://github.com/apache/flink/commit/4f65c7950f2c3ef849f2094deab0e199ffedf57b</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.zh.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-4-3 01:00:00" id="26460" opendate="2022-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Unsupported type when convertTypeToSpec: MAP</summary>
      <description>CREATE TABLE zm_test ( `a` BIGINT, `m` MAP&lt;STRING,BIGINT&gt;);if we insert into zm_test useINSERT INTO zm_test(`a`) SELECT `a` FROM MyTable;then will throw ExceptionUnsupported type when convertTypeToSpec: MAPwe must useINSERT INTO zm_test SELECT `a`, cast(null AS MAP&lt;STRING,BIGINT&gt;) FROM MyTable;</description>
      <version>1.13.1,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-10-3 01:00:00" id="26469" opendate="2022-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adaptive job shows error in WebUI when not enough resource are available</summary>
      <description>When there is no resource and job is in CREATED state, the job page shows the error: "Job failed during initialization of JobManager".</description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0,1.17.0,1.15.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.html</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-3-4 01:00:00" id="26494" opendate="2022-3-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing logs during retry</summary>
      <description>The FutureRetry.retry functionality doesn't log the errors but just trigger a retry. This makes it harder for the user to figure out what's wrong.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.cleanup.DefaultResourceCleanerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.cleanup.DispatcherResourceCleanerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.cleanup.DefaultResourceCleaner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-3-6 01:00:00" id="26501" opendate="2022-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstarts Scala nightly end-to-end test failed on azure due to checkponts failed and logs contains exceptions</summary>
      <description>2022-03-05T02:35:36.4040037Z Mar 05 02:35:36 2022-03-05 02:35:34,334 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Triggering checkpoint 1 (type=CHECKPOINT) @ 1646447734295 for job b236087395260dc34648b84c2b86d6e8.2022-03-05T02:35:36.4041701Z Mar 05 02:35:36 2022-03-05 02:35:34,387 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Decline checkpoint 1 by task e8a324cae6bf452d32db6797bbbafad0 of job b236087395260dc34648b84c2b86d6e8 at 127.0.0.1:45911-0a50f5 @ localhost (dataPort=44047).2022-03-05T02:35:36.4043279Z Mar 05 02:35:36 org.apache.flink.util.SerializedThrowable: Task name with subtask : Source: Sequence Source (Deprecated) -&gt; Map -&gt; Sink: Unnamed (1/1)#0 Failure reason: Checkpoint was declined (task is closing)2022-03-05T02:35:36.4044531Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1389) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4045729Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1382) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4047172Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.triggerCheckpointBarrier(Task.java:1348) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4049092Z Mar 05 02:35:36 at org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint(TaskExecutor.java:956) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4050158Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_322]2022-03-05T02:35:36.4050929Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_322]2022-03-05T02:35:36.4051776Z Mar 05 02:35:36 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_322]2022-03-05T02:35:36.4052559Z Mar 05 02:35:36 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_322]2022-03-05T02:35:36.4053373Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]2022-03-05T02:35:36.4054849Z Mar 05 02:35:36 at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]2022-03-05T02:35:36.4055685Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]2022-03-05T02:35:36.4056461Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]2022-03-05T02:35:36.4057219Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]2022-03-05T02:35:36.4057899Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]2022-03-05T02:35:36.4059666Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4061005Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4062324Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4063941Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4065009Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4066205Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4067514Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4068255Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]2022-03-05T02:35:36.4069019Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]2022-03-05T02:35:36.4069638Z Mar 05 02:35:36 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]2022-03-05T02:35:36.4070271Z Mar 05 02:35:36 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]2022-03-05T02:35:36.4070862Z Mar 05 02:35:36 at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]2022-03-05T02:35:36.4071453Z Mar 05 02:35:36 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]2022-03-05T02:35:36.4072430Z Mar 05 02:35:36 at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]2022-03-05T02:35:36.4073023Z Mar 05 02:35:36 at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]2022-03-05T02:35:36.4073687Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_322]2022-03-05T02:35:36.4074596Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_322]2022-03-05T02:35:36.4075712Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_322]2022-03-05T02:35:36.4076437Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) ~[?:1.8.0_322]2022-03-05T02:35:36.4077754Z Mar 05 02:35:36 2022-03-05 02:35:34,410 WARN org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger checkpoint 1 for job b236087395260dc34648b84c2b86d6e8. (0 consecutive failed attempts so far)2022-03-05T02:35:36.4078865Z Mar 05 02:35:36 org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint was declined (task is closing)2022-03-05T02:35:36.4080161Z Mar 05 02:35:36 at org.apache.flink.runtime.messages.checkpoint.SerializedCheckpointException.unwrap(SerializedCheckpointException.java:51) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4081619Z Mar 05 02:35:36 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:988) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4083063Z Mar 05 02:35:36 at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4085407Z Mar 05 02:35:36 at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4086635Z Mar 05 02:35:36 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_322]2022-03-05T02:35:36.4087419Z Mar 05 02:35:36 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_322]2022-03-05T02:35:36.4088438Z Mar 05 02:35:36 at java.lang.Thread.run(Thread.java:750) [?:1.8.0_322]2022-03-05T02:35:36.4089614Z Mar 05 02:35:36 Caused by: org.apache.flink.util.SerializedThrowable: Task name with subtask : Source: Sequence Source (Deprecated) -&gt; Map -&gt; Sink: Unnamed (1/1)#0 Failure reason: Checkpoint was declined (task is closing)2022-03-05T02:35:36.4090937Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1389) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4092177Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1382) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4093430Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.triggerCheckpointBarrier(Task.java:1348) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4094740Z Mar 05 02:35:36 at org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint(TaskExecutor.java:956) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4095836Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_322]2022-03-05T02:35:36.4096579Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_322]2022-03-05T02:35:36.4097766Z Mar 05 02:35:36 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_322]2022-03-05T02:35:36.4098684Z Mar 05 02:35:36 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_322]2022-03-05T02:35:36.4101381Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]2022-03-05T02:35:36.4102353Z Mar 05 02:35:36 at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]2022-03-05T02:35:36.4103218Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]2022-03-05T02:35:36.4104019Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]2022-03-05T02:35:36.4104801Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]2022-03-05T02:35:36.4105719Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]2022-03-05T02:35:36.4108356Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4110333Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4112523Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4113601Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4114790Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4116110Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4117636Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4118641Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]2022-03-05T02:35:36.4119307Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]2022-03-05T02:35:36.4120161Z Mar 05 02:35:36 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]2022-03-05T02:35:36.4120842Z Mar 05 02:35:36 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]2022-03-05T02:35:36.4121482Z Mar 05 02:35:36 at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]2022-03-05T02:35:36.4122113Z Mar 05 02:35:36 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]2022-03-05T02:35:36.4122736Z Mar 05 02:35:36 at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]2022-03-05T02:35:36.4123332Z Mar 05 02:35:36 at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]2022-03-05T02:35:36.4123984Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_322]2022-03-05T02:35:36.4124749Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_322]2022-03-05T02:35:36.4125750Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_322]2022-03-05T02:35:36.4126591Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) ~[?:1.8.0_322]2022-03-05T02:35:36.4128133Z Mar 05 02:35:36 2022-03-05 02:35:34,430 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Sequence Source (Deprecated) -&gt; Map -&gt; Sink: Unnamed (1/1) (e8a324cae6bf452d32db6797bbbafad0) switched from RUNNING to FINISHED. https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32553&amp;view=logs&amp;j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&amp;t=cc5499f8-bdde-5157-0d76-b6528ecd808e&amp;l=18735</description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-7 01:00:00" id="26506" opendate="2022-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support StreamExecutionEnvironment.registerCachedFile in Python DataStream API</summary>
      <description>This API is missed in Python DataStream API.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-3-7 01:00:00" id="26520" opendate="2022-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement SEARCH operator</summary>
      <description>The codegen right now is not implementing the SEARCH operator, but it's using the rex builder to circumvent it. We should implement the SEARCH operator directly, to remove the usage of the flink type factory</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ConvertToNotInOrInRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.ConvertToNotInOrInRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.LikeCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleProvider.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-8 01:00:00" id="26534" opendate="2022-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>shuffle by sink&amp;#39;s primary key should cover the case that input changelog stream has a different parallelism</summary>
      <description>FLINK-20370 fix the wrong result when sink primary key is not the same with query and introduced a new auto-keyby sink's primary key strategy for append stream if the sink's parallelism differs from input stream's.But still exists one case to be solved:for a changelog stream, its changelog upsert key same as sink's primary key, but sink's parallelism changed by user (via those sinks which implement the `ParallelismProvider` interface, e.g., KafkaDynamicSink), we should fix it.And a minor change: keyby canbe omitted when sink has single parallism (because none partitioner will cause worse disorder)</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-9 01:00:00" id="26543" opendate="2022-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the issue that exceptions generated in startup are missed in Python loopback mode</summary>
      <description/>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.boot.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-7-9 01:00:00" id="26549" opendate="2022-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>INSERT INTO with VALUES leads to wrong type inference with nested types</summary>
      <description>While working on casting, I've found out we have an interesting bug in the insert values type inference. This comes from the KafkaTableITCase#testKafkaSourceSinkWithMetadata (look at this version in particular https://github.com/apache/flink/blob/567440115bcacb5aceaf3304e486281c7da8c14f/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaTableITCase.java).The test scenario is an INSERT INTO VALUES query which is also pushing some metadata to a Kafka table, in particular is writing the headers metadata.The table is declared like that: CREATE TABLE kafka ( `physical_1` STRING, `physical_2` INT, `timestamp-type` STRING METADATA VIRTUAL, `timestamp` TIMESTAMP(3) METADATA, `leader-epoch` INT METADATA VIRTUAL, `headers` MAP&lt;STRING, BYTES&gt; METADATA, `partition` INT METADATA VIRTUAL, `topic` STRING METADATA VIRTUAL, `physical_3` BOOLEAN) WITH ( 'connector' = 'kafka', [...])The insert into query looks like:INSERT INTO kafka VALUES('data 1', 1, TIMESTAMP '2020-03-08 13:12:11.123', MAP['k1', x'C0FFEE', 'k2', x'BABE'], TRUE),('data 2', 2, TIMESTAMP '2020-03-09 13:12:11.123', CAST(NULL AS MAP&lt;STRING, BYTES&gt;), FALSE),('data 3', 3, TIMESTAMP '2020-03-10 13:12:11.123', MAP['k1', X'10', 'k2', X'20'], TRUE)Note that in the first row, the byte literal is of length 3, while in the last row the byte literal is of length 1.The generated plan of this INSERT INTO is:== Abstract Syntax Tree ==LogicalSink(table=[default_catalog.default_database.kafka], fields=[physical_1, physical_2, physical_3, headers, timestamp])+- LogicalProject(physical_1=[$0], physical_2=[$1], physical_3=[$4], headers=[CAST($3):(VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP], timestamp=[CAST($2):TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)]) +- LogicalUnion(all=[true]) :- LogicalProject(EXPR$0=[_UTF-16LE'data 1'], EXPR$1=[1], EXPR$2=[2020-03-08 13:12:11.123:TIMESTAMP(3)], EXPR$3=[MAP(_UTF-16LE'k1', X'c0ffee':VARBINARY(3), _UTF-16LE'k2', X'babe':VARBINARY(3))], EXPR$4=[true]) : +- LogicalValues(tuples=[[{ 0 }]]) :- LogicalProject(EXPR$0=[_UTF-16LE'data 2'], EXPR$1=[2], EXPR$2=[2020-03-09 13:12:11.123:TIMESTAMP(3)], EXPR$3=[null:(VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP], EXPR$4=[false]) : +- LogicalValues(tuples=[[{ 0 }]]) +- LogicalProject(EXPR$0=[_UTF-16LE'data 3'], EXPR$1=[3], EXPR$2=[2020-03-10 13:12:11.123:TIMESTAMP(3)], EXPR$3=[MAP(_UTF-16LE'k1', X'10':BINARY(1), _UTF-16LE'k2', X'20':BINARY(1))], EXPR$4=[true]) +- LogicalValues(tuples=[[{ 0 }]])== Optimized Physical Plan ==Sink(table=[default_catalog.default_database.kafka], fields=[physical_1, physical_2, physical_3, headers, timestamp])+- Union(all=[true], union=[physical_1, physical_2, physical_3, headers, timestamp]) :- Calc(select=[_UTF-16LE'data 1' AS physical_1, 1 AS physical_2, true AS physical_3, CAST(CAST(MAP(_UTF-16LE'k1', X'c0ffee':VARBINARY(3), _UTF-16LE'k2', X'babe':VARBINARY(3)) AS (CHAR(2) CHARACTER SET "UTF-16LE" NOT NULL, BINARY(1) NOT NULL) MAP) AS (VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP) AS headers, CAST(2020-03-08 12:12:11.123:TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) : +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]]) :- Calc(select=[_UTF-16LE'data 2' AS physical_1, 2 AS physical_2, false AS physical_3, null:(VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP AS headers, CAST(2020-03-09 12:12:11.123:TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) : +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]]) +- Calc(select=[_UTF-16LE'data 3' AS physical_1, 3 AS physical_2, true AS physical_3, CAST(MAP(_UTF-16LE'k1', X'10':BINARY(1), _UTF-16LE'k2', X'20':BINARY(1)) AS (VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP) AS headers, CAST(2020-03-10 12:12:11.123:TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])== Optimized Execution Plan ==Sink(table=[default_catalog.default_database.kafka], fields=[physical_1, physical_2, physical_3, headers, timestamp])+- Union(all=[true], union=[physical_1, physical_2, physical_3, headers, timestamp]) :- Calc(select=['data 1' AS physical_1, 1 AS physical_2, true AS physical_3, CAST(CAST(MAP('k1', X'c0ffee', 'k2', X'babe') AS (CHAR(2), BINARY(1)) MAP) AS (VARCHAR(2147483647), VARBINARY(2147483647)) MAP) AS headers, CAST(2020-03-08 12:12:11.123 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) : +- Values(tuples=[[{ 0 }]])(reuse_id=[1]) :- Calc(select=['data 2' AS physical_1, 2 AS physical_2, false AS physical_3, null:(VARCHAR(2147483647), VARBINARY(2147483647)) MAP AS headers, CAST(2020-03-09 12:12:11.123 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) : +- Reused(reference_id=[1]) +- Calc(select=['data 3' AS physical_1, 3 AS physical_2, true AS physical_3, CAST(MAP('k1', X'10', 'k2', X'20') AS (VARCHAR(2147483647), VARBINARY(2147483647)) MAP) AS headers, CAST(2020-03-10 12:12:11.123 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) +- Reused(reference_id=[1])As you see, in the Abstract Syntax Tree section a casting for the headers is injected (although unnecessary, as it should be an identity cast), but then in Optimized Physical Plan another casting is injected:CAST(CAST(MAP(_UTF-16LE'k1', X'c0ffee':VARBINARY(3), _UTF-16LE'k2', X'babe':VARBINARY(3)) AS (CHAR(2) CHARACTER SET "UTF-16LE" NOT NULL, BINARY(1) NOT NULL) MAP) AS (VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP) AS headersWhich makes no sense, as it's casting the values of the map first to BINARY(1) and then to BYTES, causing to trim the last 2 bytes. Removing the last row to insert makes the VALUES type inference work properly:== Abstract Syntax Tree ==LogicalSink(table=[default_catalog.default_database.kafka], fields=[physical_1, physical_2, physical_3, headers, timestamp])+- LogicalProject(physical_1=[$0], physical_2=[$1], physical_3=[$4], headers=[$3], timestamp=[CAST($2):TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)]) +- LogicalUnion(all=[true]) :- LogicalProject(EXPR$0=[_UTF-16LE'data 1'], EXPR$1=[1], EXPR$2=[2020-03-08 13:12:11.123:TIMESTAMP(3)], EXPR$3=[MAP(_UTF-16LE'k1', X'c0ffee':VARBINARY(3), _UTF-16LE'k2', X'babe':VARBINARY(3))], EXPR$4=[true]) : +- LogicalValues(tuples=[[{ 0 }]]) +- LogicalProject(EXPR$0=[_UTF-16LE'data 2'], EXPR$1=[2], EXPR$2=[2020-03-09 13:12:11.123:TIMESTAMP(3)], EXPR$3=[null:(VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP], EXPR$4=[false]) +- LogicalValues(tuples=[[{ 0 }]])== Optimized Physical Plan ==Sink(table=[default_catalog.default_database.kafka], fields=[physical_1, physical_2, physical_3, headers, timestamp])+- Union(all=[true], union=[physical_1, physical_2, physical_3, headers, timestamp]) :- Calc(select=[_UTF-16LE'data 1' AS physical_1, 1 AS physical_2, true AS physical_3, CAST(MAP(_UTF-16LE'k1', X'c0ffee':VARBINARY(3), _UTF-16LE'k2', X'babe':VARBINARY(3)) AS (VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP) AS headers, CAST(2020-03-08 12:12:11.123:TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) : +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]]) +- Calc(select=[_UTF-16LE'data 2' AS physical_1, 2 AS physical_2, false AS physical_3, null:(VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP AS headers, CAST(2020-03-09 12:12:11.123:TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])== Optimized Execution Plan ==Sink(table=[default_catalog.default_database.kafka], fields=[physical_1, physical_2, physical_3, headers, timestamp])+- Union(all=[true], union=[physical_1, physical_2, physical_3, headers, timestamp]) :- Calc(select=['data 1' AS physical_1, 1 AS physical_2, true AS physical_3, CAST(MAP('k1', X'c0ffee', 'k2', X'babe') AS (VARCHAR(2147483647), VARBINARY(2147483647)) MAP) AS headers, CAST(2020-03-08 12:12:11.123 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) : +- Values(tuples=[[{ 0 }]])(reuse_id=[1]) +- Calc(select=['data 2' AS physical_1, 2 AS physical_2, false AS physical_3, null:(VARCHAR(2147483647), VARBINARY(2147483647)) MAP AS headers, CAST(2020-03-09 12:12:11.123 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) +- Reused(reference_id=[1])</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.calcite.FlinkTypeFactoryTest.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-3-10 01:00:00" id="26583" opendate="2022-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The user is not informed in any way when a job is resubmitted but already globally-terminated</summary>
      <description>-We experience some unwanted behavior if a clean JobResult is listed in the JobResultStore and a job with the same Job ID is submitted in Application Mode.We would expect that the second submission fails with a DuplicateJobSubmissionException. Instead, the submission succeeds with the job not running anymore.-Update:We reiterated over the problem and decided that the Exception is not the desired failure because it would cause a failover in k8s setups, for instance. We rather want to inform the user through a warning because Flink still behaves as expected.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-5-10 01:00:00" id="26588" opendate="2022-3-10 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Translate the new SQL CAST documentation to Chinese</summary>
      <description>Since FLINK-26125 is now merged, this content change should also be translated to Chinese. Relevant PR is https://github.com/apache/flink/pull/18813</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.table.types.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-11 01:00:00" id="26604" opendate="2022-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update AvroParquet format user-facing document</summary>
      <description>add minimal mvn dependency setup describe the namespace use case in the Avro schema reduce the redundant information w.r.t. the bunded/unbunded data</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.formats.parquet.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.formats.parquet.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-7-14 01:00:00" id="26621" opendate="2022-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ChangelogPeriodicMaterializationITCase crashes JVM on CI in RocksDB cleanup</summary>
      <description>2022-03-11T16:20:12.6929558Z Mar 11 16:20:12 [WARNING] The requested profile "skip-webui-build" could not be activated because it does not exist.2022-03-11T16:20:12.6939269Z Mar 11 16:20:12 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (integration-tests) on project flink-tests: There are test failures.2022-03-11T16:20:12.6940062Z Mar 11 16:20:12 [ERROR] 2022-03-11T16:20:12.6940954Z Mar 11 16:20:12 [ERROR] Please refer to /__w/2/s/flink-tests/target/surefire-reports for the individual test results.2022-03-11T16:20:12.6941875Z Mar 11 16:20:12 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.2022-03-11T16:20:12.6942966Z Mar 11 16:20:12 [ERROR] ExecutionException Error occurred in starting fork, check output in log2022-03-11T16:20:12.6943919Z Mar 11 16:20:12 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException Error occurred in starting fork, check output in log2022-03-11T16:20:12.6945023Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)2022-03-11T16:20:12.6945878Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:479)2022-03-11T16:20:12.6946761Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:322)2022-03-11T16:20:12.6947532Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)2022-03-11T16:20:12.6953051Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)2022-03-11T16:20:12.6954035Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)2022-03-11T16:20:12.6954917Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:932)2022-03-11T16:20:12.6955749Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)2022-03-11T16:20:12.6956542Z Mar 11 16:20:12 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)2022-03-11T16:20:12.6957456Z Mar 11 16:20:12 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)2022-03-11T16:20:12.6958232Z Mar 11 16:20:12 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)2022-03-11T16:20:12.6959038Z Mar 11 16:20:12 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)2022-03-11T16:20:12.6960553Z Mar 11 16:20:12 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)2022-03-11T16:20:12.6962116Z Mar 11 16:20:12 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)2022-03-11T16:20:12.6963009Z Mar 11 16:20:12 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)2022-03-11T16:20:12.6963737Z Mar 11 16:20:12 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)2022-03-11T16:20:12.6964644Z Mar 11 16:20:12 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)2022-03-11T16:20:12.6965647Z Mar 11 16:20:12 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)2022-03-11T16:20:12.6966732Z Mar 11 16:20:12 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)2022-03-11T16:20:12.6967818Z Mar 11 16:20:12 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)2022-03-11T16:20:12.6968857Z Mar 11 16:20:12 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2022-03-11T16:20:12.6969986Z Mar 11 16:20:12 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2022-03-11T16:20:12.6971491Z Mar 11 16:20:12 [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2022-03-11T16:20:12.6972207Z Mar 11 16:20:12 [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)2022-03-11T16:20:12.6973134Z Mar 11 16:20:12 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)2022-03-11T16:20:12.6974067Z Mar 11 16:20:12 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)2022-03-11T16:20:12.6974828Z Mar 11 16:20:12 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)2022-03-11T16:20:12.6975753Z Mar 11 16:20:12 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)2022-03-11T16:20:12.6976620Z Mar 11 16:20:12 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: Error occurred in starting fork, check output in log2022-03-11T16:20:12.6977893Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:662)2022-03-11T16:20:12.6978949Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$700(ForkStarter.java:121)2022-03-11T16:20:12.6980306Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:465)2022-03-11T16:20:12.6981716Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:442)2022-03-11T16:20:12.6982449Z Mar 11 16:20:12 [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)2022-03-11T16:20:12.6983156Z Mar 11 16:20:12 [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2022-03-11T16:20:12.6983904Z Mar 11 16:20:12 [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2022-03-11T16:20:12.6984533Z Mar 11 16:20:12 [ERROR] at java.lang.Thread.run(Thread.java:748)2022-03-11T16:20:12.6985373Z Mar 11 16:20:12 [ERROR] -&gt; [Help 1]2022-03-11T16:20:12.6985741Z Mar 11 16:20:12 [ERROR] 2022-03-11T16:20:12.6986424Z Mar 11 16:20:12 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.2022-03-11T16:20:12.6987456Z Mar 11 16:20:12 [ERROR] Re-run Maven using the -X switch to enable full debug logging.2022-03-11T16:20:12.6987935Z Mar 11 16:20:12 [ERROR] 2022-03-11T16:20:12.6988446Z Mar 11 16:20:12 [ERROR] For more information about the errors and possible solutions, please read the following articles:2022-03-11T16:20:12.6989125Z Mar 11 16:20:12 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException2022-03-11T16:20:12.6989631Z Mar 11 16:20:12 [ERROR] 2022-03-11T16:20:12.6990284Z Mar 11 16:20:12 [ERROR] After correcting the problems, you can resume the build with the command2022-03-11T16:20:12.6991194Z Mar 11 16:20:12 [ERROR] mvn &lt;goals&gt; -rf :flink-tests2022-03-11T16:20:13.5874703Z Mar 11 16:20:13 Process exited with EXIT CODE: 1.https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32927&amp;view=logs&amp;j=5c8e7682-d68f-54d1-16a2-a09310218a49&amp;t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&amp;l=6255</description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-3-14 01:00:00" id="26634" opendate="2022-3-14 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Update Chinese version of Elasticsearch connector docs</summary>
      <description>In https://github.com/apache/flink/pull/19035 we made some smaller changes to the documentation for the Elasticsearch connector with regards to the delivery guarantee. These changes still not to be ported over to the chinese docs.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.connectors.datastream.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-3-15 01:00:00" id="26652" opendate="2022-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failing to cleanup a job should not fail the Flink Cluster in Session Mode</summary>
      <description>We introduced the option to disable the retryable cleanup in FLINK-26331. This should make Flink fall back to the 1.14- functionality with just printing a warning in session mode.Instead, a RetryException is thrown which causes Flink to fail fatally. For Job and Application Mode failing fatally is ok because it doesn't affect other builds. But for session mode, we want to print a warning, instead.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherCleanupITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CleanupOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.fixed.delay.cleanup.strategy.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.exponential.delay.cleanup.strategy.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.cleanup.configuration.html</file>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.docs.deployment.ha.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.ha.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-15 01:00:00" id="26658" opendate="2022-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate documentation build to Github Actions</summary>
      <description>INFRA recently setup the required credentials to rsync content to nightlies.apache.org via github actions. This means we can migrate out buildbot setup to github actions instead.This should make maintenance a lot easier, as we'd have more control over the environment. It'd also make it way easier to discover.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.docs.sh</file>
      <file type="M">tools.ci.build.docs.sh</file>
      <file type="M">.github.workflows.docs.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-4-16 01:00:00" id="26681" opendate="2022-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support sql statement end with ";" for Hive dialect</summary>
      <description>In FLINK-25600, the sql client won't remove ';' at the end of command, so the sql statement will keep the semicolon. When using Hive dialect, it'll be passed to HiveParser and then throw the ParseException likeorg.apache.flink.table.planner.delegation.hive.copy.HiveASTParseException: line 1:28 cannot recognize input near ';' '&lt;EOF&gt;' ..So we need to support thesql statement end with ";" for Hive dialect.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTParser.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-17 01:00:00" id="26701" opendate="2022-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Relocation of connector-base might break user jars due to changed imports</summary>
      <description>With the introduction of FLINK-25927, every connector now relocates connector-base to better support connectors compatibility with different Flink versions. Unfortunately, not all classes in connector-base are only used by connector but some are supposed to be used inside the user jar directly i.e. DeliveryGuarantee, HybridSource...Since the connector now relocates the module the existing imports are broken.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.shade.sh</file>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-streams.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-3-18 01:00:00" id="26727" opendate="2022-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the implementation of sub-interpreter in Thread Mode</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.EmbeddedPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractEmbeddedPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.embedded.EmbeddedPythonEnvironmentManager.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">docs.layouts.shortcodes.generated.python.configuration.html</file>
      <file type="M">docs.content.docs.dev.python.python.execution.mode.md</file>
      <file type="M">docs.content.zh.docs.dev.python.python.execution.mode.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-18 01:00:00" id="26728" opendate="2022-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support min max min_by max_by operation in KeyedStream</summary>
      <description>Support min max min_by max_by operation in KeyedStream</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-19 01:00:00" id="26739" opendate="2022-3-19 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Hive 2.3.8 and 2.3.9</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-21 01:00:00" id="26771" opendate="2022-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix incomparable exception between boolean type and numeric type in Hive dialect</summary>
      <description>Hive support compare boolean type with numeric type, for example such sql can be excuted in Hive:// the data type for `status` is `int`select * from employee where status = true; But in Flink, with Hive dialect, it'll throw "Incomparable types: BOOLEAN and INT NOT NULL" exception.For such case, it should be consistent with Hive while using Hive dialect in Flink.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserRexNodeConverter.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserDMLHelper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserSqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-3-21 01:00:00" id="26783" opendate="2022-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restore from a stop-with-savepoint if failed during committing</summary>
      <description>We decided stop-with-savepoint should commit side-effects and thus we should fail over to those savepoints if a failure happens when committing side effects.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.stopwithsavepoint.StopWithSavepointTerminationHandlerImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.stopwithsavepoint.StopWithSavepointTerminationHandlerImpl.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-3-25 01:00:00" id="26855" opendate="2022-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ImportError: cannot import name &amp;#39;environmentfilter&amp;#39; from &amp;#39;jinja2&amp;#39;</summary>
      <description>ar 24 17:38:39 ===========mypy checks... [SUCCESS]===========Mar 24 17:38:39 rm -rf _build/*Mar 24 17:38:39 /__w/2/s/flink-python/dev/.conda/bin/sphinx-build -b html -d _build/doctrees -a -W . _build/htmlMar 24 17:38:40 Traceback (most recent call last):Mar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/bin/sphinx-build", line 6, in &lt;module&gt;Mar 24 17:38:40 from sphinx.cmd.build import mainMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/cmd/build.py", line 23, in &lt;module&gt;Mar 24 17:38:40 from sphinx.application import SphinxMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/application.py", line 42, in &lt;module&gt;Mar 24 17:38:40 from sphinx.highlighting import lexer_classes, lexersMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/highlighting.py", line 30, in &lt;module&gt;Mar 24 17:38:40 from sphinx.ext import doctestMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/ext/doctest.py", line 28, in &lt;module&gt;Mar 24 17:38:40 from sphinx.builders import BuilderMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/builders/__init__.py", line 24, in &lt;module&gt;Mar 24 17:38:40 from sphinx.io import read_docMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/io.py", line 42, in &lt;module&gt;Mar 24 17:38:40 from sphinx.util.rst import append_epilog, docinfo_re, prepend_prologMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/util/rst.py", line 22, in &lt;module&gt;Mar 24 17:38:40 from jinja2 import environmentfilterMar 24 17:38:40 ImportError: cannot import name 'environmentfilter' from 'jinja2' (/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/jinja2/__init__.py)Mar 24 17:38:40 Makefile:76: recipe for target 'html' failedMar 24 17:38:40 make: *** [html] Error 1Mar 24 17:38:40 ==========sphinx checks... [FAILED]===========https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33717&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901&amp;l=23450</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-25 01:00:00" id="26865" opendate="2022-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the potential failure of loading library in Thread Mode</summary>
      <description>The failure occurs in session mode.</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-30 01:00:00" id="26928" opendate="2022-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary Docker network creation in Kafka connector tests</summary>
      <description>Currently each Kafka test class will create a Docker network, which could flush the network usage on Docker host, and test would fail if all IP address in the pool of Docker are occupied.</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-4-31 01:00:00" id="26961" opendate="2022-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update multiple Jackson dependencies to v2.13.2 and v2.13.2.1</summary>
      <description>There is a High CVE-2020-36518, https://github.com/advisories/GHSA-57j2-w4cx-62h2which was fixed with 2.13.2.1</description>
      <version>None</version>
      <fixedVersion>1.14.5,1.15.0,elasticsearch-3.0.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-1 01:00:00" id="26986" opendate="2022-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated string expressions in Python Table API</summary>
      <description>In FLINK-26704, it has removed the string expressions in Table API. However, there are still some APIs still using string expressions in Python Table API, however, they should not work any more as the string expressions have already been removed in the Java Table API.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.window.py</file>
      <file type="M">flink-python.pyflink.table.table.py</file>
      <file type="M">flink-python.pyflink.table.schema.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.tumble.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.sliding.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.session.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.over.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.pandas.pandas.udaf.py</file>
      <file type="M">docs.content.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.docs.dev.table.catalogs.md</file>
      <file type="M">docs.content.docs.dev.python.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.content.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.docs.dev.python.table.python.table.api.connectors.md</file>
      <file type="M">docs.content.zh.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.zh.docs.dev.table.catalogs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.python.table.api.connectors.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-1 01:00:00" id="26994" opendate="2022-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge libraries CI profile into core</summary>
      <description>The libraries profile spends more time on setting up the environment than actually running tests. Merge it with Core for more efficiency.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-4-2 01:00:00" id="27024" opendate="2022-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup surefire configuration</summary>
      <description>We have a few redundant surefire configurations in some connector modules, and overall a lot of duplication and stuff defined on the argLine which could be systemEnvironmentVariables (which are easier to extend in sub-modules).</description>
      <version>None</version>
      <fixedVersion>1.16.0,elasticsearch-3.0.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-4-6 01:00:00" id="27069" opendate="2022-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the potential memory corruption in Thread Mode</summary>
      <description>Apr 02 12:24:54 *** Error in `/usr/lib/jvm/adoptopenjdk-11-hotspot-amd64/bin/java': malloc(): memory corruption: 0x00007ff7c43bb820 ***Apr 02 12:24:54 ======= Backtrace: =========Apr 02 12:24:54 /lib/x86_64-linux-gnu/libc.so.6(+0x777f5)[0x7ff7f90be7f5]Apr 02 12:24:54 /lib/x86_64-linux-gnu/libc.so.6(+0x8215e)[0x7ff7f90c915e]Apr 02 12:24:54 /lib/x86_64-linux-gnu/libc.so.6(__libc_malloc+0x54)[0x7ff7f90cb1d4]Apr 02 12:24:54 /root/flink/flink-python/dev/.conda/envs/3.8/lib/libpython3.8.so.1.0(PyObject_Malloc+0x166)[0x7ff78c16c636]Apr 02 12:24:54 /root/flink/flink-python/dev/.conda/envs/3.8/lib/libpython3.8.so.1.0(PyBytes_FromStringAndSize+0x76)[0x7ff78c1b9316]Apr 02 12:24:54 /root/flink/flink-python/.tox/py38-cython/lib/python3.8/site-packages/pemja_core.cpython-38-x86_64-linux-gnu.so(JcpPyBytes_FromJByteArray+0x46)[0x7ff7f400c706]Apr 02 12:24:54 /root/flink/flink-python/.tox/py38-cython/lib/python3.8/site-packages/pemja_core.cpython-38-x86_64-linux-gnu.so(JcpPyObject_FromJObject+0x3db)[0x7ff7f400d2ab]Apr 02 12:24:54 /root/flink/flink-python/.tox/py38-cython/lib/python3.8/site-packages/pemja_core.cpython-38-x86_64-linux-gnu.so(JcpPyObject_SetJObject+0x2e)[0x7ff7f400ee6e]Apr 02 12:24:54 /root/flink/flink-python/.tox/py38-cython/lib/python3.8/site-packages/pemja_core.cpython-38-x86_64-linux-gnu.so(Java_pemja_core_PythonInterpreter_set__JLjava_lang_String_2Ljava_lang_Object_2+0x35)[0x7ff7f400a765]Apr 02 12:24:54 [0x7ff7d8887630]</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-4-7 01:00:00" id="27108" opendate="2022-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>State cache clean up doesn&amp;#39;t work as expected</summary>
      <description>The test case test_session_window_late_merge failed when working on FLINK-26190. After digging into this problem, I found that the reason should be that the logic to determine whether a key &amp; namespace exists in state cache is wrong is wrong. It causes the state cache isn't clean up when it becomes invalidate.</description>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-7 01:00:00" id="27111" opendate="2022-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update docs regarding EnvironmentSettings / TableConfig</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-7 01:00:00" id="27119" opendate="2022-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup JobMasters</summary>
      <description>Several tests create JobMasters but don't make sure that it is shut down. We should change the tests to use a try-with-resource statement.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterQueryableStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterExecutionDeploymentReconciliationTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-8 01:00:00" id="27140" opendate="2022-4-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move JobResultStore dirty entry creation into ioExecutor</summary>
      <description>The FileSystemJobResultStore is thread-safe and, therefore, we can move the dirty entry creation into the ioExecutor</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.MiniDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-8-12 01:00:00" id="27199" opendate="2022-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Pulsar to 2.10.0 for fixing the unstable Pulsar test environment.</summary>
      <description>Pulsar's transaction is not stable. The standalone cluster often hangs the test, then we will meet a timeout for the tests at last.The latest Pulsar 2.10.0 drops the zookeeper and fixes a lot of issues in the Pulsar transaction. Bump to this version would resolve the current test issues.</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.PulsarMockRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockZooKeeperClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockPulsarService.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockBookKeeperClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.PulsarSinkOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.SinkConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.PulsarSinkConfigUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.sink.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.producer.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.client.configuration.html</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.split.PulsarPartitionSplit.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.start.MessageIdStartCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.SourceConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.router.MessageKeyHash.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-4-13 01:00:00" id="27229" opendate="2022-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cassandra overrides netty version in tests</summary>
      <description>flink-connector-cassandra declares: &lt;dependency&gt; &lt;!-- Bump cassandra netty dependency --&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.46.Final&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;which overrides the project wide version of netty just for tests.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-13 01:00:00" id="27230" opendate="2022-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary entries in connector-kinesis NOTICE file</summary>
      <description>flink-connector-kinesis lists but does not bundle:- commons-logging:commons-logging:1.1.3- com.fasterxml.jackson.core:jackson-core:2.13.2[INFO] Excluding commons-logging:commons-logging:jar:1.1.3 from the shaded jar.[INFO] Excluding com.fasterxml.jackson.core:jackson-core:jar:2.13.2 from the shaded jar.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-13 01:00:00" id="27231" opendate="2022-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL pulsar connector lists dependencies under wrong license</summary>
      <description>Pulsar sql connector lists following dependencies under ASL2 license while they are licensed with Bouncy Castle license (variant of MIT?).- org.bouncycastle:bcpkix-jdk15on:1.69- org.bouncycastle:bcprov-ext-jdk15on:1.69- org.bouncycastle:bcprov-jdk15on:1.69- org.bouncycastle:bcutil-jdk15on:1.69</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-13 01:00:00" id="27233" opendate="2022-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary entries in connector-elasticsearch7 in NOTICE file</summary>
      <description>flink-sql-connector-elasticsearch7 lists following dependencies in the NOTICE file, which are not bundled in the jar:- com.fasterxml.jackson.core:jackson-databind:2.13.2.2- com.fasterxml.jackson.core:jackson-annotations:2.13.2- org.apache.lucene:lucene-spatial:8.7.0- org.elasticsearch:elasticsearch-plugin-classloader:7.10.2- org.lz4:lz4-java:1.8.0</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-13 01:00:00" id="27234" opendate="2022-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable fork-reuse for connector-jdbc</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProviderDriverClassConcurrentLoadingTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-jdbc.archunit-violations.6b9ab1b0-c14d-4667-bab5-407b81fba98b</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-4-14 01:00:00" id="27252" opendate="2022-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove surefire fork options from connector-hive</summary>
      <description>Cleanup of unnecessary settings, that will also slightly speed up testing.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-14 01:00:00" id="27253" opendate="2022-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove custom surefire config from connector-cassandra</summary>
      <description>With the recent improvements around the cassandra test stability we can clean up some technical debt.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-5-19 01:00:00" id="27297" opendate="2022-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the StreamExecutionEnvironment#getExecutionEnvironment(Configuration) method in PyFlink</summary>
      <description>StreamExecutionEnvironment#getExecutionEnvironment(Configuration) method has been added in Java side since release-1.12, we need to add this method in Python too</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.common.tests.test.execution.config.py</file>
      <file type="M">docs.content.docs.dev.python.python.config.md</file>
      <file type="M">docs.content.zh.docs.dev.python.python.config.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-4-19 01:00:00" id="27308" opendate="2022-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the Hadoop implementation for filesystems to 3.3.2</summary>
      <description>Flink currently uses Hadoop version 3.2.2 for the Flink filesystem implementations. Upgrading this to version 3.3.2 would provide users the features listed in HADOOP-17566</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.java.org.apache.flink.fs.s3hadoop.HadoopS3AccessHelper.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.pom.xml</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-20 01:00:00" id="27319" opendate="2022-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Duplicated "-t" option for savepoint format and deployment target</summary>
      <description>The two options savepoint format and deployment target have the same short option which causes a clash and the CLI to fail.I suggest to drop the short "-t" for savepoint format.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendStopWithSavepointTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendSavepointTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-12-21 01:00:00" id="27341" opendate="2022-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TaskManager running together with JobManager are bind to 127.0.0.1</summary>
      <description>If some TaskManagers running with JobManager on the same machine while some other TaskManager not, the TaskManagers running together with JobManager would bind to localhost or 127.0.01, which makes the Netty connections across the TaskManagers fail.</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.17.0,1.16.1,1.15.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.ConnectionUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-4-25 01:00:00" id="27382" opendate="2022-4-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Job mode wait with cluster shutdown until the cleanup is done</summary>
      <description>The shutdown is triggered as soon as the job terminates globally without waiting for any cleanup. This behavior was ok'ish in 1.14- because we didn't bother so much about the cleanup. In 1.15+ we might want to wait for the cleanup to finish.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.MiniDispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.MiniDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-25 01:00:00" id="27386" opendate="2022-4-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>throw NPE if multi MAPJOIN hint union all</summary>
      <description>We can reproduce through a UTAdd test case in HiveDialectITCase@Testpublic void testHiveMultiMapJoinUnionAll() { tableEnv.executeSql("create table t1 (id bigint, name string)"); tableEnv.executeSql("create table t2 (id bigint, name string)"); tableEnv.executeSql("select /*+ mapjoin(t2) */ t1.id from t1 join t2 on t1.id = t2.id union all select /*+ mapjoin(t2) */ t1.id from t1 join t2 on t1.name = t2.name");}</description>
      <version>1.13.1,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveASTParseUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-7-25 01:00:00" id="27387" opendate="2022-4-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Insert Multi-Table</summary>
      <description>We can reproduce through a UTAdd test case in HiveDialectITCase@Testpublic void testInsertMultiTable() { tableEnv.executeSql("create table t1 (id bigint, name string)"); tableEnv.executeSql("create table t2 (id bigint, name string)"); tableEnv.executeSql("create table t3 (id bigint, name string, age int)"); tableEnv.executeSql("from (select id, name, age from t3) t " + "insert overwrite table t1 select id, name where age &lt; 20 " + "insert overwrite table t2 select id, name where age &gt; 20 ");} This is a very common case for batch.</description>
      <version>1.13.1,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-22 01:00:00" id="2740" opendate="2015-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create data consumer for Apache NiFi</summary>
      <description>Create a connector to Apache NiFi to create Flink DataStreams from NiFi flows</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-5-28 01:00:00" id="27441" opendate="2022-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scrollbar is missing for particular UI elements (Accumulators, Backpressure, Watermarks)</summary>
      <description>The angular version bump introduced a bug, where for nzScroll does not support percentage in CSS calc, so the scrollbar will be invisible. There is an easy workaround, the linked Angular discussion covers it.Angular issue: https://github.com/NG-ZORRO/ng-zorro-antd/issues/3090</description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.job-overview-drawer-taskmanagers.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.accumulators.job-overview-drawer-accumulators.component.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-6-2 01:00:00" id="27466" opendate="2022-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC metaspace leak fix is misleading</summary>
      <description>To ensure that these classes are only loaded once you should either add the driver jars to Flinks lib/ folder, or add the driver classes to the list of parent-first loaded class via classloader.parent-first-patterns-additional.This reads as if adding the driver to classloader.parent-first-patterns-additional can solve the issue in all cases, but this only works if the driver is already in lib/.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.zh.docs.ops.debugging.debugging.classloading.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-6-9 01:00:00" id="27544" opendate="2022-5-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Example code in &amp;#39;Structure of Table API and SQL Programs&amp;#39; is out of date and cannot run</summary>
      <description>The example code in Structure of Table API and SQL Programs of 'Concepts &amp; Common API' is out of date and when user run this piece of code, they will get the following result:Exception in thread "main" org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.SinkTable'.Table options are:'connector'='blackhole''rows-per-second'='1' at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:262) at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:421) at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:222) at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:178) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) at scala.collection.Iterator.foreach(Iterator.scala:937) at scala.collection.Iterator.foreach$(Iterator.scala:937) at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) at scala.collection.IterableLike.foreach(IterableLike.scala:70) at scala.collection.IterableLike.foreach$(IterableLike.scala:69) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike.map(TraversableLike.scala:233) at scala.collection.TraversableLike.map$(TraversableLike.scala:226) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:178) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:782) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:861) at org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:56) at com.yck.TestTableAPI.main(TestTableAPI.java:43)Caused by: org.apache.flink.table.api.ValidationException: Unsupported options found for 'blackhole'.Unsupported options:rows-per-secondSupported options:connectorproperty-version at org.apache.flink.table.factories.FactoryUtil.validateUnconsumedKeys(FactoryUtil.java:624) at org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validate(FactoryUtil.java:914) at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.validate(FactoryUtil.java:978) at org.apache.flink.connector.blackhole.table.BlackHoleTableSinkFactory.createDynamicTableSink(BlackHoleTableSinkFactory.java:64) at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:259) ... 19 moreI think this mistake would drive users crazy when they first fry Table API &amp; Flink SQL since this is the very first code they see.Overall this code is outdated in two places:1. The Query creating temporary table should be CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable (EXCLUDING OPTIONS) instead of CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable which missed (EXCLUDING OPTIONS) sql_like_pattern2. The part creating a source table should be tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 1L) .build());instead of tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenOptions.ROWS_PER_SECOND, 100) .build());since the class DataGenOptions was replaced by class DataGenConnectorOptions in this commitThe test code is in my github Repository(version 1.15) and version 1.14The affected versions are 1.15 and 1.14.</description>
      <version>1.14.0,1.14.2,1.14.3,1.14.4,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-5-10 01:00:00" id="27565" opendate="2022-5-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump minimist from 1.2.5 to 1.2.6 in /flink-runtime-web/web-dashboard</summary>
      <description>It's recommended to use version 1.2.6 or later: https://security.snyk.io/vuln/SNYK-JS-MINIMIST-2429795 (version &lt;=1.2.5) https://snyk.io/vuln/SNYK-JS-MINIMIST-559764 (version &lt;=1.2.3)</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2022-5-19 01:00:00" id="27699" opendate="2022-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Align atPublishTime method of StopCursor class for Pulsar connector</summary>
      <description>StopCursor#atEventTime is deprecated, align to StopCursor#atPublishTime.</description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.pulsar.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-5-20 01:00:00" id="27711" opendate="2022-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the typo of set_topics_pattern by changing it to set_topic_pattern for Pulsar Connector</summary>
      <description>Update set_topics_pattern to set_topic_pattern</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.pulsar.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-9-20 01:00:00" id="27718" opendate="2022-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix fail to count mutiple fields excpetion in Hive dialect</summary>
      <description>In Hive, it's support to count multiple fields using the following sql:select count(distinct a, b) from srcWe are also expected to support it while using Hive dialect in Flink.</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.module.hive.HiveModule.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDAF.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-6-22 01:00:00" id="27734" opendate="2022-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Not showing checkpoint interval properly in WebUI when checkpoint is disabled</summary>
      <description>Not showing checkpoint interval properly in WebUI when checkpoint is disabled</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-5-23 01:00:00" id="27735" opendate="2022-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update testcontainers dependency to v1.17.2</summary>
      <description>testcontainers 1.17.2 is releasedAmong others there is a fix for connection leak in jdbc, performanceMain benefits (based on https://github.com/testcontainers/testcontainers-java/releases/tag/1.17.2)</description>
      <version>None</version>
      <fixedVersion>1.16.0,elasticsearch-3.0.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-5-24 01:00:00" id="27757" opendate="2022-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch connector should not use flink-table-planner but flink-table-planner-loader</summary>
      <description>Connectors should not rely on flink-table-planner but on flink-table-planner-loader by default. We can should change this for the Elasticsearch connector as this is being externalized at the moment</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-5-25 01:00:00" id="27776" opendate="2022-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw exception when UDAF used in sliding window does not implement merge method in PyFlink</summary>
      <description>We use the pane state to optimize the result of calculating the window state, which requires udaf to implement the merge method. However, due to the lack of detection of whether the merge method of udaf is implemented, the user's output result did not meet his expectations and there is no exception. Below is an example of a UDAF that implements the merge method:class SumAggregateFunction(AggregateFunction): def get_value(self, accumulator): return accumulator[0] def create_accumulator(self): return [0] def accumulate(self, accumulator, *args): accumulator[0] = accumulator[0] + args[0] def retract(self, accumulator, *args): accumulator[0] = accumulator[0] - args[0] def merge(self, accumulator, accumulators): for other_acc in accumulators: accumulator[0] = accumulator[0] + other_acc[0] def get_accumulator_type(self): return DataTypes.ARRAY(DataTypes.BIGINT()) def get_result_type(self): return DataTypes.BIGINT()</description>
      <version>1.13.6,1.14.4,1.15.0</version>
      <fixedVersion>1.14.5,1.15.1,1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.udf.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-29 01:00:00" id="2779" opendate="2015-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation to reflect new Stream/Window API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.internals.general.arch.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.apis.streaming.guide.md</file>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-5-27 01:00:00" id="27818" opendate="2022-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Model enums as references in OpenAPI spec</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-6-27 01:00:00" id="27819" opendate="2022-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate better operationIds for OpenAPI spec</summary>
      <description>There is an easy way to generate operation ids that are significantly better than the defaults.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.MessageHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.savepoints.stop.StopWithSavepointTriggerHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.savepoints.SavepointTriggerHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.savepoints.SavepointDisposalTriggerHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobSubmitHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobCancellationHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.dataset.ClusterDataSetDeleteTriggerHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.cluster.ShutdownHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.rescaling.RescalingTriggerHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanGetHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarDeleteHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.AbstractJarPlanHeaders.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.OpenApiSpecGeneratorTest.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.data.TestEmptyMessageHeaders.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-7-28 01:00:00" id="27822" opendate="2022-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the doc of checkpoint/savepoint guarantees</summary>
      <description>Translate the change of FLINK-26134</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.ops.state.savepoints.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpoints.vs.savepoints.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpoints.md</file>
      <file type="M">docs.content.zh.docs.concepts.stateful-stream-processing.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-12-30 01:00:00" id="27837" opendate="2022-5-30 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support statement set in the SQL Gateway</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-7-1 01:00:00" id="27865" opendate="2022-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add guide and example for configuring SASL and SSL in Kafka SQL connector document</summary>
      <description>Using SASL and SSL in Kafka connector is a common case and usually quite complex for new users that not quite familiar with the design of Kafka connector, so it would be helpful to add a guidance of how to enable these security options in Kafka connector.</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2,1.14.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-6-6 01:00:00" id="27903" opendate="2022-6-6 00:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce and support HYBRID resultPartitionType</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorBatchExecutionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingPipelinedRegion.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategyTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.StreamExchangeMode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.GlobalStreamExchangeMode.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionType.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ExecutionOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.BatchShuffleMode.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2022-6-8 01:00:00" id="27955" opendate="2022-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink installation failure on Windows OS</summary>
      <description>Because pemja doesn't support windows os, it makes installation failed in windows os in release-1.15. We need to fix it asap.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-9-9 01:00:00" id="27976" opendate="2022-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebUi] Allow order by jobname</summary>
      <description>Allow to order jobs (running and canceled) by job name</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-list.job-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-list.job-list.component.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-6-10 01:00:00" id="28004" opendate="2022-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce CI heap space by 25%</summary>
      <description>I've recently seen several builds where docker crashes again with out of memory errors.Our current memory configuration is a bit optimistic and relies on all forks not hitting the upper limit.If we'd reduce the heap space by 25% everything could run at maximum without memory running out.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-6-10 01:00:00" id="28006" opendate="2022-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run architecture tests in single JVM</summary>
      <description>The architecture tests, in particular the production ones, load a lot of classes into memory (~700mb). To prevent this from failing in the future I propose to run these tests in a single JVM that gets the entire memory budget.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-11-15 01:00:00" id="28083" opendate="2022-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PulsarSource cannot work with object-reusing DeserializationSchema.</summary>
      <description>This issue is the same as Kafka's https://issues.apache.org/jira/browse/FLINK-25132</description>
      <version>1.14.4,1.15.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StopCursorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarUnorderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarSourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.PulsarSourceReaderFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.message.PulsarMessageCollector.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.message.PulsarMessage.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarUnorderedFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarOrderedFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarFetcherManagerBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.emitter.PulsarRecordEmitter.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-7-16 01:00:00" id="28089" opendate="2022-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect support "tablesample (xx rows)"</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-7-16 01:00:00" id="28090" opendate="2022-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support attachAsDatastream in Python Table API</summary>
      <description>Implement attachAsDatastream. A pull request is submitted as this issue is created.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.util.java.utils.py</file>
      <file type="M">flink-python.pyflink.table.statement.set.py</file>
      <file type="M">docs.content.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.zh.docs.dev.table.data.stream.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-7-20 01:00:00" id="28136" opendate="2022-6-20 00:00:00" resolution="Done">
    <buginformation>
      <summary>Implement ExecutionTimeBasedSlowTaskDetector</summary>
      <description>In the first version of speculative execution, an ExecutionTimeBasedSlowTaskDetector will be used to detect slow tasks. For ExecutionTimeBasedSlowTaskDetector, if a task's execution time is much longer than that of most tasks of the same JobVertex, the task will be identified as slow. More specifically, it will compute an execution time baseline for each JobVertex. Tasks which execute longer than or equals to the baseline will be identified as slow tasks.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-8-20 01:00:00" id="28139" opendate="2022-6-20 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add documentation for speculative execution</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-6-20 01:00:00" id="28147" opendate="2022-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update httplib2 to at least 0.19.0</summary>
      <description>We should update httplib2 to at least 0.19.0 to address CVE-2021-21240 and avoid false flags about Flink being vulnerable.</description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-21 01:00:00" id="28161" opendate="2022-6-21 00:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce the session related API for REST endpoint</summary>
      <description>It includes openSession, closeSession and configure session. Please refer to FLIP-91 for API details.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-21 01:00:00" id="28162" opendate="2022-6-21 00:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce the operation related API for REST endpoint</summary>
      <description>It includes getOperationStatus, cancelOperation, closeOperation.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.HttpMethodWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-7-23 01:00:00" id="28222" opendate="2022-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add sql-csv/json modules</summary>
      <description>For consistency and maintainability the csv/json formats should have a dedicated sql-jar module.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.pom.xml</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-formats.flink-json.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-8-29 01:00:00" id="28296" opendate="2022-6-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive&amp;#39;s UDAF which implement GenericUDAFResolver</summary>
      <description>Some Hive's udaf implementt GenericUDAFResolver, but Flink only support the function implement UDAF andGenericUDAFResolver2</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDAFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDAF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.factories.HiveFunctionDefinitionFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-3-1 01:00:00" id="28347" opendate="2022-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update testcontainers dependency to v1.17.6</summary>
      <description>Changelog: https://github.com/testcontainers/testcontainers-java/releases/tag/1.17.6Main benefits for Flink: Elasticsearch and Pulsar improvements</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-7-1 01:00:00" id="28357" opendate="2022-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Watermark issue when recovering Finished sources</summary>
      <description>Copied mostly from email trail on the flink user mailing list:I done a lot of experimentation and Im convinced there is a problem with Flink handling Finished sources and recovery.The program consists of: Two sources: One Long Running Source  stays alive and emits a watermark of DateTime.now() every 10 seconds. Prints the console a message saying the watermark has been emitted. Throws an exception every 5 or 10 iterations to force a recovery. One Short Lived Source  emits a Long.MAX_VALUE watermark, prints a message to the console and returns. The Short Live Source feeds into a map() and then it joins with the Long Running Source with a KeyedCoProcessFunction. Moves to FINISHED state by Flink.The problem here is that the Join receives no Long.MAX_VALUE watermark from the map() in some situations after a recovery. The dashboard goes from showing this:To the below after a recovery (with the currentInput1/2Watermark metrics showing input 2 having not received a watermark from the map, saying Long.MAX_VALUE):The program is currently set to checkpoint every 5 seconds. By experimenting with 70 seconds, it seems that if only one checkpoint has been taken with the Short Lived Source in a FINISHED state since the last recovery then everything works fine and the restarted Short Lived Source emits its watermark and I see the ShortedLivedEmptySource emitting Long.MAX_VALUE watermark message on the console meaning the run() definitely executed. However, I found that if 2 or more checkpoints are taken since the last recovery with the source in a FINISHED state then the console message does not appear and the watermark is not emitted.To repeat  the Join does not get a Long.MAX_VALUE watermark from my source or Flink if I see two or more checkpoints logged in between recoveries. If zero or checkpoints are made, everything is fine  the join gets the watermark and I see my console message. You can play with the checkpointing frequency as per the code comments: // Useful checkpoint interval options: // 5 - see the problem after the first recovery // 70 - useful to see bad behaviour kick in after a recovery or two // 120 - won't see the problem as we don't have 2 checkpoints within a single recovery sessionIf I merge the Triggering/Completed checkpoint messages in the log with my console output I see something like this clearly showing the Short Lived Source run() method is not executed after 2 checkpoints with the operators marked as FINISHED:2022-06-29T11:52:31.268Z:ShortLivedEmptySourceemitting Long.MAX_VALUE watermark.2022-06-29T11:52:31.293Z: LongRunningSource emitting initial watermark=16565035512682022-06-29T11:52:41.302Z: LongRunningSource emitting loop watermark=16565035613022022-06-29T11:52:51.302Z: LongRunningSource emitting loop watermark=16565035713022022-06-29T11:53:01.303Z: LongRunningSource emitting loop watermark=16565035813032022-06-29 11:53:02.772 INFO &amp;#91;Checkpoint Timer&amp;#93; o.a.f.r.c.CheckpointCoordinator Triggering checkpoint 1 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD})2022-06-29 11:53:02.870 INFO &amp;#91;jobmanager-io-thread-10&amp;#93; o.a.f.r.c.CheckpointCoordinator Completed checkpoint 1 for job 877656d7752bc1304c2cb92790e6aefb2022-06-29T11:53:11.303Z: LongRunningSource emitting loop watermark=16565035913032022-06-29T11:53:21.304Z: LongRunningSource emitting loop watermark=16565036013042022-06-29T11:53:21.304Z: ------------------ Recovery ------------------2022-06-29T11:53:22.405Z: LongRunningSource emitting initial watermark=16565036024052022-06-29T11:53:22.408Z:ShortLivedEmptySourceemitting Long.MAX_VALUE watermark.2022-06-29T11:53:32.406Z: LongRunningSource emitting loop watermark=16565036124062022-06-29T11:53:42.406Z: LongRunningSource emitting loop watermark=16565036224062022-06-29 11:53:51.048 INFO &amp;#91;Checkpoint Timer&amp;#93; o.a.f.r.c.CheckpointCoordinator Triggering checkpoint 2 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD})2022-06-29 11:53:51.067 INFO &amp;#91;jobmanager-io-thread-4&amp;#93; o.a.f.r.c.CheckpointCoordinator Completed checkpoint 2 for job 877656d7752bc1304c2cb92790e6aefb2022-06-29T11:53:52.407Z: LongRunningSource emitting loop watermark=16565036324072022-06-29T11:54:02.407Z: LongRunningSource emitting loop watermark=16565036424072022-06-29T11:54:12.408Z: LongRunningSource emitting loop watermark=16565036524082022-06-29T11:54:22.408Z: LongRunningSource emitting loop watermark=16565036624082022-06-29T11:54:32.409Z: LongRunningSource emitting loop watermark=16565036724092022-06-29T11:54:42.409Z: LongRunningSource emitting loop watermark=16565036824092022-06-29T11:54:52.410Z: LongRunningSource emitting loop watermark=16565036924102022-06-29 11:55:01.048 INFO &amp;#91;Checkpoint Timer&amp;#93; o.a.f.r.c.CheckpointCoordinator Triggering checkpoint 3 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD})2022-06-29 11:55:01.057 INFO &amp;#91;jobmanager-io-thread-10&amp;#93; o.a.f.r.c.CheckpointCoordinator Completed checkpoint 3 for job 877656d7752bc1304c2cb92790e6aefb2022-06-29T11:55:02.410Z: LongRunningSource emitting loop watermark=16565037024102022-06-29T11:55:02.411Z: ------------------ Recovery ------------------2022-06-29T11:55:03.445Z: LongRunningSource emitting initial watermark=1656503703444&lt;&lt;&lt;&lt;&lt; NO ShortLivedEmptySource message after recovery2022-06-29T11:55:13.446Z: LongRunningSource emitting loop watermark=16565037134452022-06-29T11:55:23.446Z: LongRunningSource emitting loop watermark=16565037234462022-06-29T11:55:33.446Z: LongRunningSource emitting loop watermark=1656503733446I have also attached a longer example with shows everything working fine after 5 recoveries, and then breaking after the 6th.I am guessing here it has something to do with the checkpointing and recovery of a FINISHED source.Finally, here are some ways that allows the code to work: Change the code so the Short Lived Source doesnt return from run() and stays RUNNING (uncomment the Thread.sleep) As I mentioned before, if I remove the map() operator the problem in the join also goes away. (I dont see the console output but the join is happy) Use a long enough checkpoint interval (e.g. 120 seconds) so we dont have two checkpoints with FINISHED state per recovery.The fact these changes prevent the issue means I really think theres some bug or inconsistency here  if somebody could explain I would really appreciate it.</description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0,1.15.2,1.14.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2022-1-9 01:00:00" id="28475" opendate="2022-7-9 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>kafka connector won&amp;#39;t stop when the stopping offset is zero</summary>
      <description>when use kafka connector in bounded mode,and the stopping offset hapends to be 0,the kafka connector won't stop,which is not expected.I had traced the code, and found the stopping offset will be set to empty when it is zero, and an empty stopping offset means no stopping offset when serialized. This leads to a wrong execution.I had fixed this in my personal branch,now I am logging this issue in Jira so that I can make merge request.</description>
      <version>1.15.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.split.KafkaPartitionSplit.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-12-21 01:00:00" id="28617" opendate="2022-7-21 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support stop job statement in SqlGatewayService</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestUtils.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.utils.Constants.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-7-21 01:00:00" id="28636" opendate="2022-7-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add utility to test POJO compliance</summary>
      <description>Users should be encouraged to eagerly verify that their POJOs satisfy all the requirements that Flink imposes, however we provide no convenient way to test that.They currently have to resort to something like below, which isn't obvious at all:TypeSerializer&lt;Event&gt; eventSerializer = TypeInformation.of(Event.class).createSerializer(new ExecutionConfig());assertThat(eventSerializer).isInstanceOf(PojoSerializer.class);</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.serialization.types.serialization.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.serialization.types.serialization.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-24 01:00:00" id="28655" opendate="2022-7-24 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support show jobs statement in SqlGatewayService</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.utils.Constants.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-24 01:00:00" id="28658" opendate="2022-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for job lifecycle statements</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0,1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.docs.dev.table.sql.jar.md</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.docs.dev.table.sql-gateway.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.jar.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-7-29 01:00:00" id="28743" opendate="2022-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support validating the determinism for StreamPhysicalMatchRecognize</summary>
      <description>MatchRecognize has complex expressions and is not commonly used in traditional SQLs, so mark this as a minor issue (for 1.16)</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.NonDeterministicDagTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.NonDeterministicDagTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.optimize.StreamNonDeterministicUpdatePlanVisitor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-6-29 01:00:00" id="28744" opendate="2022-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite version to 1.31</summary>
      <description>We should upgrade to Calcite 1.31 so we can benefit from https://issues.apache.org/jira/browse/CALCITE-4865</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.subquery.SubqueryCorrelateVariablesValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.subquery.SubqueryCorrelateVariablesValidationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.ShuffleMergeJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.ShuffleHashJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.NestLoopJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.BroadcastJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.tools.RelBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.SqlValidatorImpl.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql2rel.SqlToRelConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql2rel.RelDecorrelator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalWindow.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalValues.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalUnion.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalSort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalMinus.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalIntersect.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalFilter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.hint.HintPredicates.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Window.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Values.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Union.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Sort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Snapshot.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.SetOp.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Minus.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Intersect.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Filter.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-calcite-bridge.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkDDLDataTypeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.validate.FlinkSqlConformance.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-1 01:00:00" id="28759" opendate="2022-8-1 00:00:00" resolution="Done">
    <buginformation>
      <summary>Enable speculative execution for in AdaptiveBatchScheduler TPC-DS e2e tests</summary>
      <description>To verify the correctness of speculative execution, we can enabled it in AdaptiveBatchScheduler TPC-DS e2e tests, which runs a lot of different batch jobs and verifies the result.Note that we need to disable the blocklist (by setting block duration to 0) in such single machine e2e tests.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-8-5 01:00:00" id="28821" opendate="2022-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adjust join cost for dpp query pattern which could help more plans use dpp</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRule.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-5 01:00:00" id="28841" opendate="2022-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document dynamic property support for startup scripts</summary>
      <description>The support for dynamic properties in startup scripts isn't documented anywhere.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.docker.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-10-6 01:00:00" id="29531" opendate="2022-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump protoc and protobuf-java dependencies to 3.21.7</summary>
      <description>Bump protoc and protobuf-java dependencies to at least 3.21.7 to avoid false positive scans on Protobuf vulnerabilities</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
      <file type="M">flink-formats.flink-sql-protobuf.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-10-6 01:00:00" id="29532" opendate="2022-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Pulsar dependency to 2.10.1</summary>
      <description>Update the Pulsar dependency to 2.10.1 to benefit of the fixes highlights at https://github.com/apache/pulsar/releases/tag/v2.10.1</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-31 01:00:00" id="29803" opendate="2022-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API Scala APIs lack proper source jars</summary>
      <description/>
      <version>1.15.0</version>
      <fixedVersion>1.17.0,1.15.3,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-scala.pom.xml</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-12-30 01:00:00" id="30250" opendate="2022-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The flame graph type is wrong</summary>
      <description>When the flame graph type is switched from On-CPU to Mixed. It still show the graph of On-CPU.Root cause:When click the other types, the web frontend will call the requestFlameGraph and update the graphType. However, the graphType is the old type during requestFlameGraph. So the graph type show the new type, but the flame graph is the result of old type.code link</description>
      <version>1.15.0,1.16.0,1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-flamegraph.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.components.flame-graph.flame-graph.component.ts</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-2-11 01:00:00" id="3216" opendate="2016-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define pattern specification</summary>
      <description>In order to detect event patterns we first have to define the pattern. This issue tracks the progress of implementing a user facing API to define event patterns. Patterns should support the following operations next(): The given event has to follow directly after the preceding eventfollowedBy(): The given event has to follow the preceding event. There might occur other events in-between every(): In a follow-by relationship a starting event can be matched with multiple successive events. Consider the pattern a  b where  denotes the follow-by relationship. The event sequence a, b, b can be matched as a, b or a, (b), b where the first b is left out. The essential question is whether a is allowed to match multiple times or only the first time. The method every specifies exactly that. Every events in a pattern can match with multiple successive events. This makes only sense in a follow-by relationship, though. followedByEvery(): Similar to followedBy just that the specified element can be matched with multiple successive events or(): Alternative event which can be matched instead of the original event: every(e1).where().or(e2).where() within(): Defines a time interval in which the pattern has to be completed, otherwise an incomplete pattern can be emitted (timeout case)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-libraries.pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>