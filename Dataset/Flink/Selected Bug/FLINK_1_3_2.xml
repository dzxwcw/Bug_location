<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2017-7-17 01:00:00" id="7211" opendate="2017-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude Gelly javadoc jar from release</summary>
      <description/>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-18 01:00:00" id="7220" opendate="2017-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update RocksDB dependency to 5.5.5</summary>
      <description>The latest release of RocksDB (5.5.5) fixes the issues from previous versions (slow merge performance, segfaults) in connection with Flink and seems stable for us to use. We can move away from our custom FRocksDB build, back to the latest release.</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.RocksDBPerformanceTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.PredefinedOptions.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-8-3 01:00:00" id="7363" opendate="2017-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add hashes and signatures to the download page</summary>
      <description>As part of the releases, we also generate MD5 hashes and cryptographic signatures but neither link to those nor do we explain which keys are valid release-signing keys. This should be added.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualProducerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualConsumerProducerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.examples.ProduceIntoKinesis.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ProducerConfigConstants.java</file>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-3 01:00:00" id="7366" opendate="2017-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade kinesis producer library in flink-connector-kinesis</summary>
      <description>We need to upgrade KPL and KCL to pick up the enhanced performance and stability for Flink to work better with Kinesis. Upgrading KPL is specially necessary, because the KPL version Flink uses is old, and doesn't have good retry and error handling logic.KPL:flink-connector-kinesis currently uses kinesis-producer-library 0.10.2, which is released in Nov 2015 by AWS. It's old. It's the fourth release, and thus problematic. It doesn't even have good retry logic, therefore Flink fails really frequently (about every 10 mins as we observed) when Flink writes too fast to Kinesis and receives RateLimitExceededException, Quotes from https://github.com/awslabs/amazon-kinesis-producer/issues/56, "With the newer version of the KPL it uses the AWS C++ SDK which should offer additional retries." on Oct 2016. 0.12.5, the version we are upgrading to, is released in May 2017 and should have the enhanced retry logic.</description>
      <version>1.3.2</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-9-10 01:00:00" id="7422" opendate="2017-8-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Kinesis Client Library (KCL) and AWS SDK in flink-connector-kinesis</summary>
      <description>Upgrade KCL from 1.6.2 to 1.8.1 (https://mvnrepository.com/artifact/com.amazonaws/amazon-kinesis-client)Since FLINK-7366, we may also need to bump aws sdk version as well in this ticket.KCL:Upgrade KCL from 1.6.2 to 1.8.1AWS SDKfrom 1.10.71 to 1.11.171</description>
      <version>1.3.2</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-14 01:00:00" id="7441" opendate="2017-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Double quote string literals is not supported in Table API and SQL</summary>
      <description>Code generation doesn't handle double quote string literals and some control characters which leads to compile error.Caused by: org.codehaus.commons.compiler.CompileException: Line 50, Column 48: Expression "hello" is not an rvalue</description>
      <version>1.3.2</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-14 01:00:00" id="7442" opendate="2017-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option for using a child-first classloader for loading user code</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerComponentsStartupShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.JobManagerLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManagerTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.FlinkMiniCluster.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.client.JobClient.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDbMultiClassLoaderTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.JobWithJars.java</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-3-31 01:00:00" id="7564" opendate="2017-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Watermark semantics in Table API</summary>
      <description>For reference, see https://lists.apache.org/thread.html/3541e72ba3842192e58a487e54c2817f6b2b9d12af5fee97af83e5df@%3Cdev.flink.apache.org%3E.</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.OverWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeUnboundedOver.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-1 01:00:00" id="7568" opendate="2017-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bring window documentation up-to-date with latest changes and improve</summary>
      <description>Off the top of my head: Make ProcessWindowFunction the primary window function, threat WindowFunction as legacy Document more specifically how windowing behaves: window boundaries, chaining of several windowed operations, timestamps of emitted data, what is "key" in a ProcessWindowFunction...</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.windows.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-3-7 01:00:00" id="7596" opendate="2017-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix bug when Set Operation handles ANY type</summary>
      <description>If two inputs with Any(GenericRelDataType), when they comes to Set Operation(UNION, MINUS,...), it will cause a TableException with info is "Type is not supported: ANY"Here is the test case:@Test def testUnion(): Unit = { val list = List((1, new NODE), (2, new NODE)) val list2 = List((3, new NODE), (4, new NODE)) val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env) val s1 = tEnv.fromDataStream(env.fromCollection(list)) val s2 = tEnv.fromDataStream(env.fromCollection(list2)) val result = s1.unionAll(s2).toAppendStream[Row] result.addSink(new StreamITCase.StringSink[Row]) env.execute() } class NODE { val x = new util.HashMap[String, String]()}This bug happens because Flink doesn't handle createSqlType(ANY) and Calcite doesn't know the differences between ANY and ANY(GenericRelDataType), so the createSqlType(ANY) of Calcite will return a BasicSqlType instead.</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-12-8 01:00:00" id="7603" opendate="2017-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support time constraint in MatchRecognize</summary>
      <description>It is a common use case to search for patterns within some time limit. This also allows more fine grained state control.One option is to support WITHIN clause provided by CALCITE, the downside of this approach is that it is not SQL standard compliant.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.match.PatternTranslatorTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.match.PatternTranslatorTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamMatch.scala</file>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-10-14 01:00:00" id="7623" opendate="2017-9-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Detecting whether an operator is restored doesn&amp;#39;t work with chained state</summary>
      <description>Originally reported on the ML: https://lists.apache.org/thread.html/22a2cf83de3107aa81a03a921325a191c29df8aa8676798fcd497199@%3Cuser.flink.apache.org%3EIf we have a chain of operators where multiple of them have operator state, detection of the context.isRestored() flag (of CheckpointedFunction) does not work correctly. It's best exemplified using this minimal example where both the source and the flatMap have state:final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env .addSource(new MaSource()).uid("source-1") .flatMap(new MaFlatMap()).uid("flatMap-1");env.execute("testing");If I do a savepoint with these UIDs, then change "source-1" to "source-2" and restore from the savepoint context.isRestored() still reports true for the source.</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-16 01:00:00" id="7632" opendate="2017-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better documentation and examples on C* sink usage for Pojo and Tuples data types</summary>
      <description>Cassandra sink supports Pojo and Java Tuple data types. We should improve documentation on its usage as well as some concrete / meaningful examples for both cases.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.cassandra.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-9-20 01:00:00" id="7649" opendate="2017-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port JobStoppingHandler to new REST endpoint</summary>
      <description>Port existing JobStoppingHandler to new REST endpoint</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobMessageParameters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobCancellationHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.JobCancellationHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-20 01:00:00" id="7658" opendate="2017-9-20 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support COLLECT Aggregate function in Flink TABLE API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.AggregateStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.aggregations.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-23 01:00:00" id="7678" opendate="2017-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL UserDefineTableFunction does not take CompositeType input correctly</summary>
      <description>UDF is using FlinkTypeFactory to infer operand type while UDTF does not go through the same code path. This result in:Unable to find source-code formatter for language: console. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yamlorg.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 38 to line 1, column 44: No match found for function signature func(&lt;COMPOSITE(Row(f0: Integer, f1: String))&gt;) Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 38 to line 1, column 44: No match found for function signature func(&lt;COMPOSITE(Row(f0: Integer, f1: String))&gt;)Please see github code for more info:https://github.com/walterddr/flink/blob/bug_report/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/batch/sql/UDTFCompositeTypeTestFailure.scala</description>
      <version>1.3.2</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.UserDefinedTableFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.CorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.userDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.CorrelateTest.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-23 01:00:00" id="7679" opendate="2017-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade maven enforcer plugin to 3.0.0-M1</summary>
      <description>I got the following build error against Java 9:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.4.1:enforce (enforce-maven) on project flink-parent: Execution enforce-maven of goal org.apache.maven.plugins:maven-enforcer-plugin:1.4.1:enforce failed: An API incompatibility was encountered while executing org.apache.maven.plugins:maven-enforcer-plugin:1.4.1:enforce: java.lang.ExceptionInInitializerError: null[ERROR] -----------------------------------------------------[ERROR] realm = plugin&gt;org.apache.maven.plugins:maven-enforcer-plugin:1.4.1[ERROR] strategy = org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy[ERROR] urls[0] = file:/home/hbase/.m2/repository/org/apache/maven/plugins/maven-enforcer-plugin/1.4.1/maven-enforcer-plugin-1.4.1.jarUpgrading maven enforcer plugin to 3.0.0-M1 would get over the above error.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.force-shading.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-27 01:00:00" id="7701" opendate="2017-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>IllegalArgumentException in Netty bootstrap with small memory state segment size</summary>
      <description>FLINK-7258 broke setting high and low watermarks for small segment sizes. We should tackle both use cases.</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyServerLowAndHighWatermarkTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyServer.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.AbstractServerBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-27 01:00:00" id="7702" opendate="2017-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadocs are not being built</summary>
      <description>The "Javadocs" link in the left side menu of this page doesn't work:https://ci.apache.org/projects/flink/flink-docs-master/Note that it works in 1.3:https://ci.apache.org/projects/flink/flink-docs-release-1.3/</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-filesystem.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-29 01:00:00" id="7738" opendate="2017-9-29 00:00:00" resolution="Abandoned">
    <buginformation>
      <summary>Create WebSocket handler (server)</summary>
      <description>An abstract handler is needed to support websocket communication.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.triggers.Trigger.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-10-1 01:00:00" id="7744" opendate="2017-10-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing top links to documentation</summary>
      <description>The links to the top of the page are missing on many pages. Those are very useful for reading the documentation on mobile.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.testing.md</file>
      <file type="M">docs.search-results.md</file>
      <file type="M">docs.quickstart.setup.quickstart.md</file>
      <file type="M">docs.quickstart.scala.api.quickstart.md</file>
      <file type="M">docs.quickstart.run.example.quickstart.md</file>
      <file type="M">docs.quickstart.java.api.quickstart.md</file>
      <file type="M">docs.ops.upgrading.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.state.savepoints.md</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">docs.ops.state.checkpoints.md</file>
      <file type="M">docs.ops.security-ssl.md</file>
      <file type="M">docs.ops.security-kerberos.md</file>
      <file type="M">docs.ops.production.ready.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
      <file type="M">docs.ops.deployment.mapr.setup.md</file>
      <file type="M">docs.ops.deployment.gce.setup.md</file>
      <file type="M">docs.ops.deployment.aws.md</file>
      <file type="M">docs.ops.config.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.monitoring.rest.api.md</file>
      <file type="M">docs.monitoring.logging.md</file>
      <file type="M">docs.monitoring.historyserver.md</file>
      <file type="M">docs.monitoring.debugging.event.time.md</file>
      <file type="M">docs.monitoring.debugging.classloading.md</file>
      <file type="M">docs.monitoring.checkpoint.monitoring.md</file>
      <file type="M">docs.monitoring.back.pressure.md</file>
      <file type="M">docs.monitoring.application.profiling.md</file>
      <file type="M">docs.internals.task.lifecycle.md</file>
      <file type="M">docs.internals.stream.checkpointing.md</file>
      <file type="M">docs.internals.job.scheduling.md</file>
      <file type="M">docs.internals.ide.setup.md</file>
      <file type="M">docs.internals.filesystems.md</file>
      <file type="M">docs.internals.components.md</file>
      <file type="M">docs.dev.types.serialization.md</file>
      <file type="M">docs.dev.batch.connectors.md</file>
      <file type="M">docs.dev.batch.dataset.transformations.md</file>
      <file type="M">docs.dev.batch.examples.md</file>
      <file type="M">docs.dev.batch.hadoop.compatibility.md</file>
      <file type="M">docs.dev.batch.iterations.md</file>
      <file type="M">docs.dev.batch.zip.elements.guide.md</file>
      <file type="M">docs.dev.best.practices.md</file>
      <file type="M">docs.dev.cluster.execution.md</file>
      <file type="M">docs.dev.connectors.cassandra.md</file>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
      <file type="M">docs.dev.connectors.filesystem.sink.md</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
      <file type="M">docs.dev.connectors.kinesis.md</file>
      <file type="M">docs.dev.connectors.nifi.md</file>
      <file type="M">docs.dev.connectors.rabbitmq.md</file>
      <file type="M">docs.dev.connectors.twitter.md</file>
      <file type="M">docs.dev.custom.serializers.md</file>
      <file type="M">docs.dev.event.time.md</file>
      <file type="M">docs.dev.event.timestamps.watermarks.md</file>
      <file type="M">docs.dev.event.timestamp.extractors.md</file>
      <file type="M">docs.dev.java8.md</file>
      <file type="M">docs.dev.libs.cep.md</file>
      <file type="M">docs.dev.libs.gelly.bipartite.graph.md</file>
      <file type="M">docs.dev.libs.ml.als.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.md</file>
      <file type="M">docs.dev.libs.ml.cross.validation.md</file>
      <file type="M">docs.dev.libs.ml.distance.metrics.md</file>
      <file type="M">docs.dev.libs.ml.knn.md</file>
      <file type="M">docs.dev.libs.ml.min.max.scaler.md</file>
      <file type="M">docs.dev.libs.ml.multiple.linear.regression.md</file>
      <file type="M">docs.dev.libs.ml.optimization.md</file>
      <file type="M">docs.dev.libs.ml.pipelines.md</file>
      <file type="M">docs.dev.libs.ml.polynomial.features.md</file>
      <file type="M">docs.dev.libs.ml.quickstart.md</file>
      <file type="M">docs.dev.libs.ml.sos.md</file>
      <file type="M">docs.dev.libs.ml.standard.scaler.md</file>
      <file type="M">docs.dev.libs.ml.svm.md</file>
      <file type="M">docs.dev.libs.storm.compatibility.md</file>
      <file type="M">docs.dev.linking.md</file>
      <file type="M">docs.dev.local.execution.md</file>
      <file type="M">docs.dev.migration.md</file>
      <file type="M">docs.dev.scala.api.extensions.md</file>
      <file type="M">docs.dev.scala.shell.md</file>
      <file type="M">docs.dev.stream.operators.asyncio.md</file>
      <file type="M">docs.dev.stream.operators.windows.md</file>
      <file type="M">docs.dev.stream.side.output.md</file>
      <file type="M">docs.dev.stream.state.custom.serialization.md</file>
      <file type="M">docs.dev.stream.state.queryable.state.md</file>
      <file type="M">docs.dev.stream.state.state.md</file>
      <file type="M">docs.dev.stream.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-2 01:00:00" id="7755" opendate="2017-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Null values are not correctly handled by batch inner and outer joins</summary>
      <description>Join predicates of batch joins are not correctly evaluated according to three-value logic.This affects inner as well as outer joins.The problem is that some equality predicates are only evaluated by the internal join algorithms of Flink which are based on TypeComparator. The field TypeComparator for Row are implemented such that null == null results in TRUE to ensure correct ordering and grouping. However, three-value logic requires that null == null results to UNKNOWN (or null). The code generator implements this logic correctly, but for equality predicates, no code is generated.For outer joins, the problem is a bit tricker because these do not support code-generated predicates yet (see FLINK-5520). FLINK-5498 proposes a solution for this issue.We also need to extend several of the existing tests and add null values to ensure that the join logic is correctly implemented.</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.validation.JoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.FunctionCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-5 01:00:00" id="7765" opendate="2017-10-5 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Enable dependency convergence</summary>
      <description>For motivation check https://issues.apache.org/jira/browse/FLINK-7739SubTasks of this task depends on one another - to enable convergence in `flink-runtime` it has to be enabled for `flink-shaded-hadoop` first.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-libraries.pom.xml</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-fs-tests.pom.xml</file>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-3-17 01:00:00" id="7853" opendate="2017-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reject table function outer joins with predicates in Table API</summary>
      <description>Due to CALCITE-2004, the table function outer joins can not be normally executed. We should cover it up by rejecting join predicates temporarily, until the issue is fixed in Calcite.</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.CorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.CorrelateValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-17 01:00:00" id="7854" opendate="2017-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reject lateral table outer joins with predicates in SQL</summary>
      <description>Due to CALCITE-2004, lateral table outer joins can not be normally executed. We should cover it up by rejecting join predicates temporarily, until the issue is fixed in Calcite.</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkCalciteSqlValidator.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-18 01:00:00" id="7866" opendate="2017-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Weigh list of preferred locations for scheduling</summary>
      <description>Sihua Zhou proposed to not only use the list of preferred locations to decide where to schedule a task, but to also weigh the list according to how often a location appeared and then select the location based on the weight. That way, we would obtain better locality in some cases.Example:Preferred locations list: &amp;#91;location1, location2, location2&amp;#93;Weighted preferred locations list &amp;#91;(location2 , 2), (location1, 1)&amp;#93;</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.SlotProfileTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.SlotProfile.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-9 01:00:00" id="795" opendate="2014-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Possibly extend the cost model of the optimizer</summary>
      <description>I have started the task to integrate the AbstractCachedBuildSideMatchDriver into the optimizer. The driver caches one side of the join and thereby can accellerate iterations if there are joins with static (non-changing) datasets inside the iteration.The current way of calculating the cost of operators inside of iterations is basically to multiply them by the number of iterations. I would like to propose to extend this to have one static part of costs, that is counted only once for all iterations, and one dynamic part that is multiplied by the number of iterations.In my opinion that would be the cleanest way to intergrate the cached match, by assigning it a higher starting cost then the regular match and a cheaper dynamic part.One other approach would be to always use the cached match inside of iterations. For that I would probably have to add a new RequestedLocalProperty that tells the optimizer if the operator is used inside of a iteration.A simple hacked solution could also be to simply exchange all suitable regular matches inside of an iteration by the cached alternative.What do you think is the best approach?---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/795Created by: markus-hLabels: Created at: Mon May 12 18:51:51 CEST 2014State: open</description>
      <version>None</version>
      <fixedVersion>0.6-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">stratosphere-compiler.src.test.java.eu.stratosphere.pact.compiler.UnionReplacementTest.java</file>
      <file type="M">stratosphere-compiler.src.test.java.eu.stratosphere.pact.compiler.CompilerTestBase.java</file>
      <file type="M">stratosphere-compiler.src.main.java.eu.stratosphere.compiler.dag.PactConnection.java</file>
      <file type="M">stratosphere-compiler.src.main.java.eu.stratosphere.compiler.dag.OptimizerNode.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-3 01:00:00" id="7973" opendate="2017-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix service shading relocation for S3 file systems</summary>
      <description>The shade plugin relocates services incorrectly currently, applying relocation patterns multiple times.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test-infra.end-to-end-test.common.sh</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.README.md</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.README.md</file>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-5 01:00:00" id="8199" opendate="2017-12-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Annotation for Elasticsearch connector</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.IndexRequestBuilderWrapperFunction.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.Elasticsearch1ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.Elasticsearch5ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.OldNewRequestIndexerBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.OldNewElasticsearchSinkFunctionBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.Elasticsearch2ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.util.RetryRejectedExecutionFailureHandler.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.util.NoOpFailureHandler.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.util.ElasticsearchUtils.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.BulkProcessorIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ActionRequestFailureHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-12-18 01:00:00" id="8278" opendate="2017-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala examples in Metric documentation do not compile</summary>
      <description>The Scala examples in the Metrics documentation do not compile.The line @transient private var counter: Counterneeds to be extended to@transient private var counter: Counter = _</description>
      <version>1.3.2,1.4.0,1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">docs.dev.stream.state.state.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-22 01:00:00" id="8479" opendate="2018-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement time-bounded inner join of streams as a TwoInputStreamOperator</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TestHarnessUtil.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-3-3 01:00:00" id="8847" opendate="2018-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modules containing package-info.java are always recompiled</summary>
      <description>All modules that contain a package-info.java file (that do not contain annotations which applies to all instances in Flink) will always be recompiled by the maven-compiler-plugin.To detect modified files the compiler compares timestamps of the source and .class file. In the case of package-info.java no .class file is created if it doesn't contain annotations, which the compiler interprets as a missing .class file.We can add -Xpkginfo:always to the compiler configuration to force the generation of these files to prevent this from happening.</description>
      <version>1.3.2,1.4.1,1.5.0</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-8 01:00:00" id="8897" opendate="2018-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rowtime materialization causes "mismatched type" AssertionError</summary>
      <description>As raised in this thread, the query created by the following code will throw a calcite "mismatch type" (Timestamp(3) and TimeIndicator) exception.String sql1 = "select id, eventTs as t1, count(*) over (partition by id order by eventTs rows between 100 preceding and current row) as cnt1 from myTable1";String sql2 = "select distinct id as r_id, eventTs as t2, count(*) over (partition by id order by eventTs rows between 50 preceding and current row) as cnt2 from myTable2";Table left = tableEnv.sqlQuery(sql1);Table right = tableEnv.sqlQuery(sql2);left.join(right).where("id === r_id &amp;&amp; t1 === t2").select("id, t1").writeToSink(...)The logical plan is as follows.LogicalProject(id=[$0], t1=[$1]) LogicalFilter(condition=[AND(=($0, $3), =($1, $4))]) LogicalJoin(condition=[true], joinType=[inner]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]])That is because the the rowtime field after an aggregation will be materialized while the RexInputRef type for the filter's operands (t1 === t2) is still TimeIndicator. We should make them unified.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamWindowJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-1 01:00:00" id="9499" opendate="2018-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow REST API for running a job to provide job configuration as body of POST request</summary>
      <description>Based on this documentation, the REST API provides a way to submit a request for running a Flink job. The POST request must include the job configuration information as query parameters using the documented parameter names ("program-args", "entry-class", "parallelism", etc.) Depending on the job parameters, the full URL for the POST request can reach a size that is over the maximum size (currently at 4096 bytes) of what is allowed by the configuration of Netty. To overcome this, it would be useful to allow users to provide the job configuration not only as query parameters but also as POST parameters. For the most part, it is the "program-args" parameter that can make the URL grow in size based on the needs of the developer and the job. All other attributes should be pretty constant.  </description>
      <version>1.3.2</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.hs.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.submit.submit.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.submit.submit.ctrl.coffee</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.pom.xml</file>
      <file type="M">docs..includes.generated.rest.dispatcher.html</file>
    </fixedFiles>
  </bug>
</bugrepository>