<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2015-8-7 01:00:00" id="3138" opendate="2015-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Method References are not supported as lambda expressions</summary>
      <description>For many functions (here for example KeySelectors), one can use lambda expressions:DataStream&lt;MyType&gt; stream = ...;stream.keyBy( v -&gt; v.getId() )Java's other syntax for this, Method References, do not work:DataStream&lt;MyType&gt; stream = ...;stream.keyBy( MyType::getId )</description>
      <version>0.10.2</version>
      <fixedVersion>1.0.0,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java8.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.MapITCase.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.api.java.type.lambdas.LambdaExtractionTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-16 01:00:00" id="3421" opendate="2016-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove all unused ClassTag context bounds in the Streaming Scala API</summary>
      <description>Many methods have a ClassTag as a context bound, but they are never used / needed.The same information as can be drawn from a ClassTag should also be available in the TypeInformation.</description>
      <version>0.10.2</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedStream.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.KeyedStream.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.JoinedStreams.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.ConnectedStreams.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.CoGroupedStreams.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.AllWindowedStream.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-2-18 01:00:00" id="3440" opendate="2016-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka should also checkpoint partitions where no initial offset was retrieved</summary>
      <description>For the discussion, see here: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Kafka-partition-alignment-for-event-time-td4782.html#a4998</description>
      <version>0.10.2</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08ITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.LegacyFetcher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-23 01:00:00" id="3475" opendate="2016-2-23 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>DISTINCT aggregate function support for SQL queries</summary>
      <description>DISTINCT aggregate function may be able to reuse the aggregate function instead of separate implementation, and let Flink runtime take care of duplicate records.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.QueryDecorrelationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateWithNullValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-25 01:00:00" id="3503" opendate="2016-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ProjectJoinTransposeRule fails to push down project.</summary>
      <description>val ds1 = CollectionDataSets.getSmall3TupleDataSet(env).as('a, 'b, 'c) val ds2 = CollectionDataSets.get5TupleDataSet(env).as('d, 'e, 'f, 'g, 'h) val joinT = ds1.join(ds2).where('b === 'e).select('c, 'g)For this query, ProjectJoinTransposeRule should pushes a Project past a Join by splitting the projection into a projection on top of each child of the join.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.TranslationContext.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.DataSetTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-25 01:00:00" id="3504" opendate="2016-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Join fails while have expression inside join condition.</summary>
      <description>val ds1 = CollectionDataSets.get3TupleDataSet(env).as('a, 'b, 'c) val ds2 = CollectionDataSets.get5TupleDataSet(env).as('d, 'e, 'f, 'g, 'h) val joinT = ds1.join(ds2).filter('a + 3 === 'd).select('c, 'g)This query would throw exception:Caused by: org.apache.flink.api.table.TableException: Joins should have at least one equality condition at org.apache.flink.api.table.plan.rules.dataset.DataSetJoinRule.convert(DataSetJoinRule.scala:57) at org.apache.calcite.rel.convert.ConverterRule.onMatch(ConverterRule.java:116) at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:228) ... 44 moreThere are 2 issues here: DataSetJoinRule does not support expression inside join condition. JoinPushExpressionsRulewould add a Project to calculate expression value before Join, so the join condition does not include expression any more, however, it's not returned after the logical optimization.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.logical.FlinkJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.JavaBatchTranslator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-25 01:00:00" id="3508" opendate="2016-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more test cases to verify the rules of logical plan optimization</summary>
      <description>We have enabled many rules in logical plan optimization phase, more complicated test cases should be added to verify whether these rules actally work.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.SelectITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.FilterITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.AggregationsITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-25 01:00:00" id="3517" opendate="2016-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Number of job and task managers not checked in scripts</summary>
      <description>The start up scripts determine whether a job or task manager is running via a pids file. If a process, which is part of the pid file, is destroyed (for example on failure) outside of the scripts, a warning for multiple job managers are printed even though they are not running.</description>
      <version>None</version>
      <fixedVersion>1.0.0,1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-daemon.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-26 01:00:00" id="3520" opendate="2016-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Periodic watermark operator should emit current watermark in close()</summary>
      <description>This makes sure that for bounded data sets with watermarks, the final elements get properly reflected in window results</description>
      <version>0.10.2</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPeriodicWatermarksOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-26 01:00:00" id="3526" opendate="2016-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ProcessingTime Window Assigner and Trigger are broken</summary>
      <description/>
      <version>0.10.2</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.triggers.ProcessingTimeTrigger.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-4-27 01:00:00" id="3529" opendate="2016-2-27 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add pull request template</summary>
      <description>Add a template for pull requests, checking if prerequisites of opening a PR have been fulfilled.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CONTRIBUTING.md</file>
    </fixedFiles>
  </bug>
</bugrepository>