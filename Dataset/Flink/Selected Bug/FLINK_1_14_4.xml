<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  <bug fixdate="2021-10-9 01:00:00" id="24492" opendate="2021-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect implicit type conversion between numeric and (var)char</summary>
      <description>The result of the sql "select 1 = '1'" is false. This is caused by the CodeGen. CodeGen  incorrectly transform this "=" to "BinaryStringData.equals (int 1)". And "&lt;&gt;" has the same wrong result.In my opinion, "=" should have the same behavior with "&gt;" and "&lt;", which have the correct results. So before calcite solves this bug or flink supports this kind of implicit type conversion, we'd better temporarily forbidding this implicit type conversion in "=" and "&lt;&gt;".</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionTypeCoerceRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-5-9 01:00:00" id="26052" opendate="2022-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update chinese documentation regarding FLIP-203</summary>
      <description>Relevant english commits: c1f5c5320150402fc0cb4fbf3a31f9a27b1e4d9a cd8ea8d5b207569f68acc5a3c8db95cd2ca47ba6</description>
      <version>None</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.ops.state.savepoints.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-9 01:00:00" id="26053" opendate="2022-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix parser generator warnings</summary>
      <description>When building flink-sql-parser, the javacc logs a couple of warnings:Warning: Choice conflict in [...] construct at line 2920, column 5. Expansion nested within construct and expansion following construct have common prefixes, one of which is: "PLAN" Consider using a lookahead of 2 or more for nested expansion.Warning: Choice conflict involving two expansions at line 2930, column 9 and line 2932, column 9 respectively. A common prefix is: "STATEMENT" Consider using a lookahead of 2 for earlier expansion.Warning: Choice conflict involving two expansions at line 2952, column 9 and line 2954, column 9 respectively. A common prefix is: "STATEMENT" Consider using a lookahead of 2 for earlier expansion.Took from https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30871&amp;view=logs&amp;j=0c940707-2659-5648-cbe6-a1ad63045f0a&amp;t=075c2716-8010-5565-fe08-3c4bb45824a4&amp;l=3911We should investigate them, as they're often symptom of some bug in our parser template, which might result in unexpected parsing errors.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-9 01:00:00" id="26054" opendate="2022-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable maven-enforcer to disable table-planner and table-runtime as dependencies</summary>
      <description>From https://github.com/apache/flink/pull/18676#discussion_r802502438, we should enable the enforcer for table-planner and table-runtime modules, as described in the flink-table/README.md</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner-loader-bundle.pom.xml</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  
  
  
  <bug fixdate="2022-4-31 01:00:00" id="26961" opendate="2022-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update multiple Jackson dependencies to v2.13.2 and v2.13.2.1</summary>
      <description>There is a High CVE-2020-36518, https://github.com/advisories/GHSA-57j2-w4cx-62h2which was fixed with 2.13.2.1</description>
      <version>None</version>
      <fixedVersion>1.14.5,1.15.0,elasticsearch-3.0.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-4-12 01:00:00" id="27196" opendate="2022-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove old format interfaces</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.TableFormatFactoryBaseTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.TableFormatFactoryBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.TableFormatFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.TableFactoryService.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.SerializationSchemaFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.DeserializationSchemaFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.descriptors.SchemaValidator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-12 01:00:00" id="27199" opendate="2022-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Pulsar to 2.10.0 for fixing the unstable Pulsar test environment.</summary>
      <description>Pulsar's transaction is not stable. The standalone cluster often hangs the test, then we will meet a timeout for the tests at last.The latest Pulsar 2.10.0 drops the zookeeper and fixes a lot of issues in the Pulsar transaction. Bump to this version would resolve the current test issues.</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.PulsarMockRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockZooKeeperClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockPulsarService.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockBookKeeperClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.PulsarSinkOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.SinkConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.PulsarSinkConfigUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.sink.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.producer.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.client.configuration.html</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.split.PulsarPartitionSplit.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.start.MessageIdStartCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.SourceConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.router.MessageKeyHash.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2022-4-20 01:00:00" id="27325" opendate="2022-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary forkCount settings</summary>
      <description>Several modules configure an explicit forkCount of 1. As far as I can tell this is simply unnecessary.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-10-22 01:00:00" id="2740" opendate="2015-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create data consumer for Apache NiFi</summary>
      <description>Create a connector to Apache NiFi to create Flink DataStreams from NiFi flows</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-6-9 01:00:00" id="27544" opendate="2022-5-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Example code in &amp;#39;Structure of Table API and SQL Programs&amp;#39; is out of date and cannot run</summary>
      <description>The example code in Structure of Table API and SQL Programs of 'Concepts &amp; Common API' is out of date and when user run this piece of code, they will get the following result:Exception in thread "main" org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.SinkTable'.Table options are:'connector'='blackhole''rows-per-second'='1' at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:262) at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:421) at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:222) at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:178) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) at scala.collection.Iterator.foreach(Iterator.scala:937) at scala.collection.Iterator.foreach$(Iterator.scala:937) at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) at scala.collection.IterableLike.foreach(IterableLike.scala:70) at scala.collection.IterableLike.foreach$(IterableLike.scala:69) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike.map(TraversableLike.scala:233) at scala.collection.TraversableLike.map$(TraversableLike.scala:226) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:178) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:782) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:861) at org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:56) at com.yck.TestTableAPI.main(TestTableAPI.java:43)Caused by: org.apache.flink.table.api.ValidationException: Unsupported options found for 'blackhole'.Unsupported options:rows-per-secondSupported options:connectorproperty-version at org.apache.flink.table.factories.FactoryUtil.validateUnconsumedKeys(FactoryUtil.java:624) at org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validate(FactoryUtil.java:914) at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.validate(FactoryUtil.java:978) at org.apache.flink.connector.blackhole.table.BlackHoleTableSinkFactory.createDynamicTableSink(BlackHoleTableSinkFactory.java:64) at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:259) ... 19 moreI think this mistake would drive users crazy when they first fry Table API &amp; Flink SQL since this is the very first code they see.Overall this code is outdated in two places:1. The Query creating temporary table should be CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable (EXCLUDING OPTIONS) instead of CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable which missed (EXCLUDING OPTIONS) sql_like_pattern2. The part creating a source table should be tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 1L) .build());instead of tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenOptions.ROWS_PER_SECOND, 100) .build());since the class DataGenOptions was replaced by class DataGenConnectorOptions in this commitThe test code is in my github Repository(version 1.15) and version 1.14The affected versions are 1.15 and 1.14.</description>
      <version>1.14.0,1.14.2,1.14.3,1.14.4,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-9-25 01:00:00" id="2764" opendate="2015-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebClient cannot display multiple Jobs</summary>
      <description>If multiple jars are uploaded to the WebClient, only a single job is listed.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.resources.web-docs.js.program.js</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.web.JobsServlet.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-5-16 01:00:00" id="27649" opendate="2022-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce the number of outputted log lines by Elasticsearch6SinkE2ECase</summary>
      <description>The current ElasticSearch tests create a large number of log lines, see https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35694&amp;view=logs&amp;j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&amp;t=160c9ae5-96fd-516e-1c91-deb81f59292a&amp;l=14702 as an example.We should disable the logging by default.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-25 01:00:00" id="2765" opendate="2015-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hbase version for hadoop-2 to 1.2 release</summary>
      <description>Currently 0.98.11 is used: &lt;hbase.hadoop2.version&gt;0.98.11-hadoop2&lt;/hbase.hadoop2.version&gt;Stable release for hadoop-2 is 1.2.x lineWe should upgrade to 1.2.1</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-batch-connectors.flink-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-5-25 01:00:00" id="27776" opendate="2022-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw exception when UDAF used in sliding window does not implement merge method in PyFlink</summary>
      <description>We use the pane state to optimize the result of calculating the window state, which requires udaf to implement the merge method. However, due to the lack of detection of whether the merge method of udaf is implemented, the user's output result did not meet his expectations and there is no exception. Below is an example of a UDAF that implements the merge method:class SumAggregateFunction(AggregateFunction): def get_value(self, accumulator): return accumulator[0] def create_accumulator(self): return [0] def accumulate(self, accumulator, *args): accumulator[0] = accumulator[0] + args[0] def retract(self, accumulator, *args): accumulator[0] = accumulator[0] - args[0] def merge(self, accumulator, accumulators): for other_acc in accumulators: accumulator[0] = accumulator[0] + other_acc[0] def get_accumulator_type(self): return DataTypes.ARRAY(DataTypes.BIGINT()) def get_result_type(self): return DataTypes.BIGINT()</description>
      <version>1.13.6,1.14.4,1.15.0</version>
      <fixedVersion>1.14.5,1.15.1,1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.udf.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-29 01:00:00" id="2779" opendate="2015-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation to reflect new Stream/Window API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.internals.general.arch.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.apis.streaming.guide.md</file>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-12-30 01:00:00" id="27837" opendate="2022-5-30 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support statement set in the SQL Gateway</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-7-1 01:00:00" id="27865" opendate="2022-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add guide and example for configuring SASL and SSL in Kafka SQL connector document</summary>
      <description>Using SASL and SSL in Kafka connector is a common case and usually quite complex for new users that not quite familiar with the design of Kafka connector, so it would be helpful to add a guidance of how to enable these security options in Kafka connector.</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2,1.14.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-11-15 01:00:00" id="28083" opendate="2022-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PulsarSource cannot work with object-reusing DeserializationSchema.</summary>
      <description>This issue is the same as Kafka's https://issues.apache.org/jira/browse/FLINK-25132</description>
      <version>1.14.4,1.15.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StopCursorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarUnorderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarSourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.PulsarSourceReaderFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.message.PulsarMessageCollector.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.message.PulsarMessage.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarUnorderedFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarOrderedFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarFetcherManagerBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.emitter.PulsarRecordEmitter.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-7-16 01:00:00" id="28089" opendate="2022-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect support "tablesample (xx rows)"</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-7-16 01:00:00" id="28090" opendate="2022-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support attachAsDatastream in Python Table API</summary>
      <description>Implement attachAsDatastream. A pull request is submitted as this issue is created.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.util.java.utils.py</file>
      <file type="M">flink-python.pyflink.table.statement.set.py</file>
      <file type="M">docs.content.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.zh.docs.dev.table.data.stream.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-16 01:00:00" id="28094" opendate="2022-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade AWS SDK to support ap-southeast-3</summary>
      <description>The AWS base module pulls AWS SDK v2.17.52 which does not support ap-southeast-3. Update to the latest version. Ensure to cover connectors (KDS/KDF/DDB) and formats (avro-glue-schema-registry and json-glue-schema-registry)</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.15.2,1.14.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-streams.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-firehose.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.JobManagerWatermarkTrackerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.test.java.org.apache.flink.connector.aws.util.AWSAsyncSinkUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.DynamoDBStreamsProxy.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-6-16 01:00:00" id="28095" opendate="2022-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace IOUtils dependency on oss filesystem</summary>
      <description>The oss fs has an undeclared dependency on commons-io for a single call to IOUtils.We can make our lives a little bit easier by using the Flink IOUtils instead.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.java.org.apache.flink.fs.osshadoop.writer.OSSRecoverableFsDataOutputStream.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-16 01:00:00" id="28096" opendate="2022-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect support set variable</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveOperationExecutor.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveDialectFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2022-10-13 01:00:00" id="29271" opendate="2022-9-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change to byte array from bytebuffer to improve performance when reading parquet file</summary>
      <description>I have done a test to compare byte array and bytebuffer,  40% performance improvement when using byte array to read parquet file.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.RunLengthDecoder.java</file>
    </fixedFiles>
  </bug>
  
</bugrepository>