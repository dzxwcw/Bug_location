<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  <bug fixdate="2018-12-19 01:00:00" id="10938" opendate="2018-11-19 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add e2e test for natively running Flink session cluster on Kubernetes</summary>
      <description>Add E2E tests to verify Flink on K8s integration</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.container.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.kubernetes.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-19 01:00:00" id="10939" opendate="2018-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documents for natively running Flink session cluster on k8s</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-1-5 01:00:00" id="11074" opendate="2018-12-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the harness test to make it possible test with state backend</summary>
      <description>Currently, the harness test can only test without state backend. If you use a DataView in the accumulator of the aggregate function, the DataView is a java object and held in heap, not replaced with StateMapView/StateListView which values are actually held in the state backend. We should improve the harness test to make it possible to test with state backend. Otherwise, issues such as FLINK-10674 could have never been found. With this harness test available, we could test the built-in aggregate functions which use the DataView more fine grained.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.CollectAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-12 01:00:00" id="11136" opendate="2018-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the logical of merge for DISTINCT aggregates</summary>
      <description>The logic of merge for DISTINCT aggregates has bug. For the following query:SELECT c, COUNT(DISTINCT b), SUM(DISTINCT b), SESSION_END(rowtime, INTERVAL '0.005' SECOND)FROM MyTableGROUP BY SESSION(rowtime, INTERVAL '0.005' SECOND), cthe following exception will be thrown:Caused by: java.lang.ClassCastException: org.apache.flink.types.Row cannot be cast to java.lang.Integerat scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:101)at scala.math.Numeric$IntIsIntegral$.plus(Numeric.scala:58)at org.apache.flink.table.functions.aggfunctions.SumAggFunction.accumulate(SumAggFunction.scala:50)at GroupingWindowAggregateHelper$18.mergeAccumulatorsPair(Unknown Source)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:66)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:33)at org.apache.flink.runtime.state.heap.HeapAggregatingState.mergeState(HeapAggregatingState.java:117)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState$MergeTransformation.apply(AbstractHeapMergingState.java:102)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:463)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:341)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState.mergeNamespaces(AbstractHeapMergingState.java:91)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:341)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:311)at org.apache.flink.streaming.runtime.operators.windowing.MergingWindowSet.addWindow(MergingWindowSet.java:212)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.processElement(WindowOperator.java:311)at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:202)at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:105)at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)at org.apache.flink.runtime.taskmanager.Task.run(Task.java:704)at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-9-15 01:00:00" id="13277" opendate="2019-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add documentation of Hive source/sink</summary>
      <description>add documentation of Hive source/sink in batch/connector.mdits corresponding Chinese one is FLINK-13278cc xuefuz lirui Terry1897</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.read.write.hive.zh.md</file>
      <file type="M">docs.dev.table.hive.read.write.hive.md</file>
      <file type="M">docs.dev.batch.connectors.zh.md</file>
      <file type="M">docs.dev.batch.connectors.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-17 01:00:00" id="13299" opendate="2019-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-python failed on Travis</summary>
      <description>Log: https://api.travis-ci.com/v3/job/216620643/log.txtError:___________________________________ summary ____________________________________ERROR: py27: InvocationError for command /home/travis/build/flink-ci/flink/flink-python/dev/.conda/bin/python3.7 -m virtualenv --no-download --python /home/travis/build/flink-ci/flink/flink-python/dev/.conda/envs/2.7/bin/python2.7 py27 (exited with code 1) py33: commands succeeded ERROR: py34: InvocationError for command /home/travis/build/flink-ci/flink/flink-python/dev/.conda/bin/python3.7 -m virtualenv --no-download --python /home/travis/build/flink-ci/flink/flink-python/dev/.conda/envs/3.4/bin/python3.4 py34 (exited with code 100) py35: commands succeeded py36: commands succeeded py37: commands succeeded ============tox checks... &amp;#91;FAILED&amp;#93;============ PYTHON exited with EXIT CODE: 1. Trying to KILL watchdog (12896). ./tools/travis_watchdog.sh: line 229: 12896 Terminated watchdog</description>
      <version>1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-18 01:00:00" id="13314" opendate="2019-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct resultType of some PlannerExpression when operands contains DecimalTypeInfo or BigDecimalTypeInfo in Blink planner</summary>
      <description>Correct resultType of the following PlannerExpression when operands contains DecimalTypeInfo or BigDecimalTypeInfo in Blink planner:Minus/plus/Div/Mul/Ceil/Floor/Round </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.DecimalITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.mathExpressions.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.arithmetic.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-18 01:00:00" id="13315" opendate="2019-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port wmstrategies to api-java-bridge</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.wmstrategies.watermarkStrategies.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.wmstrategies.BoundedOutOfOrderTimestamps.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.wmstrategies.AscendingTimestamps.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.wmstrategies.watermarkStrategies.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.wmstrategies.AscendingTimestamps.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-16 01:00:00" id="1333" opendate="2014-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Getter/Setter recognition for POJO fields with generics is not working</summary>
      <description>Fields likeprivate List&lt;Contributors&gt; contributors;Are not recognized correctly, even if they have getters and setters.Workaround: make them public.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.PojoTypeExtractionTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-10-20 01:00:00" id="13339" opendate="2019-7-20 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add an implementation of pipeline&amp;#39;s api</summary>
      <description>Add an implement PipelineStage, Estimator, Transformer, Model. Add MLSession to hold the execution environment and others session shared variable. Add AlgoOperator for the implementation of algorithms. Add BatchOperator and StreamOperator based on AlgoOperator Add TableSourceBatchOp and TableSourceStreamOp</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-ml-parent.flink-ml-lib.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-22 01:00:00" id="13353" opendate="2019-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove 2 args constructor in REPLACE expression</summary>
      <description>Replace definition in stringExpression.scala has another constructor with 2 arguments.According to source code, the args' meaning are str, begin. And it call other constructor with 3 args adding the 3rd arg which is the length of str.But its expectTypes is (String, String, String), but actually is (String, int, int).So I think the 2 args defined constructor means search and replacement is "" default, not begin and length of str. </description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.stringExpressions.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-22 01:00:00" id="13354" opendate="2019-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for how to use blink planner</summary>
      <description>Add documentation for how to use different planner “Overview”: add pom dependency “Concepts &amp; Common API”: add description about how to use different planner in code</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-22 01:00:00" id="13355" opendate="2019-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Temporal Table Join in blink planner</summary>
      <description>Add documentation for Temporal Table Join in blink planner “Streaming Concepts / Temporal Tables”: introduce concepts of temporal table in blink planner and the difference and sameness to flink planner temporal table “Joins in Continuous Queries”: how to use temporal join in bink planner “SQL”: join with temporal table in SQL</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.streaming.joins.zh.md</file>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.sql.zh.md</file>
      <file type="M">docs.dev.table.sql.md</file>
      <file type="M">docs.dev.table.sourceSinks.zh.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-22 01:00:00" id="13356" opendate="2019-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for TopN and Deduplication in blink planner</summary>
      <description>Add documentation for TopN in blink planner“SQL”: how to write TopN in SQL and some tips</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.zh.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-22 01:00:00" id="13359" opendate="2019-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for DDL introduction</summary>
      <description>Add documentation for DDL introduction “Concepts &amp; Common API”: Add a section to describe how to execute DDL on TableEnvironment. “SQL Client”: Add a section and example in SQL CLI page too?</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.zh.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-29 01:00:00" id="13453" opendate="2019-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump shade plugin to 3.2.1</summary>
      <description>The shade plugin fails with an IllegalArgumentException when run on Java 11.</description>
      <version>None</version>
      <fixedVersion>shaded-9.0,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-29 01:00:00" id="13454" opendate="2019-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump japicmp jaxb dependencies</summary>
      <description>The japicmp plugins fails with a ClassNotFoundExceptions with the currently defined jaxb dependencies when run on java 11.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-29 01:00:00" id="13455" opendate="2019-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move jdk.tools exclusions out of dependency management</summary>
      <description>Defining exclusions via dependencyManagement is a bit unreliable, since the shade-plugin ignores them during dependency resolution  if they are defined for transitive dependencies.</description>
      <version>None</version>
      <fixedVersion>shaded-8.0,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-fs-tests.pom.xml</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.pom.xml</file>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-filesystem.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-29 01:00:00" id="13456" opendate="2019-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump lombok to 1.16.22</summary>
      <description>Compiling the tests for flink-core fails with an ErrorDuringInitialization due to lombok,</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-30 01:00:00" id="13488" opendate="2019-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-python fails to build on Travis due to PackagesNotFoundError</summary>
      <description>https://api.travis-ci.org/v3/job/564925115/log.txtinstall conda ... [SUCCESS]install miniconda... [SUCCESS]installing python environment...installing python2.7...install python2.7... [SUCCESS]installing python3.3...PackagesNotFoundError: The following packages are not available from current channels: - python=3.3Current channels: - https://repo.anaconda.com/pkgs/main/linux-64 - https://repo.anaconda.com/pkgs/main/noarch - https://repo.anaconda.com/pkgs/r/linux-64 - https://repo.anaconda.com/pkgs/r/noarch</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.high.parallelism.iterations.sh</file>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-8-31 01:00:00" id="13504" opendate="2019-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NoSuchFieldError when executing DDL via tEnv.sqlUpdate in application project</summary>
      <description>When we create a quickstart project to try flink 1.9/1.10, a NoSuchFieldError is thrown.The dependencies (the flink 1.0 is installed locally for commit 70fe6aa747ad021bbb8dd8cdc0beecc863f010be, flink 1.9 has the same problem): &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-java&lt;/artifactId&gt; &lt;version&gt;1.10-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner_2.11&lt;/artifactId&gt; &lt;version&gt;1.10-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;The program code:package com.github.wuchong;import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.TableEnvironment;public class DDLTest { public static void main(String[] args) { EnvironmentSettings settings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build(); TableEnvironment tEnv = TableEnvironment.create(settings); tEnv.sqlUpdate("CREATE TABLE MyTable (\n" + " a int, \n" + " b bigint, \n" + " c varchar \n" + ")\n comment 'table comment'\n" + "partitioned by (b)\n" + "with (\n" + " connector = 'csv', \n" + " csv.path = '/tmp/path'\n" + ")"); }}The exception:Exception in thread "main" java.lang.NoSuchFieldError: names at org.apache.flink.sql.parser.ddl.SqlCreateTable.fullTableName(SqlCreateTable.java:326) at org.apache.flink.table.sqlexec.SqlToOperationConverter.convertCreateTable(SqlToOperationConverter.java:140) at org.apache.flink.table.sqlexec.SqlToOperationConverter.convert(SqlToOperationConverter.java:86) at org.apache.flink.table.planner.StreamPlanner.parse(StreamPlanner.scala:115) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:335) at com.github.wuchong.DDLTest.main(DDLTest.java:29)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-uber.pom.xml</file>
      <file type="M">flink-table.flink-table-uber-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-31 01:00:00" id="13509" opendate="2019-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Forbidden `IS NOT DISTINCT FROM `(or an expanded version) in LookupJoin</summary>
      <description>Example1:`SELECT T.id, T.len, T.content, D.name FROM T JOIN userTable for system_time as of T.proctime AS D ON T.id = D.id OR (T.id is null and D.id is null)`Example2:"SELECT T.id, T.len, T.content, D.name FROM T JOIN userTable for system_time as of T.proctime AS D ON T.id IS NOT DISTINCT FROM D.id"In 1.9 version, we can simply throw exception in compile phase for the above sql and add proper support in FLINK-13648.</description>
      <version>1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.common.CommonLookupJoinRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-31 01:00:00" id="13518" opendate="2019-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable Hive tests</summary>
      <description>Hive straight up doesn't support Java 11 (or anything above Java 8 really), so we might as well disable all tests on Java 11.15:49:57.131 [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.208 s - in org.apache.flink.batch.connectors.hive.HiveTableFactoryTestSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/travis/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.6.2/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/travis/.m2/repository/org/slf4j/slf4j-log4j12/1.7.15/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.15:50:11.610 [INFO] Running org.apache.flink.batch.connectors.hive.TableEnvHiveConnectorTest15:50:11.625 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.009 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.batch.connectors.hive.TableEnvHiveConnectorTest15:50:11.627 [ERROR] org.apache.flink.batch.connectors.hive.TableEnvHiveConnectorTest Time elapsed: 0.007 s &lt;&lt;&lt; ERROR!java.lang.IllegalStateException: Failed to create HiveServer :Error applying authorization policy on hive configuration: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')Caused by: java.lang.RuntimeException: Error applying authorization policy on hive configuration: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')Caused by: java.lang.ClassCastException: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-1 01:00:00" id="13523" opendate="2019-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify and correct arithmetic function&amp;#39;s semantic for Blink planner</summary>
      <description/>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.MiniBatchGroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdFilteredColumnIntervalTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.ReturnTypeInference.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.AvgAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.AvgAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.DeclarativeAggregateFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeSystem.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.AggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DeclarativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.codegen.agg.TestLongAvgFunc.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.AggregateReduceGroupingRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQuerySemiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.WindowGroupReorderRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.AggWithoutKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.BatchAggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.SortAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.GroupingSetsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.DecimalITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.MiscITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.AggregationITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.GroupWindowITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-1 01:00:00" id="13529" opendate="2019-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify and correct agg function&amp;#39;s semantic for Blink planner</summary>
      <description/>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.AggregateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWsWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlConcatAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWsWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlFirstLastValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortDistinctAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicity.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlIncrSumAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-2 01:00:00" id="13545" opendate="2019-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JoinToMultiJoinRule should not match SEMI/ANTI LogicalJoin</summary>
      <description>run tpcds 14.a on blink planner, an exception will thrownjava.lang.ArrayIndexOutOfBoundsException: 84 at org.apache.calcite.rel.rules.JoinToMultiJoinRule$InputReferenceCounter.visitInputRef(JoinToMultiJoinRule.java:564) at org.apache.calcite.rel.rules.JoinToMultiJoinRule$InputReferenceCounter.visitInputRef(JoinToMultiJoinRule.java:555) at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112) at org.apache.calcite.rex.RexVisitorImpl.visitCall(RexVisitorImpl.java:80) at org.apache.calcite.rex.RexCall.accept(RexCall.java:191) at org.apache.calcite.rel.rules.JoinToMultiJoinRule.addOnJoinFieldRefCounts(JoinToMultiJoinRule.java:481) at org.apache.calcite.rel.rules.JoinToMultiJoinRule.onMatch(JoinToMultiJoinRule.java:166) at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:319) at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:560) at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:419) at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:284) at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74) at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:215) at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:202)the reason is JoinToMultiJoinRule should match SEMI/ANTI LogicalJoin. before calcite-1.20, SEMI join is represented by SemiJoin which is not matched JoinToMultiJoinRule.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-8-3 01:00:00" id="13563" opendate="2019-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TumblingGroupWindow should implement toString method</summary>
      <description>@Test def testAllEventTimeTumblingGroupWindowOverTime(): Unit = { val util = streamTestUtil() val table = util.addDataStream[(Long, Int, String)]( "T1", 'long, 'int, 'string, 'rowtime.rowtime) val windowedTable = table .window(Tumble over 5.millis on 'rowtime as 'w) .groupBy('w) .select('int.count) util.verifyPlan(windowedTable) }currently, it's physical plan is HashWindowAggregate(window=[TumblingGroupWindow], select=[Final_COUNT(count$0) AS EXPR$0])+- Exchange(distribution=[single]) +- LocalHashWindowAggregate(window=[TumblingGroupWindow], select=[Partial_COUNT(int) AS count$0]) +- TableSourceScan(table=[[default_catalog, default_database, Table1, source: [TestTableSource(long, int, string)]]], fields=[long, int, string])we know nothing about the TumblingGroupWindow except its name. the expected plan isHashWindowAggregate(window=[TumblingGroupWindow('w, long, 5)], select=[Final_COUNT(count$0) AS EXPR$0])+- Exchange(distribution=[single]) +- LocalHashWindowAggregate(window=[TumblingGroupWindow('w, long, 5)], select=[Partial_COUNT(int) AS count$0]) +- TableSourceScan(table=[[default_catalog, default_database, Table1, source: [TestTableSource(long, int, string)]]], fields=[long, int, string])</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.AggregateReduceGroupingRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.logical.groupWindows.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-3 01:00:00" id="13564" opendate="2019-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>blink planner should also throw exception if constant with YEAR TO MONTH resolution was used for group windows</summary>
      <description>just as FLINK-11017, blink planner should also throw exception if constant with YEAR TO MONTH resolution was used for group windows</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.StreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalWindowAggregateRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.BatchLogicalWindowAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-4 01:00:00" id="13567" opendate="2019-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro Confluent Schema Registry nightly end-to-end test failed on Travis</summary>
      <description>The Avro Confluent Schema Registry nightly end-to-end test failed on Travis with[FAIL] 'Avro Confluent Schema Registry nightly end-to-end test' failed after 2 minutes and 11 seconds! Test exited with exit code 1No taskexecutor daemon (pid: 29044) is running anymore on travis-job-b0823aec-c4ec-4d4b-8b59-e9f968de9501.No standalonesession daemon to stop on host travis-job-b0823aec-c4ec-4d4b-8b59-e9f968de9501.rm: cannot remove '/home/travis/build/apache/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/plugins': No such file or directoryhttps://api.travis-ci.org/v3/job/567273939/log.txt</description>
      <version>1.8.2,1.9.0,1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.confluent.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">tools.travis.splits.split.misc.hadoopfree.sh</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-11-7 01:00:00" id="13634" opendate="2019-8-7 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add Hadoop compression format for use with StreamingFileSink</summary>
      <description>I have developed a CompressFileWriter base on BulkWriter to compress data using hadoop compressor</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">docs.dev.connectors.streamfile.sink.zh.md</file>
      <file type="M">docs.dev.connectors.streamfile.sink.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-9 01:00:00" id="13663" opendate="2019-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client end-to-end test for modern Kafka failed on Travis</summary>
      <description>The SQL Client end-to-end test for modern Kafka failed on Travis because it could not download https://archive.apache.org/dist/kafka/0.11.0.2/kafka_2.11-0.11.0.2.tgz.Maybe we could add a similar retry logic as with the Kinesis end-to-end test FLINK-13599.https://api.travis-ci.org/v3/job/569262834/log.txthttps://api.travis-ci.org/v3/job/569262828/log.txt</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-9 01:00:00" id="13667" opendate="2019-8-9 00:00:00" resolution="Won&amp;#39;t Do">
    <buginformation>
      <summary>Add the utility class for the Table</summary>
      <description>Add the utility class for the Table the operations on column name the operations on column type</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-ml-parent.flink-ml-lib.src.test.java.org.apache.flink.ml.common.utils.TableUtilTest.java</file>
      <file type="M">flink-ml-parent.flink-ml-lib.src.main.java.org.apache.flink.ml.common.utils.TableUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-7 01:00:00" id="1367" opendate="2015-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add field aggregations to Streaming Scala api</summary>
      <description>Field aggregations are missing from the streaming scala api for case classes.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedDataStream.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.package.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.TopSpeedWindowing.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-8-14 01:00:00" id="13723" opendate="2019-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use liquid-c for faster doc generation</summary>
      <description>Jekyll requires liquid and only optionally uses liquid-c if available. The latter uses natively-compiled code and reduces generation time by ~5% for me.</description>
      <version>1.10.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.Gemfile.lock</file>
      <file type="M">docs.Gemfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-14 01:00:00" id="13725" opendate="2019-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use sassc for faster doc generation</summary>
      <description>Jekyll requires sass but can optionally also use a C-based implementation provided by sassc. Although we do not use sass directly, there may be some indirect use inside jekyll. It doesn't seem to hurt to upgrade here.</description>
      <version>1.10.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.Gemfile.lock</file>
      <file type="M">docs.Gemfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-14 01:00:00" id="13729" opendate="2019-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update website generation dependencies</summary>
      <description>The website generation dependencies are quite old. By upgrading some of them we get improvements like a much nicer code highlighting and prepare for the jekyll update of FLINK-13726 and FLINK-13727.</description>
      <version>1.10.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.Gemfile.lock</file>
      <file type="M">docs.Gemfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-16 01:00:00" id="13745" opendate="2019-8-16 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Flink cache on Travis does not exist</summary>
      <description>More and more often I observe that Flink builds fail on Travis because of missing Flink caches:Cached flink dir /home/travis/flink_cache/40072/flink does not exist. Exiting build.It seems as if Travis cannot guarantee that a cache survives as long as the different profiles of a build are running. It would be good to solve this problem because now we have regularly failing builds:https://travis-ci.org/apache/flink/builds/572559629https://travis-ci.org/apache/flink/builds/572523730https://travis-ci.org/apache/flink/builds/571576734</description>
      <version>1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-16 01:00:00" id="13746" opendate="2019-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch (v2.3.5) sink end-to-end test fails on Travis</summary>
      <description>The Elasticsearch (v2.3.5) sink end-to-end test fails on Travis because it logs contain the following line:INFO org.elasticsearch.plugins - [Terror] modules [], plugins [], sites []Due to this, the error check is triggered.https://api.travis-ci.org/v3/job/572255901/log.txt</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.8.3,1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-8 01:00:00" id="1375" opendate="2015-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove incubator references &amp; rename to new project urls</summary>
      <description>We need to change all urls and remove the "incubator" string from it.Also, the website doesn't need the incubator disclaimer.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.DISCLAIMER</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-18 01:00:00" id="13759" opendate="2019-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>All builds for master branch are failed during compile stage</summary>
      <description>Here is an instance: https://api.travis-ci.org/v3/job/572950228/log.txtThere is an error in the log.==============================================================================find: ‘flink-connectors/flink-connector-elasticsearch/target/flink-connector-elasticsearch*.jar’: No such file or directory==============================================================================Previous build failure detected, skipping cache setup.==============================================================================The flink-connector-elasticsearch is not exist. But recent commits didn't modify this.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-18 01:00:00" id="13760" opendate="2019-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hardcode Scala version dependency in hive connector</summary>
      <description>FLINK-13688 introduced a flink-test-utils dependency in flink-connector-hive. However, the Scala version of the artifactId is hardcoded, this result in recent CRON jobs failed. Here is an instance: https://api.travis-ci.org/v3/job/573092374/log.txt11:46:09.078 [INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-connector-hive_2.12 ---11:46:09.134 [WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:Found Banned Dependency: com.typesafe.akka:akka-slf4j_2.11:jar:2.5.21Found Banned Dependency: com.typesafe.akka:akka-actor_2.11:jar:2.5.21Found Banned Dependency: com.typesafe:ssl-config-core_2.11:jar:0.3.7Found Banned Dependency: org.scala-lang.modules:scala-java8-compat_2.11:jar:0.7.0Found Banned Dependency: com.typesafe.akka:akka-protobuf_2.11:jar:2.5.21Found Banned Dependency: org.apache.flink:flink-clients_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-streaming-java_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: com.typesafe.akka:akka-stream_2.11:jar:2.5.21Found Banned Dependency: com.github.scopt:scopt_2.11:jar:3.5.0Found Banned Dependency: org.apache.flink:flink-test-utils_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-runtime_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-runtime_2.11:test-jar:tests:1.10-SNAPSHOTFound Banned Dependency: org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.1.1Found Banned Dependency: com.twitter:chill_2.11:jar:0.7.6Found Banned Dependency: org.clapper:grizzled-slf4j_2.11:jar:1.3.2Found Banned Dependency: org.apache.flink:flink-optimizer_2.11:jar:1.10-SNAPSHOTUse 'mvn dependency:tree' to locate the source of the banned dependencies.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-24 01:00:00" id="13841" opendate="2019-8-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend Hive version support to all 1.2 and 2.3 versions</summary>
      <description>This is to support all 1.2 (1.2.0, 1.2.1, 1.2.2) and 2.3 (2.3.0-5) versions.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV2.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV1.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-25 01:00:00" id="13845" opendate="2019-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop all the content of removed "Checkpointed" interface</summary>
      <description>From FLINK-7461, we have already removed the backward compatibility before Flink-1.1 and the deprecated Checkpointed interface has been totally removed. However, we still have many contents including java docs, documentation talked about this non-existing interface. I think it's time to remove these contents now.</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.checkpoint.ListCheckpointed.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.checkpoint.CheckpointedFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-26 01:00:00" id="13853" opendate="2019-8-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Running HA (file, async) end-to-end test failed on Travis</summary>
      <description>Running HA (file, async) end-to-end test failed on Travis:https://api.travis-ci.org/v3/job/576002743/log.txthttps://api.travis-ci.org/v3/job/576002736/log.txthttps://api.travis-ci.org/v3/job/576002730/log.txthttps://api.travis-ci.org/v3/job/576002724/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.ha.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-8-29 01:00:00" id="13891" opendate="2019-8-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increment flink-shaded version</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-2 01:00:00" id="13937" opendate="2019-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix wrong hive dependency version in documentation</summary>
      <description>There is a wrong maven dependency in the hive connector's documentation.</description>
      <version>1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..config.yml</file>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-2 01:00:00" id="13942" opendate="2019-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Overview page for Getting Started section</summary>
      <description>The Getting Started section provide different types of tutorials that target users with different interests and backgrounds.We should add a brief overview page that describes the different tutorials such that users easily find the material that they need to get started with Flink.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.getting-started.index.zh.md</file>
      <file type="M">docs.getting-started.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-3 01:00:00" id="13943" opendate="2019-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide api to convert flink table to java List (blink planner)</summary>
      <description>It would be nice to convert flink table to java List so that I can do other data manipulation in client side after execution flink job. For flink planner, I can convert flink table to DataSet and use DataSet#collect, but for blink planner, there's no such api.EDIT from FLINK-14807:Currently, it is very unconvinient for user to fetch data of flink job unless specify sink expclitly and then fetch data from this sink via its api (e.g. write to hdfs sink, then read data from hdfs). However, most of time user just want to get the data and do whatever processing he want. So it is very necessary for flink to provide api Table#collect for this purpose. Other apis such as Table#head, Table#print is also helpful.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TableUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-5-5 01:00:00" id="13978" opendate="2019-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch to Azure Pipelines as a CI tool for Flink</summary>
      <description>See ML discussion: https://lists.apache.org/thread.html/b90aa518fcabce94f8e1de4132f46120fae613db6e95a2705f1bd1ea@%3Cdev.flink.apache.org%3E We want to try out Azure Pipelines for the following reasons: more mature system (compared to travis) 10 parallel, 6 hrs builds for open source ability to add custom machines (See also INFRA-17030) </description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.elasticsearch.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-5 01:00:00" id="13979" opendate="2019-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate new streamfilesink docs to chinese</summary>
      <description>The StreamFileSink docs have been reworked as part of FLINK-13842</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.streamfile.sink.zh.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-13 01:00:00" id="1399" opendate="2015-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for registering Serializers with Kryo</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.KryoSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-9 01:00:00" id="14009" opendate="2019-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cron jobs broken due to verifying incorrect NOTICE-binary file</summary>
      <description>With FLINK-13968 we introduced an automatic NOTICE-binary file check. However, since we don't use the correct NOTICE-binary file (FLINK-14008) for Scala 2.12 it fails currently our cron jobs.I suggest to only enable the automatic NOTICE-binary files for Scala 2.11 until FLINK-14008 has been fixed.</description>
      <version>1.10.0</version>
      <fixedVersion>1.8.3,1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-9-9 01:00:00" id="14014" opendate="2019-9-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Introduce PythonScalarFunctionRunner to handle the communication with Python worker for Python ScalarFunction execution</summary>
      <description>PythonScalarFunctionRunner is responsible for Python ScalarFunction execution and it only handles the Python ScalarFunction execution and nothing else. So its logic should be very simple, forwarding an input element to Python worker and fetching the execution results back: Internally, it uses Apache Beam’s portability for Python UDF execution and this is transparent for the caller of PythonScalarFunctionRunner By default, each runner will startup a separate Python worker The Python worker can run in a docker, a separate process or even an non-managed external service. It has the ability to execute multiple Python ScalarFunctions It also supports chained Python ScalarFunctions</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.projectsetup.dependencies.zh.md</file>
      <file type="M">docs.dev.projectsetup.dependencies.md</file>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-9 01:00:00" id="14016" opendate="2019-9-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Introduce RelNodes FlinkLogicalPythonScalarFunctionExec and DataStreamPythonScalarFunctionExec which are containers for Python PythonScalarFunctions</summary>
      <description>Dedicated RelNodes such as FlinkLogicalPythonScalarFunctionExec and DataStreamPythonScalarFunctionExec should be introduced for Python ScalarFunction execution. These nodes exists as containers for Python ScalarFunctions which could be executed in a batch and then we can employ PythonScalarFunctionOperator for Python ScalarFunction execution.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.calcite.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.StreamOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.Optimizer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.CalciteConfig.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.FunctionDefinition.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-9 01:00:00" id="14017" opendate="2019-9-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support to start up Python worker in process mode</summary>
      <description>We employ Apache Beam's portability frameowork for the Python UDF execution. However, there is only a golang implementation for the boot script to start up SDK harness in Beam. It’s used by both the Python SDK harness and the Go SDK harness. This is not a problem for Beam. However, it’s indeed a problem for Flink as it indicates that the whole stack of Beam’s Go SDK harness will be depended if we use the golang implementation of the boot script. We want to avoid this by adding a Python boot script.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-9 01:00:00" id="14021" opendate="2019-9-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add rules to push down the Python ScalarFunctions contained in the join condition of Correlate node</summary>
      <description>The Python ScalarFunctions contained in the join condition of Correlate node should be extracted to make sure the TableFunction works well.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.CorrelateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-9 01:00:00" id="14022" opendate="2019-9-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add validation check for places where Python ScalarFunction cannot be used</summary>
      <description>Currently, there are places where Python ScalarFunction could not be used, for example: Python UDF could not be used in MatchRecognize Python UDFs could not be used in Join condition which take the columns from both the left table and the right table as inputsWe should add validation check for places where it’s not supported.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.JoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.MatchRecognizeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamMatch.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.stream.sql.validation.MatchRecognizeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMatch.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.Optimizer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-9 01:00:00" id="14023" opendate="2019-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support accessing job parameters in Python user-defined functions</summary>
      <description>Currently, it’s possible to access job parameters in the Java user-defined functions. It could be used to define the behavior according to job parameters. It should also be supported for Python user-defined functions.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.EmbeddedPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.EmbeddedPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.ProtoUtils.java</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.table.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.metrics.tests.test.metric.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">docs.content.docs.dev.python.table.udfs.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-9 01:00:00" id="14026" opendate="2019-9-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Manage the resource of Python worker properly</summary>
      <description>For a Flink Table API &amp; SQL job, if it uses Python user-defined functions, the Java operator will launch separate Python process for Python user-defined function execution. We should make sure that the resources used by the Python process are managed by Flink’s resource management framework.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonOptionsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-9 01:00:00" id="14027" opendate="2019-9-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add documentation for Python user-defined functions</summary>
      <description>We should add documentation about how to use Python user-defined functions.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
      <file type="M">docs.dev.table.udfs.zh.md</file>
      <file type="M">docs.dev.table.udfs.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-11-12 01:00:00" id="14062" opendate="2019-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set managed memory fractions according to slot sharing groups</summary>
      <description>For operators with specified ResourceSpecs, calculate fractions according to operators ResourceSpecs For operators with unknown ResourceSpecs, calculate fractions according to number of operators using managed memoryThis step should not introduce any behavior changes.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-16 01:00:00" id="1411" opendate="2015-1-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PlanVisualizer is not working</summary>
      <description>In the current master, the PlanVisualizer is no longer working. The reason is that the resources folder containing the web resources has been moved to the flink-runtime and flink-clients jar. Maybe we should pick up FLINK-1317 and make the PlanVisualizer accessible through the flink website.</description>
      <version>None</version>
      <fixedVersion>0.8.0,0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.tools.planVisualizer.html</file>
      <file type="M">flink-dist.src.main.assemblies.yarn.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-18 01:00:00" id="14117" opendate="2019-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate changes on index page to Chinese</summary>
      <description>The changes of commit ee0d6fdf0604d74bd1cf9a6eb9cf5338ac1aa4f9 on the documentation index page should be translated to Chinese.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-19 01:00:00" id="14128" opendate="2019-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the description of restart strategy customization</summary>
      <description>Restart strategy customization was not a public interface since it was not documented.Since existing RestartStrategy implementation will not be supported with the new scheduler introduced in FLINK-10429, we'd better not mark restart strategy customization a public interface.This way we' need to remove the documentation of restart strategy customization which was recently added in FLINK-13898.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestartStrategyOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs..includes.generated.restart.strategy.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-10-20 01:00:00" id="14134" opendate="2019-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce LimitableTableSource to optimize limit</summary>
      <description>SQL: select *from t1 limit 1Now source will scan full table, if we can introduce LimitableTableSource, let source know the limit line, source can just read one row is OK.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LimitTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-9-20 01:00:00" id="14150" opendate="2019-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary __pycache__ directories appears in pyflink.zip</summary>
      <description>It seems we are packaging _pycache_ directories into pyflink.zip. These directories contain bytecode cache files that are automatically generated by python3. We should remove them from the python source code folder before packaging.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-9-20 01:00:00" id="14157" opendate="2019-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temporarily remove S3 StreamingFileSink end-to-end test</summary>
      <description>This issue temporarily disables the failing test for Java 11 so that we can have a green travis build, until a proper solution is found. In addition, it removes the relocations for Java 8 so that the production code works for Java 8, the main java version Flink supports.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-10-24 01:00:00" id="14178" opendate="2019-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>maven-shade-plugin 3.2.1 doesn&amp;#39;t work on ARM for Flink</summary>
      <description>recently, maven-shade-plugin is bumped from 3.0.0 to 3.2.1 by the commit. While with my test locally on ARM, The Flink build process will be jammed. After debugging, I found there is an infinite loop.Downgrade maven-shade-plugin to 3.1.0 can solve this problem.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">docs.dev.projectsetup.dependencies.zh.md</file>
      <file type="M">docs.dev.projectsetup.dependencies.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-9-24 01:00:00" id="14195" opendate="2019-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nightly connectors-jdk11 fails because of missing jaxb classes</summary>
      <description>As titled, https://api.travis-ci.org/v3/job/588652149/log.txt is one example. We could see below errors from the log message:23:48:12.419 [ERROR] testResumeAfterCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 5.117 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.IOException&gt; but was&lt;java.lang.NoClassDefFoundError&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit(HadoopS3RecoverableWriterExceptionITCase.java:165)Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit(HadoopS3RecoverableWriterExceptionITCase.java:165)23:48:12.419 [ERROR] testResumeWithWrongOffset(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 0.205 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.IOException&gt; but was&lt;java.lang.NoClassDefFoundError&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset(HadoopS3RecoverableWriterExceptionITCase.java:185)Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset(HadoopS3RecoverableWriterExceptionITCase.java:185)</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-25 01:00:00" id="14205" opendate="2019-9-25 00:00:00" resolution="Done">
    <buginformation>
      <summary>Distinguish duplicate job submissions from other job submission errors</summary>
      <description>In order to better handle duplicate job submissions, I propose to add a new exception type DuplicateJobSubmissionException which inherits from JobSubmissionException and which is returned if the submitted job is a duplicate.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherResourceCleanupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-26 01:00:00" id="14227" opendate="2019-9-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Razorpay to Chinese Powered By page</summary>
      <description>Razorpay was added to the English Powered By page with commit: 87a034140e97be42616e1a3dbe58e4f7a014e560.It should be added to the Chinese Powered By (and index.html) page as well.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.state.checkpointing.zh.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-10-27 01:00:00" id="14247" opendate="2019-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use NoResourceAvailableException to wrap TimeoutException on slot allocation (Scheduler NG)</summary>
      <description>This makes the error to be more user friendly.It also helps MiniClusterITCases to pass.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-27 01:00:00" id="14254" opendate="2019-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce FileSystemOutputFormat for batch</summary>
      <description>Introduce FileSystemOutputFormat to support all table file system connector with partition support in batch mode.FileSystemOutputFormat use PartitionWriter to write: DynamicPartitionWriter GroupedPartitionWriter NonPartitionWriterFileSystemOutputFormat use FileCommitter to commit temporary files. </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-4-27 01:00:00" id="14258" opendate="2019-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate file system connector to streaming sink</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.PartitionPathUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-10-28 01:00:00" id="14265" opendate="2019-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t use ContinuousFileReaderOperator to support multiple paths</summary>
      <description>Now, blink planner use ContinuousFileReaderOperator to support InputFormat, but ContinuousFileReaderOperator not support multiple paths.If read partitioned source, after partition pruning, we need let InputFormat to read multiple partitions which are multiple paths.We can use InputFormatSourceFunction directly to support InputFormat.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.PhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-28 01:00:00" id="14266" opendate="2019-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce Row Csv InputFormat</summary>
      <description>Now, we have an old CSV, but that is not standard CSV support. we should support the RFC-compliant CSV format for table/sql.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-29 01:00:00" id="14272" opendate="2019-9-29 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Blink planner for Python UDF</summary>
      <description>Currently, the Python UDF only works in the legacy planner, we should also support it in the Blink planner.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ExpressionReductionRulesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalc.scala</file>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-29 01:00:00" id="14276" opendate="2019-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala quickstart project does not compile on Java9+</summary>
      <description>The Quickstarts Scala nightly end-to-end test fails on Travis when running the e2e - misc - jdk11 profile. The failure cause is19:32:57.344 [ERROR] error: java.lang.NoClassDefFoundError: javax/tools/ToolProvider19:32:57.344 [INFO] at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)19:32:57.344 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)19:32:57.344 [INFO] at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)19:32:57.344 [INFO] at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)19:32:57.344 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)19:32:57.344 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)19:32:57.344 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)19:32:57.344 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)19:32:57.344 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)19:32:57.345 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)19:32:57.345 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)19:32:57.345 [INFO] at scala.collection.Iterator$class.foreach(Iterator.scala:891)19:32:57.345 [INFO] at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)19:32:57.345 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)19:32:57.345 [INFO] at scala.collection.AbstractIterable.foreach(Iterable.scala:54)19:32:57.345 [INFO] at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)19:32:57.345 [INFO] at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)19:32:57.345 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)19:32:57.345 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)19:32:57.345 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)19:32:57.345 [INFO] at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)19:32:57.346 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)19:32:57.346 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)19:32:57.346 [INFO] at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)19:32:57.346 [INFO] at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)19:32:57.346 [INFO] at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)19:32:57.346 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:71)19:32:57.346 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:39)19:32:57.346 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)19:32:57.346 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)19:32:57.346 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)19:32:57.346 [INFO] at scala.tools.nsc.Global$Run.&lt;init&gt;(Global.scala:1242)19:32:57.346 [INFO] at scala.tools.nsc.Driver.doCompile(Driver.scala:31)19:32:57.346 [INFO] at scala.tools.nsc.MainClass.doCompile(Main.scala:23)19:32:57.346 [INFO] at scala.tools.nsc.Driver.process(Driver.scala:51)19:32:57.346 [INFO] at scala.tools.nsc.Driver.main(Driver.scala:64)19:32:57.347 [INFO] at scala.tools.nsc.Main.main(Main.scala)19:32:57.347 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)19:32:57.347 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)19:32:57.347 [INFO] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)19:32:57.347 [INFO] at java.base/java.lang.reflect.Method.invoke(Method.java:566)19:32:57.347 [INFO] at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)19:32:57.347 [INFO] at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)19:32:57.347 [INFO] java.lang.reflect.InvocationTargetException19:32:57.347 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)19:32:57.347 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)19:32:57.347 [INFO] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)19:32:57.347 [INFO] at java.base/java.lang.reflect.Method.invoke(Method.java:566)19:32:57.348 [INFO] at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)19:32:57.348 [INFO] at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)19:32:57.348 [ERROR] Caused by: java.lang.NoClassDefFoundError: javax/tools/ToolProvider19:32:57.348 [INFO] at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)19:32:57.348 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)19:32:57.349 [INFO] at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)19:32:57.349 [INFO] at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)19:32:57.349 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)19:32:57.349 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)19:32:57.349 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)19:32:57.350 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)19:32:57.350 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)19:32:57.350 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)19:32:57.351 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)19:32:57.351 [INFO] at scala.collection.Iterator$class.foreach(Iterator.scala:891)19:32:57.351 [INFO] at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)19:32:57.352 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)19:32:57.352 [INFO] at scala.collection.AbstractIterable.foreach(Iterable.scala:54)19:32:57.352 [INFO] at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)19:32:57.352 [INFO] at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)19:32:57.352 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)19:32:57.352 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)19:32:57.352 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)19:32:57.352 [INFO] at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)19:32:57.352 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)19:32:57.352 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)19:32:57.352 [INFO] at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)19:32:57.352 [INFO] at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)19:32:57.352 [INFO] at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)19:32:57.352 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:71)19:32:57.352 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:39)19:32:57.352 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)19:32:57.352 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)19:32:57.353 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)19:32:57.353 [INFO] at scala.tools.nsc.Global$Run.&lt;init&gt;(Global.scala:1242)19:32:57.353 [INFO] at scala.tools.nsc.Driver.doCompile(Driver.scala:31)19:32:57.353 [INFO] at scala.tools.nsc.MainClass.doCompile(Main.scala:23)19:32:57.353 [INFO] at scala.tools.nsc.Driver.process(Driver.scala:51)19:32:57.353 [INFO] at scala.tools.nsc.Driver.main(Driver.scala:64)19:32:57.353 [INFO] at scala.tools.nsc.Main.main(Main.scala)19:32:57.353 [INFO] ... 6 morehttps://api.travis-ci.org/v3/job/590390311/log.txtThe issue might be a Java 11 problem and might be fixable with passing -nobootcp to the scala compiler plugin.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-29 01:00:00" id="14282" opendate="2019-9-29 00:00:00" resolution="Done">
    <buginformation>
      <summary>Simplify DispatcherResourceManagerComponent</summary>
      <description>With the completion of the FLINK-14281 it is now possible to encapsulate the shutdown logic of the MiniDispatcher within the DispatcherRunner. Consequently, it is no longer necessary to have separate DispatcherResourceManagerComponent implementations. I suggest to remove the special case implementations.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.TestingMiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.SessionDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.SessionDispatcherResourceManagerComponent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.JobDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.JobDispatcherResourceManagerComponent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.StandaloneDispatcherRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.MiniDispatcherRunnerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.MiniDispatcherRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.MiniDispatcherRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DispatcherRunnerFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosSessionClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosJobClusterEntrypoint.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-29 01:00:00" id="14283" opendate="2019-9-29 00:00:00" resolution="Done">
    <buginformation>
      <summary>Update Kinesis consumer documentation for watermarks and event time alignment</summary>
      <description>Periodic per shard watermarking and event time alignment have been added over past releases but the doc has not been updated.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-29 01:00:00" id="14285" opendate="2019-9-29 00:00:00" resolution="Done">
    <buginformation>
      <summary>Simplify Dispatcher factories by removing generics</summary>
      <description>The Dispatcher factories can be simplified by removing the generics. For better maintainability of the code base, I propose to do this.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.TestingMiniCluster.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.SessionDispatcherWithUUIDFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.entrypoint.component.TestingDefaultDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.SessionDispatcherFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.StandaloneDispatcherRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.MiniDispatcherRunnerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.MiniDispatcherRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DispatcherRunnerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.JobDispatcherFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-10-1 01:00:00" id="14305" opendate="2019-10-1 00:00:00" resolution="Done">
    <buginformation>
      <summary>Move ownership of JobManagerMetricGroup to Dispatcher</summary>
      <description>With FLINK-14303 and FLINK-14299, it is now possible to move the ownership of the JobManagerMetricGroup into the Dispatcher. This makes the lifespan of the metric group shorter and the lifecycle management easier.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.SessionDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.SessionDispatcherResourceManagerComponent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.JobDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.JobDispatcherResourceManagerComponent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-2 01:00:00" id="14310" opendate="2019-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get ExecutionVertexID from ExecutionVertex rather than creating new instances</summary>
      <description>ExecutionVertexID is now added as a field to ExecutionVertex.Many components, however, are still creating ExecutionVertexID from ExecutionVertex by themselves. This may lead to more memory consumption of JM. It also slows down the ExecutionVertexID equality check.We should change them to use the field ExecutionVertex#executionVertexID directly.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adapter.ExecutionGraphToSchedulingTopologyAdapterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionGraphToInputsLocationsRetrieverAdapter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adapter.ExecutionGraphToSchedulingTopologyAdapter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.adapter.DefaultFailoverTopology.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.AdaptedRestartPipelinedRegionStrategyNG.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-2 01:00:00" id="14311" opendate="2019-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming File Sink end-to-end test failed on Travis</summary>
      <description>The Streaming File Sink end-to-end test fails on Travis because it does not produce output for 10 minutes.https://api.travis-ci.org/v3/job/591992274/log.txt</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-2 01:00:00" id="14312" opendate="2019-10-2 00:00:00" resolution="Done">
    <buginformation>
      <summary>Support building logical pipelined regions from JobGraph</summary>
      <description>Logical pipelined region partitioning is needed by FLINK-14060 to determine JobVertex slot sharing group.We can leverage PipelinedRegionComputeUtil#computePipelinedRegions to do this by adapting JobGraph to a base Topology.With changes from FLINK-14453, we can build LogicalPipelinedRegions from JobGraph by:1. Introduce LogicalTopology which extends Topology2. Implement DefaultLogicalTopology as an adapter of JobGraph to LogicalTopology3. Add DefaultLogicalTopology#getLogicalPipelinedRegions to return the logical pipelined regions of a JobGraph</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobVertexID.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.IntermediateDataSetID.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-4 01:00:00" id="14318" opendate="2019-10-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDK11 build stalls during shading</summary>
      <description>JDK11 build stalls during shading.Travis stage: e2d - misc - jdk11https://travis-ci.org/apache/flink/builds/593022581?utm_source=slack&amp;utm_medium=notificationhttps://api.travis-ci.org/v3/job/593022629/log.txtRelevant excerpt from logs:01:53:43.889 [INFO] ------------------------------------------------------------------------01:53:43.889 [INFO] Building flink-metrics-reporter-prometheus-test 1.10-SNAPSHOT01:53:43.889 [INFO] ------------------------------------------------------------------------...01:53:44.508 [INFO] Including org.apache.flink:force-shading:jar:1.10-SNAPSHOT in the shaded jar.01:53:44.508 [INFO] Excluding org.slf4j:slf4j-api:jar:1.7.15 from the shaded jar.01:53:44.508 [INFO] Excluding com.google.code.findbugs:jsr305:jar:1.3.9 from the shaded jar.01:53:44.508 [INFO] No artifact matching filter io.netty:netty01:53:44.522 [INFO] Replacing original artifact with shaded artifact.01:53:44.523 [INFO] Replacing /home/travis/build/apache/flink/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/flink-metrics-reporter-prometheus-test-1.10-SNAPSHOT.jar with /home/travis/build/apache/flink/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/flink-metrics-reporter-prometheus-test-1.10-SNAPSHOT-shaded.jar01:53:44.524 [INFO] Replacing original test artifact with shaded test artifact.01:53:44.524 [INFO] Replacing /home/travis/build/apache/flink/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/flink-metrics-reporter-prometheus-test-1.10-SNAPSHOT-tests.jar with /home/travis/build/apache/flink/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/flink-metrics-reporter-prometheus-test-1.10-SNAPSHOT-shaded-tests.jar01:53:44.524 [INFO] Dependency-reduced POM written at: /home/travis/build/apache/flink/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/dependency-reduced-pom.xmlNo output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-receivedThe build has been terminated</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.nightly.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-12-11 01:00:00" id="14378" opendate="2019-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup rocksDB lib folder if fail to load library</summary>
      <description>This improvement is inspired due to some of our machines need some time to load the rocksDB library. When some other unrecoverable exceptions continue to happen and the process to load library would be interrupted which cause the rocksdb-lib folder created but not cleaned up. As the job continues to failover, the rocksdb-lib folder would be created more and more. We even come across that machine was running out of inodes!Details could refer to current implementation</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBInitResetTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-15 01:00:00" id="14393" opendate="2019-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add an option to enable/disable cancel job in web ui</summary>
      <description>add the option to enable/disable cancel job in web uiwhen disabled, user can not cancel a job through the web ui</description>
      <version>1.10.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.DashboardConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.RestHandlerConfigurationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.DashboardConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RestHandlerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.configuration.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServer.java</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.WebOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.web.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">docs.content.docs.deployment.config.md</file>
      <file type="M">docs.content.zh.docs.deployment.config.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-10-16 01:00:00" id="14413" opendate="2019-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade-plugin ApacheNoticeResourceTransformer uses platform-dependent encoding</summary>
      <description>Some NOTICE files contain quotes that, at least on my system, result in some encoding errors when generating the binary licensing. One example can be found here; the closing quotes would be replaced with a question mark.This is due to the ApacheNoticeResourceTransformer using the platform encoding.</description>
      <version>shaded-8.0,1.8.2,1.9.0,1.10.0</version>
      <fixedVersion>shaded-9.0,1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-16 01:00:00" id="14416" opendate="2019-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Module interface and ModuleManager</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.MockTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RexProgramExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PushFilterIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.internal.BatchTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.java.internal.BatchTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.BatchTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.SelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.test.scala.org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImplTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.TableEnvironmentMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImplTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.BatchTableEnvironment.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-16 01:00:00" id="14417" opendate="2019-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Develop CoreModule to provide Flink built-in functions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.module.ModuleManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-11-16 01:00:00" id="14419" opendate="2019-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ModuleFactory, ModuleDescriptor, ModuleValidator for factory discovery service</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.module.ModuleConfig.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.module.ModuleManager.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-11-20 01:00:00" id="14468" opendate="2019-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Kubernetes docs</summary>
      <description>Two minor improvements to documented Kubernetes resource definitions: avoid referencing deprecated extensions/v1beta1/Deployment run unprivileged</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-23 01:00:00" id="14505" opendate="2019-10-23 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>SQL Client end-to-end test for Kafka 0.10 nightly end-to-end test failed on travis</summary>
      <description>The SQL Client end-to-end test for Kafka 0.10 nightly end-to-end test failed on Travis with[FAIL] 'SQL Client end-to-end test for Kafka 0.10' failed after 0 minutes and 37 seconds! Test exited with exit code 1No taskexecutor daemon (pid: 26336) is running anymore on travis-job-2c704099-0645-4182-942d-3fb5c2e10e54.No standalonesession daemon to stop on host travis-job-2c704099-0645-4182-942d-3fb5c2e10e54.https://api.travis-ci.org/v3/job/600710614/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.misc.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.kafka.common.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-10-29 01:00:00" id="14556" opendate="2019-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the package of cloud pickle</summary>
      <description>Currently the package structure of cloud pickle is as following:cloudpickle-1.2.2/cloudpickle-1.2.2/cloudpickle/cloudpickle-1.2.2/cloudpickle/__init__.py cloudpickle-1.2.2/cloudpickle/cloudpickle.py cloudpickle-1.2.2/cloudpickle/cloudpickle_fast.py cloudpickle-1.2.2/LICENSEIt should be:cloudpickle/ cloudpickle/__init__.py  cloudpickle/cloudpickle.py  cloudpickle/cloudpickle_fast.py  cloudpickle/LICENSEOtherwise, the following error will be thrown when running in a standalone cluster :"ImportError: No module named cloudpickle". </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.lib.cloudpickle-1.2.2-src.zip</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-29 01:00:00" id="14557" opendate="2019-10-29 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Clean up the package of py4j</summary>
      <description>Currently it contains a directory __MACOSX in the Py4j package. It's useless and should be removed. </description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.lib.py4j-0.10.8.1-src.zip</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-29 01:00:00" id="14558" opendate="2019-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the ClassNotFoundException issue for run python job in standalone mode</summary>
      <description>java.lang.ClassNotFoundException: org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperator will be thrown when running a Python UDF job in a standalone cluster.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-30 01:00:00" id="14573" opendate="2019-10-30 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support time data types in Python user-defined functions</summary>
      <description>This jira is a sub-task of FLINK-14388. In this jira, only time types are dedicated to be supported for python UDF.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.coders.test.common.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-11-31 01:00:00" id="14580" opendate="2019-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add HiveModuleFactory, HiveModuleDescriptor, and HiveModuleValidator</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-11-31 01:00:00" id="14588" opendate="2019-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 1.0.0 and 1.0.1</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveReflectionUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV110.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-1-5 01:00:00" id="14613" opendate="2019-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add validation check when applying UDF to tempral table key in Temporal Table Join condition</summary>
      <description>In Temporal Table Join, We don't support using  UDF in tempral table join key. For we can't analyze LookupKeys  when call is an expression. When users use like this, the program run normally,  and the result will be wrong. So we should add validation to prevent it.The SQL as following:INSERT INTO ASELECT B.amount, B.currency, C.amount, C.product FROM B join C FOR SYSTEM_TIME AS OF B.proctime on B.amount = cancat(C.amount, 'r') and C.product = '1'</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-7 01:00:00" id="14641" opendate="2019-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix description of metric `fullRestarts`</summary>
      <description>The metric `fullRestarts` counts both full restarts and fine grained restarts since 1.9.2.We should update the metric description doc accordingly.We need to pointing out the the metric counts full restarts in 1.9.1 or earlier versions, and turned to count all kinds of restarts since 1.9.2.</description>
      <version>1.9.2,1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-7 01:00:00" id="14652" opendate="2019-11-7 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Refactor checkpointing related parts into one place on task side</summary>
      <description>As suggested by sewen within review for PR-8693, it would be worthy to refactor all checkpointing parts into a single place on task side.This issue focus on refactoring these parts.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskStateManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-31 01:00:00" id="1466" opendate="2015-1-31 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add InputFormat to read HCatalog tables</summary>
      <description>HCatalog is a metadata repository and InputFormat to make Hive tables accessible to other frameworks such as Pig.Adding support for HCatalog would give access to Hive managed data.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-hcatalog.src.main.scala.org.apache.flink.hcatalog.scala.HCatInputFormat.scala</file>
      <file type="M">flink-staging.flink-hcatalog.src.main.java.org.apache.flink.hcatalog.java.HCatInputFormat.java</file>
      <file type="M">flink-staging.flink-hcatalog.src.main.java.org.apache.flink.hcatalog.HCatInputFormatBase.java</file>
      <file type="M">flink-staging.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-7 01:00:00" id="14660" opendate="2019-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add &amp;#39;SHOW MODULES&amp;#39; sql command</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-12-14 01:00:00" id="14781" opendate="2019-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[ZH] clarify that a RocksDB dependency in pom.xml may not be needed</summary>
      <description>The English version was clarified with respect when and how to add the maven dependencies via https://github.com/apache/flink/commit/d36ce5ff77fae2b01b8fbe8e5c15d610de8ed9f5. The Chinese version still needs that update</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-15 01:00:00" id="14796" opendate="2019-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document about limitations of different Hive versions</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-1-15 01:00:00" id="14814" opendate="2019-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show the vertex that produces the backpressure source in the job</summary>
      <description>By checking the status of output and input buffer pools exposed via FLINK-14815 (output buffer empty, input buffer full) it is possible to display which node is a source of the back pressure. This information could be displayed/accessible in the Web Frontend.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.metrics.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-detail.ts</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-12-17 01:00:00" id="14834" opendate="2019-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kerberized YARN on Docker test fails on Travis</summary>
      <description>https://api.travis-ci.org/v3/job/612782888/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">tools.travis.splits.split.container.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-5-21 01:00:00" id="14894" opendate="2019-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HybridOffHeapUnsafeMemorySegmentTest#testByteBufferWrap failed on Travis</summary>
      <description>HybridOffHeapUnsafeMemorySegmentTest&gt;MemorySegmentTestBase.testByteBufferWrapping:2465 expected:&lt;992288337&gt; but was:&lt;196608&gt;https://api.travis-ci.com/v3/job/258950527/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.MemorySegmentChecksTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemorySegmentFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.HybridMemorySegment.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-25 01:00:00" id="14939" opendate="2019-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamingKafkaITCase fails due to distDir property not being set</summary>
      <description>https://api.travis-ci.org/v3/job/616462870/log.txt08:12:34.965 [INFO] -------------------------------------------------------08:12:34.965 [INFO] T E S T S08:12:34.965 [INFO] -------------------------------------------------------08:12:35.868 [INFO] Running org.apache.flink.tests.util.kafka.StreamingKafkaITCase08:12:35.893 [ERROR] Tests run: 3, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 0.02 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.tests.util.kafka.StreamingKafkaITCase08:12:35.893 [ERROR] testKafka[0: kafka-version:0.10.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 0.009 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: The distDir property was not set. You can set it when running maven via -DdistDir=&lt;path&gt; . at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.&lt;init&gt;(StreamingKafkaITCase.java:71)08:12:35.893 [ERROR] testKafka[1: kafka-version:0.11.0.2](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 0.001 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: The distDir property was not set. You can set it when running maven via -DdistDir=&lt;path&gt; . at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.&lt;init&gt;(StreamingKafkaITCase.java:71)08:12:35.893 [ERROR] testKafka[2: kafka-version:2.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 0.001 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: The distDir property was not set. You can set it when running maven via -DdistDir=&lt;path&gt; . at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.&lt;init&gt;(StreamingKafkaITCase.java:71)08:12:36.233 [INFO] 08:12:36.233 [INFO] Results:08:12:36.233 [INFO] 08:12:36.233 [ERROR] Failures: 08:12:36.233 [ERROR] StreamingKafkaITCase.&lt;init&gt;:71 The distDir property was not set. You can set it when running maven via -DdistDir=&lt;path&gt; .08:12:36.233 [ERROR] StreamingKafkaITCase.&lt;init&gt;:71 The distDir property was not set. You can set it when running maven via -DdistDir=&lt;path&gt; .08:12:36.233 [ERROR] StreamingKafkaITCase.&lt;init&gt;:71 The distDir property was not set. You can set it when running maven via -DdistDir=&lt;path&gt; .08:12:36.233 [INFO] 08:12:36.233 [ERROR] Tests run: 3, Failures: 3, Errors: 0, Skipped: 008:12:36.233 [INFO]</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-25 01:00:00" id="14940" opendate="2019-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Travis build passes despite Test failures</summary>
      <description>Build https://travis-ci.org/apache/flink/jobs/616462870 is green despite the presence of Test failures.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-11-27 01:00:00" id="14968" opendate="2019-11-27 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Kerberized YARN on Docker test (custom fs plugin) fails on Travis</summary>
      <description>This change made the test flaky: https://github.com/apache/flink/commit/749965348170e4608ff2a23c9617f67b8c341df5. It changes the job to have two sources instead of one which, under normal circumstances, requires too many slots to run and therefore the job will fail.The setup of this test is very intricate, we configure YARN to have two NodeManagers with 2500mb memory each: https://github.com/apache/flink/blob/413a77157caf25dbbfb8b0caaf2c9e12c7374d98/flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster/config/yarn-site.xml#L39. We run the job with parallelism 3 and configure Flink to use 1000mb as TaskManager memory and 1000mb of JobManager memory. This means that the job fits into the YARN memory budget but more TaskManagers would not fit. We also don't simply increase the YARN resources because we want the Flink job to use TMs on different NMs because we had a bug where Kerberos config file shipping was not working correctly but the bug was not materialising if all TMs where on the same NM.https://api.travis-ci.org/v3/job/612782888/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.container.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-27 01:00:00" id="14972" opendate="2019-11-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement RemoteExecutor as a new Executor</summary>
      <description/>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.environment.RemoteStreamExecutionEnvironmentTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.RemoteStreamEnvironment.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.RemoteEnvironment.java</file>
      <file type="M">flink-clients.src.main.resources.META-INF.services.org.apache.flink.core.execution.ExecutorFactory</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-27 01:00:00" id="14978" opendate="2019-11-27 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce constraint class hierarchy required for primary keys</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.api.TableSchemaTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-28 01:00:00" id="14984" opendate="2019-11-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove old WebUI</summary>
      <description>Following the discussion on the ML, remove the old WebUI.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.mstile-310x150.png</file>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.taskmanager.taskmanager.stdout.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.taskmanager.taskmanager.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.taskmanager.taskmanager.log.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.taskmanager.taskmanager.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.taskmanager.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.submit.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.overview.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.running-jobs.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.timeline.vertex.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.timeline.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.properties.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.watermarks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.taskmanagers.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.subtasks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.summary.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.statistics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.overview.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.operator.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.job.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.history.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.details.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.counts.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.config.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoint-history.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.accumulators.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node-list.watermarks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node-list.subtasks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node-list.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node-list.checkpoints.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node-list.backpressure.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node-list.accumulators.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.exceptions.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.config.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.completed-jobs.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobmanager.stdout.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobmanager.log.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobmanager.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobmanager.config.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.js.vendor.js</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.js.hs.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.index.hs.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.safari-pinned-tab.svg</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.mstile-70x70.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.mstile-310x310.png</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.@angular</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.angular</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.angular-drag-and-drop-list</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.angular-moment</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.angular-ui-router</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.ant-design-palettes</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.bootstrap</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.core-js</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.d3</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.dagre</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.dagre-d3</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.ev-emitter</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.font-awesome</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.graphlib</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.imagesloaded</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.jquery</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.lodash</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.moment</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.moment-duration-format</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.monaco-editor</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.ng-zorro-antd</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.qtip2</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.rxjs</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.split</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.tinycolor2</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.tslib</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.zone</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-runtime-web.web-dashboard.angular.json</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.css.vendor.css</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.fonts.fontawesome-webfont.eot</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.fonts.fontawesome-webfont.svg</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.fonts.fontawesome-webfont.ttf</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.fonts.fontawesome-webfont.woff</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.fonts.fontawesome-webfont.woff2</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.fonts.FontAwesome.otf</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.android-chrome-192x192.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.android-chrome-512x512.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.apple-touch-icon.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.browserconfig.xml</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.favicon-16x16.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.favicon-32x32.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.favicon.ico</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.flink-logo.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.grips.horizontal.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.grips.vertical.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.manifest.json</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.mstile-144x144.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.mstile-150x150.png</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-2 01:00:00" id="15006" opendate="2019-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to close shuffle when dynamic partition inserting</summary>
      <description>When partition values are rare or have skew, if we shuffle by dynamic partitions, will break the performance.We can have an option to close shuffle in such cases:‘connector.sink.shuffle-by-partition.enable’ = ...</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSinkRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-2 01:00:00" id="15008" opendate="2019-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests in flink-yarn-tests fail with ClassNotFoundException (JDK11)</summary>
      <description>1) Error injecting constructor, java.lang.NoClassDefFoundError: javax/activation/DataSource at org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver.&lt;init&gt;(JAXBContextResolver.java:41) at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:51) while locating org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver1 error at com.google.inject.internal.InjectorImpl$4.get(InjectorImpl.java:987) at com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1013) at com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory$GuiceInstantiatedComponentProvider.getInstance(GuiceComponentProviderFactory.java:332) at com.sun.jersey.core.spi.component.ioc.IoCProviderFactory$ManagedSingleton.&lt;init&gt;(IoCProviderFactory.java:179) at com.sun.jersey.core.spi.component.ioc.IoCProviderFactory.wrap(IoCProviderFactory.java:100) at com.sun.jersey.core.spi.component.ioc.IoCProviderFactory._getComponentProvider(IoCProviderFactory.java:93) at com.sun.jersey.core.spi.component.ProviderFactory.getComponentProvider(ProviderFactory.java:153) at com.sun.jersey.core.spi.component.ProviderServices.getComponent(ProviderServices.java:251) at com.sun.jersey.core.spi.component.ProviderServices.getProviders(ProviderServices.java:148) at com.sun.jersey.core.spi.factory.ContextResolverFactory.init(ContextResolverFactory.java:83) at com.sun.jersey.server.impl.application.WebApplicationImpl._initiate(WebApplicationImpl.java:1271) at com.sun.jersey.server.impl.application.WebApplicationImpl.access$700(WebApplicationImpl.java:169) at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:775) at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:771) at com.sun.jersey.spi.inject.Errors.processWithErrors(Errors.java:193) at com.sun.jersey.server.impl.application.WebApplicationImpl.initiate(WebApplicationImpl.java:771) at com.sun.jersey.guice.spi.container.servlet.GuiceContainer.initiate(GuiceContainer.java:121) at com.sun.jersey.spi.container.servlet.ServletContainer$InternalWebComponent.initiate(ServletContainer.java:318) at com.sun.jersey.spi.container.servlet.WebComponent.load(WebComponent.java:609) at com.sun.jersey.spi.container.servlet.WebComponent.init(WebComponent.java:210) at com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:373) at com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:710) at com.google.inject.servlet.FilterDefinition.init(FilterDefinition.java:114) at com.google.inject.servlet.ManagedFilterPipeline.initPipeline(ManagedFilterPipeline.java:98) at com.google.inject.servlet.GuiceFilter.init(GuiceFilter.java:172) at org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50) at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713) at org.mortbay.jetty.servlet.Context.startContext(Context.java:140) at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282) at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518) at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50) at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152) at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50) at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130) at org.mortbay.jetty.Server.doStart(Server.java:224) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50) at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:936) ... 50 moreCaused by: java.lang.NoClassDefFoundError: javax/activation/DataSource at com.sun.xml.bind.v2.model.impl.RuntimeBuiltinLeafInfoImpl.&lt;clinit&gt;(RuntimeBuiltinLeafInfoImpl.java:457) at com.sun.xml.bind.v2.model.impl.RuntimeTypeInfoSetImpl.&lt;init&gt;(RuntimeTypeInfoSetImpl.java:65) at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.createTypeInfoSet(RuntimeModelBuilder.java:133) at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.createTypeInfoSet(RuntimeModelBuilder.java:85) at com.sun.xml.bind.v2.model.impl.ModelBuilder.&lt;init&gt;(ModelBuilder.java:156) at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.&lt;init&gt;(RuntimeModelBuilder.java:93) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.getTypeInfoSet(JAXBContextImpl.java:473) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.&lt;init&gt;(JAXBContextImpl.java:319) at com.sun.xml.bind.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1170) at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:145) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:297) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:286) at javax.xml.bind.ContextFinder.find(ContextFinder.java:409) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:721) at com.sun.jersey.api.json.JSONJAXBContext.&lt;init&gt;(JSONJAXBContext.java:246) at org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver.&lt;init&gt;(JAXBContextResolver.java:65) at org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver$$FastClassByGuice$$6a7be7f6.newInstance(&lt;generated&gt;) at com.google.inject.internal.cglib.reflect.$FastConstructor.newInstance(FastConstructor.java:40) at com.google.inject.internal.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:60) at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:85) at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:254) at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46) at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1031) at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40) at com.google.inject.Scopes$1$1.get(Scopes.java:65) at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:40) at com.google.inject.internal.InjectorImpl$4$1.call(InjectorImpl.java:978) at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1024) at com.google.inject.internal.InjectorImpl$4.get(InjectorImpl.java:974) ... 89 moreCaused by: java.lang.ClassNotFoundException: javax.activation.DataSource at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:583) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521) ... 122 morehttps://api.travis-ci.org/v3/job/619217945/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-2 01:00:00" id="15009" opendate="2019-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a utility for creating type inference logic via reflection</summary>
      <description>This is the last missing piece for completing FLIP-65 from an API point of view. The utility will take a UserDefinedFunction and return TypeInference that can be used for validation and planning.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.utils.ExtractionUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.utils.DataTypeTemplate.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.extraction.DataTypeExtractorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.DataTypeExtractor.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.types.extraction.DataTypeExtractorScalaTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-12-3 01:00:00" id="15034" opendate="2019-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump FRocksDB version for memory control</summary>
      <description>Since FLINK-14483 has already been resolved and a new version of FRocksDB has been released, we should bump the FRocksDB version in Flink for next steps in memory control.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-3 01:00:00" id="15042" opendate="2019-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix python compatibility by excluding the Env.executeAsync() (FLINK-14854)</summary>
      <description>Click to add description</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.dataset.tests.test.execution.environment.completeness.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-4 01:00:00" id="15047" opendate="2019-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>YarnDistributedCacheITCase is unstable</summary>
      <description>See also https://api.travis-ci.com/v3/job/262854881/log.txtcc Zhenqiu Huang</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ActiveResourceManagerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-4 01:00:00" id="15053" opendate="2019-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configurations with values contains space may cause TM failures on Yarn</summary>
      <description>Currently on Yarn setups, we are passing task executor specific configurations through dynamic properties in the starting command (see FLINK-13184).If the value of configuration contains space, the dynamic properties may not be correctly parsed, which could cause task executor failures. On occurrence can be found in FLINK-15047.It would be good to allow spaces when passing dynamic properties. E.g., surrounding the values with double quotation marks, or escaping special characters.cc &amp;#91;~fly_in_gis&amp;#93;</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.BootstrapToolsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-4 01:00:00" id="15057" opendate="2019-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set taskmanager.memory.total-process.size in jepsen tests</summary>
      <description>Set taskmanager.memory.total-process.size in flink-conf.yaml used by tests. Currently the taskmanager process fails due toorg.apache.flink.configuration.IllegalConfigurationException: Either Task Heap Memory size and Managed Memory size, or Total Flink Memory size, or Total Process Memory size need to be configured explicitly. at org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.resourceSpecFromConfig(TaskExecutorResourceUtils.java:110) at org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfiguration.fromConfiguration(TaskManagerServicesConfiguration.java:219) at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.startTaskManager(TaskManagerRunner.java:357) at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.&lt;init&gt;(TaskManagerRunner.java:153) at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManager(TaskManagerRunner.java:327) at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$1.call(TaskManagerRunner.java:298) at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$1.call(TaskManagerRunner.java:295) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.main(TaskManagerRunner.java:295)</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-4 01:00:00" id="15058" opendate="2019-12-4 00:00:00" resolution="Done">
    <buginformation>
      <summary>Log required config keys if TaskManager memory configuration is invalid</summary>
      <description>Currently the error message isEither Task Heap Memory size and Managed Memory size, or Total Flink Memory size, or Total Process Memory size need to be configured explicitlyHowever, it would be good to immediately see which config keys are expected to be configured.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-5 01:00:00" id="15073" opendate="2019-12-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>sql client fails to run same query multiple times</summary>
      <description>Flink SQL&gt; select abs(-1);&amp;#91;INFO&amp;#93; Result retrieval cancelled.Flink SQL&gt; select abs(-1);&amp;#91;ERROR&amp;#93; Could not execute SQL statement. Reason:org.apache.flink.table.api.ValidationException: Table 'default: select abs(-1)' already exists. Please choose a different name.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-6 01:00:00" id="15091" opendate="2019-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JoinITCase.testFullJoinWithNonEquiJoinPred failed in travis</summary>
      <description>04:45:22.404 &amp;#91;ERROR&amp;#93; Tests run: 21, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.909 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.planner.runtime.batch.table.JoinITCase 04:45:22.406 &amp;#91;ERROR&amp;#93; testFullJoinWithNonEquiJoinPred(org.apache.flink.table.planner.runtime.batch.table.JoinITCase) Time elapsed: 0.168 s &lt;&lt;&lt; ERROR! org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.table.planner.runtime.batch.table.JoinITCase.testFullJoinWithNonEquiJoinPred(JoinITCase.scala:344) Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy Caused by: org.apache.flink.runtime.memory.MemoryAllocationException: Could not allocate 32 pages. Only 0 pages are remaining. details: https://api.travis-ci.org/v3/job/621407747/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-3-6 01:00:00" id="15100" opendate="2019-12-6 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add the interface and base implementation for SourceReader.</summary>
      <description>Add the interface and base implementation for SourceReader. Including threading model, SplitReader / RecordEmitter. This ticket should also integrate the SourceReader into the SourceReaderStreamTask.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertexCoordinatorContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SourceReaderContext.java</file>
      <file type="M">flink-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-6 01:00:00" id="15105" opendate="2019-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resuming Externalized Checkpoint after terminal failure (rocks, incremental) end-to-end test stalls on travis</summary>
      <description>Resuming Externalized Checkpoint after terminal failure (rocks, incremental) end-to-end test fails on release-1.9 nightly build stalls with "The job exceeded the maximum log length, and has been terminated".https://api.travis-ci.org/v3/job/621090394/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.checkpoints.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-6 01:00:00" id="15106" opendate="2019-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive 3.x tests leave metastore_db folder under build directory</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.FlinkStandaloneHiveServerContext.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-6 01:00:00" id="15107" opendate="2019-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL-CLI can not execute insert into statement with lowercase "INSERT INTO" keyword</summary>
      <description>This was introduced by FLINK-15026 which has a always uppercase case "INSERT INTO" pattern matcher check.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-6 01:00:00" id="15118" opendate="2019-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make flink-scala-shell use Executors</summary>
      <description>Click to add description</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.java.org.apache.flink.api.java.FlinkILoopTest.java</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellRemoteStreamEnvironment.java</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellRemoteEnvironment.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-12-8 01:00:00" id="15128" opendate="2019-12-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document support for Hive timestamp type</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-5-9 01:00:00" id="15138" opendate="2019-12-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add e2e Test for PyFlink</summary>
      <description>Currently, both the Table API and Python native UDF are supported in 1.10, I would like to add the e2e test for PyFlink.What do you think?</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-9 01:00:00" id="15139" opendate="2019-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>misc end to end test failed cause loss jars in converting to jobgraph</summary>
      <description>The test Running 'SQL Client end-to-end test (Old planner)' in misc e2e test failedlog:(a94d1da25baf2a5586a296d9e933743c) switched from RUNNING to FAILED.org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot load user class: org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkClassLoader info: URL ClassLoader:Class not resolvable through given classloader. at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:266) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:430) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:353) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:419) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:353) at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:144) at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:432) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:460) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.ClassNotFoundException: org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:60) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at org.apache.flink.util.InstantiationUtil$ClassLoaderObjectInputStream.resolveClass(InstantiationUtil.java:78) at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1868) at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:254) ... 10 morelink: https://travis-ci.org/apache/flink/jobs/622261358 </description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ExecutionConfigAccessor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-9 01:00:00" id="15146" opendate="2019-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix check that incremental cleanup size must be greater than zero</summary>
      <description> Hi , the value of cleanupSize is grater than or equal 0. Whether that the value is grater than 0 is more practical. </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.state.StateTtlConfigTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateTtlConfig.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-9 01:00:00" id="15163" opendate="2019-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>japicmp should use 1.9 as the old version</summary>
      <description>We should configure the japicmp-maven-plugin to use the latest Flink 1.9 release as the reference version to compare against. Currently 1.8.0 is used.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-1-10 01:00:00" id="15171" opendate="2019-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression in serialisation benchmarks</summary>
      <description>There is quite significant performance regression in serialisation benchmarks in the commit range 2ecf7ca..9320f34 (which includes FLINK-14346).http://codespeed.dak8s.net:8000/timeline/?ben=serializerTuple&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=serializerRow&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=serializerPojo&amp;env=2it coincides with the performance improvement for heavy stringshttp://codespeed.dak8s.net:8000/timeline/?ben=serializerHeavyString&amp;env=2it might be caused by some accidental change in the benchmarking code (changing parallelism in one benchmarks is carried on to the next one?) or in the code itself.CC Roman Grebennikov Arvid Heise</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.StringValue.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-12-11 01:00:00" id="15197" opendate="2019-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add resource related config options to dynamical properties for Kubernetes</summary>
      <description>Since FLIP-49 and FLIP-53 have been completely merged. The new introduced resource config options should be passed to taskmanager via dynamic properties for Kubernetes. The dynamic properties are generated in KubernetesResourceManager.TaskExecutorResourceUtils#generateDynamicConfigsStr could be used to generate the dynamic properties. cc &amp;#91;~xintongsong&amp;#93;</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesUtilsTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-9-11 01:00:00" id="1520" opendate="2015-2-11 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Read edges and vertices from CSV files</summary>
      <description>Add methods to create Vertex and Edge Datasets directly from CSV file inputs.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.GraphCreationWithCsvITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.GatherSumApplyConfigurationITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.library.GSATriangleCount.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.GraphCsvReader.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.Graph.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.SingleSourceShortestPaths.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.IncrementalSSSP.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.GSASingleSourceShortestPaths.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.GraphMetrics.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.ConnectedComponents.java</file>
      <file type="M">docs.libs.gelly.guide.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-11 01:00:00" id="15203" opendate="2019-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>rephrase Hive&amp;#39;s data types doc</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-11 01:00:00" id="15205" opendate="2019-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add doc and exmaple of INSERT OVERWRITE and insert into partitioned table for Hive connector</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.read.write.hive.zh.md</file>
      <file type="M">docs.dev.table.hive.read.write.hive.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-12 01:00:00" id="15221" opendate="2019-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support exactly once sink delivery semantic for Kafka in Table API</summary>
      <description>The Table Api doesn't support End to End Exactly once sematic like datastream Api.  Does Flink have a plan for this?</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaOptions.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.table.Kafka011DynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.table.Kafka011DynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.table.Kafka011DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.table.Kafka010DynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.table.Kafka010DynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.table.Kafka010DynamicSink.java</file>
      <file type="M">docs.dev.table.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-12 01:00:00" id="15228" opendate="2019-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop vendor specific deployment documentation</summary>
      <description>Based on a mailing list discussion we want to drop vendor specific deployment documentationml discussion: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Drop-vendor-specific-deployment-documentation-td35457.html</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
      <file type="M">docs.ops.filesystems.s3.md</file>
      <file type="M">docs.ops.deployment.mapr.setup.zh.md</file>
      <file type="M">docs.ops.deployment.mapr.setup.md</file>
      <file type="M">docs.ops.deployment.gce.setup.zh.md</file>
      <file type="M">docs.ops.deployment.gce.setup.md</file>
      <file type="M">docs.ops.deployment.aws.zh.md</file>
      <file type="M">docs.ops.deployment.aws.md</file>
      <file type="M">docs.internals.components.zh.md</file>
      <file type="M">docs.internals.components.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-13 01:00:00" id="15245" opendate="2019-12-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink running in one cluster cannot write data to Hive tables in another cluster</summary>
      <description>Launch Flink cluster and write some data to a Hive table in another cluster. The job finishes successfully but data is not really written.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOutputFormatFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-12-15 01:00:00" id="15263" opendate="2019-12-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add dedicated page for HiveCatalog</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-16 01:00:00" id="15268" opendate="2019-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shaded Hadoop S3A end-to-end test fails on travis</summary>
      <description>As titled, the 'Shaded Hadoop S3A end-to-end test' case failed with below error:java.io.IOException: regular upload failed: java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AUtils.extractException(S3AUtils.java:291) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3ABlockOutputStream.putObject(S3ABlockOutputStream.java:448) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:360) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101) at org.apache.flink.fs.s3.common.hadoop.HadoopDataOutputStream.close(HadoopDataOutputStream.java:52) at org.apache.flink.core.fs.ClosingFSDataOutputStream.close(ClosingFSDataOutputStream.java:64) at java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188) at java.base/sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:341) at java.base/sun.nio.cs.StreamEncoder.close(StreamEncoder.java:161) at java.base/java.io.OutputStreamWriter.close(OutputStreamWriter.java:258) at org.apache.flink.api.java.io.CsvOutputFormat.close(CsvOutputFormat.java:170) at org.apache.flink.runtime.operators.DataSinkTask.invoke(DataSinkTask.java:227) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527) at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException at org.apache.flink.fs.s3base.shaded.com.amazonaws.util.Md5Utils.md5AsBase64(Md5Utils.java:104) at org.apache.flink.fs.s3base.shaded.com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1647) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AFileSystem.putObjectDirect(S3AFileSystem.java:1531) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$putObject$5(WriteOperationHelper.java:426) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:260) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)https://api.travis-ci.org/v3/job/625037121/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-16 01:00:00" id="15269" opendate="2019-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hive dialect limitation to overwrite and partition syntax</summary>
      <description>As http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Overwrite-and-partition-inserting-support-in-1-10-td35829.html#a35885 discussed.We should: Remove hive dialect limitation for supported "INSERT OVERWRITE" and "INSERT ... PARTITION(...)". Limit "CREATE TABLE ... PARTITIONED BY" to hive dialect.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.resources.org.apache.flink.sql.parser.utils.ParserResource.properties</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.validate.FlinkSqlConformance.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.utils.ParserResource.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-16 01:00:00" id="15270" opendate="2019-12-16 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add documentation about how to specify third-party dependencies via API for Python UDFs</summary>
      <description>Currently we have already provided APIs and command line options to allow users to specify third-part dependencies which may be used in Python UDFs. There are already documentation about how to specify third-part dependencies in the command line options. We should also add documentation about how to specify third-party dependencies via API.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.table.config.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-16 01:00:00" id="15271" opendate="2019-12-16 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add documentation about the Python environment requirements</summary>
      <description>Python UDF has specific requirements about the Python environments, such as Python 3.5+, Beam 2.15.0, etc.  We should add clear documentation about these requirements.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-16 01:00:00" id="15272" opendate="2019-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better error message when insert partition with values</summary>
      <description>Now, we not support insert partition with values like:Insert into mytable partition (date='2019-08-08') values ('jason', 25)Will throw a exception:schema not match.We should improve error message to tell user we not support this pattern.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-16 01:00:00" id="15274" opendate="2019-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Filesystem documentation to reflect changes in shading</summary>
      <description/>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.filesystems.index.zh.md</file>
      <file type="M">docs.ops.filesystems.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-16 01:00:00" id="15275" opendate="2019-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update CLI documentation to include only current valid options</summary>
      <description>Currently the documentation for the CLI contains outdated/invalid information, such as deprecated and removed options. This has to be fixed before the release.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-16 01:00:00" id="15278" opendate="2019-12-16 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Update StreamingFileSink documentation</summary>
      <description>Many times in the ML we have seen questions about the StreamingFileSink that could have been answered with better documentation that includes:1) shortcomings (especially in the case of S3 and also bulk formats)2) file lifecycle</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.streamfile.sink.md</file>
      <file type="M">docs.dev.connectors.streamfile.sink.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-16 01:00:00" id="15279" opendate="2019-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document new `executeAsync()` method and the newly introduced `JobClient`</summary>
      <description/>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.api.concepts.zh.md</file>
      <file type="M">docs.dev.api.concepts.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-16 01:00:00" id="15281" opendate="2019-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Map Flink&amp;#39;s TypeInference to Calcite&amp;#39;s interfaces</summary>
      <description>After a TypeInference is available (either through reflective extraction or manual definition), the information needs to be connected to Calcite's interfaces. In particular, SqlOperandTypeInference, SqlOperandTypeChecker, SqlReturnTypeInference.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeInferenceUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.ValueLiteralExpression.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-17 01:00:00" id="15287" opendate="2019-12-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ack flink-hadoop-compatibility and flink-orc into flink-hive</summary>
      <description>flink-connector-hive should contain flink-hadoop-compatibility to reduce users' efforts in figuring out dependency jars, and flink-orc for adding orc dependency which is missing in hive 2.2.x.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-17 01:00:00" id="15288" opendate="2019-12-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Starting jobmanager pod should respect containerized.heap-cutoff</summary>
      <description>Starting jobmanager pod should respect containerized.heap-cutoff. The cutoff will be used to leave some memory for jvm off-heap, for example meta space, thread native memory and etc. public static final ConfigOption&lt;Float&gt; CONTAINERIZED_HEAP_CUTOFF_RATIO = ConfigOptions .key("containerized.heap-cutoff-ratio") .defaultValue(0.25f) .withDeprecatedKeys("yarn.heap-cutoff-ratio") .withDescription("Percentage of heap space to remove from containers (YARN / Mesos / Kubernetes), to compensate" + " for other JVM memory usage.");</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesUtilsTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClusterDescriptorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs..includes.generated.resource.manager.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-18 01:00:00" id="15311" opendate="2019-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lz4BlockCompressionFactory should use native compressor instead of java unsafe</summary>
      <description>According to:https://lz4.github.io/lz4-java/1.7.0/lz4-compression-benchmark/Java java unsafe compressor has lower performance than native lz4 compressor.After FLINK-14845 , we use lz4 compression for shuffler.In testing, I found shuffle using java unsafe compressor.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-18 01:00:00" id="15313" opendate="2019-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can not insert decimal with precision into sink using TypeInformation</summary>
      <description>Sink DDL: val sinkDDL = s""" |CREATE TABLE T2 ( | d DECIMAL(10, 2), | cnt INT |) with ( | 'connector.type' = 'filesystem', | 'connector.path' = '$sinkFilePath', | 'format.type' = 'csv', | 'format.field-delimiter' = ',' |) """.stripMarginUsing blink with batch mode. (ensure insert BinaryRow into sink table) In FLINK-15124 , but we still use wrong precision to construct DataFormatConverter. This will lead to exception when inserting BinaryRow. (BinaryRow need correct precision to get) </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamTestSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.TableSinkValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.factories.utils.TestCollectionTableFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sinks.TableSinkUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sinks.DataStreamTableSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.BatchLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.SinkCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-19 01:00:00" id="15322" opendate="2019-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet test fails with Hive versions prior to 1.2.0</summary>
      <description>Some data types are not supported by parquet tables in older Hive versions. Related to: https://issues.apache.org/jira/browse/HIVE-6384</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-19 01:00:00" id="15332" opendate="2019-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen tests are broken due to copying un-relocated flink-s3-fs-hadoop* into lib</summary>
      <description>The Jepsen tests are currently broken because we copy flink-s3-fs-hadoop* into lib. With FLINK-11956 we removed the relocations from these classes and hence they need to reside in plugins.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-20 01:00:00" id="15338" opendate="2019-12-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TM Metaspace memory leak when submitting PyFlink UDF jobs multiple times</summary>
      <description>Start a standalone cluster and after submit PyFlink UDF jobs multiple times, the TM will fail with the following exception: Caused by: java.lang.OutOfMemoryError: Metaspace at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:788) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:448) at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:60) at java.lang.ClassLoader.loadClass(ClassLoader.java:380) at org.apache.flink.api.python.shaded.com.fasterxml.jackson.databind.ObjectMapper.&lt;init&gt;(ObjectMapper.java:628) at org.apache.flink.api.python.shaded.com.fasterxml.jackson.databind.ObjectMapper.&lt;init&gt;(ObjectMapper.java:531) at org.apache.beam.sdk.options.PipelineOptionsFactory.&lt;clinit&gt;(PipelineOptionsFactory.java:469) at org.apache.flink.python.AbstractPythonFunctionRunner.open(AbstractPythonFunctionRunner.java:173) at org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator$ProjectUdfInputPythonScalarFunctionRunner.open(AbstractPythonScalarFunctionOperator.java:193) at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.open(AbstractPythonFunctionOperator.java:139) at org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator.open(AbstractPythonScalarFunctionOperator.java:143) at org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperator.open(BaseRowPythonScalarFunctionOperator.java:86) at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1018) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454) at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$125/800044563.run(Unknown Source) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94) at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527) at java.lang.Thread.run(Thread.java:834)</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-20 01:00:00" id="15342" opendate="2019-12-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify querying Hive view</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-20 01:00:00" id="15344" opendate="2019-12-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update limitations in hive udf document</summary>
      <description>The limitation is not valid now.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.hive.functions.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.functions.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-1-24 01:00:00" id="15369" opendate="2019-12-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MiniCluster use fixed network / managed memory sizes by default</summary>
      <description>Currently, Mini Cluster may allocate off-heap memory (managed &amp; network) according to the JVM free heap size and configured off-heap fractions. This could lead to unnecessary large off-heap memory usage and unpredictable / hard-to-understand behaviors.We believe a fix value for managed / network memory would be enough for a such a setup that runs Flink as a library.Detailed discussion can be found in this ML thread.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.MiniClusterConfigurationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-24 01:00:00" id="15373" opendate="2019-12-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update descriptions for framework / task off-heap memory config options</summary>
      <description>Update descriptions for "taskmanager.memory.framework.off-heap.size" and "taskmanager.memory.task.off-heap.size" to explicitly state that: Both direct and native memory are accounted Will be fully counted into MaxDirectMemorySizeDetailed discussion can be found in this ML thread.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.task.manager.memory.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-24 01:00:00" id="15374" opendate="2019-12-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update descriptions for jvm overhead config options</summary>
      <description>Update descriptions for "taskmanager.memory.jvm-overhead.&amp;#91;min|max|fraction&amp;#93;" to remove "I/O direct memory" and explicitly state that it's not counted into MaxDirectMemorySize.Detailed discussion can be found in this ML thread.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.task.manager.memory.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-24 01:00:00" id="15377" opendate="2019-12-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mesos WordCount test fails on travis</summary>
      <description>The "Run Mesos WordCount test" fails nightly run on travis with below error:rm: cannot remove '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test-data/log/mesos-sl/mesos-slave.INFO': Permission deniedrm: cannot remove '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test-data/log/mesos-sl/mesos-fetcher.INFO': Permission deniedrm: cannot remove '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test-data/log/mesos-sl/mesos-slave.4a4fda410c57.invalid-user.log.INFO.20191224-031307.1': Permission denied...[FAIL] 'Run Mesos WordCount test' failed after 5 minutes and 26 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out fileshttps://api.travis-ci.org/v3/job/628795106/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-mesos-cluster.Dockerfile</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-mesos-cluster.docker-compose.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-24 01:00:00" id="15380" opendate="2019-12-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to set number of TM and number of Slot for MiniCluster in Scala shell</summary>
      <description>It is possible to set that for MiniCluster in other places, but unable to do that in Scala shell.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-24 01:00:00" id="15381" opendate="2019-12-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>INSERT INTO VALUES statement fails if a cast project is applied</summary>
      <description>The following query will fail: @Test def test(): Unit = { val sinkDDL = """ |create table t2( | a int, | b string |) with ( | 'connector' = 'COLLECTION' |) """.stripMargin val query = """ |insert into t2 select cast(a as int), cast(b as varchar) from (values (3, 'c')) T(a,b) """.stripMargin tableEnv.sqlUpdate(sinkDDL) tableEnv.sqlUpdate(query) execJob("testJob") }exception:org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: LogicalSink(name=[`default_catalog`.`default_database`.`t2`], fields=[a, b])+- LogicalProject(EXPR$0=[$0], EXPR$1=[CAST($1):VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL]) +- LogicalValues(type=[RecordType(INTEGER a, CHAR(1) b)], tuples=[[{ 3, _UTF-16LE'c' }]])This exception indicates that the query uses an unsupported SQL feature.Please check the documentation for the set of currently supported SQL features.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkDefaultRelMetadataProvider.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-3-26 01:00:00" id="15396" opendate="2019-12-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to ignore parse errors for JSON format</summary>
      <description>We already support 'format.ignore-parse-errors' to skip dirty records in CSV format. We can also support it in JSON format.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.table.descriptors.JsonTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.table.descriptors.JsonValidator.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.table.descriptors.Json.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDeserializationSchema.java</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-11-26 01:00:00" id="15408" opendate="2019-12-26 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Interval join support no equi-condition</summary>
      <description>For Now, Interval join must has at least one equi-condition. Should we need to allow no equi-condition like regular join?For that, if sql like as follow:INSERT INTO A SELECT * FROM B join C on B.rowtime BETWEEN C.rowtime - INTERVAL '20' SECOND AND C.rowtime + INTERVAL '30' SECONDIt will has no matched rule to convert.Caused by: org.apache.calcite.plan.RelOptPlanner$CannotPlanException: There are not enough rules to produce a node with desired properties: convention=STREAM_PHYSICAL, FlinkRelDistributionTraitDef=any, MiniBatchIntervalTraitDef=None: 0, UpdateAsRetractionTraitDef=false, AccModeTraitDef=UNKNOWN.Missing conversion is FlinkLogicalJoin[convention: LOGICAL -&gt; STREAM_PHYSICAL]</description>
      <version>1.10.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.IntervalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecIntervalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecIntervalJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-27 01:00:00" id="15420" opendate="2019-12-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cast string to timestamp will loose precision</summary>
      <description>cast('2010-10-14 12:22:22.123456' as timestamp(9))Will produce "2010-10-14 12:22:22.123" in blink planner, this should not happen.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlDateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-1-27 01:00:00" id="15427" opendate="2019-12-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>State TTL RocksDb backend end-to-end test stalls on travis</summary>
      <description>The 'State TTL RocksDb backend end-to-end test' case stalls and finally timedout with error message:The job exceeded the maximum log length, and has been terminated.https://api.travis-ci.org/v3/job/629699416/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.misc.hadoopfree.sh</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.stream.state.ttl.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.TtlVerifyUpdateFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-13 01:00:00" id="1543" opendate="2015-2-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Proper exception handling in actors</summary>
      <description>With Akka's actors it is important to not throw exceptions in the actor thread, if one does not want that the actor restarts or stops. Many of the Java components which are called from the actor's receive method throw exceptions which are not properly handled by the actor thread. Therefore, we have to catch these exceptions and handle them properly.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.ActorLogMessages.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.accumulators.AccumulatorManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.InstanceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-29 01:00:00" id="15435" opendate="2019-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ExecutionConfigTests.test_equals_and_hash in pyFlink fails when cpu core numbers is 6</summary>
      <description>My laptop's cpu core number is 6, so the default parallelism of the ExecutionEnvironment is 12. And the test will fail.</description>
      <version>1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.common.tests.test.execution.config.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-31 01:00:00" id="15439" opendate="2019-12-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix incorrect description about unsupported DDL in "Queries" page</summary>
      <description>DDL is supported now, document should be updated.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sql.index.zh.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-1-31 01:00:00" id="15442" opendate="2019-12-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden the Avro Confluent Schema Registry nightly end-to-end test</summary>
      <description>We have already harden the Avro Confluent Schema Registry test in FLINK-13567. However, there are still some defects in current mechanism. The loop variable i is not safe, it could be modified by the command. The process of downloading kafka 0.10 is not included in the scope of retry_times . I think we need to include it to tolerent transient network issue.We need to fix those issue to harden the Avro Confluent Schema Registry nightly end-to-end test.cc: Till Rohrmann Chesnay Schepler</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-31 01:00:00" id="15446" opendate="2019-12-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve "Connect to External Systems" documentation page</summary>
      <description>1. Remove documentation for format schema, which is not necessary any more and is deprecated.2. Add DDL documentation for "Table Schema" and "Rowtime Attribute" sections.3. Update the comments in DDL for better rendering (do not wrap line).</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-3 01:00:00" id="15468" opendate="2020-1-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>INSERT OVERWRITE not supported from SQL CLI</summary>
      <description>Running INSERT OVERWRITE from SQL CLI will get:[ERROR] Unknown or invalid SQL statement.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-1-6 01:00:00" id="15489" opendate="2020-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebUI log refresh not working</summary>
      <description>There is no way to query the latest state of logs of jobmanager/taskmanager.The Web UI show only the first version that was ever displayed.How to reproduce: (not sure if necessary) configure logback as described here: https://ci.apache.org/projects/flink/flink-docs-stable/dev/best_practices.html#use-logback-when-running-flink-on-a-cluster start a cluster show jobmanager logs in the Web UI run example job check again the jobmanager logs, there is no trace of the job. Clicking the refresh button does not help</description>
      <version>1.9.1,1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-1-7 01:00:00" id="15495" opendate="2020-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set default planner for SQL Client to Blink planner</summary>
      <description>As discussed in the mailing list &amp;#91;1&amp;#93;, we will change the default planner to Blink planner for SQL CLI. &amp;#91;1&amp;#93;: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Set-default-planner-for-SQL-Client-to-Blink-planner-in-1-10-release-td36379.html</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.conf.sql-client-defaults.yaml</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-7 01:00:00" id="15499" opendate="2020-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>No debug log describes the host of a TM before any task is deployed to it in YARN mode</summary>
      <description>When troubleshooting FLINK-15456, I noticed a TM hang in starting and not able to register to RM. However, there is no debug log on which host the TM located on and thus I can hardly find the logs of the problematic TM.I think we should print the host name when starting a TM, i.e. in this logs"TaskExecutor container_XXXX will be started ...".This would make it possible for us to troubleshoot similar problems. (not only for cases that TM hang in starting, but also for cases that TM exits in starting)</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-8 01:00:00" id="15512" opendate="2020-1-8 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Refactor the mechanism of how to constructure the cache and write buffer manager shared across RocksDB instances</summary>
      <description>FLINK-14484 introduce a LRUCache to share among RocksDB instances, so that the memory usage by RocksDB could be controlled well. However, due to the implementation and some bugs in RocksDB (issue-6247), we cannot limit the memory strictly.The way to walk around this issue is to consider the buffer which memtable would overuse (1/2 write buffer manager size). By introducing this, the actual cache size for user to share is not the same as the managed off-heap memory or user configured memory.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMemoryConfiguration.java</file>
      <file type="M">docs..includes.generated.rocks.db.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-8 01:00:00" id="15515" opendate="2020-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document that Hive connector should be used with blink planner</summary>
      <description>HiveCatalog works with both old and blink planner. But read/write Hive tables only works with blink planner.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.scala.shell.hive.zh.md</file>
      <file type="M">docs.dev.table.hive.scala.shell.hive.md</file>
      <file type="M">docs.dev.table.hive.read.write.hive.zh.md</file>
      <file type="M">docs.dev.table.hive.read.write.hive.md</file>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-8 01:00:00" id="15518" opendate="2020-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t hide web frontend side pane automatically</summary>
      <description>As mentioned in FLINK-13386 the side pane hides automatically in some cases but not all cases. When I was debugging or trying the web frontend I found this behaviour a bit disconcerting. Could we disable the hiding by default? The user can still manually hide if they want to.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app-routing.module.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-8 01:00:00" id="15520" opendate="2020-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prometheus E2E test should use DownloadCache</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.src.test.java.org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-1-9 01:00:00" id="15542" opendate="2020-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>lz4-java licensing is incorrect</summary>
      <description>In FLINK-14845 an lz4-java dependency was moved from flink-table to flink-runtime. With this commit lz4 was included in the flink-runtime jar, and the NOTICE file for flink-runtime was adjusted appropriately.The follow-up that removed the shading (FLINK-15311) did not update the NOTICE files however; it is still listed in flink-runtime when it should be in flink-dist.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-10 01:00:00" id="15554" opendate="2020-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump jetty-util-ajax to 9.3.24</summary>
      <description>flink-fs-hadoop-azure has transitive dependency on jetty-util-ajax:9.3.19, which has a security vulnerability: https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-7657This was fixed in 9.3.24.v20180605 (source). Starting from version 3.2.1 hadoop-azure is using this version as well, but for a quick resolution I propose bumping this single dependency for the time being.</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-11 01:00:00" id="15558" opendate="2020-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Elasticsearch version from 7.3.2 to 7.5.1 for es7 connector</summary>
      <description>It would be better to track the newest ES 7.x client version just like we have done for Kafka universal connector.Currently, the ES7 connector track version 7.3.2 and the latest ES 7.x version is 7.5.1. We can upgrade it.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-13 01:00:00" id="15562" opendate="2020-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document example settings.xml for configuring snapshot repository</summary>
      <description>For SNAPSHOT versions, if using 3.0 or higher version maven, the "archetypeCatalog" is not valid. In FLINK-7839, we add note to inform that issue. In this ticket, we'd like to documents an example settings.xml and pass it to maven.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.getting-started.walkthroughs.table.api.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.table.api.md</file>
      <file type="M">docs.getting-started.walkthroughs.datastream.api.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.datastream.api.md</file>
      <file type="M">docs.getting-started.project-setup.scala.api.quickstart.zh.md</file>
      <file type="M">docs.getting-started.project-setup.scala.api.quickstart.md</file>
      <file type="M">docs.getting-started.project-setup.java.api.quickstart.zh.md</file>
      <file type="M">docs.getting-started.project-setup.java.api.quickstart.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-13 01:00:00" id="15565" opendate="2020-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incompatible types of expression and result type thrown in codegen</summary>
      <description>The sql is: CREATE TABLE `over10k` ( t tinyint, si smallint, i int, b bigint, f float, d double, bo boolean, s varchar, ts timestamp, deci decimal(4,2), bin varchar ) WITH ( 'connector.path'='/daily_regression_batch_hive_1.10/test_window_with_specific_behavior/sources/over10k.csv', 'format.empty-column-as-null'='true', 'format.field-delimiter'='|', 'connector.type'='filesystem', 'format.derive-schema'='true', 'format.type'='csv' ); select s, rank() over (partition by s order by si), sum(b) over (partition by s order by si) from over10k limit 100;The data is : 109|277|65620|4294967305|97.25|7.80|true|nick quirinius|2013-03-01 09:11:58.703226|27.72|undecided 93|263|65725|4294967341|6.06|4.12|false|calvin king|2013-03-01 09:11:58.703299|32.44|values clariffication 108|383|65629|4294967510|39.55|47.67|false|jessica zipper|2013-03-01 09:11:58.703133|74.23|nap time 89|463|65537|4294967493|64.82|13.79|true|ethan white|2013-03-01 09:11:58.703243|89.52|nap time 88|372|65645|4294967358|34.48|11.18|true|quinn thompson|2013-03-01 09:11:58.703168|84.86|forestry 123|432|65626|4294967435|2.39|16.49|true|david white|2013-03-01 09:11:58.703136|61.24|joggying 57|486|65551|4294967397|36.11|9.88|true|katie xylophone|2013-03-01 09:11:58.703142|57.10|zync studies 59|343|65787|4294967312|66.89|6.54|true|mike laertes|2013-03-01 09:11:58.703209|27.56|xylophone band 74|267|65671|4294967409|21.14|14.64|true|priscilla miller|2013-03-01 09:11:58.703197|89.06|undecided 25|336|65587|4294967336|71.01|14.90|true|tom ichabod|2013-03-01 09:11:58.703127|74.32|zync studies 48|346|65712|4294967315|45.01|16.08|true|zach brown|2013-03-01 09:11:58.703108|21.68|zync studies 84|385|65776|4294967452|35.80|32.13|false|xavier zipper|2013-03-01 09:11:58.703311|99.46|education 58|389|65766|4294967416|95.55|20.62|false|sarah miller|2013-03-01 09:11:58.703215|70.92|history 22|403|65565|4294967381|99.65|35.42|false|yuri johnson|2013-03-01 09:11:58.703154|94.47|geology 55|428|65733|4294967535|99.54|5.35|false|jessica king|2013-03-01 09:11:58.703233|30.30|forestry 117|410|65706|4294967391|50.15|0.21|false|quinn johnson|2013-03-01 09:11:58.703248|65.99|yard duty 95|423|65573|4294967378|47.59|17.37|true|alice robinson|2013-03-01 09:11:58.703133|54.57|linguistics 87|332|65748|4294967320|19.83|41.67|false|fred ellison|2013-03-01 09:11:58.703289|79.02|mathematics 114|263|65674|4294967405|84.44|33.18|true|victor van buren|2013-03-01 09:11:58.703092|63.74|linguistics 5|369|65780|4294967488|92.02|38.59|true|zach polk|2013-03-01 09:11:58.703271|67.29|yard duty -3|430|65667|4294967469|65.50|40.46|true|yuri xylophone|2013-03-01 09:11:58.703258|30.94|american history 120|264|65769|4294967486|89.97|41.18|false|xavier hernandez|2013-03-01 09:11:58.703140|66.89|philosophy 107|317|65634|4294967488|5.68|18.89|false|priscilla ichabod|2013-03-01 09:11:58.703196|39.42|joggying 29|386|65723|4294967328|71.48|6.13|false|ulysses ichabod|2013-03-01 09:11:58.703215|86.65|xylophone band 22|434|65768|4294967543|44.25|27.56|false|tom polk|2013-03-01 09:11:58.703306|12.30|kindergarten -1|274|65755|4294967300|22.01|35.52|false|oscar king|2013-03-01 09:11:58.703141|33.35|chemistry 6|365|65603|4294967522|18.51|5.60|false|gabriella king|2013-03-01 09:11:58.703104|34.20|geology 97|414|65757|4294967325|31.82|22.37|false|rachel nixon|2013-03-01 09:11:58.703127|61.00|nap time 72|448|65538|4294967524|80.09|7.73|true|luke brown|2013-03-01 09:11:58.703090|95.81|american history 51|280|65589|4294967486|57.46|23.35|false|zach xylophone|2013-03-01 09:11:58.703299|11.54|education 12|447|65583|4294967389|0.98|29.79|true|yuri polk|2013-03-01 09:11:58.703305|1.89|wind surfing -1|360|65539|4294967464|4.08|39.51|false|oscar davidson|2013-03-01 09:11:58.703144|59.47|nap time 0|380|65569|4294967425|0.94|28.93|false|sarah robinson|2013-03-01 09:11:58.703176|88.81|xylophone band 66|478|65669|4294967339|23.66|38.34|true|yuri carson|2013-03-01 09:11:58.703228|64.68|opthamology 12|322|65771|4294967545|84.87|10.76|false|sarah allen|2013-03-01 09:11:58.703271|0.79|joggying 79|308|65563|4294967347|4.06|44.84|false|nick underhill|2013-03-01 09:11:58.703097|76.53|industrial engineering 4|382|65719|4294967329|7.26|39.92|true|fred polk|2013-03-01 09:11:58.703073|73.64|mathematics 10|448|65675|4294967392|26.20|16.30|true|rachel laertes|2013-03-01 09:11:58.703200|18.01|xylophone band 45|281|65685|4294967513|81.33|32.22|true|oscar allen|2013-03-01 09:11:58.703285|71.38|religion 57|288|65599|4294967422|90.33|44.25|false|bob young|2013-03-01 09:11:58.703185|11.16|biology 77|452|65706|4294967512|22.90|5.35|true|bob van buren|2013-03-01 09:11:58.703290|14.58|debate 103|492|65773|4294967404|58.29|48.28|false|yuri thompson|2013-03-01 09:11:58.703249|84.38|undecided 84|411|65737|4294967486|63.13|1.10|true|katie ichabod|2013-03-01 09:11:58.703086|29.57|american history 28|378|65589|4294967511|26.41|39.79|true|yuri polk|2013-03-01 09:11:58.703267|28.62|values clariffication 88|478|65752|4294967364|80.59|45.13|true|victor garcia|2013-03-01 09:11:58.703081|34.90|chemistry 37|388|65608|4294967350|32.94|39.06|false|mike polk|2013-03-01 09:11:58.703273|42.48|quiet hour 25|264|65648|4294967402|90.83|30.96|false|tom ichabod|2013-03-01 09:11:58.703268|65.58|history 17|455|65738|4294967508|15.73|27.01|false|david young|2013-03-01 09:11:58.703254|26.24|american history 62|438|65655|4294967511|91.77|1.90|false|sarah steinbeck|2013-03-01 09:11:58.703150|16.41|chemistry 65|298|65669|4294967328|68.89|2.75|true|david miller|2013-03-01 09:11:58.703077|51.86|values clariffication 25|491|65641|4294967387|94.82|10.04|false|ulysses thompson|2013-03-01 09:11:58.703124|63.75|linguistics 25|497|65708|4294967497|2.45|49.99|false|ethan laertes|2013-03-01 09:11:58.703320|49.72|yard duty 117|288|65591|4294967530|75.18|2.71|false|fred quirinius|2013-03-01 09:11:58.703221|99.58|geology 62|404|65706|4294967549|86.06|40.01|true|irene zipper|2013-03-01 09:11:58.703139|13.38|kindergarten 99|362|65709|4294967399|50.48|26.34|false|jessica white|2013-03-01 09:11:58.703294|83.53|kindergarten 62|395|65685|4294967446|56.73|14.87|false|victor johnson|2013-03-01 09:11:58.703194|31.42|history 62|386|65615|4294967359|44.03|43.78|true|luke underhill|2013-03-01 09:11:58.703099|86.73|nap time 15|302|65698|4294967526|91.38|3.59|true|wendy carson|2013-03-01 09:11:58.703111|9.46|religion 92|507|65699|4294967512|8.44|34.72|false|calvin xylophone|2013-03-01 09:11:58.703198|66.89|study skills 3|279|65756|4294967439|87.65|24.72|false|david white|2013-03-01 09:11:58.703233|47.19|study skills 114|330|65754|4294967500|76.20|39.35|true|rachel quirinius|2013-03-01 09:11:58.703145|76.16|undecided 24|500|65717|4294967535|60.96|21.51|false|victor falkner|2013-03-01 09:11:58.703318|82.83|nap time -2|331|65707|4294967335|67.12|13.51|false|bob ovid|2013-03-01 09:11:58.703285|62.32|joggying 101|463|65740|4294967425|52.27|11.58|true|priscilla robinson|2013-03-01 09:11:58.703078|13.09|yard duty 106|269|65577|4294967524|17.11|38.45|true|rachel falkner|2013-03-01 09:11:58.703197|79.89|xylophone band 121|500|65690|4294967517|49.31|9.85|false|luke robinson|2013-03-01 09:11:58.703074|37.91|topology 37|351|65587|4294967410|99.66|20.51|false|quinn falkner|2013-03-01 09:11:58.703221|80.69|history 6|340|65612|4294967345|54.08|3.53|true|oscar white|2013-03-01 09:11:58.703279|68.67|debate 115|366|65785|4294967330|90.00|25.79|true|jessica carson|2013-03-01 09:11:58.703143|2.72|xylophone band 124|307|65649|4294967368|81.66|19.35|true|wendy ichabod|2013-03-01 09:11:58.703254|73.76|opthamology 11|286|65752|4294967355|72.33|20.94|false|xavier carson|2013-03-01 09:11:58.703109|23.28|history 15|320|65716|4294967505|49.25|27.53|false|fred carson|2013-03-01 09:11:58.703263|18.08|industrial engineering 76|316|65706|4294967460|12.99|35.53|true|rachel davidson|2013-03-01 09:11:58.703300|85.43|quiet hour -2|485|65788|4294967510|9.99|22.75|false|luke carson|2013-03-01 09:11:58.703217|82.56|mathematics 87|482|65612|4294967327|16.51|22.21|true|katie nixon|2013-03-01 09:11:58.703083|47.09|xylophone band 21|400|65777|4294967354|4.05|11.10|false|david quirinius|2013-03-01 09:11:58.703205|25.69|geology 97|343|65764|4294967427|47.79|18.94|true|ethan miller|2013-03-01 09:11:58.703308|39.81|topology 2|292|65783|4294967420|38.86|12.14|true|wendy robinson|2013-03-01 09:11:58.703239|72.70|wind surfing 48|440|65570|4294967438|41.44|13.11|true|bob thompson|2013-03-01 09:11:58.703122|57.67|american history 87|333|65592|4294967296|71.77|8.28|false|yuri nixon|2013-03-01 09:11:58.703302|87.58|quiet hour -1|344|65616|4294967444|29.44|19.94|false|oscar falkner|2013-03-01 09:11:58.703203|28.22|geology 1|425|65625|4294967531|51.83|38.18|false|holly xylophone|2013-03-01 09:11:58.703198|0.31|geology 108|363|65715|4294967467|99.69|17.10|true|yuri xylophone|2013-03-01 09:11:58.703177|44.91|geology 93|500|65778|4294967442|82.52|38.24|true|xavier falkner|2013-03-01 09:11:58.703277|25.41|history 112|260|65612|4294967500|51.90|24.53|false|rachel falkner|2013-03-01 09:11:58.703211|65.45|american history 89|294|65754|4294967450|94.21|35.55|true|gabriella falkner|2013-03-01 09:11:58.703156|18.36|topology 32|389|65700|4294967525|42.65|32.59|true|yuri king|2013-03-01 09:11:58.703253|1.70|undecided 13|395|65715|4294967317|64.24|36.77|false|fred ovid|2013-03-01 09:11:58.703168|74.25|yard duty 5|262|65726|4294967543|8.85|12.89|true|rachel garcia|2013-03-01 09:11:58.703222|45.65|yard duty 65|324|65569|4294967315|93.15|41.46|false|alice brown|2013-03-01 09:11:58.703110|77.23|topology 73|477|65764|4294967542|27.96|44.68|false|bob steinbeck|2013-03-01 09:11:58.703173|90.95|undecided 6|337|65616|4294967456|38.34|34.04|true|rachel hernandez|2013-03-01 09:11:58.703223|60.63|debate 51|384|65649|4294967423|14.62|5.33|true|oscar king|2013-03-01 09:11:58.703232|21.96|history 87|369|65626|4294967403|20.94|26.46|true|ulysses hernandez|2013-03-01 09:11:58.703076|35.79|values clariffication 48|365|65558|4294967361|66.17|6.28|true|alice xylophone|2013-03-01 09:11:58.703081|51.13|study skills 12|388|65642|4294967298|58.26|34.09|false|jessica brown|2013-03-01 09:11:58.703081|92.61|linguistics 12|353|65703|4294967414|54.55|5.92|true|jessica johnson|2013-03-01 09:11:58.703289|91.71|chemistry 117|499|65566|4294967328|32.18|19.59|true|priscilla king|2013-03-01 09:11:58.703214|66.88|philosophy 116|363|65719|4294967513|18.59|48.19|false|priscilla johnson|2013-03-01 09:11:58.703237|55.47|history 21|433|65551|4294967366|84.35|34.09|false|oscar thompson|2013-03-01 09:11:58.703291|7.99|values clariffication -2|409|65717|4294967343|39.62|9.79|true|irene ichabod|2013-03-01 09:11:58.703315|64.80|joggying 23|495|65785|4294967473|30.91|21.95|true|fred robinson|2013-03-01 09:11:58.703240|66.34|nap time 30|507|65673|4294967453|83.51|40.92|true|oscar thompson|2013-03-01 09:11:58.703281|65.25|values clariffication 13|365|65594|4294967446|13.41|34.03|true|irene white|2013-03-01 09:11:58.703084|52.53|topology 92|419|65771|4294967310|64.82|3.01|false|yuri brown|2013-03-01 09:11:58.703271|18.05|undecided 81|351|65781|4294967473|48.46|15.80|false|bob nixon|2013-03-01 09:11:58.703254|99.35|debate 105|490|65543|4294967334|32.91|42.91|false|yuri steinbeck|2013-03-01 09:11:58.703233|42.19|xylophone band 25|402|65619|4294967340|6.28|49.92|true|victor xylophone|2013-03-01 09:11:58.703210|84.32|philosophy 88|485|65557|4294967391|95.95|46.22|true|irene xylophone|2013-03-01 09:11:58.703141|63.31|mathematics 81|285|65758|4294967338|37.83|38.23|true|irene ichabod|2013-03-01 09:11:58.703322|43.31|quiet hour 96|316|65764|4294967442|86.76|32.89|false|wendy miller|2013-03-01 09:11:58.703190|10.35|geology 43|321|65538|4294967422|81.78|6.07|false|zach van buren|2013-03-01 09:11:58.703273|26.02|topology 60|496|65614|4294967376|34.40|45.59|true|jessica steinbeck|2013-03-01 09:11:58.703076|81.95|xylophone band 44|395|65611|4294967443|15.58|1.53|false|gabriella thompson|2013-03-01 09:11:58.703295|11.00|values clariffication 73|409|65767|4294967371|36.93|36.16|true|quinn ellison|2013-03-01 09:11:58.703105|82.70|religion 121|330|65772|4294967508|70.46|44.50|true|quinn zipper|2013-03-01 09:11:58.703272|11.31|philosophy 61|421|65541|4294967410|34.59|27.52|false|calvin johnson|2013-03-01 09:11:58.703299|3.52|history 65|370|65674|4294967474|6.94|4.38|false|tom falkner|2013-03-01 09:11:58.703142|63.24|wind surfing 41|462|65699|4294967391|58.03|17.26|false|calvin xylophone|2013-03-01 09:11:58.703322|92.60|study skills 97|460|65591|4294967515|46.39|2.16|false|mike carson|2013-03-01 09:11:58.703265|97.16|values clariffication -1|435|65624|4294967377|73.60|45.63|true|irene hernandez|2013-03-01 09:11:58.703208|31.35|study skills 22|282|65782|4294967318|75.19|40.78|false|quinn ichabod|2013-03-01 09:11:58.703122|44.85|topology 46|487|65748|4294967318|67.01|24.13|false|victor zipper|2013-03-01 09:11:58.703273|95.40|linguistics 18|275|65757|4294967307|80.45|18.92|false|bob hernandez|2013-03-01 09:11:58.703307|38.25|education 103|264|65587|4294967306|97.65|11.36|false|david ovid|2013-03-01 09:11:58.703265|42.76|wind surfing 86|466|65642|4294967333|40.96|26.06|true|david young|2013-03-01 09:11:58.703155|2.99|kindergarten 119|437|65637|4294967494|18.93|31.04|true|calvin brown|2013-03-01 09:11:58.703241|30.45|debate 62|285|65593|4294967518|83.43|2.05|false|rachel xylophone|2013-03-01 09:11:58.703084|45.21|quiet hour 1|283|65752|4294967528|95.01|1.76|false|ethan ichabod|2013-03-01 09:11:58.703072|16.68|history 8|333|65732|4294967503|22.43|21.80|false|mike polk|2013-03-01 09:11:58.703160|71.80|industrial engineering 90|425|65648|4294967323|50.68|40.41|false|victor allen|2013-03-01 09:11:58.703146|58.75|kindergarten 110|319|65620|4294967332|32.36|35.17|true|ethan davidson|2013-03-01 09:11:58.703269|73.03|history 111|313|65711|4294967418|70.04|10.88|true|priscilla nixon|2013-03-01 09:11:58.703206|66.32|mathematics 96|399|65719|4294967401|52.35|4.01|true|rachel hernandez|2013-03-01 09:11:58.703076|32.45|values clariffication 83|353|65714|4294967384|10.12|15.81|false|rachel miller|2013-03-01 09:11:58.703110|16.39|philosophy 11|475|65747|4294967303|98.29|32.30|false|yuri king|2013-03-01 09:11:58.703285|11.06|forestry 84|295|65682|4294967463|17.75|23.28|true|alice zipper|2013-03-01 09:11:58.703306|79.77|industrial engineering 8|348|65626|4294967373|52.54|31.29|false|bob underhill|2013-03-01 09:11:58.703189|82.40|undecided 0|339|65603|4294967356|32.42|31.31|false|katie young|2013-03-01 09:11:58.703238|49.14|forestry 82|280|65688|4294967427|19.11|0.10|false|holly young|2013-03-01 09:11:58.703256|71.39|chemistry 119|465|65781|4294967467|23.83|0.95|false|yuri zipper|2013-03-01 09:11:58.703094|96.06|history 10|356|65586|4294967339|71.96|32.54|true|oscar zipper|2013-03-01 09:11:58.703091|73.01|quiet hour 25|364|65682|4294967449|50.96|34.46|true|sarah steinbeck|2013-03-01 09:11:58.703139|18.28|philosophy 47|270|65652|4294967393|85.46|33.87|true|luke zipper|2013-03-01 09:11:58.703173|96.68|philosophy 89|470|65676|4294967314|39.34|37.35|false|ulysses miller|2013-03-01 09:11:58.703303|69.67|values clariffication 105|393|65703|4294967359|19.00|45.80|false|oscar johnson|2013-03-01 09:11:58.703086|99.42|linguistics 120|415|65785|4294967498|54.68|32.92|true|calvin hernandez|2013-03-01 09:11:58.703086|93.09|linguistics 94|486|65649|4294967549|33.47|35.42|false|jessica carson|2013-03-01 09:11:58.703089|34.30|mathematics 38|288|65634|4294967304|5.10|44.83|false|ethan white|2013-03-01 09:11:58.703083|0.94|xylophone band 91|268|65578|4294967501|43.98|2.77|false|jessica white|2013-03-01 09:11:58.703195|51.68|joggying 123|409|65629|4294967431|29.23|27.30|false|ulysses garcia|2013-03-01 09:11:58.703141|70.01|philosophy 7|454|65697|4294967394|62.25|3.38|false|tom underhill|2013-03-01 09:11:58.703121|47.97|values clariffication 13|488|65662|4294967457|25.08|4.01|false|quinn van buren|2013-03-01 09:11:58.703272|35.40|history 118|388|65642|4294967438|52.78|15.67|true|rachel falkner|2013-03-01 09:11:58.703158|61.13|opthamology 1|315|65713|4294967509|43.80|24.95|false|nick brown|2013-03-01 09:11:58.703287|83.95|mathematics 11|416|65658|4294967433|19.94|8.97|false|jessica nixon|2013-03-01 09:11:58.703117|63.58|joggying 42|457|65669|4294967534|13.45|16.47|true|calvin polk|2013-03-01 09:11:58.703257|59.51|yard duty 119|467|65639|4294967304|57.17|35.89|false|nick nixon|2013-03-01 09:11:58.703088|0.98|history 5|383|65629|4294967302|70.92|32.41|false|rachel young|2013-03-01 09:11:58.703314|1.72|opthamology 108|304|65557|4294967498|26.30|33.01|true|tom nixon|2013-03-01 09:11:58.703189|70.64|opthamology 60|447|65778|4294967546|65.11|14.36|true|yuri robinson|2013-03-01 09:11:58.703284|45.69|joggying 65|406|65613|4294967522|93.10|16.27|false|xavier laertes|2013-03-01 09:11:58.703178|25.19|philosophy 113|482|65739|4294967311|51.17|36.29|true|priscilla steinbeck|2013-03-01 09:11:58.703084|13.07|kindergarten 58|453|65780|4294967484|25.45|1.99|false|alice ichabod|2013-03-01 09:11:58.703307|25.71|nap time 24|320|65759|4294967315|23.99|43.22|false|irene robinson|2013-03-01 09:11:58.703095|24.36|chemistry 112|438|65622|4294967483|62.47|21.21|false|tom laertes|2013-03-01 09:11:58.703257|54.45|nap time 89|382|65708|4294967459|40.10|45.17|false|luke ovid|2013-03-01 09:11:58.703325|59.38|yard duty 63|410|65561|4294967330|86.99|24.01|false|fred underhill|2013-03-01 09:11:58.703288|29.48|religion 103|462|65658|4294967533|48.98|46.63|true|wendy laertes|2013-03-01 09:11:58.703272|85.64|philosophy 97|279|65563|4294967322|79.42|41.65|false|yuri thompson|2013-03-01 09:11:58.703308|43.37|mathematics 122|375|65717|4294967513|99.32|27.37|true|rachel falkner|2013-03-01 09:11:58.703095|65.37|philosophy 25|481|65672|4294967454|98.90|37.58|false|oscar ovid|2013-03-01 09:11:58.703293|73.85|biology 71|409|65667|4294967420|1.98|44.05|true|alice brown|2013-03-01 09:11:58.703117|38.55|religion 86|399|65568|4294967404|26.97|34.10|true|priscilla ichabod|2013-03-01 09:11:58.703283|87.92|yard duty 114|348|65752|4294967368|18.90|42.15|false|irene zipper|2013-03-01 09:11:58.703154|63.92|debate 31|464|65683|4294967364|20.61|48.84|false|irene garcia|2013-03-01 09:11:58.703219|80.62|american history 30|302|65688|4294967477|7.75|5.34|false|quinn polk|2013-03-01 09:11:58.703085|80.36|geology 72|423|65665|4294967353|54.78|15.57|false|fred quirinius|2013-03-01 09:11:58.703219|56.86|philosophy 78|408|65609|4294967534|83.25|24.25|false|quinn falkner|2013-03-01 09:11:58.703074|29.42|quiet hour 35|308|65659|4294967371|89.52|45.35|true|luke carson|2013-03-01 09:11:58.703276|78.07|wind surfing 13|310|65558|4294967399|60.05|38.39|false|priscilla polk|2013-03-01 09:11:58.703194|53.92|mathematics 80|450|65537|4294967548|74.10|8.87|true|ulysses falkner|2013-03-01 09:11:58.703139|56.48|nap time 30|295|65743|4294967359|17.51|44.20|true|bob hernandez|2013-03-01 09:11:58.703242|59.71|quiet hour 25|372|65606|4294967412|99.40|36.98|false|yuri quirinius|2013-03-01 09:11:58.703242|87.18|zync studies -3|454|65733|4294967544|73.83|18.42|false|bob ichabod|2013-03-01 09:11:58.703240|95.56|debate 9|440|65773|4294967362|30.46|44.91|true|xavier falkner|2013-03-01 09:11:58.703098|62.35|religion 105|289|65576|4294967342|76.65|29.47|false|ulysses garcia|2013-03-01 09:11:58.703282|71.95|chemistry 116|263|65757|4294967525|94.04|37.06|false|priscilla hernandez|2013-03-01 09:11:58.703072|13.75|linguistics 124|458|65726|4294967483|7.96|0.29|false|zach laertes|2013-03-01 09:11:58.703281|1.46|study skills -3|507|65671|4294967305|60.28|41.50|false|quinn polk|2013-03-01 09:11:58.703244|77.17|industrial engineering -3|458|65679|4294967331|64.29|43.80|true|irene young|2013-03-01 09:11:58.703084|2.61|american history 17|435|65739|4294967438|44.39|9.29|false|alice thompson|2013-03-01 09:11:58.703241|68.01|undecided 33|390|65564|4294967305|8.20|17.36|false|calvin laertes|2013-03-01 09:11:58.703176|65.07|zync studies 73|474|65789|4294967421|62.00|40.44|true|alice quirinius|2013-03-01 09:11:58.703101|98.80|geology 46|313|65692|4294967310|93.40|34.70|true|fred hernandez|2013-03-01 09:11:58.703196|26.80|geology 50|302|65581|4294967387|2.73|18.54|false|jessica carson|2013-03-01 09:11:58.703282|58.24|study skills 115|311|65651|4294967423|44.94|33.29|true|ethan laertes|2013-03-01 09:11:58.703116|63.49|biology 88|368|65556|4294967428|37.79|47.21|true|tom laertes|2013-03-01 09:11:58.703149|7.26|topology 59|476|65560|4294967341|26.00|21.70|true|irene ovid|2013-03-01 09:11:58.703224|37.32|wind surfing 33|489|65723|4294967491|52.08|36.13|false|quinn robinson|2013-03-01 09:11:58.703174|29.70|chemistry 69|329|65580|4294967527|45.37|25.36|true|irene ichabod|2013-03-01 09:11:58.703267|95.34|joggying 8|342|65542|4294967486|86.51|30.05|true|ulysses johnson|2013-03-01 09:11:58.703164|4.89|kindergarten 47|327|65660|4294967329|53.96|10.07|false|fred white|2013-03-01 09:11:58.703313|48.34|zync studies 77|296|65771|4294967420|94.25|12.67|true|ulysses underhill|2013-03-01 09:11:58.703080|45.67|biology 63|451|65581|4294967493|44.66|40.63|true|alice miller|2013-03-01 09:11:58.703071|97.98|geology 103|303|65605|4294967540|54.00|47.97|true|fred davidson|2013-03-01 09:11:58.703087|68.42|zync studies 68|300|65577|4294967395|8.00|27.76|false|quinn quirinius|2013-03-01 09:11:58.703124|14.35|values clariffication 41|424|65684|4294967396|44.97|44.01|false|calvin polk|2013-03-01 09:11:58.703161|31.72|linguistics 84|448|65649|4294967425|5.81|28.49|true|ulysses ichabod|2013-03-01 09:11:58.703317|96.87|history 30|398|65577|4294967306|71.32|39.24|false|katie zipper|2013-03-01 09:11:58.703310|97.22|wind surfing 70|361|65695|4294967371|6.97|45.29|false|oscar falkner|2013-03-01 09:11:58.703268|79.32|opthamology 92|371|65702|4294967518|29.30|18.48|false|david ellison|2013-03-01 09:11:58.703192|30.01|topology 10|298|65666|4294967460|82.71|16.06|true|irene white|2013-03-01 09:11:58.703198|64.62|quiet hour 109|496|65699|4294967536|36.99|14.91|true|holly hernandez|2013-03-01 09:11:58.703123|66.43|geology 68|383|65597|4294967334|84.64|1.14|true|holly falkner|2013-03-01 09:11:58.703210|96.35|kindergarten 95|433|65738|4294967363|95.88|45.88|false|rachel steinbeck|2013-03-01 09:11:58.703308|34.85|history 37|262|65773|4294967482|26.04|4.86|true|oscar hernandez|2013-03-01 09:11:58.703285|92.63|linguistics 24|421|65676|4294967355|23.99|14.11|true|ulysses ovid|2013-03-01 09:11:58.703281|19.16|forestry 91|485|65607|4294967315|55.90|17.62|false|zach nixon|2013-03-01 09:11:58.703305|83.23|joggying 67|387|65790|4294967318|93.14|31.43|false|irene king|2013-03-01 09:11:58.703188|6.25|industrial engineering 82|262|65571|4294967465|56.70|30.18|true|irene van buren|2013-03-01 09:11:58.703167|3.00|study skills 98|505|65582|4294967365|17.40|40.51|false|sarah polk|2013-03-01 09:11:58.703121|56.65|history 22|268|65612|4294967462|9.69|4.64|false|xavier ichabod|2013-03-01 09:11:58.703304|3.86|linguistics 10|332|65685|4294967332|76.12|20.13|true|priscilla laertes|2013-03-01 09:11:58.703170|82.71|opthamology 36|317|65641|4294967471|56.22|36.78|true|tom johnson|2013-03-01 09:11:58.703296|53.38|biology 60|501|65555|4294967313|13.57|11.68|true|yuri davidson|2013-03-01 09:11:58.703183|10.42|religion 123|267|65560|4294967438|40.69|11.41|true|ethan allen|2013-03-01 09:11:58.703086|91.03|undecided -2|482|65558|4294967487|36.92|49.78|true|nick johnson|2013-03-01 09:11:58.703204|39.91|industrial engineering 59|270|65726|4294967372|48.94|37.15|false|oscar polk|2013-03-01 09:11:58.703221|12.67|quiet hour 119|385|65595|4294967373|36.66|15.82|true|jessica nixon|2013-03-01 09:11:58.703127|5.26|zync studies 122|306|65751|4294967471|56.79|48.37|true|bob hernandez|2013-03-01 09:11:58.703186|50.61|kindergarten 64|402|65777|4294967481|77.49|13.11|false|nick carson|2013-03-01 09:11:58.703264|66.64|study skills 48|465|65758|4294967485|75.39|30.96|false|ethan allen|2013-03-01 09:11:58.703076|10.00|joggying 117|458|65603|4294967342|53.32|32.59|true|ethan garcia|2013-03-01 09:11:58.703204|47.35|yard duty 23|283|65557|4294967415|24.61|14.57|false|fred white|2013-03-01 09:11:58.703082|12.44|chemistry 56|507|65538|4294967507|67.82|42.13|false|alice king|2013-03-01 09:11:58.703297|54.64|american history 96|436|65737|4294967528|81.66|27.09|false|tom zipper|2013-03-01 09:11:58.703199|85.16|debate 88|292|65578|4294967546|91.57|37.42|false|nick zipper|2013-03-01 09:11:58.703294|96.08|religion 73|481|65717|4294967391|40.07|27.66|true|yuri xylophone|2013-03-01 09:11:58.703120|18.21|history 80|280|65620|4294967482|58.09|40.39|false|fred polk|2013-03-01 09:11:58.703136|23.61|xylophone band 96|464|65659|4294967493|74.22|21.71|true|jessica ichabod|2013-03-01 09:11:58.703226|92.72|undecided 103|485|65707|4294967436|94.57|21.16|true|zach van buren|2013-03-01 09:11:58.703313|3.93|study skills 31|410|65566|4294967518|36.11|16.72|true|nick ellison|2013-03-01 09:11:58.703305|61.53|biology -3|270|65702|4294967512|38.05|1.07|true|david carson|2013-03-01 09:11:58.703136|28.07|philosophy 3|404|65709|4294967473|14.86|48.87|true|mike quirinius|2013-03-01 09:11:58.703099|37.99|xylophone band 124|473|65644|4294967314|65.16|19.33|false|oscar white|2013-03-01 09:11:58.703194|33.17|debate 103|321|65572|4294967353|64.79|0.22|false|david robinson|2013-03-01 09:11:58.703187|20.31|linguistics 41|395|65686|4294967428|61.99|11.61|false|sarah steinbeck|2013-03-01 09:11:58.703278|17.45|biology -3|469|65752|4294967350|55.41|32.11|true|oscar johnson|2013-03-01 09:11:58.703110|47.32|philosophy 98|336|65641|4294967519|82.11|7.91|true|tom davidson|2013-03-01 09:11:58.703320|83.43|debate 54|422|65655|4294967551|15.74|34.11|true|bob garcia|2013-03-01 09:11:58.703086|46.93|yard duty 70|462|65671|4294967385|82.68|7.94|false|fred white|2013-03-01 09:11:58.703167|45.89|joggying 62|325|65751|4294967342|36.71|28.42|true|priscilla garcia|2013-03-01 09:11:58.703239|0.56|mathematics 56|504|65635|4294967318|93.88|34.87|true|holly polk|2013-03-01 09:11:58.703227|89.14|american history 50|275|65697|4294967322|58.10|27.56|false|priscilla johnson|2013-03-01 09:11:58.703096|6.19|biology 114|428|65680|4294967498|62.68|3.90|true|yuri nixon|2013-03-01 09:11:58.703086|53.28|xylophone band 100|277|65739|4294967382|1.61|18.22|true|wendy garcia|2013-03-01 09:11:58.703137|78.35|industrial engineering 7|494|65601|4294967403|20.76|19.41|false|david underhill|2013-03-01 09:11:58.703164|70.81|topology 79|448|65744|4294967479|18.18|36.26|true|david xylophone|2013-03-01 09:11:58.703310|76.40|joggying 19|289|65562|4294967344|56.25|33.81|true|sarah van buren|2013-03-01 09:11:58.703301|64.05|forestry 10|508|65589|4294967473|96.49|7.56|false|priscilla brown|2013-03-01 09:11:58.703134|2.08|education 89|451|65686|4294967396|21.20|13.22|true|oscar king|2013-03-01 09:11:58.703127|49.12|undecided 45|323|65540|4294967436|29.79|5.69|false|tom falkner|2013-03-01 09:11:58.703102|53.85|nap time 34|319|65780|4294967523|80.40|9.05|true|sarah falkner|2013-03-01 09:11:58.703179|75.06|yard duty 30|510|65632|4294967373|60.94|21.31|true|gabriella steinbeck|2013-03-01 09:11:58.703146|69.16|undecided 72|350|65742|4294967491|3.33|30.48|false|katie johnson|2013-03-01 09:11:58.703315|55.83|topology 96|402|65620|4294967320|19.38|49.45|false|oscar steinbeck|2013-03-01 09:11:58.703303|25.84|yard duty 95|405|65536|4294967338|18.26|1.46|false|sarah thompson|2013-03-01 09:11:58.703073|29.27|education 80|396|65675|4294967379|30.21|28.41|false|rachel white|2013-03-01 09:11:58.703316|11.37|topology 5|507|65715|4294967297|87.39|16.09|true|sarah xylophone|2013-03-01 09:11:58.703321|0.46|nap time 52|322|65635|4294967296|13.25|10.02|false|wendy falkner|2013-03-01 09:11:58.703094|2.51|industrial engineering 64|345|65744|4294967316|23.26|29.25|true|sarah brown|2013-03-01 09:11:58.703245|96.45|kindergarten 97|502|65654|4294967405|0.09|3.10|false|victor robinson|2013-03-01 09:11:58.703141|29.03|religion 25|424|65599|4294967303|49.92|33.86|true|calvin miller|2013-03-01 09:11:58.703095|76.80|study skills 115|298|65599|4294967457|78.69|11.89|false|luke steinbeck|2013-03-01 09:11:58.703245|22.81|geology 49|496|65722|4294967407|17.46|33.62|false|ethan underhill|2013-03-01 09:11:58.703158|7.67|forestry 77|315|65592|4294967532|28.72|38.15|false|nick robinson|2013-03-01 09:11:58.703296|78.69|debate 33|258|65780|4294967448|5.78|19.07|true|calvin davidson|2013-03-01 09:11:58.703133|18.12|study skills 98|390|65592|4294967397|36.40|29.61|false|sarah young|2013-03-01 09:11:58.703314|74.60|wind surfing 41|415|65618|4294967426|2.23|46.43|true|nick van buren|2013-03-01 09:11:58.703225|14.78|yard duty 62|427|65671|4294967359|75.01|38.93|false|bob ovid|2013-03-01 09:11:58.703195|17.17|values clariffication -2|294|65588|4294967301|8.51|2.16|false|zach zipper|2013-03-01 09:11:58.703208|35.15|debate 94|309|65653|4294967447|6.14|5.65|false|yuri van buren|2013-03-01 09:11:58.703279|94.47|study skills 120|377|65615|4294967364|24.99|12.26|true|oscar nixon|2013-03-01 09:11:58.703250|71.62|industrial engineering 3|500|65756|4294967445|98.38|39.43|true|luke nixon|2013-03-01 09:11:58.703243|29.49|yard duty -1|505|65611|4294967338|75.26|22.98|false|mike allen|2013-03-01 09:11:58.703123|95.80|linguistics 124|466|65612|4294967456|72.76|15.57|false|calvin polk|2013-03-01 09:11:58.703235|37.15|biology 1|490|65591|4294967329|69.89|40.29|false|luke laertes|2013-03-01 09:11:58.703104|58.27|quiet hour 70|385|65553|4294967506|69.14|44.05|false|ethan xylophone|2013-03-01 09:11:58.703150|93.69|chemistry 68|330|65573|4294967506|66.87|17.31|true|jessica hernandez|2013-03-01 09:11:58.703124|30.57|zync studies 82|421|65699|4294967550|84.77|40.40|false|gabriella white|2013-03-01 09:11:58.703292|29.99|history 9|346|65646|4294967449|66.32|24.07|false|jessica xylophone|2013-03-01 09:11:58.703084|94.86|undecided 116|336|65638|4294967327|64.45|11.24|true|jessica falkner|2013-03-01 09:11:58.703087|60.05|study skills 19|376|65770|4294967536|79.12|20.11|false|victor carson|2013-03-01 09:11:58.703243|72.69|industrial engineering 27|433|65767|4294967395|22.53|18.81|false|bob polk|2013-03-01 09:11:58.703097|52.68|linguistics 31|468|65654|4294967361|33.08|29.95|false|bob young|2013-03-01 09:11:58.703210|16.48|philosophy 84|411|65564|4294967493|49.25|7.84|true|oscar nixon|2013-03-01 09:11:58.703274|47.54|american history 37|409|65769|4294967384|25.89|42.27|false|katie underhill|2013-03-01 09:11:58.703172|66.93|zync studies 10|356|65628|4294967475|98.07|13.86|false|david carson|2013-03-01 09:11:58.703222|7.37|nap time 105|437|65664|4294967535|2.05|17.01|true|holly laertes|2013-03-01 09:11:58.703144|5.69|industrial engineering 117|508|65788|4294967319|66.86|25.25|false|ulysses davidson|2013-03-01 09:11:58.703283|85.22|industrial engineering 108|322|65697|4294967529|20.24|40.23|true|mike carson|2013-03-01 09:11:58.703083|6.04|philosophy 80|426|65735|4294967533|73.85|41.99|false|quinn hernandez|2013-03-01 09:11:58.703098|69.55|mathematics 49|434|65692|4294967336|89.33|14.24|true|yuri underhill|2013-03-01 09:11:58.703127|3.91|quiet hour 74|501|65657|4294967451|88.85|11.09|true|bob king|2013-03-01 09:11:58.703175|51.36|quiet hour 8|380|65734|4294967369|84.11|10.24|false|victor underhill|2013-03-01 09:11:58.703291|78.90|opthamology 89|364|65735|4294967334|12.41|24.02|false|nick nixon|2013-03-01 09:11:58.703272|34.80|debate 53|479|65579|4294967303|7.50|43.05|false|rachel ellison|2013-03-01 09:11:58.703148|48.50|yard duty 67|493|65626|4294967489|98.74|32.74|false|katie thompson|2013-03-01 09:11:58.703263|87.95|geology 56|390|65676|4294967456|42.59|1.64|true|wendy king|2013-03-01 09:11:58.703307|39.31|joggying 13|431|65624|4294967330|94.05|30.76|false|quinn ichabod|2013-03-01 09:11:58.703180|1.72|biology 85|366|65627|4294967356|37.14|35.57|true|alice king|2013-03-01 09:11:58.703170|6.78|yard duty -2|286|65549|4294967493|9.20|1.23|true|ulysses king|2013-03-01 09:11:58.703218|93.35|study skills 51|344|65698|4294967309|83.66|6.12|false|zach ellison|2013-03-01 09:11:58.703158|29.28|yard duty 89|489|65610|4294967353|64.70|8.13|true|katie polk|2013-03-01 09:11:58.703120|56.34|education 95|327|65747|4294967522|1.16|12.00|true|bob van buren|2013-03-01 09:11:58.703284|3.45|opthamology 50|508|65541|4294967451|37.38|46.94|true|quinn steinbeck|2013-03-01 09:11:58.703081|20.90|forestry 6|301|65693|4294967454|89.07|41.96|true|alice ichabod|2013-03-01 09:11:58.703297|16.13|religion 7|322|65719|4294967434|1.02|29.24|false|quinn carson|2013-03-01 09:11:58.703293|47.99|forestry 99|469|65751|4294967356|10.10|42.47|false|wendy young|2013-03-01 09:11:58.703180|63.14|opthamology 18|269|65751|4294967544|87.84|0.60|true|mike steinbeck|2013-03-01 09:11:58.703167|36.04|religion 22|361|65729|4294967328|67.51|15.52|false|zach ovid|2013-03-01 09:11:58.703317|26.96|quiet hour 114|455|65723|4294967481|4.94|33.44|false|alice van buren|2013-03-01 09:11:58.703074|72.22|philosophy -3|384|65676|4294967453|71.97|31.52|false|alice davidson|2013-03-01 09:11:58.703226|14.28|xylophone band 37|334|65775|4294967518|17.88|45.96|false|zach ellison|2013-03-01 09:11:58.703260|9.92|nap time 28|427|65648|4294967309|45.65|3.90|true|bob robinson|2013-03-01 09:11:58.703308|89.89|chemistry 86|469|65780|4294967466|64.61|24.76|true|david steinbeck|2013-03-01 09:11:58.703241|0.68|linguistics 61|455|65567|4294967315|84.80|25.83|false|alice robinson|2013-03-01 09:11:58.703127|26.03|zync studies -3|387|65550|4294967355|84.75|22.75|true|holly thompson|2013-03-01 09:11:58.703073|52.01|biology 14|492|65690|4294967388|98.07|15.98|true|david miller|2013-03-01 09:11:58.703096|15.69|forestry 8|318|65687|4294967551|44.02|14.70|false|quinn thompson|2013-03-01 09:11:58.703205|23.43|joggying 117|502|65789|4294967441|55.39|8.22|false|tom allen|2013-03-01 09:11:58.703129|74.48|xylophone band 20|285|65783|4294967424|99.34|21.19|false|alice thompson|2013-03-01 09:11:58.703223|9.55|opthamology 4|478|65538|4294967312|21.90|0.85|false|sarah thompson|2013-03-01 09:11:58.703089|79.07|xylophone bandThe conf is:execution: planner: blink type: batchAfter excuse the sql above, there will be the exception : &amp;#91;ERROR&amp;#93; Could not execute SQL statement. Reason: org.apache.flink.table.planner.codegen.CodeGenException: Incompatible types of expression and result type. Expression&amp;#91;GeneratedExpression(((int) 0),false,,INT NOT NULL,Some(0))&amp;#93; type is &amp;#91;INT NOT NULL&amp;#93;, result type is &amp;#91;SMALLINT&amp;#93;</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.ExpressionConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-13 01:00:00" id="15567" opendate="2020-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for INSERT statements for Flink SQL</summary>
      <description>We missed to add documentation for INSERT statements which should be added under "SQL" page.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.index.zh.md</file>
      <file type="M">docs.dev.table.sql.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-13 01:00:00" id="15575" opendate="2020-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Azure Filesystem Shades Wrong Package "httpcomponents"</summary>
      <description>Instead of shading "org.apache.httpcomponents" (this package does not exist) the azure filesystem should shade "org.apache.http". This e.g. causes problems when the azure filesystem and elasticsearch6 connector are both on the classpath.</description>
      <version>1.9.1,1.10.0</version>
      <fixedVersion>1.9.3,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-14 01:00:00" id="15583" opendate="2020-1-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala walkthrough archetype does not compile on Java 11</summary>
      <description>While compiling a projected created with walkthrough archetype, the following error occurs02:55:58.048 [ERROR] error: java.lang.NoClassDefFoundError: javax/tools/ToolProvider02:55:58.048 [INFO] at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-walkthroughs.flink-walkthrough-table-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-14 01:00:00" id="15584" opendate="2020-1-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Give nested data type of ROWs in ValidationException</summary>
      <description>In INSERT INTO baz_sinkSELECT  a, ROW(b, c)FROM foo_sourceSchema mismatch mistakes will not get proper detail level, yielding the following:Caused by: org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink &amp;#91;baz_sink&amp;#93; do not match. Query result schema: &amp;#91;a: Integer, EXPR$2: Row&amp;#93; TableSink schema: &amp;#91;a: Integer, payload: Row&amp;#93;Leaving the user with an opaque 'Row' type to debug. </description>
      <version>1.9.1,1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sinks.TableSinkUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-15 01:00:00" id="15589" opendate="2020-1-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove beta tag from catalog and hive doc</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
      <file type="M">docs.dev.table.catalogs.zh.md</file>
      <file type="M">docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-1-15 01:00:00" id="15597" opendate="2020-1-15 00:00:00" resolution="Done">
    <buginformation>
      <summary>Relax sanity check of JVM memory overhead to be within its min/max</summary>
      <description>When the explicitly configured process and Flink memory sizes are verified with the JVM meta space and overhead, JVM overhead does not have to be the exact fraction.It can be just within its min/max range, similar to how it is now for network/shuffle memory check after FLINK-15300.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-1-16 01:00:00" id="15623" opendate="2020-1-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Buildling flink-python with maven profile docs-and-source fails</summary>
      <description>DescriptionBuilding flink-python with maven profile docs-and-source fails due to checkstyle violations. How to reproduceRunningmvn clean install -pl flink-python -Pdocs-and-source -DskipTests -DretryFailedDeploymentCount=10should fail with the following error[...][ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8343] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8344] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8345] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8346] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8347] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8348] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8349] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8350] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 18.046 s[INFO] Finished at: 2020-01-16T16:44:01+00:00[INFO] Final Memory: 158M/2826M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (validate) on project flink-python_2.11: You have 7603 Checkstyle violations. -&gt; [Help 1][ERROR]</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-17 01:00:00" id="15633" opendate="2020-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadocs cannot be built on Java 11</summary>
      <description>The javadoc-plugin fails when run Java 11. This isn't a big issue since we do our releases on JDK 8 anyway, but we should still fix it so users can reproduce releases on JDK 11.java.lang.ExceptionInInitializerError at org.apache.maven.plugin.javadoc.AbstractJavadocMojo.&lt;clinit&gt;(AbstractJavadocMojo.java:190) at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488) at com.google.inject.internal.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:86) at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:105) at com.google.inject.internal.ConstructorInjector.access$000(ConstructorInjector.java:32) at com.google.inject.internal.ConstructorInjector$1.call(ConstructorInjector.java:89) at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:115) at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:133) at com.google.inject.internal.ProvisionListenerStackCallback.provision(ProvisionListenerStackCallback.java:68) at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:87) at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:267) at com.google.inject.internal.InjectorImpl$2$1.call(InjectorImpl.java:1016) at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1103) at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1012) at com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1051) at org.eclipse.sisu.space.AbstractDeferredClass.get(AbstractDeferredClass.java:48) at com.google.inject.internal.ProviderInternalFactory.provision(ProviderInternalFactory.java:81) at com.google.inject.internal.InternalFactoryToInitializableAdapter.provision(InternalFactoryToInitializableAdapter.java:53) at com.google.inject.internal.ProviderInternalFactory$1.call(ProviderInternalFactory.java:65) at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:115) at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:133) at com.google.inject.internal.ProvisionListenerStackCallback.provision(ProvisionListenerStackCallback.java:68) at com.google.inject.internal.ProviderInternalFactory.circularGet(ProviderInternalFactory.java:63) at com.google.inject.internal.InternalFactoryToInitializableAdapter.get(InternalFactoryToInitializableAdapter.java:45) at com.google.inject.internal.InjectorImpl$2$1.call(InjectorImpl.java:1016) at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1092) at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1012) at org.eclipse.sisu.inject.Guice4$1.get(Guice4.java:162) at org.eclipse.sisu.inject.LazyBeanEntry.getValue(LazyBeanEntry.java:81) at org.eclipse.sisu.plexus.LazyPlexusBean.getValue(LazyPlexusBean.java:51) at org.codehaus.plexus.DefaultPlexusContainer.lookup(DefaultPlexusContainer.java:263) at org.codehaus.plexus.DefaultPlexusContainer.lookup(DefaultPlexusContainer.java:255) at org.apache.maven.plugin.internal.DefaultMavenPluginManager.getConfiguredMojo(DefaultMavenPluginManager.java:519) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:121) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:154) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:146) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:309) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:194) at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:107) at org.apache.maven.cli.MavenCli.execute(MavenCli.java:993) at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:345) at org.apache.maven.cli.MavenCli.main(MavenCli.java:191) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289) at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415) at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)Caused by: java.lang.StringIndexOutOfBoundsException: begin 0, end 3, length 1 at java.base/java.lang.String.checkBoundsBeginEnd(String.java:3116) at java.base/java.lang.String.substring(String.java:1885) at org.apache.commons.lang.SystemUtils.getJavaVersionAsFloat(SystemUtils.java:1133) at org.apache.commons.lang.SystemUtils.&lt;clinit&gt;(SystemUtils.java:818) ... 58 more</description>
      <version>1.10.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-17 01:00:00" id="15636" opendate="2020-1-17 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDF for flink planner under batch mode</summary>
      <description>Currently, Python UDF has been supported under flink planner(only stream) and blink planner(stream&amp;batch). This jira dedicates to add Python UDF support for flink planner under batch mode.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.BatchOptimizer.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-17 01:00:00" id="15639" opendate="2020-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to set toleration for jobmanager and taskmanger</summary>
      <description>Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. Navigate to Kubernetes doc for more information.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesTaskManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-19 01:00:00" id="15647" opendate="2020-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support user-specified annotations for JM/TM pods</summary>
      <description>One can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels. Kubernetes Annotation</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesTaskManagerParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesTaskManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-6-17 01:00:00" id="1565" opendate="2015-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document object reuse behavior</summary>
      <description>The documentation needs to be extended and describe the object reuse behavior of Flink and its implications for how to implement functions.The documentation must at least cover the default reuse mode: new objects through iterators and in reduce functions chaining behavior (objects are passed on to the next function which might modify it)Optionally, the documentation could describe the object reuse switch introduced by FLINK-1137.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-19 01:00:00" id="15657" opendate="2020-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the python table api doc link in Python API tutorial</summary>
      <description>Fix the python table api doc link</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.getting-started.walkthroughs.python.table.api.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-19 01:00:00" id="15658" opendate="2020-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The same sql run in a streaming environment producing a Exception, but a batch env can run normally.</summary>
      <description>summary:The same sql can run in a batch environment normally, but in a streaming environment there will be a exception like this:&amp;#91;ERROR&amp;#93; Could not execute SQL statement. Reason:org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: &amp;#91;f1&amp;#93;The sql is:CREATE TABLE `tenk1` ( unique1 int, unique2 int, two int, four int, ten int, twenty int, hundred int, thousand int, twothousand int, fivethous int, tenthous int, odd int, even int, stringu1 varchar, stringu2 varchar, string4 varchar) WITH ( 'connector.path'='/daily_regression_test_stream_postgres_1.10/test_join/sources/tenk1.csv', 'format.empty-column-as-null'='true', 'format.field-delimiter'='|', 'connector.type'='filesystem', 'format.derive-schema'='true', 'format.type'='csv');CREATE TABLE `int4_tbl` ( f1 INT) WITH ( 'connector.path'='/daily_regression_test_stream_postgres_1.10/test_join/sources/int4_tbl.csv', 'format.empty-column-as-null'='true', 'format.field-delimiter'='|', 'connector.type'='filesystem', 'format.derive-schema'='true', 'format.type'='csv');select a.f1, b.f1, t.thousand, t.tenthous from tenk1 t, (select sum(f1)+1 as f1 from int4_tbl i4a) a, (select sum(f1) as f1 from int4_tbl i4b) bwhere b.f1 = t.thousand and a.f1 = b.f1 and (a.f1+b.f1+999) = t.tenthous;</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.utils.KeySelectorUtil.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-6-20 01:00:00" id="15687" opendate="2020-1-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Potential test instabilities due to concurrent access to TaskSlotTable.</summary>
      <description>Working on FLINK-14742 revealed that the problem with that test instability was the modification of the taskSlotTable of the TaskManager under test from multiple threads, namely the test thread and the main thread of the rpcEnpoint. This data-structure is not thread-safe and this should not happen.This anti-pattern seems to be repeated in multiple tests like most of the tests in the TaskExecutorSubmissionTest (look for the call to the TaskSlotTable.allocateSlot()). There we seem to call taskSlotTable.allocateSlot() and then tmGateway.submitTask() which is essentially accessing the slot table from within the main rpc-endpoint thread.This JIRA is just to investigate if this is also a problem in those tests or not.cc Till Rohrmann, Chesnay Schepler , &amp;#91;~yangwang166&amp;#93;</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-1-20 01:00:00" id="15689" opendate="2020-1-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass execution configuration from BatchTableEnvironment to ExecutionEnvironment</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.BatchTableEnvImpl.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-2-20 01:00:00" id="15694" opendate="2020-1-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove HDFS Section of Configuration Page</summary>
      <description>The section "HDFS" is outdated (and flagged as such).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-21 01:00:00" id="15701" opendate="2020-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allows transfer.sh to retry when fails to uploads logs</summary>
      <description>Occasionally we met with the error COMPRESSING build artifacts.build_infocontainer.logmvn-1.logmvn-2.logmvn.outUploading to transfer.shCould not save metadatawhen the travis uploading logs to transfer.sh. This makes many fail tests cannot be analyzed due to lack of detail logs. To amend this situation, we should allow uploading to retry when the uploading fails. </description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-21 01:00:00" id="15702" opendate="2020-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sqlClient classloader aligned with other components</summary>
      <description>Currently, Flink sqlClient still use hardcoded `parentFirst` classloader to load user specified jars and libraries, this is easily causing classes conflicts. In FLINK-13749 , we already make the classloader consistent in both client and remote components.So I think we should do the same for sqlClient.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-23 01:00:00" id="15736" opendate="2020-1-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Java 17 (LTS)</summary>
      <description>Long-term issue for preparing Flink for Java 17.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.memory..index.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-9-29 01:00:00" id="15804" opendate="2020-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support the new type inference in Scala Table API scalar functions</summary>
      <description>Currently, we cannot distinguish between old and new type inference for Scala Table API because those functions are not registered in a catalog but are used "inline". We should support them as well.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.UserDefinedTableFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.table.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.utils.userDefinedScalarFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.UserDefinedFunctionValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonMapMergeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testPythonTableFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonFunctionInWhereClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-29 01:00:00" id="15807" opendate="2020-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document Java 11 support</summary>
      <description>At several places we list Java 8 as the only supported Java version; we should Java 11 to the list.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.local.zh.md</file>
      <file type="M">docs.ops.deployment.local.md</file>
      <file type="M">docs.getting-started.walkthroughs.table.api.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.table.api.md</file>
      <file type="M">docs.getting-started.walkthroughs.datastream.api.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.datastream.api.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-31 01:00:00" id="15817" opendate="2020-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kubernetes Resource leak while deployment exception happens</summary>
      <description>When we deploy a new session cluster on Kubernetes cluster, usually there are four steps to create the Kubernetes components, and the creation order is as below: internal Service -&gt; rest Service -&gt; ConfigMap -&gt; JobManager Deployment.After the internal Service is created, any Exceptions that fail the cluster deployment progress would cause Kubernetes Resource leak, for example:  If failed to create rest Service due to service name constraint(FLINK-15816), the internal Service would not be cleaned up when the deploy progress terminates. If failed to create JobManager Deployment(a case is that jobmanager.heap.size is too small such as 512M, which is less than the default configuration value of 'containerized.heap-cutoff-min'), the internal Service, the rest Service, and the ConfigMap all leaks.This ticket proposes to do some clean-ups(cleans the residual Services and ConfigMap) if the cluster deployment progress terminates accidentally on the client-side.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-6-3 01:00:00" id="15849" opendate="2020-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update SQL-CLIENT document from type to data-type</summary>
      <description>There are documentation of type instead of data-type in sql-client.</description>
      <version>None</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-2-3 01:00:00" id="15864" opendate="2020-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jackson-databind dependency to 2.10.1 for security reasons</summary>
      <description>The module flink-kubernetes defines an explicit dependency on jackson-databind:2.9.8. This is problematic since this jackson version contains security vulnerabilities. See FLINK-14104 for more information.If possible, I would suggest to remove the explicit version tag and to rely on the parent's dependency management.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-5 01:00:00" id="15913" opendate="2020-2-5 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add Python Table Function Runner And Operator</summary>
      <description>This Jira will include two PRs: Add Python Table Function Runner and Operator in legacy planner Add Python Table Function Runner and Operator in blink planner</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonTableFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonTableFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.PythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.PythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.BaseRowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.PythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractPythonTableFunctionOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-5 01:00:00" id="15916" opendate="2020-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove outdated sections for Network Buffers and Async Checkpoints from the Large State Tuning Guide</summary>
      <description>The documentation page "Tuning Checkpoints and Large State" has a section on network buffers and async checkpoints.The network buffers section is not relevant any more and say so as well (before Flink 1.3 ...)The async snapshots section is also outdated (all state snapshots are async by default now) refers to a setup (rocks with heap timers) that is no longer the default and has its own warning already.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.large.state.tuning.zh.md</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-2-5 01:00:00" id="15920" opendate="2020-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show thread name in logs on CI</summary>
      <description>Having thread names in log lines make it much easier to understand from which task they come.Enabling that by default on the CI setup helps with analyzing bugs and unstable tests.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.log4j-travis.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-5 01:00:00" id="15921" opendate="2020-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Travis-ci error: PYTHON exited with EXIT CODE: 143</summary>
      <description>Currently, some Travis CI failures occur, such as: &amp;#91;1&amp;#93;,&amp;#91;2&amp;#93;. The reason for the failure is that the python dependent `grpcio` released the latest version 1.27.0 &amp;#91;3&amp;#93; today, which resulted in the test cache not having the latest dependency, and the timeout of downloading in the repo. If the problem will be fixed after the first download when the network is in good condition. I am still watching the latest build &amp;#91;4&amp;#93;. If it fails for a long time, we will try to set a lower version of `grpcio` or optimize the current test case. I would like to watch for a while . What do you think?  &amp;#91;1&amp;#93;https://travis-ci.org/apache/flink/builds/646250268 &amp;#91;2&amp;#93;https://travis-ci.org/apache/flink/jobs/646281060&amp;#91;3&amp;#93; https://pypi.org/project/grpcio/#files&amp;#91;4&amp;#93;https://travis-ci.org/apache/flink/builds/646355253</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.tox.ini</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-2-6 01:00:00" id="15937" opendate="2020-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the Development Status for PyFlink</summary>
      <description>Correct the  `Development Status` value. From `'Development Status :: 1 - Planning'` to `'Development Status :: 5 - Production/Stable'`. The `Planning status` in PyPI means tell user that the package cannot be using in production &amp;#91;1&amp;#93;. So, correct the Development Status is very important for user. I would like to contains this fix in 1.10.0 release.  [1] https://pypi.org/search/?c=Development+Status+%3A%3A+1+-+Planning</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-6 01:00:00" id="15941" opendate="2020-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ConfluentSchemaRegistryCoder should not perform HTTP requests for all request</summary>
      <description>The true reason is described by user Stephen Whelan: https://issues.apache.org/jira/browse/FLINK-15941?focusedCommentId=17033121&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17033121</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.RegistryAvroSerializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-7 01:00:00" id="15948" opendate="2020-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resource will be wasted when the task manager memory is not a multiple of Yarn minimum allocation</summary>
      <description>If the taskmanager.memory.process.size is set to 2000m and the Yarn minimum allocation is 128m, we will get a container with 2048m. Currently, TaskExecutorProcessSpec is built with 2000m, so we will have 48m wasted and they could not be used by Flink.I think Flink has accounted all the jvm heap, off-heap, overhead resources. So we should not leave these free memory there. And i suggest to update the TaskExecutorProcessSpec according to the Yarn allocated container.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-7 01:00:00" id="15949" opendate="2020-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden jackson dependency constraints</summary>
      <description>Replace the individual jackson dependency management entries with the jackson bom, and introduce an enforcer check to ban older jackson dependencies.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-7 01:00:00" id="15953" opendate="2020-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job Status is hard to read for some Statuses</summary>
      <description>The job status RESTARTING is rendered in a white font on white background which makes it hard to read (see attachments).</description>
      <version>1.9.2,1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.config.ts</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-10 01:00:00" id="15977" opendate="2020-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update pull request template to include Kubernetes as deployment candidates</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.github.PULL.REQUEST.TEMPLATE.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-11 01:00:00" id="15999" opendate="2020-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extract “Concepts” material from API/Library sections and start proper concepts section</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.fig.times.clocks.svg</file>
      <file type="M">docs.internals.stream.checkpointing.zh.md</file>
      <file type="M">docs.internals.stream.checkpointing.md</file>
      <file type="M">docs.fig.event.ingestion.processing.time.svg</file>
      <file type="M">docs.fig.processes.svg</file>
      <file type="M">docs.concepts.index.zh.md</file>
      <file type="M">docs.concepts.index.md</file>
      <file type="M">docs.concepts.stream-processing.md</file>
      <file type="M">docs.concepts.programming-model.zh.md</file>
      <file type="M">docs.concepts.glossary.zh.md</file>
      <file type="M">docs.concepts.glossary.md</file>
      <file type="M">docs.dev.event.time.md</file>
      <file type="M">docs.concepts.timely-stream-processing.md</file>
      <file type="M">docs.dev.stream.operators.process.function.zh.md</file>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
      <file type="M">docs.concepts.runtime.zh.md</file>
      <file type="M">docs.concepts.runtime.md</file>
      <file type="M">docs.concepts.flink-architecture.md</file>
      <file type="M">docs.dev.stream.state.state.md</file>
      <file type="M">docs.dev.stream.state.index.md</file>
      <file type="M">docs.dev.stream.state.broadcast.state.md</file>
      <file type="M">docs.concepts.stateful-stream-processing.md</file>
      <file type="M">docs.concepts.programming-model.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-11 01:00:00" id="16004" opendate="2020-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude flink-rocksdb-state-memory-control-test jars from the dist</summary>
      <description>Currently flink-rocksdb-state-memory-control-test will be included in the dist as shown here. We should remove it.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-rocksdb-state-memory-control-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-12 01:00:00" id="16007" opendate="2020-2-12 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add rules to push down the Java Calls contained in Python Correlate node</summary>
      <description>The Java Calls contained in Python Correlate node should be extracted to make sure the TableFunction works well.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.CorrelateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-12 01:00:00" id="16008" opendate="2020-2-12 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add rules to transpose the join condition of Python Correlate node</summary>
      <description>Because the conditions can’t be executed in Python correlate node, this rule will transpose the conditions after the Python correlate node if the join type is inner join.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.SplitPythonConditionFromCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitPythonConditionFromCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCorrelate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-12 01:00:00" id="16014" opendate="2020-2-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>S3 plugin ClassNotFoundException SAXParser</summary>
      <description>While stress-testing s3 plugin on EMR. org.apache.flink.util.FlinkRuntimeException: Could not perform checkpoint 2 for operator Map (114/160). at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:839) at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:104) at org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner.notifyBarrierReceived(CheckpointBarrierUnaligner.java:149) at org.apache.flink.streaming.runtime.io.InputProcessorUtil$1.lambda$notifyBarrierReceived$0(InputProcessorUtil.java:80) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87) at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:255) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:508) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:492) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.RuntimeException: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3://emr-unaligned-checkpoints-testing-eu-central-1/inflight/9ae223e41008b17568d7f63c12360268_output/part-file.-1: com.amazonaws.SdkClientException: Couldn't initialize a SAX driver to create an XMLReader: Couldn't initialize a SAX driver to create an XMLReader at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.checkErroneousUnsafe(BufferPersisterImpl.java:262) at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.add(BufferPersisterImpl.java:137) at org.apache.flink.runtime.io.network.BufferPersisterImpl.addBuffers(BufferPersisterImpl.java:66) at org.apache.flink.streaming.runtime.tasks.StreamTask.prepareInflightDataSnapshot(StreamTask.java:935) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:898) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94) at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:870) at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:826) ... 12 moreCaused by: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3://emr-unaligned-checkpoints-testing-eu-central-1/inflight/9ae223e41008b17568d7f63c12360268_output/part-file.-1: com.amazonaws.SdkClientException: Couldn't initialize a SAX driver to create an XMLReader: Couldn't initialize a SAX driver to create an XMLReader at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:177) at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:145) at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2251) at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2149) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2088) at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1734) at org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:2970) at org.apache.flink.fs.s3hadoop.common.HadoopFileSystem.exists(HadoopFileSystem.java:152) at org.apache.flink.core.fs.PluginFileSystemFactory$ClassLoaderFixingFileSystem.exists(PluginFileSystemFactory.java:143) at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.exists(SafetyNetWrapperFileSystem.java:102) at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.get(BufferPersisterImpl.java:213) at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.run(BufferPersisterImpl.java:167)Caused by: com.amazonaws.SdkClientException: Couldn't initialize a SAX driver to create an XMLReader at com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.&lt;init&gt;(XmlResponsesSaxParser.java:118) at com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:87) at com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:77) at com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:62) at com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:31) at com.amazonaws.http.response.AwsResponseHandlerAdapter.handle(AwsResponseHandlerAdapter.java:70) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleResponse(AmazonHttpClient.java:1554) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1272) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1056) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667) at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4325) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4272) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4266) at com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:876) at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$5(S3AFileSystem.java:1262) at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317) at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:280) at org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:1255) at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2223) ... 9 moreCaused by: org.xml.sax.SAXException: SAX2 driver class org.apache.xerces.parsers.SAXParser not foundjava.lang.ClassNotFoundException: org.apache.xerces.parsers.SAXParser at org.xml.sax.helpers.XMLReaderFactory.loadClass(XMLReaderFactory.java:230) at org.xml.sax.helpers.XMLReaderFactory.createXMLReader(XMLReaderFactory.java:191) at com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.&lt;init&gt;(XmlResponsesSaxParser.java:115) ... 32 moreCaused by: java.lang.ClassNotFoundException: org.apache.xerces.parsers.SAXParser at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at org.apache.flink.core.plugin.PluginLoader$PluginClassLoader.loadClass(PluginLoader.java:149) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at org.xml.sax.helpers.NewInstance.newInstance(NewInstance.java:82) at org.xml.sax.helpers.XMLReaderFactory.loadClass(XMLReaderFactory.java:228) ... 34 more</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-12 01:00:00" id="16024" opendate="2020-2-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support filter pushdown in JDBC connector</summary>
      <description/>
      <version>None</version>
      <fixedVersion>jdbc-3.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.resources.org.apache.flink.connector.jdbc.table.JdbcTablePlanTest.xml</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcTablePlanTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSource.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-13 01:00:00" id="16040" opendate="2020-2-13 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Change local import to global import</summary>
      <description>There two reasons support doing this: Execute import will cost time. PEP8  claims that "Imports are always put at the top of the file" https://www.python.org/dev/peps/pep-0008/#imports</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-13 01:00:00" id="16041" opendate="2020-2-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expand "popular" documentation sections by default</summary>
      <description>Currently, when the documentation page is loaded all sections are collapsed, this means that some prominent subsections are not easily discoverable. I think we should expand the "Getting Started", "Concepts", and "API" sections by default. (See also https://cwiki.apache.org/confluence/display/FLINK/FLIP-42%3A+Rework+Flink+Documentation, because "API" doesn't exist yet in the current documentation.I attached screenshots to show what I mean.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs.getting-started.index.zh.md</file>
      <file type="M">docs.getting-started.index.md</file>
      <file type="M">docs.dev.index.zh.md</file>
      <file type="M">docs.dev.index.md</file>
      <file type="M">docs.concepts.index.zh.md</file>
      <file type="M">docs.concepts.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-14 01:00:00" id="16053" opendate="2020-2-14 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Remove redundant metrics in PyFlink</summary>
      <description>We have recorded the metrics about how many elements it has processed in Python UDF. This kind of information is not necessary as there is also this kind of information in the Java operator. I have performed a simple test and find that removing it could improve the performance about 5% - 10%. Besides, as these metrics are still not exposed and it will be safe to remove it.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-3-15 01:00:00" id="16070" opendate="2020-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Blink planner can not extract correct unique key for UpsertStreamTableSink</summary>
      <description>I reproduce an Elasticsearch6UpsertTableSink issue which user reported in mail list&amp;#91;1&amp;#93; that Blink planner can not extract correct unique key for following query, but legacy planner works well. // user codeINSERT INTO ES6_ZHANGLE_OUTPUT  SELECT aggId, pageId, ts_min as ts,  count(case when eventId = 'exposure' then 1 else null end) as expoCnt,  count(case when eventId = 'click' then 1 else null end) as clkCnt  FROM  (    SELECT        'ZL_001' as aggId,        pageId,        eventId,        recvTime,        ts2Date(recvTime) as ts_min    from kafka_zl_etrack_event_stream    where eventId in ('exposure', 'click')  ) as t1  group by aggId, pageId, ts_minI  found that blink planner can not extract correct unique key in `FlinkRelMetadataQuery.getUniqueKeys(relNode)`, legacy planner works well in  `org.apache.flink.table.plan.util.UpdatingPlanChecker.getUniqueKeyFields(...) `. A simple ETL job to reproduce this issue can refers&amp;#91;2&amp;#93; &amp;#91;1&amp;#93;http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Flink-1-10-es-sink-exception-td32773.html&amp;#91;2&amp;#93;https://github.com/leonardBang/flink-sql-etl/blob/master/etl-job/src/main/java/kafka2es/Kafka2UpsertEs.java  </description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-16 01:00:00" id="16071" opendate="2020-2-16 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Introduce FlattenRowCoder to solve the performance issue of __get_item__ in Row</summary>
      <description>Optimizing the cost of the get item of the Row will gain tremendous improvements in python udf performance</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-16 01:00:00" id="16072" opendate="2020-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the performance of the write/read null mask in FlattenRowCoder</summary>
      <description>Optimizing the write/read null mask in RowCoder will gain some performance improvements in python udf.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.tests.coders.test.common.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-17 01:00:00" id="16115" opendate="2020-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aliyun oss filesystem could not work with plugin mechanism</summary>
      <description>From release-1.9, Flink suggest users to load all filesystem with plugin, including oss. However, it could not work for oss filesystem. The root cause is it does not shade the org.apache.flink.runtime.fs.hdfs and org.apache.flink.runtime.util. So they will always be loaded by system classloader and throw the following exceptions. 2020-02-17 17:28:47,247 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint - Could not start cluster entrypoint StandaloneSessionClusterEntrypoint.org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint StandaloneSessionClusterEntrypoint. at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:187) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:518) at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:64)Caused by: java.lang.NoSuchMethodError: org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.&lt;init&gt;(Lorg/apache/flink/fs/shaded/hadoop3/org/apache/hadoop/fs/FileSystem;)V at org.apache.flink.fs.osshadoop.OSSFileSystemFactory.create(OSSFileSystemFactory.java:85) at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:61) at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:441) at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:362) at org.apache.flink.core.fs.Path.getFileSystem(Path.java:298) at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:100) at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:89) at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:125) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:305) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:263) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:207) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:169) at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:168) ... 2 more</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-1-18 01:00:00" id="16152" opendate="2020-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Operator/index" into Chinese</summary>
      <description>The page is located at docs/dev/stream/operators/index.zh.md</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-19 01:00:00" id="16162" opendate="2020-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump flink-shaded to 10.0</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-19 01:00:00" id="16164" opendate="2020-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable maven-site-plugin</summary>
      <description>The maven-site-plugin can be used to generate a site for the project, like the sites used by various maven modules.We inherit an execution for this plugin, but don't actually use it.Hence, we can disable it to remove some noise from the build.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-19 01:00:00" id="16173" opendate="2020-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce noise for used undeclared dependencies</summary>
      <description>When analyzing dependency usages there are a bunch of noisy entries, particularly around transitive Flink dependencies, hamcrest and powermock, that would create more work than benefit.These should be allowed to be used without having an explicit dependency.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-20 01:00:00" id="16191" opendate="2020-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error message on Windows when RocksDB Paths are too long</summary>
      <description>Paths on Windows have a length limit by default (247 chars).If RocksDB tries to open a longer path, it throws a misleading exception (directory not found).We can check for this situation and improve the error message.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-21 01:00:00" id="16217" opendate="2020-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client crashed when any uncatched exception is thrown</summary>
      <description>Currently, SQL CLI doesn't catch all the exceptions, for example, Calcite exceptions. We should catch any possible exceptions thrown by the planner, and print the root cause.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.utils.TerminalUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutorBuilder.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-22 01:00:00" id="16231" opendate="2020-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector is missing jdk.tools exclusion against Hive 2.x.x</summary>
      <description>Click to add description</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-24 01:00:00" id="16251" opendate="2020-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the cost of function call in ScalarFunctionOpertation</summary>
      <description>Currently, there are too many extra function calls cost in  ScalarFunctionOpertation.We need to optimize it to improve performance in Python UDF.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-24 01:00:00" id="16252" opendate="2020-2-24 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Optimize the output emit logic to remove unnecessary overhead</summary>
      <description>We need to optimize the function call chains in process_outputs to improve the performance in Python UDF</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-2-25 01:00:00" id="16271" opendate="2020-2-25 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Introduce ArrowScalarFunctionOperator for vectorized Python UDF execution</summary>
      <description>The aim of this Jira is to introduce ArrowScalarFunctionOperator for vectorized Python UDF execution.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.PythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.BaseRowPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.AbstractPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.BaseRowPythonTableFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PassThroughPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.BaseRowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.ArrowUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonOptionsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonConfigTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.AbstractGeneralPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.BaseRowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractBaseRowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonConfig.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.AbstractPythonFunctionRunner.java</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-26 01:00:00" id="16288" opendate="2020-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setting the TTL for discarding task pods on Kubernetes.</summary>
      <description>I'm experimenting with running Flink 1.10.0 on native Kubernetes (version 1.17).After a job ends the task pods that were used to run it are discarded quite quickly.I found that if my job goes wrong I have too little time to look at all of the logs.I propose having a new config setting that allows me to run Flink on k8s where I can set the minimum time before an idle task pod is discarded.That way I can start Flink with a pod ttl of an hour (or something like that) so I have enough time to go through the logs and figure out what I did wrong.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-26 01:00:00" id="16292" opendate="2020-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute all end to end tests on AZP</summary>
      <description>Ensure that we execute all end to end tests on AZP: Make sure that all the e2e tests referenced in the splits are also referenced in the "run nightly tests" script make sure the java e2e tests are executed</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.setup.kubernetes.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-26 01:00:00" id="16293" opendate="2020-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document using plugins in Kubernetes</summary>
      <description>It took me some time to figure out how to enable plugins when running Flink on Kubernetes.So I'm writing some documentation to save other people trying the same a lot of time.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
      <file type="M">docs.ops.deployment.docker.zh.md</file>
      <file type="M">docs.ops.deployment.docker.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-2-28 01:00:00" id="16331" opendate="2020-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove source licenses for old WebUI</summary>
      <description>When we removed the old WebUI we only removed the licenses from flink-runtime-web, but missed the ones for the source release.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">licenses.LICENSE.split</file>
      <file type="M">licenses.LICENSE.qtip2</file>
      <file type="M">licenses.LICENSE.moment-duration-format</file>
      <file type="M">licenses.LICENSE.moment</file>
      <file type="M">licenses.LICENSE.lodash</file>
      <file type="M">licenses.LICENSE.jquery</file>
      <file type="M">licenses.LICENSE.imagesloaded</file>
      <file type="M">licenses.LICENSE.graphlib</file>
      <file type="M">licenses.LICENSE.ev-emitter</file>
      <file type="M">licenses.LICENSE.dagre-d3</file>
      <file type="M">licenses.LICENSE.dagre</file>
      <file type="M">licenses.LICENSE.d3</file>
      <file type="M">licenses.LICENSE.bootstrap</file>
      <file type="M">licenses.LICENSE.angular-ui-router</file>
      <file type="M">licenses.LICENSE.angular-moment</file>
      <file type="M">licenses.LICENSE.angular-drag-and-drop-list</file>
      <file type="M">licenses.LICENSE.angular</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-28 01:00:00" id="16343" opendate="2020-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve exception message when reading an unbounded source in batch mode</summary>
      <description>We can just ignore watermark in batch mode. cc jark</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-28 01:00:00" id="16345" opendate="2020-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Computed column can not refer time attribute column</summary>
      <description>If a computed column refer a time attribute column, computed column will lose  time attribute and cause validation fail.CREATE TABLE orders ( order_id STRING, order_time TIMESTAMP(3), amount DOUBLE, amount_kg as amount * 1000, // can not select computed column standard_ts which from column order_time that used as WATERMARK standard_ts as order_time + INTERVAL '8' HOUR, WATERMARK FOR order_time AS order_time) WITH ( 'connector.type' = 'kafka', 'connector.version' = '0.10', 'connector.topic' = 'flink_orders', 'connector.properties.zookeeper.connect' = 'localhost:2181', 'connector.properties.bootstrap.servers' = 'localhost:9092', 'connector.properties.group.id' = 'testGroup', 'connector.startup-mode' = 'earliest-offset', 'format.type' = 'json', 'format.derive-schema' = 'true');The query `select amount_kg from orders` runs normally,  the` he query `select standard_ts from orders` throws a validation exception message as following:[ERROR] Could not execute SQL statement. Reason: java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes: validated type: RecordType(VARCHAR(2147483647) CHARACTER SET "UTF-16LE" order_id, TIME ATTRIBUTE(ROWTIME) order_time, DOUBLE amount, DOUBLE amount_kg, TIMESTAMP(3) ts) NOT NULL converted type: RecordType(VARCHAR(2147483647) CHARACTER SET "UTF-16LE" order_id, TIME ATTRIBUTE(ROWTIME) order_time, DOUBLE amount, DOUBLE amount_kg, TIME ATTRIBUTE(ROWTIME) ts) NOT NULL rel: LogicalProject(order_id=[$0], order_time=[$1], amount=[$2], amount_kg=[$3], ts=[$4]) LogicalWatermarkAssigner(rowtime=[order_time], watermark=[$1]) LogicalProject(order_id=[$0], order_time=[$1], amount=[$2], amount_kg=[*($2, 1000)], ts=[+($1, 28800000:INTERVAL HOUR)]) LogicalTableScan(table=[[default_catalog, default_database, orders, source: [Kafka010TableSource(order_id, order_time, amount)]]])   </description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-3-2 01:00:00" id="16378" opendate="2020-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable JDK 11 Docker tests on AZP</summary>
      <description>Build log: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5779&amp;view=logs&amp;j=eec879f1-c5a2-5810-2b49-ba5c6bfecb27&amp;t=484f04d6-55db-5161-9f93-391b1677737dError: A JNI error has occurred, please check your installation and try againException in thread "main" java.lang.UnsupportedClassVersionError: org/apache/flink/client/cli/CliFrontend has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0 at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)Running the job failed.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-3-3 01:00:00" id="16410" opendate="2020-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PrometheusReporterEndToEndITCase fails with ClassNotFoundException</summary>
      <description>Logs: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5883&amp;view=logs&amp;j=b1623ac9-0979-5b0d-2e5e-1377d695c991&amp;t=e7804547-1789-5225-2bcf-269eeaa37447[INFO] Running org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.005 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase[ERROR] testReporter(org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase) Time elapsed: 0.005 s &lt;&lt;&lt; ERROR!java.lang.NoClassDefFoundError: org/apache/flink/runtime/rest/messages/RequestBody at org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.&lt;init&gt;(PrometheusReporterEndToEndITCase.java:119)Caused by: java.lang.ClassNotFoundException: org.apache.flink.runtime.rest.messages.RequestBody at org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.&lt;init&gt;(PrometheusReporterEndToEndITCase.java:119)[INFO]</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-6 01:00:00" id="16455" opendate="2020-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce flink-sql-connector-hive modules to provide hive uber jars</summary>
      <description>Discussed in: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Introduce-flink-connector-hive-xx-modules-td38440.html</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-7 01:00:00" id="16485" opendate="2020-3-7 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support vectorized Python UDF in the batch mode of old planner</summary>
      <description>Currently, vectorized Python UDF is only supported in the batch/stream mode for the blink planner and stream mode for the old planner. The aim of this Jira is to add support in the batch mode for the old planner.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetPythonCalc.scala</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonTableFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonStatelessFunctionFlatMap.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-7 01:00:00" id="16486" opendate="2020-3-7 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add documentation for vectorized Python UDF</summary>
      <description>As the title described, the aim of this JIRA is to add documentation for vectorized Python UDF.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.python.index.zh.md</file>
      <file type="M">docs.dev.table.python.index.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-6-9 01:00:00" id="16508" opendate="2020-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Name the ports exposed by the main Container in Pod</summary>
      <description>Currently, we expose some ports via the main Container of the JobManager and the TaskManager, but we forget to name those ports so that people could be confused because there is no description of the port usage. This ticket proposes to explicitly name the ports in the Container to help people understand the usage of those ports.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InternalServiceDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.AbstractServiceDecorator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-10 01:00:00" id="16524" opendate="2020-3-10 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Optimize the execution of Python UDF to use generator to eliminate unnecessary function calls</summary>
      <description>Optimize the result of FlattenRowCoder and ArrowCoder to generator to eliminate unnecessary function calls.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.tests.coders.test.common.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-10 01:00:00" id="16526" opendate="2020-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix exception when computed column expression references a keyword column name</summary>
      <description>json_row ROW&lt;`timestamp` BIGINT&gt;,`timestamp` AS `json_row`.`timestamp`It translate to "SELECT json_row.timestamp FROM _temp_table_"Throws exception "Encountered ". timestamp" at line 1, column 157. Was expecting one of:..."</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-11 01:00:00" id="16538" opendate="2020-3-11 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Restructure Python Table API documentation</summary>
      <description>Python Table API documentation is currently spread across a number of pages and it's difficult for a user to find out all the documentations available. Besides, there are also a few documentations which deserves specific page to describe the functionality, such as the environment setup, the Python dependency management, the vectorized Python UDF, etc. We want to improve the documentation by adding an item under the Table API as "Python Table API" and the structure of "Python Table API" is as following: Python Table API Installation User-defined Functions Vectorized Python UDF Dependency Management Metrics &amp; Logging Configuration  </description>
      <version>None</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.getting-started.walkthroughs.python.table.api.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.python.table.api.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.dev.table.config.zh.md</file>
      <file type="M">docs.dev.table.config.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-6-13 01:00:00" id="16577" opendate="2020-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exception will be thrown when computing columnInterval relmetadata in some case</summary>
      <description>Consider the following SQL // a: INT, c: LONGSELECT c, SUM(a) FROM T WHERE a &gt; 0.1 AND a &lt; 1 GROUP BY c Here the sql type of 0.1 is Decimal and 1 is Integer, and they are both in NUMERIC type family, and do not trigger type coercion, so the plan is:FlinkLogicalAggregate(group=[{0}], EXPR$1=[SUM($1)])+- FlinkLogicalCalc(select=[c, a], where=[AND(&gt;(a, 0.1:DECIMAL(2, 1)), &lt;(a, 1))]) +- FlinkLogicalTableSourceScan(table=[[...]], fields=[a, b, c])When we calculate the filtered column interval of calc, it'll lead to validation exception of `FiniteValueInterval`:</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.ColumnIntervalUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdFilteredColumnIntervalTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnIntervalTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.catalog.CatalogStatisticsTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-13 01:00:00" id="16579" opendate="2020-3-13 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Upgrade Calcite version to 1.26 for Flink SQL</summary>
      <description>A taks to upgrade Calcite version to 1.23.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.ProjectSemiAntiJoinTransposeRule.scala</file>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.collections.ByteHashSet.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlFunctionUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlDateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RexProgramExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.NormalizationRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.calcite.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.CorrelateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.SortTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.CorrelateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.RexProgramExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalUnnestRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.EnumerableToLogicalTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.common.WindowPropertiesRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.common.LogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.TableSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.subquery.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.catalog.BasicOperatorTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.CalciteConfig.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.PlanningConfigurationBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.QueryOperationCatalogViewTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.calcite.FlinkCalciteSqlValidator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.calcite.CalciteParser.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.ProcedureNamespace.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateJoinTransposeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQueryTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQuerySemiJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQueryAntiJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.RewriteMultiJoinConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushLimitIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.ProjectPruneAggregateCallRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.JoinDependentConditionDerivationRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkPruneEmptyRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkJoinToMultiJoinRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateRemoveRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateJoinTransposeRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.CalcPruneAggregateCallRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgramTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgramTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgramTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.SelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdSelectivityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdPopulationSizeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdDistinctRowCountTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.CorrelateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactoryTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.calcite.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.PythonOverWindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.OverWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.ColumnFunctionsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.LegacySinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalRankRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalHashAggRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.WindowGroupReorderRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQuerySemiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQueryAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.subquery.FlinkRewriteSubQueryRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RankNumberColumnRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoLegacyTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ProjectSemiAntiJoinTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.JoinDependentConditionDerivationRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkSemiAntiJoinJoinTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkSemiAntiJoinFilterTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForRangeEndTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLimit0RemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkJoinPushExpressionsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateExpandDistinctAggregatesRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.DecomposeGroupingSetsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ConvertToNotInOrInRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.CalcRankTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.AggregateReduceGroupingRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.stringexpr.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.PythonOverWindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SingleRowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testExecuteSqlWithExplainInsert.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushLimitIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelShuttles.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RankUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.WindowPropertiesRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.WindowGroupReorderRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.table.shaded.dependencies.sh</file>
      <file type="M">flink-table.flink-sql-parser-hive.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.validate.FlinkSqlConformance.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.CreateTableLikeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkDDLDataTypeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.TableApiIdentifierParsingTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.calcite.sql.validate.ProcedureNamespace.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.calcite.CalciteParser.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.PlannerContext.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.CustomizedConvertRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.utils.HiveTableSqlFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.metadata.FlinkRelMdCollation.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.FlinkSemiAntiJoinFilterTransposeRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.FlinkSemiAntiJoinJoinTransposeRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.FlinkSemiAntiJoinProjectTransposeRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.SubQueryDecorrelator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.CalciteConfig.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.TableSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.SelectivityEstimator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableFunctionScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.ConvertToNotInOrInRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.EnumerableToLogicalTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkLimit0RemoveRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalUnnestRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalWindowAggregateRuleBase.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-13 01:00:00" id="16589" opendate="2020-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Table SQL fails/crashes with big queries with lots of fields</summary>
      <description>Hi,My use case is a streaming application with a few streaming tables.I was trying to build a SELECT query (and registering it as a temporary view) with about 200 fields/expressions out of another streaming table. The application is successfully submitted to Flink cluster. However the worker processes keep crashing, with the exception as quoted below. It clearly mentioned in the log that this is a bug, so I fire this ticket. By the way, if I lower the number of fields down to 100 then it works nicely.Please advice.Thanks a lot for all the efforts bring Flink up. It is really amazing!java.lang.RuntimeException: Could not instantiate generated class 'GroupAggsHandler$9687'    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:57)    at org.apache.flink.table.runtime.operators.aggregate.MiniBatchGroupAggFunction.open(MiniBatchGroupAggFunction.java:136)    at org.apache.flink.table.runtime.operators.bundle.AbstractMapBundleOperator.open(AbstractMapBundleOperator.java:84)    at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1007)    at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)    at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)    at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68)    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78)    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:52)    ... 10 moreCaused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66)    ... 12 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81)    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)    ... 15 moreCaused by: org.codehaus.janino.InternalCompilerException: Compiling "GroupAggsHandler$9687": Code of method "retract(Lorg/apache/flink/table/dataformat/BaseRow;)V" of class "GroupAggsHandler$9687" grows beyond 64 KB    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:78)    ... 21 moreCaused by: org.codehaus.janino.InternalCompilerException: Code of method "retract(Lorg/apache/flink/table/dataformat/BaseRow;)V" of class "GroupAggsHandler$9687" grows beyond 64 KB    at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1009)    at org.codehaus.janino.CodeContext.write(CodeContext.java:901)    at org.codehaus.janino.CodeContext.writeShort(CodeContext.java:1026)    at org.codehaus.janino.UnitCompiler.writeConstantLongInfo(UnitCompiler.java:12274)    at org.codehaus.janino.UnitCompiler.pushConstant(UnitCompiler.java:10679)    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4936)    at org.codehaus.janino.UnitCompiler.access$8400(UnitCompiler.java:215)    at org.codehaus.janino.UnitCompiler$16.visitUnaryOperation(UnitCompiler.java:4414)    at org.codehaus.janino.UnitCompiler$16.visitUnaryOperation(UnitCompiler.java:4394)    at org.codehaus.janino.Java$UnaryOperation.accept(Java.java:4719)    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394)    at org.codehaus.janino.UnitCompiler.fakeCompile(UnitCompiler.java:3719)    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5569)    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2580)    at org.codehaus.janino.UnitCompiler.access$2700(UnitCompiler.java:215)    at org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1503)    at org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1487)    at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:3511)    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)    ... 28 more</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-3-15 01:00:00" id="16604" opendate="2020-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column key in JM configuration is too narrow</summary>
      <description>See the screenshot </description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.configuration.job-manager-configuration.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-16 01:00:00" id="16608" opendate="2020-3-16 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support primitive data types for vectorized Python UDF</summary>
      <description>As the title described, the aim of this JIRA is to support the primitive types for vectorized Python UDF.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.ArrowPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.BaseRowArrowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.BaseRowArrowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.ArrowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.BaseRowArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.serializers.python.DateSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.util.StreamRecordUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.ArrowUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-16 01:00:00" id="16613" opendate="2020-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Builds on Azure pipeline are not triggered</summary>
      <description>This is actually an issue at azure, but we can mitigate it, by making our build config more verbose: https://status.dev.azure.com/_event/179641421</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-5-17 01:00:00" id="16624" opendate="2020-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support user-specified annotations for the rest Service</summary>
      <description>There are some scenarios that people would like to customize annotations for the rest Service, for example: Specify the LB type to decide whether it should have an external IP. Specify the Security Policy for the LB.It's a common need, especially in the Cloud Environment.  This ticket proposes to add support for it.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-17 01:00:00" id="16635" opendate="2020-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incompatible okio dependency in flink-metrics-influxdb module</summary>
      <description>With FLINK-12147 we bumped influxdb-java from version 2.14 to 2.16. At the same time we fix the okio dependency to version 1.14.0. Since influxdb-java transitive dependency converter-moshi:jar:2.6.1 requires moshi:jar:1.8.0 which requires okio:jar:1.16.0, the influxdb metric reporter fails as described here. We should fix this incompatibility by removing the dependency management entry for okio.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-3-18 01:00:00" id="16652" opendate="2020-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BytesColumnVector should init buffer in Hive 3.x</summary>
      <description>The failed test is TableEnvHiveConnectorTest#testDifferentFormats when hive 3.x.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.vector.AbstractOrcColumnVector.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-18 01:00:00" id="16653" opendate="2020-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce ResultPartitionWriterTestBase for simplifying tests</summary>
      <description>At the moment there are at-least four implementations of `ResultPartitionWriter` interface used in unit tests. And there are about ten methods to be implemented for `ResultPartitionWriter` and most of them are dummy in tests.When we want to extend the methods for `ResultPartitionWriter`, the above four dummy implementations in tests have to be adjusted as well, to waste a bit efforts.Therefore abstract ResultPartitionWriterTestBase is proposed to implement the basic dummy methods for `ResultPartitionWriter`, and the previous four instances can all extend it to only implement one or two methods based on specific requirements in tests. And we will probably only need to adjust the ResultPartitionWriterTestBase when extending the `ResultPartitionWriter` interface.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.AvailabilityTestResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.AbstractCollectingResultPartitionWriter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-18 01:00:00" id="16657" opendate="2020-3-18 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Wire the EmbeddedExecutor to the Web Submission logic.</summary>
      <description>This will replace the current logic of web submission in the JarRunHandler with one that uses the newly introduced EmbeddedExecutor .</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlers.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebSubmissionExtension.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.StreamContextEnvironment.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ContextEnvironment.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.application.DetachedApplicationRunner.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.ClientUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-19 01:00:00" id="16669" opendate="2020-3-19 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDF in SQL function DDL</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.CreateTempSystemFunctionOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.functions.FunctionDefinitionUtil.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonDriverTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonDriverEnvUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonGatewayServer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonDriverEnvUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonDriver.java</file>
      <file type="M">flink-python.pyflink.java.gateway.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-4-20 01:00:00" id="16697" opendate="2020-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CVE-2020-1960] Disable JMX rebinding</summary>
      <description>Disable JMX rebinding.</description>
      <version>1.10.0</version>
      <fixedVersion>1.9.3,1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.main.java.org.apache.flink.metrics.jmx.JMXReporter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-20 01:00:00" id="16699" opendate="2020-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support accessing secured services via K8s secrets</summary>
      <description>Kubernetes Secrets can be used to provide credentials for a Flink application to access secured services.  This ticket proposes to Support to mount user-specified K8s Secrets into the JobManager/TaskManager Container Support to use a user-specified K8s Secret through an environment variable.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesTaskManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-23 01:00:00" id="16711" opendate="2020-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet columnar row reader read footer from wrong end</summary>
      <description>readFooter(conf, path, range(splitStart, splitLength))Should be:readFooter(conf, path, range(splitStart, splitStart + splitLength)) </description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-23 01:00:00" id="16727" opendate="2020-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix cast exception when having time point literal as parameters</summary>
      <description>I defined as ScalarFunction as follow: public class DateFunc extends ScalarFunction { public String eval(Date date) { return date.toString(); } @Override public TypeInformation&lt;?&gt; getResultType(Class&lt;?&gt;[] signature) { return Types.STRING; } @Override public TypeInformation&lt;?&gt;[] getParameterTypes(Class&lt;?&gt;[] signature) { return new TypeInformation[]{Types.INT}; }}I ues it in sql: `select func(DATE '2020-11-12') as a from source` , Flink throws 'cannot cast 2020-11-12 as class java.time.LocalDate ' The full code is in the Flinktest.zip Main class is com.lorinda.template.TestDateFunction</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.call.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarFunctionCallGen.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-25 01:00:00" id="16766" opendate="2020-3-25 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support create StreamTableEnvironment without passing StreamExecutionEnvironment</summary>
      <description>Currently, when we create a BatchTableEnvironment, the ExecutionEnvironment is an optional parameter, while for the StreamTableEnvironment, the ExecutionEnvironment is not optional. We should make them consistent</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-9-25 01:00:00" id="16768" opendate="2020-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart hangs</summary>
      <description>Logs: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6584&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=d26b3528-38b0-53d2-05f7-37557c2405e42020-03-24T15:52:18.9196862Z "main" #1 prio=5 os_prio=0 tid=0x00007fd36c00b800 nid=0xc21 runnable [0x00007fd3743ce000]2020-03-24T15:52:18.9197235Z java.lang.Thread.State: RUNNABLE2020-03-24T15:52:18.9197536Z at java.net.SocketInputStream.socketRead0(Native Method)2020-03-24T15:52:18.9197931Z at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)2020-03-24T15:52:18.9198340Z at java.net.SocketInputStream.read(SocketInputStream.java:171)2020-03-24T15:52:18.9198749Z at java.net.SocketInputStream.read(SocketInputStream.java:141)2020-03-24T15:52:18.9199171Z at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)2020-03-24T15:52:18.9199840Z at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:593)2020-03-24T15:52:18.9200265Z at sun.security.ssl.InputRecord.read(InputRecord.java:532)2020-03-24T15:52:18.9200663Z at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975)2020-03-24T15:52:18.9201213Z - locked &lt;0x00000000927583d8&gt; (a java.lang.Object)2020-03-24T15:52:18.9201589Z at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933)2020-03-24T15:52:18.9202026Z at sun.security.ssl.AppInputStream.read(AppInputStream.java:105)2020-03-24T15:52:18.9202583Z - locked &lt;0x0000000092758c00&gt; (a sun.security.ssl.AppInputStream)2020-03-24T15:52:18.9203029Z at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137)2020-03-24T15:52:18.9203558Z at org.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198)2020-03-24T15:52:18.9204121Z at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176)2020-03-24T15:52:18.9204626Z at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)2020-03-24T15:52:18.9205121Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9205679Z at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)2020-03-24T15:52:18.9206164Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9206786Z at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125)2020-03-24T15:52:18.9207361Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9207839Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9208327Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9208809Z at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)2020-03-24T15:52:18.9209273Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9210003Z at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107)2020-03-24T15:52:18.9210658Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9211154Z at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$read$3(S3AInputStream.java:445)2020-03-24T15:52:18.9211631Z at org.apache.hadoop.fs.s3a.S3AInputStream$$Lambda$42/1936375962.execute(Unknown Source)2020-03-24T15:52:18.9212044Z at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)2020-03-24T15:52:18.9212553Z at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:260)2020-03-24T15:52:18.9212972Z at org.apache.hadoop.fs.s3a.Invoker$$Lambda$23/1457226878.execute(Unknown Source)2020-03-24T15:52:18.9213408Z at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)2020-03-24T15:52:18.9213866Z at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:256)2020-03-24T15:52:18.9214273Z at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:231)2020-03-24T15:52:18.9214701Z at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:441)2020-03-24T15:52:18.9215443Z - locked &lt;0x00000000926e88b0&gt; (a org.apache.hadoop.fs.s3a.S3AInputStream)2020-03-24T15:52:18.9215852Z at java.io.DataInputStream.read(DataInputStream.java:149)2020-03-24T15:52:18.9216305Z at org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.read(HadoopDataInputStream.java:94)2020-03-24T15:52:18.9216781Z at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)2020-03-24T15:52:18.9217187Z at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)2020-03-24T15:52:18.9217571Z at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)2020-03-24T15:52:18.9218108Z - locked &lt;0x00000000926ea000&gt; (a java.io.InputStreamReader)2020-03-24T15:52:18.9218475Z at java.io.InputStreamReader.read(InputStreamReader.java:184)2020-03-24T15:52:18.9218876Z at java.io.BufferedReader.fill(BufferedReader.java:161)2020-03-24T15:52:18.9219261Z at java.io.BufferedReader.readLine(BufferedReader.java:324)2020-03-24T15:52:18.9219890Z - locked &lt;0x00000000926ea000&gt; (a java.io.InputStreamReader)2020-03-24T15:52:18.9220256Z at java.io.BufferedReader.readLine(BufferedReader.java:389)2020-03-24T15:52:18.9220914Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getContentsOfFile(HadoopS3RecoverableWriterITCase.java:423)2020-03-24T15:52:18.9221704Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:411)2020-03-24T15:52:18.9222457Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364)2020-03-24T15:52:18.9223222Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:330)2020-03-24T15:52:18.9223817Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-03-24T15:52:18.9224232Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-03-24T15:52:18.9224729Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-03-24T15:52:18.9225160Z at java.lang.reflect.Method.invoke(Method.java:498)2020-03-24T15:52:18.9225675Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-03-24T15:52:18.9226171Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-03-24T15:52:18.9226682Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-03-24T15:52:18.9227187Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-03-24T15:52:18.9227661Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2020-03-24T15:52:18.9228145Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2020-03-24T15:52:18.9228718Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2020-03-24T15:52:18.9229112Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-03-24T15:52:18.9229582Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-03-24T15:52:18.9230029Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-03-24T15:52:18.9230525Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-03-24T15:52:18.9230963Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-03-24T15:52:18.9231546Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-03-24T15:52:18.9231999Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-03-24T15:52:18.9232432Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-03-24T15:52:18.9232862Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-03-24T15:52:18.9233307Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2020-03-24T15:52:18.9233833Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2020-03-24T15:52:18.9234284Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-03-24T15:52:18.9234700Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-03-24T15:52:18.9235076Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-03-24T15:52:18.9235599Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-03-24T15:52:18.9236124Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-03-24T15:52:18.9236648Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-03-24T15:52:18.9237167Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-03-24T15:52:18.9237688Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-03-24T15:52:18.9238244Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-03-24T15:52:18.9238745Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-03-24T15:52:18.9239202Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2020-03-24T15:52:18.9239549Z 2020-03-24T15:52:18.9239794Z "VM Thread" os_prio=0 tid=0x00007fd36c260800 nid=0xc58 runnable  </description>
      <version>1.10.0,1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.watchdog.sh</file>
      <file type="M">tools.ci.test.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-25 01:00:00" id="16771" opendate="2020-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE when filtering by decimal column</summary>
      <description>The following SQL can trigger the issue:create table foo (d decimal(15,8));insert into foo values (cast('123.123' as decimal(15,8)));select * from foo where d&gt;cast('123456789.123' as decimal(15,8));</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.DecimalTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-25 01:00:00" id="16772" opendate="2020-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump derby to 10.12.1.1+ or exclude it</summary>
      <description>hive-metastore depends on derby 10.10/10.4, which are vulnerable to CVE-2015-1832.We should bump the version to at least 10.12.1.1 .Assuming that derby is only required for the server and not the client we could potentially even exclude it.phoenixjiangnan Can you help with this?</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-26 01:00:00" id="16786" opendate="2020-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix pyarrow version incompatible problem</summary>
      <description>As reported in FLINK-16483, we should make the version of pyarrow consistent between pyflink and beam. Other dependencies should also be checked.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2020-4-26 01:00:00" id="16807" opendate="2020-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve reporting of errors during resource initialization</summary>
      <description>The factories for resources currently returns Optionals for handling failed instantiations, which are an expected occurrence. The factories themselves are only logging the exception.This has the downside that no error is encoded in the optional, so if no resource can be instantiated we cannot enrich the final exception in any way.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.src.test.java.org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-availability-test.src.test.java.org.pache.flink.metrics.tests.MetricsAvailabilityITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.util.FactoryUtils.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.LocalStandaloneFlinkResourceFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkResourceFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.cache.TravisDownloadCacheFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.cache.PersistingDownloadCacheFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.cache.LolCacheFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.cache.DownloadCacheFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.main.java.org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResourceFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.main.java.org.apache.flink.tests.util.kafka.KafkaResourceFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-26 01:00:00" id="16815" opendate="2020-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add e2e tests for reading primitive data types from postgres with JDBCTableSource and PostgresCatalog</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.catalog.PostgresCatalogTestBase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.catalog.PostgresCatalogTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.catalog.PostgresCatalogITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-27 01:00:00" id="16822" opendate="2020-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The config set by SET command does not work</summary>
      <description>Users can add or change the properties for execution behavior through SET command in SQL client CLI, e.g. SET execution.parallelism=10, SET table.optimizer.join-reorder-enabled=true. But the table.xx config can't change the TableEnvironment behavior, because the property set from CLI does not be set into TableEnvironment's table config.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2020-4-2 01:00:00" id="16945" opendate="2020-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute CheckpointFailureManager.FailJobCallback directly in main thread executor</summary>
      <description>Since we have put all non-IO operations of CheckpointCoordinator into main thread executor, the CheckpointFailureManager.FailJobCallback could be executed directly now. In this way execution graph would fail immediately when CheckpointFailureManager invokes the callback. We could avoid the inconsistent scenario of FLINK-13497.</description>
      <version>1.10.0,1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-2 01:00:00" id="16946" opendate="2020-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update user documentation for job manager memory model</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.memory.mem.tuning.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.md</file>
      <file type="M">docs.ops.memory.mem.setup.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.md</file>
      <file type="M">docs.ops.memory.mem.detail.zh.md</file>
      <file type="M">docs.ops.memory.mem.detail.md</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-2 01:00:00" id="16947" opendate="2020-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ArtifactResolutionException: Could not transfer artifact. Entry [...] has not been leased from this pool</summary>
      <description>https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6982&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5Build of flink-metrics-availability-test failed with:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (end-to-end-tests) on project flink-metrics-availability-test: Unable to generate classpath: org.apache.maven.artifact.resolver.ArtifactResolutionException: Could not transfer artifact org.apache.maven.surefire:surefire-grouper:jar:2.22.1 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): Entry [id:13][route:{s}-&gt;https://maven-central-eu.storage-download.googleapis.com:443][state:null] has not been leased from this pool[ERROR] org.apache.maven.surefire:surefire-grouper:jar:2.22.1[ERROR] [ERROR] from the specified remote repositories:[ERROR] google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/, releases=true, snapshots=false),[ERROR] apache.snapshots (https://repository.apache.org/snapshots, releases=false, snapshots=true)[ERROR] Path to dependency:[ERROR] 1) dummy:dummy:jar:1.0[ERROR] 2) org.apache.maven.surefire:surefire-junit47:jar:2.22.1[ERROR] 3) org.apache.maven.surefire:common-junit48:jar:2.22.1[ERROR] 4) org.apache.maven.surefire:surefire-grouper:jar:2.22.1[ERROR] -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException[ERROR] [ERROR] After correcting the problems, you can resume the build with the command[ERROR] mvn &lt;goals&gt; -rf :flink-metrics-availability-test</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-13 01:00:00" id="1695" opendate="2015-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create machine learning library</summary>
      <description>Create the infrastructure for Flink's machine learning library. This includes the creation of the module structure and the implementation of basic types such as vectors and matrices.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-3 01:00:00" id="16970" opendate="2020-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bundle JMXReporter separately from dist jar</summary>
      <description>The JMXReporter is currently bundled in the flink-dist jar.There isn't a real benefit to this; it could just as well be bundled in /lib, or with FLINK-16222 even in /plugins .This would allow users to exclude the reporter from the distribution, for cases where they do not want anyone to use this reporter.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-4 01:00:00" id="16980" opendate="2020-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python UDF doesn&amp;#39;t work with protobuf 3.6.1</summary>
      <description>PyFlink UDF execution module is not compatible with protobuf 3.6.1 because it uses a newer interface to access the enum value defined in proto model. We need to fix this.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-4-7 01:00:00" id="17013" opendate="2020-4-7 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDTF in old planner under batch mode</summary>
      <description>Currently, Python UDTF has been supported under flink planner(only stream) and blink planner. This jira dedicates to add Python UDTF support for flink planner under batch mode.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.datastream.DataStreamPythonCorrelateRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.PythonTableFunction.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-12 01:00:00" id="17093" opendate="2020-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python UDF doesn&amp;#39;t work when the input column is from composite field</summary>
      <description>For the following job:from pyflink.datastream import StreamExecutionEnvironmentfrom pyflink.table import BatchTableEnvironment, StreamTableEnvironment, EnvironmentSettings, CsvTableSinkfrom pyflink.table.descriptors import Schema, Kafka, Jsonfrom pyflink.table import DataTypesfrom pyflink.table.udf import ScalarFunction, udfimport os@udf(input_types=[DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING()], result_type=DataTypes.STRING())def get_host_ip(source, qr, sip, dip):    if source == "NGAF" and qr == '1':        return dip    return sip@udf(input_types=[DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING()], result_type=DataTypes.STRING())def get_dns_server_ip(source, qr, sip, dip):    if source == "NGAF" and qr == '1':        return sip    return dipdef test_case():    env = StreamExecutionEnvironment.get_execution_environment()    env.set_parallelism(1)    t_env = StreamTableEnvironment.create(env)     from pyflink.table import Row   table = t_env.from_elements(      [("DNS", Row(source="source", devid="devid", sip="sip", dip="dip", qr="qr", queries="queries", answers="answers", qtypes="qtypes", atypes="atypes", rcode="rcode", ts="ts",))],    DataTypes.ROW([DataTypes.FIELD("stype", DataTypes.STRING()), DataTypes.FIELD("data", DataTypes.ROW([DataTypes.FIELD('source', DataTypes.STRING()), DataTypes.FIELD("devid", DataTypes.STRING()), DataTypes.FIELD('sip', DataTypes.STRING()), DataTypes.FIELD('dip', DataTypes.STRING()), DataTypes.FIELD("qr", DataTypes.STRING()), DataTypes.FIELD("queries", DataTypes.STRING()), DataTypes.FIELD("answers", DataTypes.STRING()), DataTypes.FIELD("qtypes", DataTypes.STRING()), DataTypes.FIELD("atypes", DataTypes.STRING()), DataTypes.FIELD("rcode", DataTypes.STRING()), DataTypes.FIELD("ts", DataTypes.STRING())])) ] )) result_file = "/tmp/test.csv" if os.path.exists(result_file): os.remove(result_file) t_env.register_table_sink("Results", CsvTableSink(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n'], [DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING()], "/tmp/test.csv")) t_env.register_function("get_host_ip", get_host_ip) t_env.register_function("get_dns_server_ip", get_dns_server_ip) t_env.register_table("source", table) standard_table = t_env.sql_query("select data.*, stype as dns_type from source")\ .where("dns_type.in('DNSFULL', 'DNS', 'DNSFULL_FROM_LOG', 'DNS_FROM_LOG')") t_env.register_table("standard_table", standard_table) final_table = t_env.sql_query("SELECT *, get_host_ip(source, qr, sip, dip) as host_ip," "get_dns_server_ip(source, qr, sip, dip) as dns_server_ip FROM standard_table") final_table.insert_into("Results") t_env.execute("test")if __name__ == '__main__': test_case()The plan is as following which is not correct: org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: KafkaTableSource(type, data) -&gt; Map -&gt; where: (IN(type, _UTF-16LE'DNSFULL', _UTF-16LE'DNS', _UTF-16LE'DNSFULL_FROM_LOG', _UTF-16LE'DNS_FROM_LOG')), select: (data, type) -&gt; select: (type, get_host_ip(type.source, type.qr, type.sip, type.dip) AS f0, get_dns_server_ip(type.source, type.qr, type.sip, type.dip) AS f1) -&gt; select: (f0.source AS source, f0.devid AS devid, f0.sip AS sip, f0.dip AS dip, f0.qr AS qr, f0.queries AS queries, f0.answers AS answers, f0.qtypes AS qtypes, f0.atypes AS atypes, f0.rcode AS rcode, f0.ts AS ts, type AS dns_type, f0 AS host_ip, f1 AS dns_server_ip) -&gt; to: Row -&gt; Sink: KafkaTableSink(source, devid, sip, dip, qr, queries, answers, qtypes, atypes, rcode, ts, dns_type, host_ip, dns_server_ip) (1/4) (8d064ab137866a2a9040392a87bcc59d) switched from RUNNING to FAILED.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.PythonCorrelateSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PythonCalcSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.logical.PythonCorrelateSplitRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCorrelateSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCorrelateSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.CalcPythonCorrelateTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PythonCorrelateSplitRule.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-4-13 01:00:00" id="17107" opendate="2020-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CheckpointCoordinatorConfiguration#isExactlyOnce() is inconsistent with StreamConfig#getCheckpointMode()</summary>
      <description>CheckpointCoordinatorConfiguration#isExactlyOnce() is inconsistent with StreamConfig#getCheckpointMode() when checkpoint is disabled. CheckpointCoordinatorConfiguration#isExactlyOnce() returns true if checkpoint mode is  EXACTLY_ONCE mode and return false if checkpoint mode is AT_LEAST_ONCE while StreamConfig#getCheckpointMode() will always return AT_LEAST_ONCE which means always not exactly once.</description>
      <version>1.6.3,1.7.2,1.8.0,1.9.0,1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-6-13 01:00:00" id="17113" opendate="2020-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use executeSql to execute view statements and fix nullability loss problem</summary>
      <description/>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.ViewExpansionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-13 01:00:00" id="17114" opendate="2020-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>When the pyflink job runs in local mode and the command "python" points to Python 2.7, the startup of the Python UDF worker will fail.</summary>
      <description>When the PyFlink job runs in local mode and the command "python" points to Python 2.7, the startup of the Python UDF worker will fail because "python" is the default interpreter of the Python UDF worker. For this case we need to set the default value of "python.executable" to `sys.executable` i.e. the python interpreter which launches the job.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.util.utils.py</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-13 01:00:00" id="17119" opendate="2020-4-13 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add Cython support for composite types</summary>
      <description>Support Composite DataTypes in Cython</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.fast.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.fast.coder.impl.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.fast.coder.impl.pxd</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-14 01:00:00" id="17125" opendate="2020-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a Usage Notes Page to Answer Common Questions Encountered by PyFlink Users</summary>
      <description>There are several common problems that PyFlink new users often encounter. We need to support usage notes to help them solve these problems quickly.</description>
      <version>None</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.python.index.zh.md</file>
      <file type="M">docs.dev.table.python.index.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-4-18 01:00:00" id="1718" opendate="2015-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add sparse vector and sparse matrix types to machine learning library</summary>
      <description>Currently, the machine learning library only supports dense matrix and dense vectors. For future algorithms it would be beneficial to also support sparse vectors and matrices.I'd propose to use the compressed sparse column (CSC) representation, because it allows rather efficient operations compared to a map backed sparse matrix/vector implementation. Furthermore, this is also the format the Breeze library expects for sparse matrices/vectors. Thus, it is easy to convert to a sparse breeze data structure which provides us with many linear algebra operations.BIDMat &amp;#91;1&amp;#93; uses the same data representation.Resources:&amp;#91;1&amp;#93; https://github.com/BIDData/BIDMat</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.SparseVectorTest.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.SparseMatrixTest.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.DenseVectorTest.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.DenseMatrixTest.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.BreezeMathTest.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.DenseVectorSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.DenseMatrixSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.package.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Matrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-18 01:00:00" id="1720" opendate="2015-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate ScalaDoc in Scala sources into overall JavaDoc</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils.pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.pom.xml</file>
      <file type="M">flink-staging.flink-hcatalog.pom.xml</file>
      <file type="M">flink-staging.flink-expressions.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-18 01:00:00" id="1722" opendate="2015-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming not respecting FinalizeOnMaster for output formats</summary>
      <description>The Hadoop output formats execute a process in the end to move the produced files from a temp directory to the final location.The batch API is annotating output formats that execute something in the end with the FinalizeOnMaster interface.The streaming component is not respecting this interface. Hence, HadoopOutputFormats aren't writing their final data into the desired destination.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SimpleOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.OutputFormatSinkFunction.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSinkTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.JobTaskVertexTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSourceTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSinkTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.OutputFormatVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.InputFormatVertex.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-18 01:00:00" id="17232" opendate="2020-4-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rethink the implicit behavior to use the Service externalIP as the address of the Endpoint</summary>
      <description>Currently, for the LB/NodePort type Service, if we found that the LoadBalancer in the Service is null, we would use the externalIPs configured in the external Service as the address of the Endpoint. Again, this is another implicit toleration and may confuse the users.This ticket proposes to rethink the implicit toleration behaviour.</description>
      <version>1.10.0,1.10.1</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.services.LoadBalancerService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-20 01:00:00" id="17273" opendate="2020-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix not calling ResourceManager#closeTaskManagerConnection in KubernetesResourceManager in case of registered TaskExecutor failure</summary>
      <description>At the moment, the KubernetesResourceManager does not call the method of ResourceManager#closeTaskManagerConnection once it detects that a currently registered task executor has failed. This ticket propoeses to fix this problem.</description>
      <version>1.10.0,1.10.1</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.TestingResourceEventHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ResourceEventHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.TestingKubernetesPod.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPod.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-22 01:00:00" id="17311" opendate="2020-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the logic of compressed in tgz before uploading artifacts</summary>
      <description> Script files in packages downloaded from Azure will lose executable permissions https://github.com/microsoft/azure-pipelines-tasks/issues/6364.So We need to add the logic of compressing the built result to tgz before uploading artifacts</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
      <file type="M">tools.azure-pipelines.build-python-wheels.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-8-23 01:00:00" id="17330" opendate="2020-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid scheduling deadlocks caused by cyclic input dependencies between regions</summary>
      <description>Imagine a job like this:A &amp;#8211; (pipelined FORWARD) --&gt; B &amp;#8211; (blocking ALL-to-ALL) --&gt; DA &amp;#8211; (pipelined FORWARD) --&gt; C &amp;#8211; (pipelined FORWARD) --&gt; Dparallelism=2 for all vertices.We will have 2 execution pipelined regions:R1 = {A1, B1, C1, D1}R2 = {A2, B2, C2, D2}R1 has a cross-region input edge (B2-&gt;D1).R2 has a cross-region input edge (B1-&gt;D2).Scheduling deadlock will happen since we schedule a region only when all its inputs are consumable (i.e. blocking partitions to be finished). This is because R1 can be scheduled only if R2 finishes, while R2 can be scheduled only if R1 finishes.To avoid this, one solution is to force a logical pipelined region with intra-region ALL-to-ALL blocking edges to form one only execution pipelined region, so that there would not be cyclic input dependency between regions.Besides that, we should also pay attention to avoid cyclic cross-region POINTWISE blocking edges.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.PipelinedRegionComputeUtilTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.PipelinedRegionComputeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-23 01:00:00" id="17332" opendate="2020-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix restart policy not equals to Never for native task manager pods</summary>
      <description>Currently, we do not explicitly set the RestartPolicy for the task manager pods in the native K8s setups so that it is Always by default.  The task manager pod itself should not restart the failed Container, the decision should always be made by the job manager.Therefore, this ticket proposes to set the RestartPolicy to Never for the task manager pods.</description>
      <version>1.10.0,1.10.1</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecorator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-5-26 01:00:00" id="17385" opendate="2020-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix precision problem when converting JDBC numberic into Flink decimal type</summary>
      <description>This is reported in the mailing list: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/JDBC-error-on-numeric-conversion-because-of-DecimalType-MIN-PRECISION-td34668.html</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.catalog.PostgresCatalogTestBase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.catalog.PostgresCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.catalog.PostgresCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-28 01:00:00" id="17442" opendate="2020-4-28 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Cannot convert String or boxed-primitive arrays to DataStream using TypeInformation</summary>
      <description>It seems to be impossible to convert String or boxed primitive array types from table back to DataStream by specifying type info (inside a Tuple for instance).We get the following error:Query schema: &amp;#91;f0: ARRAY&lt;STRING&gt;&amp;#93;Sink schema: [f0: LEGACY('ARRAY', 'ANY&lt;[Ljava.lang.String;,....</description>
      <version>1.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.external.resources.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-5-29 01:00:00" id="17450" opendate="2020-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement function &amp; catalog DDLs for Hive dialect</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-29 01:00:00" id="17451" opendate="2020-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement view DDLs for Hive dialect</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateView.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-29 01:00:00" id="17452" opendate="2020-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support creating Hive tables with constraints</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogHiveMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV310.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV210.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-29 01:00:00" id="17460" opendate="2020-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create sql-jars for parquet and orc</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-29 01:00:00" id="17461" opendate="2020-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support JSON serialization and deseriazation schema for RowData type</summary>
      <description>Add support JsonRowDataDeserializationSchema and JsonRowDataSerializationSchema for the new data structure RowData.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-29 01:00:00" id="17462" opendate="2020-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support CSV serialization and deseriazation schema for RowData type</summary>
      <description>Add support CsvRowDataDeserializationSchema and CsvRowDataSerializationSchema for the new data structure RowData.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowSchemaConverter.java</file>
      <file type="M">flink-formats.flink-csv.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-29 01:00:00" id="17466" opendate="2020-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>toRetractStream doesn&amp;#39;t work correctly with Pojo conversion class</summary>
      <description>The toRetractStream(table, Pojo.class) does not map the query columns properly to the pojo fields.This either leads to exceptions due to type incompatibility or simply incorrect results.It can be simple reproduced by the following test code:@Testpublic void testRetract() throws Exception { EnvironmentSettings settings = EnvironmentSettings .newInstance() .useBlinkPlanner() .inStreamingMode() .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment .create(StreamExecutionEnvironment.getExecutionEnvironment(), settings); tableEnv.createTemporaryView("person", env.fromElements(new Person())); tableEnv.toRetractStream(tableEnv.sqlQuery("select name, age from person"), Person.class).print(); tableEnv.execute("Test");}public static class Person { public String name = "bob"; public int age = 1;}Runtime Error:java.lang.ClassCastException: org.apache.flink.table.dataformat.BinaryString cannot be cast to java.lang.IntegerChanging the query to "select age,name from person" in this case would resolve the problem but it also highlights the possible underlying issue.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.StreamTableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaPojos.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.SinkCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-30 01:00:00" id="17471" opendate="2020-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move LICENSE and NOTICE files to root directory of python distribution</summary>
      <description>This is observed and proposed by Robert during 1.10.1 RC1 check:Another question that I had while checking the release was the"apache-flink-1.10.1.tar.gz" binary, which I suppose is the pythondistribution.It does not contain a LICENSE and NOTICE file at the root level (which isokay [1] for binary releases), but in the "pyflink/" directory. There isalso a "deps/" directory, which contains a full distribution of Flink,without any license files.I believe it would be a little bit nicer to have the LICENSE and NOTICEfile in the root directory (if the python wheels format permits) to makesure it is obvious that all binary release contents are covered by thesefiles.http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-Release-1-10-1-release-candidate-1-tp40724p40910.html</description>
      <version>1.9.3,1.10.0,1.11.0</version>
      <fixedVersion>1.9.4,1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.MANIFEST.in</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-30 01:00:00" id="17476" opendate="2020-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests to check recovery from snapshot created with different UC mode</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-30 01:00:00" id="17483" opendate="2020-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update flink-sql-connector-elasticsearch7 NOTICE file to correctly reflect bundled dependencies</summary>
      <description>This issue is found during 1.10.1 RC1 check by Robert, that `com.carrotsearch:hppc` and `com.github:mustachejava` were included into the shaded binary to fix FLINK-16170 but not added into the NOTICE file of flink-sql-connector-elasticsearch7 module. More details please refer to the ML discussion thread.</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-2 01:00:00" id="17496" opendate="2020-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression with amazon-kinesis-producer 0.13.1 in Flink 1.10.x</summary>
      <description/>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-5-13 01:00:00" id="17657" opendate="2020-5-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix reading BIGINT UNSIGNED type field not work in JDBC</summary>
      <description>I use sql client read mysql table, but I found I can't read a table contain `BIGINT UNSIGNED` field. It will  Caused by: java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.lang.Long MySQL table: create table tb ( id BIGINT UNSIGNED auto_increment  primary key, cooper BIGINT(19) null ,user_sex VARCHAR(2) null ); my env yaml is env.yaml .</description>
      <version>1.10.0,1.10.1</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.converter.PostgresRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.converter.AbstractJdbcRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-13 01:00:00" id="17661" opendate="2020-5-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add APIs for using new WatermarkStrategy/WatermarkGenerator</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPunctuatedWatermarksOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPeriodicWatermarksOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPunctuatedWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPeriodicWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.IngestionTimeExtractor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-14 01:00:00" id="17701" opendate="2020-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude jdk:tools dependency from all Hadoop dependencies for Java 9+ compatibility</summary>
      <description>Hadoop transitively pulls the system dependency jdk:tools which is not longer available on Java 9+. This causes errors when importing the code into an IDE with runs Java 11.This dependency is anyways not needed when running the code, because the classes are always present. It can be safely excluded form the transitive dependencies.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-15 01:00:00" id="17715" opendate="2020-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supports function DDLs in SQL-CLI</summary>
      <description/>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-16 01:00:00" id="17763" opendate="2020-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>No log files when starting scala-shell</summary>
      <description>I see the following error when starting scala shell. Starting Flink Shell:ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2</description>
      <version>1.9.2,1.10.0</version>
      <fixedVersion>1.9.4,1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.start-script.start-scala-shell.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-5-19 01:00:00" id="17809" opendate="2020-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BashJavaUtil script logic does not work for paths with spaces</summary>
      <description>Multiple paths aren't quoted (class path, conf_dir) resulting in errors if they contain spaces.</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-20 01:00:00" id="17847" opendate="2020-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ArrayIndexOutOfBoundsException happens when codegen StreamExec operator</summary>
      <description>user case://source table create table json_table( w_es BIGINT, w_type STRING, w_isDdl BOOLEAN, w_data ARRAY&lt;ROW&lt;pay_info STRING, online_fee DOUBLE, sign STRING, account_pay_fee DOUBLE&gt;&gt;, w_ts TIMESTAMP(3), w_table STRING) WITH ( 'connector.type' = 'kafka', 'connector.version' = '0.10', 'connector.topic' = 'json-test2', 'connector.properties.zookeeper.connect' = 'localhost:2181', 'connector.properties.bootstrap.servers' = 'localhost:9092', 'connector.properties.group.id' = 'test-jdbc', 'connector.startup-mode' = 'earliest-offset', 'format.type' = 'json', 'format.derive-schema' = 'true')// real data:{"w_es":1589870637000,"w_type":"INSERT","w_isDdl":false,"w_data":[{"pay_info":"channelId=82&amp;onlineFee=89.0&amp;outTradeNo=0&amp;payId=0&amp;payType=02&amp;rechargeId=4&amp;totalFee=89.0&amp;tradeStatus=success&amp;userId=32590183789575&amp;sign=00","online_fee":"89.0","sign":"00","account_pay_fee":"0.0"}],"w_ts":"2020-05-20T13:58:37.131Z","w_table":"cccc111"}//queryselect w_ts, 'test' as city1_id, w_data[0].pay_info AS cate3_id, w_data as pay_order_id from json_tableexception://Caused by: java.lang.ArrayIndexOutOfBoundsException: 1427848Caused by: java.lang.ArrayIndexOutOfBoundsException: 1427848 at org.apache.flink.table.runtime.util.SegmentsUtil.getByteMultiSegments(SegmentsUtil.java:598) at org.apache.flink.table.runtime.util.SegmentsUtil.getByte(SegmentsUtil.java:590) at org.apache.flink.table.runtime.util.SegmentsUtil.bitGet(SegmentsUtil.java:534) at org.apache.flink.table.dataformat.BinaryArray.isNullAt(BinaryArray.java:117) at StreamExecCalc$10.processElement(Unknown Source) Looks like in the codegen StreamExecCalc$10 operator some operation visit a '-1' index which should be wrong, this bug exits both in 1.10 and 1.11 public class StreamExecCalc$10 extends org.apache.flink.table.runtime.operators.AbstractProcessStreamOperator implements org.apache.flink.streaming.api.operators.OneInputStreamOperator { private final Object[] references; private final org.apache.flink.table.dataformat.BinaryString str$3 = org.apache.flink.table.dataformat.BinaryString.fromString("test"); private transient org.apache.flink.table.runtime.typeutils.BaseArraySerializer typeSerializer$5; final org.apache.flink.table.dataformat.BoxedWrapperRow out = new org.apache.flink.table.dataformat.BoxedWrapperRow(4); private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null); public StreamExecCalc$10( Object[] references, org.apache.flink.streaming.runtime.tasks.StreamTask task, org.apache.flink.streaming.api.graph.StreamConfig config, org.apache.flink.streaming.api.operators.Output output) throws Exception { this.references = references; typeSerializer$5 = (((org.apache.flink.table.runtime.typeutils.BaseArraySerializer) references[0])); this.setup(task, config, output); } @Override public void open() throws Exception { super.open(); } @Override public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception { org.apache.flink.table.dataformat.BaseRow in1 = (org.apache.flink.table.dataformat.BaseRow) element.getValue(); org.apache.flink.table.dataformat.SqlTimestamp field$2; boolean isNull$2; org.apache.flink.table.dataformat.BaseArray field$4; boolean isNull$4; org.apache.flink.table.dataformat.BaseArray field$6; org.apache.flink.table.dataformat.BinaryString field$8; boolean isNull$8; org.apache.flink.table.dataformat.BinaryString result$9; boolean isNull$9; isNull$2 = in1.isNullAt(4); field$2 = null; if (!isNull$2) { field$2 = in1.getTimestamp(4, 3); } isNull$4 = in1.isNullAt(3); field$4 = null; if (!isNull$4) { field$4 = in1.getArray(3); } field$6 = field$4; if (!isNull$4) { field$6 = (org.apache.flink.table.dataformat.BaseArray) (typeSerializer$5.copy(field$6)); } out.setHeader(in1.getHeader()); if (isNull$2) { out.setNullAt(0); } else { out.setNonPrimitiveValue(0, field$2); } if (false) { out.setNullAt(1); } else { out.setNonPrimitiveValue(1, ((org.apache.flink.table.dataformat.BinaryString) str$3)); } boolean isNull$7 = isNull$4 || false || field$6.isNullAt(((int) 0) - 1); org.apache.flink.table.dataformat.BaseRow result$7 = isNull$7 ? null : field$6.getRow(((int) 0) - 1, 4); if (isNull$7) { result$9 = org.apache.flink.table.dataformat.BinaryString.EMPTY_UTF8; isNull$9 = true; } else { isNull$8 = result$7.isNullAt(0); field$8 = org.apache.flink.table.dataformat.BinaryString.EMPTY_UTF8; if (!isNull$8) { field$8 = result$7.getString(0); } result$9 = field$8; isNull$9 = isNull$8; } if (isNull$9) { out.setNullAt(2); } else { out.setNonPrimitiveValue(2, result$9); } if (isNull$4) { out.setNullAt(3); } else { out.setNonPrimitiveValue(3, field$6); } output.collect(outElement.replace(out)); } @Override public void close() throws Exception { super.close(); }}     </description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-24 01:00:00" id="17902" opendate="2020-5-24 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support the new interfaces about temporary functions in PyFlink</summary>
      <description>The interfaces such as createTemporarySystemFunction, dropTemporarySystemFunction, createFunction, dropFunction, createTemporaryFunction, dropTemporaryFunction in the Java TableEnvironment are currently not available in the PyFlink. The aim of this JIRA is to add support of them in PyFlink.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-25 01:00:00" id="17931" opendate="2020-5-25 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document fromValues clause</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-28 01:00:00" id="1797" opendate="2015-3-28 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add jumping pre-reducer for Count and Time windows</summary>
      <description>There is currently only support for sliding and tumbling windows. This should be an easy extension of the tumbling pre-reducer</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.AggregationFunctionTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.WindowUtils.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.TumblingPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingTimePreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingTimeGroupedPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingGroupedPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingCountPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingCountGroupedPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamPlanEnvironment.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamContextEnvironment.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.WindowedDataStream.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-2 01:00:00" id="18070" opendate="2020-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Time attribute been materialized after sub graph optimize</summary>
      <description>Hi, I want to use window aggregate after create temporary, and has multiple sinks. But throw exception:java.lang.AssertionError: type mismatch:ref:TIME ATTRIBUTE(PROCTIME) NOT NULLinput:TIMESTAMP(3) NOT NULLI look into the optimizer logic, there is comment at CommonSubGraphBasedOptimizer:"1. In general, for multi-sinks users tend to use VIEW which is a natural common sub-graph."After sub graph optimize, time attribute from source have been convert to basic TIMESTAMP type according to FlinkRelTimeIndicatorProgram. But my create view sql is simple query, I think didn't need to materialized time attribute in theory.Here is my code:// connector.type COLLECTION is for debug usetableEnv.sqlUpdate("CREATE TABLE source (\n" + " `ts` AS PROCTIME(),\n" + " `order_type` INT\n" + ") WITH (\n" + " 'connector.type' = 'COLLECTION',\n" + " 'format.type' = 'json'\n" + ")\n");tableEnv.createTemporaryView("source_view", tableEnv.sqlQuery("SELECT * FROM source"));tableEnv.sqlUpdate("CREATE TABLE sink (\n" + " `result` BIGINT\n" + ") WITH (\n" + " 'connector.type' = 'COLLECTION',\n" + " 'format.type' = 'json'\n" + ")\n");tableEnv.sqlUpdate("INSERT INTO sink \n" + "SELECT\n" + " COUNT(1)\n" + "FROM\n" + " `source_view`\n" + "WHERE\n" + " `order_type` = 33\n" + "GROUP BY\n" + " TUMBLE(`ts`, INTERVAL '5' SECOND)\n");tableEnv.sqlUpdate("INSERT INTO sink \n" + "SELECT\n" + " COUNT(1)\n" + "FROM\n" + " `source_view`\n" + "WHERE\n" + " `order_type` = 34\n" + "GROUP BY\n" + " TUMBLE(`ts`, INTERVAL '5' SECOND)\n");</description>
      <version>1.10.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-7 01:00:00" id="18168" opendate="2020-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error results when use UDAF with Object Array return type</summary>
      <description>I get error results when I use an UDAF with Object Array return type (e.g. Row[]). I find that the problem is we reuse 'reuseArray' as the return value of ObjectArrayConverter.toBinaryArray(). It leads to 'prevAggValue' and 'newAggValue' in GroupAggFunction.processElement() contains exactly the same BinaryArray, so 'equaliser.equalsWithoutHeader(prevAggValue, newAggValue)' is always true.</description>
      <version>None</version>
      <fixedVersion>1.10.2,1.11.0,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.DataStructureConvertersTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.DataFormatConvertersTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.data.util.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.data.conversion.ArrayObjectArrayConverter.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2020-6-16 01:00:00" id="18315" opendate="2020-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert into partitioned table can fail with values</summary>
      <description/>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-16 01:00:00" id="18329" opendate="2020-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dist NOTICE issues</summary>
      <description>akka-actor version incorrect 2.5.1 -&gt; 2.5.21</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-7-7 01:00:00" id="18507" opendate="2020-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move get_config implementation to TableEnvironment to eliminate the duplication</summary>
      <description>Currently, TableEnvironment.get_config is abstract and the implementations in the child classes BatchTableEnvironment/StreamTableEnvironment are duplicate. The implementation could be moved to TableEnvironment to eliminate the duplication.</description>
      <version>1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-7 01:00:00" id="18514" opendate="2020-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Building fails with JDK 14 installed</summary>
      <description>On a system with JDK 14 installed the build fails with [INFO] --- gmavenplus-plugin:1.8.1:execute (merge-categories) @ flink-end-to-end-tests ---[INFO] Using plugin classloader, includes GMavenPlus classpath.java.lang.NoClassDefFoundError: Could not initialize class org.codehaus.groovy.vmplugin.v7.Java7 at org.codehaus.groovy.vmplugin.VMPluginFactory.&lt;clinit&gt;(VMPluginFactory.java:43) at org.codehaus.groovy.reflection.GroovyClassValueFactory.&lt;clinit&gt;(GroovyClassValueFactory.java:35) at org.codehaus.groovy.reflection.ClassInfo.&lt;clinit&gt;(ClassInfo.java:107) at org.codehaus.groovy.reflection.ReflectionCache.getCachedClass(ReflectionCache.java:95) at org.codehaus.groovy.reflection.ReflectionCache.&lt;clinit&gt;(ReflectionCache.java:39)Which is the same exception as seen herehttps://github.com/gradle/gradle/issues/10248This is a known problem in groovy that has been fixed in version 2.5.10: https://issues.apache.org/jira/browse/GROOVY-9211</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-7-24 01:00:00" id="18703" opendate="2020-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use new data structure converters when legacy types are not present</summary>
      <description>FLINK-16999 introduce the new data structure converters that are already in place for the new sources/sinks and new scalar/table functions. We can enable it for all usages (or almost all usages) if legacy types are not present.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ScanUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.SinkCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.TableFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.ImperativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.AggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.logical.utils.LogicalTypeChecksTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeChecks.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-27 01:00:00" id="18730" opendate="2020-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Beta tag from SQL Client docs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-27 01:00:00" id="18731" opendate="2020-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The monotonicity of UNIX_TIMESTAMP function is not correct</summary>
      <description>Currently, the monotonicity of UNIX_TIMESTAMP function is always INCREASING, actually, when it has empty function arguments (UNIX_TIMESTAMP(), is equivalent to NOW()), its monotonicity is INCREASING. otherwise its monotonicity should be NOT_MONOTONIC. (e.g. UNIX_TIMESTAMP(string))</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-7 01:00:00" id="18844" opendate="2020-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support maxwell-json format to read Maxwell changelogs</summary>
      <description>Hi,i have finish these code .So, can assign this issule  to me ?</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-9 01:00:00" id="18861" opendate="2020-8-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support add_source() to get a DataStream for Python StreamExecutionEnvironment</summary>
      <description>Support add_source() to get a DataStream for Python StreamExecutionEnvironment. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-9-28 01:00:00" id="19070" opendate="2020-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector should throw a meaningful exception if user reads/writes ACID tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-9-31 01:00:00" id="19097" opendate="2020-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support add_jars() for Python StreamExecutionEnvironment</summary>
      <description>Add add_jars() interface in Python StreamExecutionEnvironment to enable users to specify jar dependencies in their Python DataStream Job.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-1 01:00:00" id="19109" opendate="2020-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split Reader eats chained periodic watermarks</summary>
      <description>Attempting to generate watermarks chained to the Split Reader / ContinuousFileReaderOperator, as inSingleOutputStreamOperator&lt;Event&gt; results = env .readTextFile(...) .map(...) .assignTimestampsAndWatermarks(bounded) .keyBy(...) .process(...);leads to the Watermarks failing to be produced. Breaking the chain, via disableOperatorChaining() or a rebalance, works around the bug. Using punctuated watermarks also avoids the issue.Looking at this in the debugger reveals that timer service is being prematurely quiesced.In many respects this is FLINK-7666 brought back to life.The problem is not present in 1.9.3.There's a minimal reproducible example in https://github.com/alpinegizmo/flink-question-001/tree/bug.</description>
      <version>1.10.0,1.10.1,1.10.2,1.11.0,1.11.1</version>
      <fixedVersion>1.10.3,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-1 01:00:00" id="19110" opendate="2020-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flatten current PyFlink documentation structure</summary>
      <description>The navigation for this entire section is overly complex. I would much rather see something flatter, like this: Python API Installation Table API Tutorial Table API User's Guide DataStream API User's Guide FAQ</description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.python.table.api.zh.md</file>
      <file type="M">docs.try-flink.python.table.api.md</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.sql.alter.zh.md</file>
      <file type="M">docs.dev.table.sql.alter.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.index.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.types.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.types.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.config.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.config.md</file>
      <file type="M">docs.dev.python.user-guide.table.metrics.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.metrics.md</file>
      <file type="M">docs.dev.python.user-guide.table.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.index.md</file>
      <file type="M">docs.dev.python.user-guide.table.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.dependency.management.md</file>
      <file type="M">docs.dev.python.user-guide.table.conversion.of.pandas.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.conversion.of.pandas.md</file>
      <file type="M">docs.dev.python.user-guide.table.built.in.functions.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.built.in.functions.md</file>
      <file type="M">docs.dev.python.user-guide.table.10.minutes.to.table.api.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.10.minutes.to.table.api.md</file>
      <file type="M">docs.dev.python.user-guide.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.index.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.index.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.data.types.zh.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.data.types.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.table.api.tutorial.zh.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.table.api.tutorial.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.index.zh.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.index.md</file>
      <file type="M">docs.dev.python.getting-started.installation.zh.md</file>
      <file type="M">docs.dev.python.getting-started.installation.md</file>
      <file type="M">docs.dev.python.getting-started.index.zh.md</file>
      <file type="M">docs.dev.python.getting-started.index.md</file>
      <file type="M">docs.dev.python.faq.zh.md</file>
      <file type="M">docs.dev.python.faq.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-11-26 01:00:00" id="19809" opendate="2020-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ServiceConnectionManager</summary>
      <description>The Slotpool has to interact with the ResourceManager to declare the resource requirements.We do not want to provide full access to the ResourceManagerGateway (and as such should wrap it in some form), but we also have to handle the case where no ResourceManager is connected.Introduce a component for handling this.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ManuallyTriggeredScheduledExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-26 01:00:00" id="19810" opendate="2020-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatically run a basic NOTICE file check on CI</summary>
      <description>For every release, we are manually validating the NOTICE files, according to this wiki page: https://cwiki.apache.org/confluence/display/FLINK/LicensingThe most time-consuming task is ensuring that all modules that deploy a shaded dependency to maven central are properly documenting this dependency in their NOTICE file.I would like to add a tool to Flink that checks if all shaded dependencies are at least mentioned in the NOTICE file.We will still need to perform a manual checks, but the tool should catch the most common, severe and difficult to find problems.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.maven-utils.sh</file>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-26 01:00:00" id="19811" opendate="2020-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlinkRexUtil#simplify should simplify search in conjunctions</summary>
      <description>The new version of Calcite introduces the SEARCH rex call to express range conditions. However SEARCH}}s in conjunctions are currently not simplified. For example, {{AND(=($2, 2020), SEARCH($2, Sarg&amp;#91;2020, 2021&amp;#93;)) is not simplified while it should be simplified to =($2, 2020).This issue is caused by CALCITE-4364. We could currently extend the RexSimplify class to temporarily cover this issue.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtilTest.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-17 01:00:00" id="20184" opendate="2020-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update hive streaming read and temporal table documents</summary>
      <description>The hive streaming read and temporal table document has been out of style, we need to update it.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.hive.hive.read.write.zh.md</file>
      <file type="M">docs.dev.table.connectors.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-23 01:00:00" id="20302" opendate="2020-11-23 00:00:00" resolution="Done">
    <buginformation>
      <summary>Recommend DataStream API with BATCH execution mode in DataSet docs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-7 01:00:00" id="20519" opendate="2020-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend HBase notice with transitively bundled dependencies</summary>
      <description>The HBase 2.2 sql connector bundles hbase which themselves bundles various dependencies. We should investigate what is being bundled and list it in our notice file.</description>
      <version>None</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-7 01:00:00" id="20520" opendate="2020-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document that metric names can contain characters that need to be escaped</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.metrics.zh.md</file>
      <file type="M">docs.ops.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-23 01:00:00" id="21925" opendate="2021-3-23 00:00:00" resolution="Done">
    <buginformation>
      <summary>FLIP-169 Support configuring fine grained resource requirements via DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.ResourceSpec.java</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.StreamExecutionEnvironmentTest.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSink.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.dag.Transformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-23 01:00:00" id="21926" opendate="2021-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document Fine Grained Resource Management</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.resource.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.cluster.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-9 01:00:00" id="22184" opendate="2021-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rest client shutdown on failure runs in netty thread</summary>
      <description>Then using the CLI to run any command, if the request fails then the RestClient is shut down from the netty thread, which causes problems because when shutting down we try to shut down netty, but the netty thread is still busy shutting things down.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.4,1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.executors.AbstractSessionClusterExecutor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-24 01:00:00" id="23936" opendate="2021-8-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python UDFs instances are reinitialized if there is no input for more than 1 minute</summary>
      <description>We receive this feedback from some PyFlink users. After some investigation, we find out that the root case is that there is a mechanism in Beam that it will released the BundleProcessors which are inactive for more than 1 minute: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/worker/sdk_worker.py#L90</description>
      <version>1.10.0,1.11.0,1.12.0,1.13.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-10-15 01:00:00" id="24560" opendate="2021-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-yarn-tests should copy examples after package phase</summary>
      <description>flink-yarn-tests currently copies some jars in the process-resources phase. As a result mvn test would fail in a clean environment because said jars don't exist yet.</description>
      <version>1.10.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-15 01:00:00" id="24561" opendate="2021-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build archetype jars in compile phase</summary>
      <description>The quickstart jars are currently build in the package phase, which conceptually makes sense.However, when running mvn test in a clean environment the build fails because the quickstart e2e test cannot resolve the quickstart dependencies.Usually maven figures out that it can use the compiled classes as a stand-in for (test-)jar dependencies, but this doesn't work for the quickstarts because those use a special packaging process.I don't see any harm in building the archetype jars in the compile phase to solve this issue.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-1 01:00:00" id="25905" opendate="2022-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Promoting Experimental methods is marked as a violation</summary>
      <description>Since @Experimental is not in the japicmp exclude list an experimental method in a public class is also considered as public.</description>
      <version>1.10.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>