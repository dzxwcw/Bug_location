<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2016-10-20 01:00:00" id="4639" opendate="2016-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Calcite features more pluggable</summary>
      <description>Some users might want to extend the feature set of the Table API by adding or replacing Calcite optimizer rules, modifying the parser etc. It would be good to have means to hook into the Table API and change Calcite behavior. We should implement something like a CalciteConfigBuilder.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableConfig.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-27 01:00:00" id="4689" opendate="2016-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement a simple slot provider for the new job manager</summary>
      <description>In flip-6 branch, we need to adjust existing scheduling model. In the first step, we should introduce a simple / naive slot provider which just ignore all the sharing or location constraint, to make whole thing work.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SlotPool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-27 01:00:00" id="4693" opendate="2016-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add session group-windows for batch tables</summary>
      <description>Add Session group-windows for batch tables as described in FLIP-11.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.TimeWindowPropertyCollector.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetWindowAggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetWindowAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-11 01:00:00" id="4799" opendate="2016-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-add build-target symlink to project root</summary>
      <description>We have previously removed the plugin which created the 'build-target' link to the build target directory. See FLINK-4732. At least one user has requested to re-add the link.</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-11-21 01:00:00" id="4876" opendate="2016-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow web interface to be bound to a specific ip/interface/inetHost</summary>
      <description>Currently the web interface automatically binds to all interfaces on 0.0.0.0. IMHO there are some use cases to only bind to a specific ipadress, (e.g. access through an authenticated proxy, not binding on the management or backup interface)</description>
      <version>1.1.2,1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorConfig.java</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-21 01:00:00" id="4881" opendate="2016-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Docker: Remove dependency on shared volumes</summary>
      <description>Our Dockerfile assumes a shared volume configuration to access the config. Instead, we can configure the Docker container to directly write the hostname into /etc/hosts and use "jobmanager" as the default hostname.This has been proposed here: https://github.com/apache/flink/pull/2667</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.docker-flink.README.md</file>
      <file type="M">flink-contrib.docker-flink.docker-entrypoint.sh</file>
      <file type="M">flink-contrib.docker-flink.docker-compose.yml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-10-26 01:00:00" id="4925" opendate="2016-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate meters into IOMetricGroups</summary>
      <description/>
      <version>1.1.3</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.OperatorIOMetricGroup.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-26 01:00:00" id="4927" opendate="2016-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement FLIP-6 YARN Resource Manager</summary>
      <description>The Flink YARN Resource Manager communicates with YARN's Resource Manager to acquire and release containers.It is also responsible to notify the JobManager eagerly about container failures.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-26 01:00:00" id="4929" opendate="2016-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement FLIP-6 YARN TaskManager Runner</summary>
      <description>The YARN TaskManager Runner has the following responsibilities: Read the configuration and all environment variables and compute the effective configuration Start all services (Rpc, High Availability, Security, etc) Instantiate and start the Task Manager Runner</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-11-31 01:00:00" id="4975" opendate="2016-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a limit for how much data may be buffered during checkpoint alignment</summary>
      <description>During checkpoint alignment, data may be buffered/spilled.We should introduce an upper limit for the spilled data volume. After exceeding that limit, the checkpoint alignment should abort and the checkpoint be canceled.</description>
      <version>1.1.3</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BufferSpiller.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierBuffer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-31 01:00:00" id="4983" opendate="2016-10-31 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Web UI: Add favicon</summary>
      <description>Makes the tab easier to find when having multiple tabs open</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.bower.json</file>
      <file type="M">flink-runtime-web.web-dashboard.app.index.jade</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-31 01:00:00" id="4985" opendate="2016-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Report Declined/Canceled Checkpoints to Checkpoint Coordinator</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeAllWindowCheckpointingITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierTrackerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierTracker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierBuffer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DummyEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.RuntimeEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.CheckpointResponder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayCheckpointResponder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.checkpoint.DeclineCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.StatefulTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.Environment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-11-4 01:00:00" id="5013" opendate="2016-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Kinesis connector doesn&amp;#39;t work on old EMR versions</summary>
      <description>A user reported on the mailing list that our Kinesis connector doesn't work with EMR 4.4.0: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Kinesis-Connector-Dependency-Problems-td9790.htmlThe problem seems to be that Flink is loading older libraries from the "YARN container classpath", which on EMR contains the default Amazon libraries.We should try to shade kinesis and its amazon dependencies into a different namespace.</description>
      <version>None</version>
      <fixedVersion>1.1.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-12-9 01:00:00" id="5039" opendate="2016-11-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro GenericRecord support is broken</summary>
      <description>Avro GenericRecord support was introduced in FLINK-3691, but it seems like the GenericRecords are not properly (de)serialized.This can be easily seen with a program like this: env.createInput(new AvroInputFormat&lt;&gt;(new Path("somefile.avro"), GenericRecord.class)) .first(10) .print();which will print records in which all fields have the same value:{"foo": 1478628723066, "bar": 1478628723066, "baz": 1478628723066, ...}{"foo": 1478628723179, "bar": 1478628723179, "baz": 1478628723179, ...}If I'm not mistaken, the AvroInputFormat does essentially TypeExtractor.getForClass(GenericRecord.class), but GenericRecords are not POJOs.Furthermore, each GenericRecord contains a pointer to the record schema. I guess the current naive approach will serialize this schema with each record, which is quite inefficient (the schema is typically more complex and much larger than the data). We probably need a TypeInformation and TypeSerializer specific to Avro GenericRecords, which could just use avro serialization.</description>
      <version>1.1.3</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-10 01:00:00" id="5047" opendate="2016-11-10 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Add sliding group-windows for batch tables</summary>
      <description>Add Slide group-windows for batch tables as described in FLIP-11.There are two ways to implement sliding windows for batch:1. replicate the output in order to assign keys for overlapping windows. This is probably the more straight-forward implementation and supports any aggregation function but blows up the data volume.2. if the aggregation functions are combinable / pre-aggregatable, we can also find the largest tumbling window size from which the sliding windows can be assembled. This is basically the technique used to express sliding windows with plain SQL (GROUP BY + OVER clauses). For a sliding window Slide(10 minutes, 2 minutes) this would mean to first compute aggregates of non-overlapping (tumbling) 2 minute windows and assembling consecutively 5 of these into a sliding window (could be done in a MapPartition with sorted input). The implementation could be done as an optimizer rule to split the sliding aggregate into a tumbling aggregate and a SQL WINDOW operator. Maybe it makes sense to implement the WINDOW clause first and reuse this for sliding windows.3. There is also a third, hybrid solution: Doing the pre-aggregation on the largest non-overlapping windows (as in 2) and replicating these results and processing those as in the 1) approach. The benefits of this is that it a) is based on the implementation that supports non-combinable aggregates (which is required in any case) and b) that it does not require the implementation of the SQL WINDOW operator. Internally, this can be implemented again as an optimizer rule that translates the SlidingWindow into a pre-aggregating TublingWindow and a final SlidingWindow (with replication).see FLINK-4692 for more discussion</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetWindowAggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleCountWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.AggregateFunction.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-11-10 01:00:00" id="5050" opendate="2016-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JSON.org license is CatX</summary>
      <description>We should exclude org.json:json dependency from hive-exec dependency.[INFO] +- org.apache.flink:flink-java:jar:1.2-SNAPSHOT:provided...[INFO] +- org.apache.hive.hcatalog:hcatalog-core:jar:0.12.0:compile...[INFO] | +- org.apache.hive:hive-exec:jar:0.12.0:compile[INFO] | | +- com.google.protobuf:protobuf-java:jar:2.4.1:compile[INFO] | | +- org.iq80.snappy:snappy:jar:0.2:compile[INFO] | | +- org.json:json:jar:20090211:compile</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-batch-connectors.flink-hcatalog.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-11-16 01:00:00" id="5075" opendate="2016-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis consumer incorrectly determines shards as newly discovered when tested against Kinesalite</summary>
      <description>A user reported that when our Kinesis connector is used against Kinesalite (https://github.com/mhart/kinesalite), we're incorrectly determining already found shards as newly discovered:http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Subtask-keeps-on-discovering-new-Kinesis-shard-when-using-Kinesalite-td10133.htmlI suspect the problem to be the mock Kinesis API implementations of Kinesalite doesn't completely match with the official AWS Kinesis behaviour.</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-16 01:00:00" id="5076" opendate="2016-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shutting down TM when shutting down new mini cluster</summary>
      <description>Currently we don't shut down task manager when shutting down mini cluster. It will cause mini cluster can not exit normally.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-16 01:00:00" id="5084" opendate="2016-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Java Table API integration tests by unit tests</summary>
      <description>The Java Table API is a wrapper on top of the Scala Table API. Instead of operating directly with Expressions like the Scala API, the Java API accepts a String parameter which is parsed into Expressions.We could therefore replace the Java Table API ITCases by tests that check that the parsing step produces a valid logical plan.This could be done by creating two Table objects for an identical query once with the Scala Expression API and one with the Java String API and comparing the logical plans of both Table objects. Basically something like the following:val ds1 = CollectionDataSets.getSmall3TupleDataSet(env).toTable(tEnv, 'a, 'b, 'c)val ds2 = CollectionDataSets.get5TupleDataSet(env).toTable(tEnv, 'd, 'e, 'f, 'g, 'h)val joinT1 = ds1.join(ds2).where('b === 'e).select('c, 'g)val joinT2 = ds1.join(ds2).where("b = e").select("c, g")val lPlan1 = joinT1.logicalPlanval lPlan2 = joinT2.logicalPlanAssert.assertEquals("Logical Plans do not match", lPlan1, lPlan2)</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.JoinITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.CastingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.CalcITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.AggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.ExplainTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-17 01:00:00" id="5091" opendate="2016-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Formalize the AppMaster environment for docker compability</summary>
      <description>For scenarios where the AppMaster is launched from a docker image, it would be ideal to use the installed Flink rather than rely on a special file layout in the sandbox directory.This is related to DCOS integration, which (in 1.2) will launch the AppMaster via Marathon (as a top-level DCOS service). The existing code assumed that only the dispatcher (coming in 1.3) would launch the AppMaster.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.CommonTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServer.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.Utils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerRunner.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManager.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosConfigKeys.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.GlobalConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-21 01:00:00" id="5107" opendate="2016-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job Manager goes out of memory from long history of prior execution attempts</summary>
      <description>We have observed that the job manager can run out of memory during long running jobs with many vertexes. Analysis of the heap dump shows, that the ever-growing history of prior execution attempts is the culprit for this problem.We should limit this history to a number of n most recent attempts.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.AllVerticesIteratorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionVertex.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.AbstractSubtaskAttemptRequestHandler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-21 01:00:00" id="5119" opendate="2016-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Last taskmanager heartbeat not showing in web frontend</summary>
      <description>The web frontend does not list anything for the last heartbeat in the web frontend.</description>
      <version>1.1.3,1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-22 01:00:00" id="5123" opendate="2016-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add description how to do proper shading to Flink docs.</summary>
      <description/>
      <version>1.1.3</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.building.md</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-11-23 01:00:00" id="5143" opendate="2016-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add EXISTS to list of supported operators</summary>
      <description>EXISTS is supported in certain cases. We should add it so that e.g. TPC-H query 4 runs properly.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-23 01:00:00" id="5144" opendate="2016-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error while applying rule AggregateJoinTransposeRule</summary>
      <description>AggregateJoinTransposeRule seems to cause errors. We have to investigate if this is a Flink or Calcite error. Here a simplified example:select sum(l_extendedprice)from lineitem, partwhere p_partkey = l_partkey and l_quantity &lt; ( select avg(l_quantity) from lineitem where l_partkey = p_partkey )Exception:Exception in thread "main" java.lang.AssertionError: Internal error: Error occurred while applying rule AggregateJoinTransposeRule at org.apache.calcite.util.Util.newInternal(Util.java:792) at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:148) at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:225) at org.apache.calcite.rel.rules.AggregateJoinTransposeRule.onMatch(AggregateJoinTransposeRule.java:342) at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:213) at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:819) at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:334) at org.apache.flink.api.table.BatchTableEnvironment.optimize(BatchTableEnvironment.scala:251) at org.apache.flink.api.table.BatchTableEnvironment.translate(BatchTableEnvironment.scala:286) at org.apache.flink.api.scala.table.BatchTableEnvironment.toDataSet(BatchTableEnvironment.scala:139) at org.apache.flink.api.scala.table.package$.table2RowDataSet(package.scala:77) at org.apache.flink.api.scala.sql.tpch.TPCHQueries$.runQ17(TPCHQueries.scala:826) at org.apache.flink.api.scala.sql.tpch.TPCHQueries$.main(TPCHQueries.scala:57) at org.apache.flink.api.scala.sql.tpch.TPCHQueries.main(TPCHQueries.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)Caused by: java.lang.AssertionError: Type mismatch:rowtype of new rel:RecordType(BIGINT l_partkey, BIGINT p_partkey) NOT NULLrowtype of set:RecordType(BIGINT p_partkey) NOT NULL at org.apache.calcite.util.Litmus$1.fail(Litmus.java:31) at org.apache.calcite.plan.RelOptUtil.equal(RelOptUtil.java:1838) at org.apache.calcite.plan.volcano.RelSubset.add(RelSubset.java:273) at org.apache.calcite.plan.volcano.RelSet.add(RelSet.java:148) at org.apache.calcite.plan.volcano.VolcanoPlanner.addRelToSet(VolcanoPlanner.java:1820) at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1766) at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:1032) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1052) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1942) at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:136) ... 17 more</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-25 01:00:00" id="5159" opendate="2016-11-25 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Improve perfomance of inner joins with a single row input</summary>
      <description>All inner joins (including a cross join) can be implemented as a MapFunction if one of their inputs is a single row. This row can be passed to a MapFunction as a BroadcastSet.This approach is going to be more lightweight than the other current strategies.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-25 01:00:00" id="5160" opendate="2016-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SecurityContextTest#testCreateInsecureHadoopCtx fails on windows</summary>
      <description>The test uses the wrong method to retrieve the current user name when run on Windows.</description>
      <version>1.1.3</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.OperatingSystem.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.SecurityUtilsTest.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-11-28 01:00:00" id="5170" opendate="2016-11-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>getAkkaConfig will use localhost if hostname is specified</summary>
      <description>in AkkaUtil.scala, def getAkkaConfig(configuration: Configuration, hostname: String, port: Int): Config = { getAkkaConfig(configuration, if (hostname == null) Some((hostname, port)) else None) }when hostname is specified, it use None.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcServiceUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-28 01:00:00" id="5173" opendate="2016-11-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade RocksDB dependency</summary>
      <description>The current RocksDB version has some bugs which have been observed to cause data corruption in at least one case.Newer RocksDB versions also support Microsoft Windows.</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-12-29 01:00:00" id="5196" opendate="2016-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t log InputChannelDescriptor</summary>
      <description>Logging the InputChannelDescriptors is very noisy and usually infeasible to parse for larger setups with all-to-all connections.In a log of a larger scale Flink job this lead to a 11 fold reduction in file size (175 to 15 MBs).</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-29 01:00:00" id="5199" opendate="2016-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve logging of submitted job graph actions in HA case</summary>
      <description>Include the involved paths (ZK and FS) when logging and make sure they happen for each operation (put, get, delete).</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.SubmittedJobGraph.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-30 01:00:00" id="5211" opendate="2016-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include an example configuration for all reporters</summary>
      <description>We should extend the reporter documentation to include an example configuration for every reporter.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-1 01:00:00" id="5219" opendate="2016-12-1 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add non-grouped session windows for batch tables</summary>
      <description>Add non-grouped session windows for batch tables as described in FLIP-11.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggregateCombineGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetWindowAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-2 01:00:00" id="5223" opendate="2016-12-2 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add documentation of UDTF in Table API &amp; SQL</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-2 01:00:00" id="5232" opendate="2016-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a Thread default uncaught exception handler on the JobManager</summary>
      <description>When some JobManager threads die because of uncaught exceptions, we should bring down the JobManager. If a thread dies from an uncaught exception, there is a high chance that the JobManager becomes dysfunctional.The only sfae thing is to rely on the JobManager being restarted by YARN / Mesos / Kubernetes / etc.I suggest to add this code to the JobManager launch:Thread.setDefaultUncaughtExceptionHandler(new UncaughtExceptionHandler() { @Override public void uncaughtException(Thread t, Throwable e) { try { LOG.error("Thread {} died due to an uncaught exception. Killing process.", t.getName()); } finally { Runtime.getRuntime().halt(-1); } }});</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.minicluster.LocalFlinkMiniClusterITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHACheckpointRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.JobManagerLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.InstanceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.akka.FlinkUntypedActorTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-2 01:00:00" id="5239" opendate="2016-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Properly unpack thrown exceptions in RPC methods</summary>
      <description>When an RPC invocation fails, the exception is an InvocationTargetException that contains the actual exception.We should report the actual exception to the sender, rather than the InvocationTargetException.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-1-9 01:00:00" id="5303" opendate="2016-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CUBE/ROLLUP/GROUPING SETS operator in SQL</summary>
      <description>Add support for such operators as CUBE, ROLLUP and GROUPING SETS in SQL.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateWithNullValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-9 01:00:00" id="5304" opendate="2016-12-9 00:00:00" resolution="Done">
    <buginformation>
      <summary>Change method name from crossApply to join in Table API</summary>
      <description>Currently, the UDTF in Table API is used with crossApply, but is used with JOIN in SQL. UDTF is something similar to Table, so it make sense to use .join("udtf(c) as (s)") in Table API too, and join is more familiar to users than crossApply.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.datastream.DataStreamCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.dataset.DataSetCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.sql.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.FlinkCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.functions.TableFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-16 01:00:00" id="5349" opendate="2016-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix code sample for Twitter connector</summary>
      <description>There is a typo in code sample for Twitter connector.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.twitter.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-16 01:00:00" id="5353" opendate="2016-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch Sink loses well-formed documents when there are malformed documents</summary>
      <description/>
      <version>1.1.3</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.Elasticsearch1ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.Elasticsearch5ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.Elasticsearch2ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchApiCallBridge.java</file>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-16 01:00:00" id="5357" opendate="2016-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WordCountTable fails</summary>
      <description>The execution of org.apache.flink.table.examples.java.WordCountTable fails:Exception in thread "main" org.apache.flink.table.api.TableException: POJO does not define field name: TMP_0 at org.apache.flink.table.typeutils.TypeConverter$$anonfun$2.apply(TypeConverter.scala:85) at org.apache.flink.table.typeutils.TypeConverter$$anonfun$2.apply(TypeConverter.scala:81) at scala.collection.immutable.List.foreach(List.scala:318) at org.apache.flink.table.typeutils.TypeConverter$.determineReturnType(TypeConverter.scala:81) at org.apache.flink.table.plan.nodes.dataset.DataSetCalc.translateToPlan(DataSetCalc.scala:110) at org.apache.flink.table.api.BatchTableEnvironment.translate(BatchTableEnvironment.scala:305) at org.apache.flink.table.api.BatchTableEnvironment.translate(BatchTableEnvironment.scala:289) at org.apache.flink.table.api.java.BatchTableEnvironment.toDataSet(BatchTableEnvironment.scala:146) at org.apache.flink.table.examples.java.WordCountTable.main(WordCountTable.java:56) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-21 01:00:00" id="5377" opendate="2016-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve savepoint docs</summary>
      <description>The savepoint docs are very detailed and focus on the internals. They should better convey what users have to take care of.The following questions should be answered:What happens if I add a new operator that requires state to my flow?What happens if I delete an operator that has state to my flow?What happens if I reorder stateful operators in my flow?What happens if I add or delete or reorder operators that have no state in my flow?Should I apply .uid to all operators in my flow?Should I apply .uid to only the operators that have state?</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.savepoints.md</file>
      <file type="M">docs.fig.savepoints-program.ids.png</file>
      <file type="M">docs.fig.savepoints-overview.png</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-21 01:00:00" id="5378" opendate="2016-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Scopt version to 3.5.0</summary>
      <description>Is it possible to increase the Scopt version to 3.5.0? This version does also support comma-separated lists of arguments.I'm using this in my project and indeed I can use Maven to use the latest Scopt version. But, once I want to deploy an uber-Jar to Flink, it obviously fails because of two different versions of Scopt in the classpath - one in my uber-Jar (Scopt 3.5.0) and the one shipped with Flink distribution (Scopt 3.2.0).I know that there is another open issue regarding refactoring the CLI parser (FLINK-1347), but as far as I can see there is no progress yet.</description>
      <version>1.1.3,1.2.0,1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>