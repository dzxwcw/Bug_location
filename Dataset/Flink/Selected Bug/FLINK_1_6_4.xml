<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  <bug fixdate="2019-1-30 01:00:00" id="11469" opendate="2019-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix example in "Tuning Checkpoints and Large State" documentation</summary>
      <description>Sample code for subtitle Tuning RocksDB in Tuning Checkpoints and Large State is wrong  Affects Version：All versions after 1.1  </description>
      <version>1.6.2,1.6.3,1.6.4,1.7.0,1.7.1,1.7.2,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-20 01:00:00" id="11665" opendate="2019-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink fails to remove JobGraph from ZK even though it reports it did</summary>
      <description>We recently have seen the following issue with Flink 1.5.5:Given Flink Job ID 1d24cad26843dcebdfca236d5e3ad82a: 1- A job is activated successfully and the job graph added to ZK:Added SubmittedJobGraph(1d24cad26843dcebdfca236d5e3ad82a, null) to ZooKeeper.2- Job is deactivated, Flink reports that the job graph has been successfully removed from ZK and the blob is deleted from the blob server (in this case S3):Removed job graph 1d24cad26843dcebdfca236d5e3ad82a from ZooKeeper.3- JM is later restarted, Flink for some reason attempts to recover the job that it reported earlier it has removed from ZK but since the blob has already been deleted the JM goes into a crash loop. The only way to recover it manually is to remove the job graph entry from ZK:Recovered SubmittedJobGraph(1d24cad26843dcebdfca236d5e3ad82a, null). andorg.apache.flink.fs.s3presto.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 1BCDFD83FC4546A2), S3 Extended Request ID: OzZtMbihzCm1LKy99s2+rgUMxyll/xYmL6ouMvU2eo30wuDbUmj/DAWoTCs9pNNCLft0FWqbhTo= (Path: s3://blam-state-staging/flink/default/blob/job_1d24cad26843dcebdfca236d5e3ad82a/blob_p-c51b25cc0b20351f6e32a628bb6e674ee48a273e-ccfa96b0fd795502897c73714185dde3)My question is under what circumstances would this happen? this seems to happen very infrequently but since the consequence is severe (JM crash loop) we'd like to understand how it would happen.This all seems a little similar to https://issues.apache.org/jira/browse/FLINK-9575 but that issue is reported fixed in Flink 1.5.2 and we are already on Flink 1.5.5</description>
      <version>1.5.5,1.6.4,1.7.2,1.8.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DispatcherRunnerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DispatcherRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDispatcherRunnerImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DispatcherRunnerImplNGFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-25 01:00:00" id="11745" opendate="2019-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TTL end-to-end test restores from the savepoint after the job cancelation</summary>
      <description>The state TTL end-to-end test currently cancels the first running job, takes savepoint and starts the job again from stratch without using the savepoint. The second job should start from the previously taken savepoint.</description>
      <version>1.6.4,1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.6.5,1.7.3,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.stream.state.ttl.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-4-13 01:00:00" id="12185" opendate="2019-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for generating optimized logical plan for join on stream</summary>
      <description>This issue aims to supports generating optimized plan for join on stream. The query will be converted to window join if join condition contains window bounds, otherwise will be converted to normal join.e.g.Queries similar to the following should be window join:SELECT t1.a, t2.b FROM MyTable t1 JOIN MyTable2 t2 ON t1.a = t2.a AND t1.proctime BETWEEN t2.proctime - INTERVAL '1' HOUR AND t2.proctime + INTERVAL '1' HOUR</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkLogicalRankRuleForRangeEndTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.ShuffledHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RankUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-22 01:00:00" id="12285" opendate="2019-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory leak in SavepointITCase and SavepointMigrationTestBase</summary>
      <description>The tests in SavepointITCase and SavepointMigrationTestBase do not cancel running jobs before exit. It will cause exceptions in {{TaskExecutor}}s and unreleased memory segments. Succeeding tests may fail due to insufficient amount of memory.The problem is caused by cancelling {{TaskExecutor}}s with running tasks. Another issue caused by the reason can be seen in FLINK-11343. Maybe we can find a more dedicated method to cancel those {{TaskExecutor}}s still having running tasks.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.MiniClusterResource.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-4-24 01:00:00" id="12323" opendate="2019-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy ActorGateway implementations</summary>
      <description>Remove the legacy ActorGateway based implementations.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskInputSplitProviderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.TaskInputSplitProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayTaskManagerActions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayResultPartitionConsumableNotifier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayPartitionProducerStateChecker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayKvStateRegistryListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayKvStateLocationOracle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayGlobalAggregateManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayCheckpointResponder.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-26 01:00:00" id="12342" opendate="2019-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn Resource Manager Acquires Too Many Containers</summary>
      <description>In currently implementation of YarnFlinkResourceManager, it starts to acquire new container one by one when get request from SlotManager. The mechanism works when job is still, say less than 32 containers. If the job has 256 container, containers can't be immediately allocated and appending requests in AMRMClient will be not removed accordingly. We observe the situation that AMRMClient ask for current pending request + 1 (the new request from slot manager) containers. In this way, during the start time of such job, it asked for 4000+ containers. If there is an external dependency issue happens, for example hdfs access is slow. Then, the whole job will be blocked without getting enough resource and finally killed with SlotManager request timeout.Thus, we should use the total number of container asked rather than pending request in AMRMClient as threshold to make decision whether we need to add one more resource request.</description>
      <version>1.6.4,1.7.2,1.8.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">docs..includes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-26 01:00:00" id="12343" opendate="2019-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow set file.replication in Yarn Configuration</summary>
      <description>Currently, FlinkYarnSessionCli upload jars into hdfs with default 3 replications. From our production experience, we find that 3 replications will block big job (256 containers) to launch, when the HDFS is slow due to big workload for batch pipelines. Thus, we want to make the factor customizable from FlinkYarnSessionCli by adding an option.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnFileStageTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNITCase.java</file>
      <file type="M">docs..includes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-6-26 01:00:00" id="12990" opendate="2019-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Date type doesn&amp;#39;t consider the local TimeZone</summary>
      <description>Currently, the python DateType is converted by an `int` which indicates the days passed since 1970-1-1 and then the Java side will create a Java Date by call `new Date(days * 86400)`. As we know that the Date constructor expected milliseconds since 1970-1-1 00:00:00 GMT and so we should convert `days * 86400` to GMT milliseconds.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.python.PythonTableUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-27 01:00:00" id="13017" opendate="2019-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Broken and irreproducible dockerized docs build</summary>
      <description>The build tools around docs/docker seem broken and (on my machine) give errors like the following while it is working on a colleague's machine:bash: /etc/bash_completion.d/git-prompt.sh: No such file or directorybash: __git_ps1: command not found/usr/bin/env: 'ruby.ruby2.5': No such file or directorybash: __git_ps1: command not foundReason seems to be that your whole user's $HOME is mounted (writable!) into the docker container. We should just mount the docs directory to get builds which are independent from the host system (making them reproducible) not have the commands in the container affect the host</description>
      <version>1.6.4,1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.docker.run.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-3 01:00:00" id="13067" opendate="2019-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix broken links to contributing docs</summary>
      <description>As contributing links change on https://github.com/apache/flink-web, all links to contributing related docs have become broken. We need to fix these broken links.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.redirects.windows.md</file>
      <file type="M">docs.redirects.setup.quickstart.md</file>
      <file type="M">docs.redirects.filesystems.md</file>
      <file type="M">docs.redirects.example.quickstart.md</file>
      <file type="M">docs.internals.components.zh.md</file>
      <file type="M">docs.internals.components.md</file>
      <file type="M">docs.index.zh.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
      <file type="M">docs.dev.projectsetup.scala.api.quickstart.zh.md</file>
      <file type="M">docs.dev.projectsetup.scala.api.quickstart.md</file>
      <file type="M">docs.dev.projectsetup.java.api.quickstart.zh.md</file>
      <file type="M">docs.dev.projectsetup.java.api.quickstart.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.zh.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.md</file>
      <file type="M">docs.dev.libs.gelly.index.zh.md</file>
      <file type="M">docs.dev.libs.gelly.index.md</file>
      <file type="M">.github.PULL.REQUEST.TEMPLATE.md</file>
      <file type="M">.github.CONTRIBUTING.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-7 01:00:00" id="16485" opendate="2020-3-7 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support vectorized Python UDF in the batch mode of old planner</summary>
      <description>Currently, vectorized Python UDF is only supported in the batch/stream mode for the blink planner and stream mode for the old planner. The aim of this Jira is to add support in the batch mode for the old planner.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetPythonCalc.scala</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonTableFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonStatelessFunctionFlatMap.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-7 01:00:00" id="16486" opendate="2020-3-7 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add documentation for vectorized Python UDF</summary>
      <description>As the title described, the aim of this JIRA is to add documentation for vectorized Python UDF.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.python.index.zh.md</file>
      <file type="M">docs.dev.table.python.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-15 01:00:00" id="8949" opendate="2018-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rest API failure with long URL</summary>
      <description>When you have jobs with high parallelism, the URL for a REST request can get very long. When the URL is longer than 4096 bytes, the  REST API will return errorFailure: 404 Not Found This can easily be seen in the Web UI, when Flink queries for the watermark using the REST API:GET /jobs/:jobId/vertices/:vertexId/metrics?get=0.currentLowWatermark,1.currentLowWatermark,2.currentLo...The request will fail with more than 170 subtasks and the watermark will not be displayed in the Web UI.</description>
      <version>1.4.2,1.5.0,1.6.4,1.7.2,1.8.2</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.metrics.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
</bugrepository>