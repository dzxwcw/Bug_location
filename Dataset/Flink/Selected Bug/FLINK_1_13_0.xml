<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2018-7-7 01:00:00" id="11103" opendate="2018-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set a default uncaught exception handler</summary>
      <description>We should set a default uncaught exception handler in Flink via Thread.setDefaultUncaughtExceptionHandler() which at least logs the exceptions. Ideally, we would even fail the job (could make this configurable) but users may have some ill-behaving threads, e.g. through libraries, which they would want to tolerate and we don't want to change behaviour now.(FLINK-5232 added this for the JobManager, we need it for the TaskManager)</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypointUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.cluster.section.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-5 01:00:00" id="13973" opendate="2019-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpoint recovery failed after user set uidHash</summary>
      <description>Checkpoint recovery failed after user set uidHash, the possible reasons are as follows:If altOperatorID is not null, operatorState will be obtained by altOperatorID and will not be given</description>
      <version>1.8.0,1.8.1,1.9.0,1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-27 01:00:00" id="17401" opendate="2020-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Labels to Mesos TM taskinfo object</summary>
      <description>Currently labels are not set in the task info object. A lot of companies can have logic specific to labels on TM. For example in criteo, based on labels we set kerberos env variables and mesos debug capabilities. It is critical for us to be able to pass these label values to the task manager.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParametersTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerDriverTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">docs.layouts.shortcodes.generated.mesos.task.manager.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-4-17 01:00:00" id="1902" opendate="2015-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web interface reports false (the default) jobmanager.rpc.port on YARN</summary>
      <description>Running Flink as YARN session mode I was completely confused by the web interface reporting a false jobmanager.rpc.port (the default).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMaster.scala</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-29 01:00:00" id="19445" opendate="2020-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Several tests for HBase connector 1.4 failed with "NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7042&amp;view=logs&amp;j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&amp;t=bfbc6239-57a0-5db0-63f3-41551b4f7d512020-09-28T21:28:29.4171075Z Running org.apache.flink.connector.hbase1.HBaseTablePlanTest2020-09-28T21:28:31.0367584Z Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.62 sec &lt;&lt;&lt; FAILURE! - in org.apache.flink.connector.hbase1.HBaseTablePlanTest2020-09-28T21:28:31.0368925Z testProjectionPushDown(org.apache.flink.connector.hbase1.HBaseTablePlanTest) Time elapsed: 0.031 sec &lt;&lt;&lt; ERROR!2020-09-28T21:28:31.0369805Z org.apache.flink.table.api.ValidationException: 2020-09-28T21:28:31.0370409Z Unable to create a source for reading table 'default_catalog.default_database.hTable'.2020-09-28T21:28:31.0370707Z 2020-09-28T21:28:31.0370976Z Table options are:2020-09-28T21:28:31.0371204Z 2020-09-28T21:28:31.0371528Z 'connector'='hbase-1.4'2020-09-28T21:28:31.0371871Z 'table-name'='my_table'2020-09-28T21:28:31.0372255Z 'zookeeper.quorum'='localhost:2021'2020-09-28T21:28:31.0372812Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125)2020-09-28T21:28:31.0373359Z at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.buildTableScan(CatalogSourceTable.scala:135)2020-09-28T21:28:31.0373905Z at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:78)2020-09-28T21:28:31.0374390Z at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)2020-09-28T21:28:31.0375224Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)2020-09-28T21:28:31.0375867Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)2020-09-28T21:28:31.0376479Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl$$anon$1.convertFrom(FlinkPlannerImpl.scala:181)2020-09-28T21:28:31.0377077Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)2020-09-28T21:28:31.0377593Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl$$anon$1.convertFrom(FlinkPlannerImpl.scala:181)2020-09-28T21:28:31.0378114Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)2020-09-28T21:28:31.0378622Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)2020-09-28T21:28:31.0379132Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)2020-09-28T21:28:31.0379872Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)2020-09-28T21:28:31.0380477Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:196)2020-09-28T21:28:31.0381128Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:154)2020-09-28T21:28:31.0381666Z at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:823)2020-09-28T21:28:31.0382264Z at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:795)2020-09-28T21:28:31.0382968Z at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:250)2020-09-28T21:28:31.0383550Z at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)2020-09-28T21:28:31.0384172Z at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:640)2020-09-28T21:28:31.0384700Z at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:346)2020-09-28T21:28:31.0385201Z at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyPlan(TableTestBase.scala:271)2020-09-28T21:28:31.0385717Z at org.apache.flink.connector.hbase1.HBaseTablePlanTest.testProjectionPushDown(HBaseTablePlanTest.java:124)2020-09-28T21:28:31.0386166Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-09-28T21:28:31.0386575Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-09-28T21:28:31.0387257Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-09-28T21:28:31.0387822Z at java.lang.reflect.Method.invoke(Method.java:498)2020-09-28T21:28:31.0388229Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-09-28T21:28:31.0388718Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-09-28T21:28:31.0389198Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-09-28T21:28:31.0389745Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-09-28T21:28:31.0390262Z at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)2020-09-28T21:28:31.0390732Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2020-09-28T21:28:31.0391179Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-09-28T21:28:31.0391582Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-09-28T21:28:31.0391964Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-09-28T21:28:31.0392382Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-09-28T21:28:31.0393053Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-09-28T21:28:31.0393617Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-09-28T21:28:31.0393997Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-09-28T21:28:31.0394407Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-09-28T21:28:31.0394817Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-09-28T21:28:31.0395211Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-09-28T21:28:31.0395608Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-09-28T21:28:31.0396041Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)2020-09-28T21:28:31.0396517Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)2020-09-28T21:28:31.0397026Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-09-28T21:28:31.0397512Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)2020-09-28T21:28:31.0398245Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)2020-09-28T21:28:31.0398778Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)2020-09-28T21:28:31.0399251Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)2020-09-28T21:28:31.0399838Z Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V2020-09-28T21:28:31.0400340Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)2020-09-28T21:28:31.0400756Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)2020-09-28T21:28:31.0401304Z at org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.createDynamicTableSource(HBase1DynamicTableFactory.java:113)2020-09-28T21:28:31.0401869Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:122)2020-09-28T21:28:31.0402307Z ... 50 more2020-09-28T21:28:31.0402624Z 2020-09-28T21:28:31.0402949Z Running org.apache.flink.connector.hbase1.HBaseDescriptorTest2020-09-28T21:28:31.0416116Z Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec - in org.apache.flink.connector.hbase1.HBaseDescriptorTest2020-09-28T21:28:31.4448287Z 2020-09-28T21:28:31.4448950Z Results :2020-09-28T21:28:31.4449082Z 2020-09-28T21:28:31.4449270Z Tests in error: 2020-09-28T21:28:31.4450556Z HBaseDynamicTableFactoryTest.testTableSourceFactory:104-&gt;createTableSource:332 Â» Validation2020-09-28T21:28:31.4451232Z HBaseTableFactoryTest.testTableSourceFactory:101 Â» NoSuchMethod com.google.com...2020-09-28T21:28:31.4451851Z HBaseTablePlanTest.testProjectionPushDown:124 Â» Validation Unable to create a ...</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-28 01:00:00" id="19844" opendate="2020-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Python UDAF</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-16 01:00:00" id="20165" opendate="2020-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARNSessionFIFOITCase.checkForProhibitedLogContents: Error occurred during initialization of boot layer java.lang.IllegalStateException: Module system already initialized</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9597&amp;view=logs&amp;j=298e20ef-7951-5965-0e79-ea664ddc435e&amp;t=8560c56f-9ec1-5c40-4ff5-9d3eaaaa882d2020-11-15T22:42:03.3053212Z 22:42:03,303 [ Time-limited test] INFO org.apache.flink.yarn.YARNSessionFIFOITCase [] - Finished testDetachedMode()2020-11-15T22:42:37.9020133Z [ERROR] Tests run: 5, Failures: 2, Errors: 0, Skipped: 2, Time elapsed: 67.485 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.yarn.YARNSessionFIFOSecuredITCase2020-11-15T22:42:37.9022015Z [ERROR] testDetachedMode(org.apache.flink.yarn.YARNSessionFIFOSecuredITCase) Time elapsed: 12.841 s &lt;&lt;&lt; FAILURE!2020-11-15T22:42:37.9023701Z java.lang.AssertionError: 2020-11-15T22:42:37.9025649Z Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-logDir-nm-1_0/application_1605480087188_0002/container_1605480087188_0002_01_000002/taskmanager.out with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:2020-11-15T22:42:37.9026730Z [2020-11-15T22:42:37.9027080Z Error occurred during initialization of boot layer2020-11-15T22:42:37.9027623Z java.lang.IllegalStateException: Module system already initialized2020-11-15T22:42:37.9033278Z java.lang.IllegalStateException: Module system already initialized2020-11-15T22:42:37.9033825Z ]2020-11-15T22:42:37.9034291Z at org.junit.Assert.fail(Assert.java:88)2020-11-15T22:42:37.9034971Z at org.apache.flink.yarn.YarnTestBase.ensureNoProhibitedStringInLogFiles(YarnTestBase.java:479)2020-11-15T22:42:37.9035814Z at org.apache.flink.yarn.YARNSessionFIFOITCase.checkForProhibitedLogContents(YARNSessionFIFOITCase.java:83)</description>
      <version>1.11.3,1.13.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-24 01:00:00" id="20310" opendate="2020-11-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Debezium, Canal, Raw support for Filesystem connector</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.zh.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-30 01:00:00" id="20423" opendate="2020-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove usage of {{site.baseurl}} from markdown files</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.execution.configuration.zh.md</file>
      <file type="M">docs.dev.execution.configuration.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.sql.gettingStarted.zh.md</file>
      <file type="M">docs.dev.table.sql.gettingStarted.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-19 01:00:00" id="2044" opendate="2015-5-19 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implementation of Gelly HITS Algorithm</summary>
      <description>Implementation of Hits Algorithm in Gelly API using Java. the feature branch can be found here: (https://github.com/JavidMayar/flink/commits/HITS)</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-3 01:00:00" id="20467" opendate="2020-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the Example in Python DataStream Doc</summary>
      <description>Currently the example of MapFunction can't work. We need to fix it.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.datastream-api-users-guide.operators.zh.md</file>
      <file type="M">docs.dev.python.datastream-api-users-guide.operators.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-19 01:00:00" id="2050" opendate="2015-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pipelining mechanism for chainable transformers and estimators</summary>
      <description>The key concept of an easy to use ML library is the quick and simple construction of data analysis pipelines. Scikit-learn's approach to define transformers and estimators seems to be a really good solution to this problem. I propose to follow a similar path, because it makes FlinkML flexible in terms of code reuse as well as easy for people coming from Scikit-learn to use the FlinkML.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.recommendation.ALS.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseVector.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery9ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery4ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery3WithUnionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery3ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery10ITCase.java</file>
      <file type="M">flink-tests.src.test.assembly.test-streamingclassloader-assembly.xml</file>
      <file type="M">flink-tests.src.test.assembly.test-kmeans-assembly.xml</file>
      <file type="M">flink-tests.src.test.assembly.test-custominput-assembly.xml</file>
      <file type="M">flink-staging.flink-tez.src.main.java.org.apache.flink.tez.examples.TPCHQuery3.java</file>
      <file type="M">flink-staging.flink-tachyon.src.test.java.org.apache.flink.tachyon.TachyonFileSystemWrapperTest.java</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.runtime.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.plan.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.expressions.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.scala.table.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.java.org.apache.flink.api.table.package-info.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.pom.xml</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.MultipleLinearRegressionITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.recommendation.ALSITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.preprocessing.StandardScalerITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.feature.PolynomialBaseITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.experimental.SciKitPipelineSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.classification.CoCoASuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">docs.libs.ml.als.md</file>
      <file type="M">docs.libs.ml.cocoa.md</file>
      <file type="M">docs.libs.ml.multiple.linear.regression.md</file>
      <file type="M">docs.libs.ml.polynomial.base.feature.mapper.md</file>
      <file type="M">docs.libs.ml.standard.scaler.md</file>
      <file type="M">flink-clients.src.test.assembly.test-assembly.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigurationTest.java</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-examples.flink-java-examples.pom.xml</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.relational.TPCHQuery3.java</file>
      <file type="M">flink-examples.flink-scala-examples.pom.xml</file>
      <file type="M">flink-java8.pom.xml</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.AvgAggregationFunction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.TupleGenerator.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.traversals.package-info.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.Job.java</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.Job.scala</file>
      <file type="M">flink-quickstart.flink-tez-quickstart.src.main.resources.archetype-resources.src.main.java.LocalJob.java</file>
      <file type="M">flink-quickstart.flink-tez-quickstart.src.main.resources.archetype-resources.src.main.java.YarnJob.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.checkpoint.package-info.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.package-info.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.JarFileCreator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.JarFileCreatorTest.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-staging.flink-avro.src.test.assembly.test-assembly.xml</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.HadoopFileOutputCommitter.java</file>
      <file type="M">flink-staging.flink-language-binding.flink-python.src.main.java.org.apache.flink.languagebinding.api.java.python.PythonPlanBinder.java</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.classification.CoCoA.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedLearner.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedTransformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Learner.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.ChainedPredictor.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.ChainedTransformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Estimator.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.KMeans.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Offset.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Predictor.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Scaler.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.feature.PolynomialBase.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Breeze.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.CanCopy.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.package.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.preprocessing.StandardScaler.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-9 01:00:00" id="20550" opendate="2020-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong savepoint config in some docs</summary>
      <description>Fix config 'state.savepoint.dir' into 'state.savepoints.dir' in docs</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.flink-operations-playground.zh.md</file>
      <file type="M">docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.ops.state.savepoints.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-10 01:00:00" id="20567" opendate="2020-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document Error in UDTF section</summary>
      <description>itemContentDocumentLinkpartInner Join with Table Function (UDTF)originTableFunction&lt;String&gt; split = new MySplitUDTF();change toTableFunction&lt;Tuple3&lt;String,String,String&gt;&gt; split = new MySplitUDTF();I have run the following the codes successfully that contain all the contents from the above.①InnerJoinwithTableFunction.java②MySplitUDTF.javaReason:In this part, it says:joinLateral(call("split", $("c")).as("s", "t", "v"))it means:the udtf has 1 input "c",and 3 outputs "s", "t", "v"So:these outputs should have 3 types.such as TableFunction&lt;Tuple3&lt;String,String,String&gt;&gt;instead of only TableFunction&lt;String&gt; split</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-14 01:00:00" id="20588" opendate="2020-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add docker-compose as appendix to Mesos documentation</summary>
      <description>Dockerfile and docker-compose.yml can be added to the Mesos documentation as appendix to provide an easy entry point for starting to work with Mesos/Marathon.</description>
      <version>1.13.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.deployment.resource-providers.mesos.zh.md</file>
      <file type="M">docs.deployment.resource-providers.mesos.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-15 01:00:00" id="20613" opendate="2020-12-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update TableResult.collect()/TableResult.print() to the new type system</summary>
      <description>Currently, TableResult.collect()/TableResult.print() use old sink interfaces and old type system. Those methods are very important in the API and should be updated.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sinks.StreamSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sinks.BatchSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.DecimalITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.AggregationITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.DataStreamJavaITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.StreamSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.SelectTableSinkSchemaConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.SelectTableSinkBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.ExternalDynamicSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.ExternalCatalogTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.BatchSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.SelectSinkOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ModifyOperationVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.SelectResultProvider.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sinks.TableSinkUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeTransformationsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.TypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.DataTypeUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeTransformations.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeTransformation.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.transforms.LegacyDecimalTypeTransformation.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.ExternalTypeInfo.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.ExternalSerializer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-16 01:00:00" id="20615" opendate="2020-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local recovery and sticky scheduling end-to-end test timeout with "IOException: Stream Closed"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10905&amp;view=logs&amp;j=6caf31d6-847a-526e-9624-468e053467d6&amp;t=0b23652f-b18b-5b6e-6eb6-a11070364610It tried to restart many times, and the final error was following:2020-12-15T23:54:00.5067862Z Dec 15 23:53:42 2020-12-15 23:53:41,538 ERROR org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Caught unexpected exception.2020-12-15T23:54:00.5068392Z Dec 15 23:53:42 java.io.IOException: Stream Closed2020-12-15T23:54:00.5068767Z Dec 15 23:53:42 at java.io.FileInputStream.readBytes(Native Method) ~[?:?]2020-12-15T23:54:00.5069223Z Dec 15 23:53:42 at java.io.FileInputStream.read(FileInputStream.java:279) ~[?:?]2020-12-15T23:54:00.5070150Z Dec 15 23:53:42 at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5071217Z Dec 15 23:53:42 at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:61) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5072295Z Dec 15 23:53:42 at org.apache.flink.runtime.util.ForwardingInputStream.read(ForwardingInputStream.java:51) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5072967Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:200) ~[?:?]2020-12-15T23:54:00.5073483Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:170) ~[?:?]2020-12-15T23:54:00.5074535Z Dec 15 23:53:42 at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5075847Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:222) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5077187Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:169) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5078495Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:152) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5079802Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:269) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5081013Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:565) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5082215Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:94) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5083500Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:299) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5084899Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5086342Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5087601Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:316) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5088924Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:155) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5090261Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:248) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5091459Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5092604Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:507) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5093748Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5094866Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5095912Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:531) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5096875Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5097814Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5098373Z Dec 15 23:53:42 at java.lang.Thread.run(Thread.java:834) [?:?]2020-12-15T23:54:00.5099549Z Dec 15 23:53:42 2020-12-15 23:53:41,557 WARN org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Exception while restoring keyed state backend for StreamFlatMap_20ba6b65f97481d5570070de90e4e791_(1/4) from alternative (1/1), will retry while more alternatives are available.2020-12-15T23:54:00.5100556Z Dec 15 23:53:42 org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.2020-12-15T23:54:00.5101480Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:328) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5102669Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:565) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5103763Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:94) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5104723Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:299) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5105700Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5106630Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5107587Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:316) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5108581Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:155) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5109505Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:248) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5110456Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5111316Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:507) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5112175Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5113012Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5113787Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:531) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5114521Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5115209Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5115635Z Dec 15 23:53:42 at java.lang.Thread.run(Thread.java:834) [?:?]2020-12-15T23:54:00.5115949Z Dec 15 23:53:42 Caused by: java.io.IOException: Stream Closed2020-12-15T23:54:00.5116246Z Dec 15 23:53:42 at java.io.FileInputStream.readBytes(Native Method) ~[?:?]2020-12-15T23:54:00.5116589Z Dec 15 23:53:42 at java.io.FileInputStream.read(FileInputStream.java:279) ~[?:?]2020-12-15T23:54:00.5117284Z Dec 15 23:53:42 at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5118080Z Dec 15 23:53:42 at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:61) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5118894Z Dec 15 23:53:42 at org.apache.flink.runtime.util.ForwardingInputStream.read(ForwardingInputStream.java:51) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5119392Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:200) ~[?:?]2020-12-15T23:54:00.5119808Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:170) ~[?:?]2020-12-15T23:54:00.5120605Z Dec 15 23:53:42 at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5121576Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:222) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5122579Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:169) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5123543Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:152) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5124476Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:269) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5124994Z Dec 15 23:53:42 ... 16 more </description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-21 01:00:00" id="2067" opendate="2015-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chained streaming operators should not throw chained exceptions</summary>
      <description>The exceptions that come from chained operators have an non-intuitive chaining structure, that makes the stack traces harder to understand.For every chained task, there is a "Failed to forward record" exception, before the actual exception comes as a cause.In the Batch API, we use a special "ExceptionInChainedStubException" that is recognized and un-nested to make chained operator exceptions surface as root exceptions. We should do the same for the streaming API.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-19 01:00:00" id="20680" opendate="2020-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fails to call var-arg function with no parameters</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.ExtractionUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-21 01:00:00" id="20694" opendate="2020-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Declarative resource management can get stuck in a loop</summary>
      <description>We've seen a few tests where the coordination layer gets stuck in a request slot -&gt; offer slot -&gt; reject slot -&gt; request slot loop.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPoolTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-22 01:00:00" id="20703" opendate="2020-12-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveSinkCompactionITCase test timeout</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11096&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=62110053-334f-5295-a0ab-80dd7e2babbfhttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11136&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=62110053-334f-5295-a0ab-80dd7e2babbfThe phenomenon is: All failure are related to the hive tests. All tests timeout when fail.</description>
      <version>None</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.compact.CompactOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.CompactionITCaseBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-22 01:00:00" id="20731" opendate="2020-12-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar Source</summary>
      <description>This is our implementation based on FLIP-27.</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.environment.MiniClusterTestEnvironment.java</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-23 01:00:00" id="20750" opendate="2020-12-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port stream python group aggregate nodes to Java</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.PythonAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-23 01:00:00" id="20751" opendate="2020-12-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port batch python group aggregate nodes to Java</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonAggregate.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-28 01:00:00" id="20779" opendate="2020-12-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for row-based operation in Python Table API</summary>
      <description/>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.docs.dev.python.table.intro.to.table.api.md</file>
      <file type="M">docs.content.zh.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.intro.to.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-28 01:00:00" id="20783" opendate="2020-12-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate the implementation of BatchExec nodes for Join</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSortMergeJoinRule.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.util.DataFormatTestUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.codegen.SortCodeGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.SortUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.OverAggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecOverAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMatch.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.sort.SortCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.sort.ComparatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.SortSpec.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortMergeJoin.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortLimit.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecRank.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.JoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecHashJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.ExecNodeUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSingleRowJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecNestedLoopJoinRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecNestedLoopJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-29 01:00:00" id="20803" opendate="2020-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Version mismatch between spotless-maven-plugin and google-java-format plugin</summary>
      <description>The spotless-maven-plugin uses version 1.7 of the google-java-format, while the IntelliJ google-java-format plugin uses 1.9, resulting in inconsistent formatting.We cannot bump the version used by the spotless plugin because it requires java 11, so instead we have to downgrade the intellij plugin to 1.7.0.5 .</description>
      <version>1.11.4,1.12.1,1.13.0</version>
      <fixedVersion>1.11.4,1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.flinkDev.ide.setup.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-29 01:00:00" id="20807" opendate="2020-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup checkstyle suppressions files</summary>
      <description>With spotless formatting having landed this is a good opportunity to remove or narrow down various checkstyle suppressions.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions-runtime.xml</file>
      <file type="M">tools.maven.suppressions-optimizer.xml</file>
      <file type="M">tools.maven.suppressions-core.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-11 01:00:00" id="20921" opendate="2021-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Date/Time/Timestamp in Python DataStream</summary>
      <description>Currently the Date/Time/Timestamp type doesn't works in Python DataStream.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.streaming.api.utils.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.PythonTypeUtils.java</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-12 01:00:00" id="20940" opendate="2021-1-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The LOCALTIME/LOCALTIMSTAMP functions should use session time zone</summary>
      <description>LOCALTIMELOCALTIME TIME(0) NOT NULL #session timezone: UTC 08:52:52 #session timezone: UTC+8 08:52:52wall clock: UTC+8:2020-12-29 08:52:52|LOCALTIMESTAMPLOCALTIMESTAMP TIMESTAMP(0) NOT NULL #session timezone: UTC 2020-12-29T08:52:52 #session timezone: UTC + 8 LOCALTIMESTAMP TIMESTAMP(0) NOT NULL 2020-12-29T08:52:52wall clock: UTC+8:2020-12-29 08:52:52|</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-12 01:00:00" id="20942" opendate="2021-1-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Digest of FLOAT literals throws UnsupportedOperationException</summary>
      <description>The recent refactoring of Calcite's digests might have caused a regression for FLOAT literals. org.apache.calcite.rex.RexLiteral#appendAsJava throws a UnsupportedOperationException for the following query:def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val source = env.fromElements( (1.0f, 11.0f, 12.0f), (2.0f, 21.0f, 22.0f), (3.0f, 31.0f, 32.0f), (4.0f, 41.0f, 42.0f), (5.0f, 51.0f, 52.0f) ) val settings = EnvironmentSettings.newInstance() .inStreamingMode() .useBlinkPlanner() .build() val tEnv = StreamTableEnvironment.create(env, settings) tEnv.createTemporaryView("myTable", source, $("id"), $("f1"), $("f2")) val query = """ |select * from myTable where id in (1.0, 2.0, 3.0) |""".stripMargin tEnv.executeSql(query).print()}Stack trace:Exception in thread "main" java.lang.UnsupportedOperationException: class org.apache.calcite.sql.type.SqlTypeName: FLOAT at org.apache.calcite.util.Util.needToImplement(Util.java:1075) at org.apache.calcite.rex.RexLiteral.appendAsJava(RexLiteral.java:703) at org.apache.calcite.rex.RexLiteral.toJavaString(RexLiteral.java:408) at org.apache.calcite.rex.RexLiteral.computeDigest(RexLiteral.java:276) at org.apache.calcite.rex.RexLiteral.&lt;init&gt;(RexLiteral.java:223) at org.apache.calcite.rex.RexLiteral.toLiteral(RexLiteral.java:737) at org.apache.calcite.rex.RexLiteral.lambda$printSarg$4(RexLiteral.java:710) at org.apache.calcite.util.RangeSets$Printer.singleton(RangeSets.java:397) at org.apache.calcite.util.RangeSets.forEach(RangeSets.java:237) at org.apache.calcite.util.Sarg.lambda$printTo$0(Sarg.java:110) at org.apache.calcite.linq4j.Ord.forEach(Ord.java:157) at org.apache.calcite.util.Sarg.printTo(Sarg.java:106) at org.apache.calcite.rex.RexLiteral.printSarg(RexLiteral.java:709) at org.apache.calcite.rex.RexLiteral.lambda$appendAsJava$1(RexLiteral.java:652) at org.apache.calcite.util.Util.asStringBuilder(Util.java:2502) at org.apache.calcite.rex.RexLiteral.appendAsJava(RexLiteral.java:651) at org.apache.calcite.rex.RexLiteral.toJavaString(RexLiteral.java:408) at org.apache.calcite.rex.RexLiteral.computeDigest(RexLiteral.java:276) at org.apache.calcite.rex.RexLiteral.&lt;init&gt;(RexLiteral.java:223) at org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:971) at org.apache.calcite.rex.RexBuilder.makeSearchArgumentLiteral(RexBuilder.java:1066) at org.apache.calcite.rex.RexSimplify$SargCollector.fix(RexSimplify.java:2786) at org.apache.calcite.rex.RexSimplify.lambda$simplifyOrs$6(RexSimplify.java:1843) at java.util.ArrayList.forEach(ArrayList.java:1257) at org.apache.calcite.rex.RexSimplify.simplifyOrs(RexSimplify.java:1843) at org.apache.calcite.rex.RexSimplify.simplifyOr(RexSimplify.java:1817) at org.apache.calcite.rex.RexSimplify.simplify(RexSimplify.java:313) at org.apache.calcite.rex.RexSimplify.simplifyUnknownAs(RexSimplify.java:282) at org.apache.calcite.rex.RexSimplify.simplify(RexSimplify.java:257) at org.apache.flink.table.planner.plan.utils.FlinkRexUtil$.simplify(FlinkRexUtil.scala:213) at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.simplify(SimplifyFilterConditionRule.scala:63) at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.onMatch(SimplifyFilterConditionRule.scala:46) at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333) at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542) at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407) at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243) at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127) at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202) at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.immutable.Range.foreach(Range.scala:160) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:79) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:286) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1329) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:707) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:1107) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:666) at org.apache.flink.table.examples.scala.basics.WordCountTable$.main(WordCountTable.scala:59)</description>
      <version>None</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-13 01:00:00" id="20946" opendate="2021-1-13 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Optimize Python ValueState Implementation In PyFlink</summary>
      <description/>
      <version>1.13.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-13 01:00:00" id="20954" opendate="2021-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize State with Cross Bundle State Cache In PyFlink</summary>
      <description/>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-13 01:00:00" id="20964" opendate="2021-1-13 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Introduce PythonStreamGroupWindowAggregateOperator</summary>
      <description>Adds PythonStreamGroupWindowAggregateOperator to support running General Python Stream Group Window Aggregate Function</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupTableAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamAggregateOperator.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-14 01:00:00" id="20975" opendate="2021-1-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveTableSourceITCase.testPartitionFilter fails on AZP</summary>
      <description>The test HiveTableSourceITCase.testPartitionFilter fails on AZP with the following exception:java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertFalse(Assert.java:64) at org.junit.Assert.assertFalse(Assert.java:74) at org.apache.flink.connectors.hive.HiveTableSourceITCase.testPartitionFilter(HiveTableSourceITCase.java:278) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.hive-site.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-19 01:00:00" id="21026" opendate="2021-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Align column list specification with Hive in INSERT statement</summary>
      <description>HIVE-9481 allows column list specification in INSERT statement. The syntax is:INSERT INTO TABLE table_name [PARTITION (partcol1[=val1], partcol2[=val2] ...)] [(column list)] select_statement FROM from_statementIn the MeanWhile, flink introduces PARTITION syntax that the PARTITION clause appears after the COLUMN LIST clause. It looks weird and luckily we don't support COLUMN LIST clause now.  I think it'a good chance to align this with Hive now.     </description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dml.RichSqlInsert.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-20 01:00:00" id="21041" opendate="2021-1-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce ExecNodeGraph to wrap the ExecNode topology</summary>
      <description>Currently, we use List&lt;ExecNode&lt;?&gt; to represent the ExecNode topology, as we will introduce more features (such as serialize/deserialize {{ExecNode}}s), It's better we can introduce an unified class to represent the topology.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.MultipleInputNodeCreationProcessorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.ExecNodePlanDumper.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.MultipleInputNodeCreationProcessor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.DeadlockBreakupProcessor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.DAGProcessor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecGraphGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-20 01:00:00" id="21048" opendate="2021-1-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor documentation related to switch memory allocator</summary>
      <description>Since we decide to change the switch of memory allocator from command to environment variable in FLINK-21034, we should also change related documentation.</description>
      <version>1.12.2,1.13.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.deployment.resource-providers.standalone.docker.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.docker.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-3-21 01:00:00" id="21076" opendate="2021-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to use the declarative scheduler and its limitations</summary>
      <description>We need to document how to use the declarative scheduler and what its limitations are. Ideally, we already create tickets for the limitations we want to fix in the foreseeable future so that we can link them.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-21 01:00:00" id="21078" opendate="2021-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add DeclarativeSlotPoolService</summary>
      <description>In order to leverage FLINK-21077, we need to implement a DeclarativeSlotPoolService which encapsulates the DeclarativeSlotPool.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridgeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridgeResourceDeclarationTest.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2021-2-22 01:00:00" id="21102" opendate="2021-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ScaleUpController</summary>
      <description>Add the ScaleUpController according to the definition in FLIP-160.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs..includes.generated.job.manager.configuration.html</file>
      <file type="M">docs..includes.generated.expert.scheduling.section.html</file>
      <file type="M">docs..includes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-22 01:00:00" id="21103" opendate="2021-1-22 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>E2e tests time out on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12377&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529Creating worker2 ... doneJan 22 13:16:17 Waiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...Jan 22 13:16:22 Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...Jan 22 13:16:27 Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...Jan 22 13:16:32 Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...Jan 22 13:16:37 Waiting for hadoop cluster to come up. We have been trying for 20 seconds, retrying ...Jan 22 13:16:43 Waiting for hadoop cluster to come up. We have been trying for 26 seconds, retrying ...Jan 22 13:16:48 Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...Jan 22 13:16:53 Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...Jan 22 13:16:58 Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...Jan 22 13:17:03 Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...Jan 22 13:17:08 We only have 0 NodeManagers up. We have been trying for 0 seconds, retrying ...21/01/22 13:17:10 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.19.0.3:803221/01/22 13:17:11 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.19.0.3:10200Jan 22 13:17:11 We now have 2 NodeManagers up.============================================================================================= WARNING: This E2E Run took already 80% of the allocated time budget of 250 minutes ====================================================================================================================================================================================================== WARNING: This E2E Run will time out in the next few minutes. Starting to upload the log output =========================================================================================================##[error]The task has timed out.Async Command Start: Upload ArtifactUploading 1 filesFile upload succeed.Upload '/tmp/_e2e_watchdog.output.0' to file container: '#/11824779/e2e-timeout-logs'Associated artifact 140921 with build 12377Async Command End: Upload ArtifactAsync Command Start: Upload ArtifactUploading 1 filesFile upload succeed.Upload '/tmp/_e2e_watchdog.output.1' to file container: '#/11824779/e2e-timeout-logs'Associated artifact 140921 with build 12377Async Command End: Upload ArtifactAsync Command Start: Upload ArtifactUploading 1 filesFile upload succeed.Upload '/tmp/_e2e_watchdog.output.2' to file container: '#/11824779/e2e-timeout-logs'Associated artifact 140921 with build 12377Async Command End: Upload ArtifactFinishing: Run e2e tests</description>
      <version>1.11.3,1.12.1,1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-23 01:00:00" id="21106" opendate="2021-1-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>google-java-format Intellij Plugin 1.7.0.5 causes UnsupportedOperationException in IntelliJ</summary>
      <description>There's a problem with google-java-format Intellij plugin version 1.7.0.5 that causes an UnsupportedOperationException when creating a new Java class file. Besides the exception, an error dialog pops up and the newly created file is not properly formatted. A simple reformat solves the issue.This problem is caused by a bug that got fixed in the google-java-format plugin's codebase in 45fb41a.Unfortunately, this fix got released with the plugin version 1.8.0.1 which we cannot upgrade to due to our limitations on sticking to Java 8 for now (see FLINK-20803).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.flinkDev.ide.setup.md</file>
      <file type="M">docs.content.zh.docs.flinkDev.ide.setup.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-5-25 01:00:00" id="21131" opendate="2021-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show alignment timeout in checkpoint configuration (web UI)</summary>
      <description> </description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-checkpoint.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-26 01:00:00" id="21149" opendate="2021-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated CatalogTable.getProperties</summary>
      <description>CatalogTable.getProperties has been deprecated in 1.11. It is time to remove it, to reduce confusion and potential bugs by calling the wrong method.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.BatchTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.factories.utils.TestCollectionTableFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.OperationMatchers.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.catalog.JavaCatalogTableTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalLegacySinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalLegacySinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTestUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.CatalogBaseTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.api.TableEnvironmentTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.AlterTablePropertiesOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogViewImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogTableImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.AbstractCatalogView.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.AbstractCatalogTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterTableProperties.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlAlterHiveTable.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSourceFactoryBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSinkFactoryBase.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.catalog.py</file>
      <file type="M">flink-python.pyflink.table.catalog.py</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-28 01:00:00" id="21168" opendate="2021-1-28 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add support for general python group window aggregate function in Physical Rule and Node</summary>
      <description>Add support for general python group window aggregate function in Physical Rule and Node</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.PythonGroupWindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.PythonGroupWindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalPythonGroupWindowAggregateRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-28 01:00:00" id="21170" opendate="2021-1-28 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add internal state hierarchy in PyFlink</summary>
      <description/>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-3-28 01:00:00" id="21189" opendate="2021-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend exception history to collect concurrent task failure that have a common root cause</summary>
      <description>We want to collect task failures that were caused by the same root cause.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.ExceptionHistoryEntryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfoWithHistoryNoRootTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionGraphInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExceptionHistoryEntry.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfoWithHistory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.exceptionhistory.ExceptionHistoryEntryMatcher.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateTrackingMockExecutionGraph.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.ExecutingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.FailureHandlingResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.Failing.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.FailureHandlingResult.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-28 01:00:00" id="21192" opendate="2021-1-28 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support setting namespace in RemoteKeyedStateBackend</summary>
      <description>Currently, RemoteKeyedStateBackend only support VoidNamespace. We need to add the support for setting namespace so that we can support window state.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-2-31 01:00:00" id="21216" opendate="2021-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamPandasConversionTests Fails</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12699&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=8d78fe4f-d658-5c70-12f8-4921589024c3 =================================== FAILURES =================================== _______________ StreamPandasConversionTests.test_empty_to_pandas _______________  self = &lt;pyflink.table.tests.test_pandas_conversion.StreamPandasConversionTests testMethod=test_empty_to_pandas&gt;   def test_empty_to_pandas(self): &gt; table = self.t_env.from_pandas(self.pdf, self.data_type)  pyflink/table/tests/test_pandas_conversion.py:144: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ pyflink/table/table_environment.py:1462: in from_pandas arrow_schema = pa.Schema.from_pandas(pdf, preserve_index=False) pyarrow/types.pxi:1315: in pyarrow.lib.Schema.from_pandas ??? .tox/py37-cython/lib/python3.7/site-packages/pyarrow/pandas_compat.py:519: in dataframe_to_types type_ = pa.lib.ndarray_to_arrow_type(values, type) pyarrow/array.pxi:53: in pyarrow.lib._ndarray_to_arrow_type ??? pyarrow/array.pxi:64: in pyarrow.lib._ndarray_to_type ??? _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  &gt; ??? E pyarrow.lib.ArrowTypeError: Did not pass numpy.dtype object  pyarrow/error.pxi:108: ArrowTypeError _________________ StreamPandasConversionTests.test_from_pandas _________________  self = &lt;pyflink.table.tests.test_pandas_conversion.StreamPandasConversionTests testMethod=test_from_pandas&gt;   def test_from_pandas(self): &gt; table = self.t_env.from_pandas(self.pdf, self.data_type, 5)  pyflink/table/tests/test_pandas_conversion.py:120: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</description>
      <version>1.11.4,1.12.2,1.13.0</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-2-4 01:00:00" id="21277" opendate="2021-2-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQLClientSchemaRegistryITCase fails to download testcontainers/ryuk:0.3.0</summary>
      <description>Tests using testcontainers fail from time to time downloading required images. Most probably caused by: https://github.com/testcontainers/testcontainers-java/issues/3574We should upgrade testcontainers to 1.15.1https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12890&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12874&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529Feb 03 19:14:14 java.lang.RuntimeException: Could not build the flink-dist imageFeb 03 19:14:14 at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.build(FlinkContainer.java:281)Feb 03 19:14:14 at org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.&lt;init&gt;(SQLClientSchemaRegistryITCase.java:88)Feb 03 19:14:14 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)Feb 03 19:14:14 at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)Feb 03 19:14:14 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)Feb 03 19:14:14 at java.lang.reflect.Constructor.newInstance(Constructor.java:423)Feb 03 19:14:14 at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)Feb 03 19:14:14 at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)Feb 03 19:14:14 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)Feb 03 19:14:14 at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)Feb 03 19:14:14 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)Feb 03 19:14:14 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)Feb 03 19:14:14 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)Feb 03 19:14:14 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)Feb 03 19:14:14 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)Feb 03 19:14:14 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)Feb 03 19:14:14 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)Feb 03 19:14:14 at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)Feb 03 19:14:14 at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)Feb 03 19:14:14 at java.util.concurrent.FutureTask.run(FutureTask.java:266)Feb 03 19:14:14 at java.lang.Thread.run(Thread.java:748)Feb 03 19:14:14 Caused by: java.lang.RuntimeException: com.github.dockerjava.api.exception.NotFoundException: Status 404: {"message":"No such image: testcontainers/ryuk:0.3.0"}Feb 03 19:14:14 Feb 03 19:14:14 at org.rnorth.ducttape.timeouts.Timeouts.callFuture(Timeouts.java:68)Feb 03 19:14:14 at org.rnorth.ducttape.timeouts.Timeouts.getWithTimeout(Timeouts.java:43)Feb 03 19:14:14 at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:45)Feb 03 19:14:14 at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.buildBaseImage(FlinkContainer.java:309)Feb 03 19:14:14 at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.build(FlinkContainer.java:268)Feb 03 19:14:14 ... 20 moreFeb 03 19:14:14 Caused by: com.github.dockerjava.api.exception.NotFoundException: Status 404: {"message":"No such image: testcontainers/ryuk:0.3.0"}Feb 03 19:14:14 Feb 03 19:14:14 at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.execute(DefaultInvocationBuilder.java:241)Feb 03 19:14:14 at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.post(DefaultInvocationBuilder.java:125)Feb 03 19:14:14 at org.testcontainers.shaded.com.github.dockerjava.core.exec.CreateContainerCmdExec.execute(CreateContainerCmdExec.java:33)Feb 03 19:14:14 at org.testcontainers.shaded.com.github.dockerjava.core.exec.CreateContainerCmdExec.execute(CreateContainerCmdExec.java:13)Feb 03 19:14:14 at org.testcontainers.shaded.com.github.dockerjava.core.exec.AbstrSyncDockerCmdExec.exec(AbstrSyncDockerCmdExec.java:21)Feb 03 19:14:14 at org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd.exec(AbstrDockerCmd.java:35)Feb 03 19:14:14 at org.testcontainers.shaded.com.github.dockerjava.core.command.CreateContainerCmdImpl.exec(CreateContainerCmdImpl.java:595)Feb 03 19:14:14 at org.testcontainers.utility.ResourceReaper.start(ResourceReaper.java:91)Feb 03 19:14:14 at org.testcontainers.DockerClientFactory.client(DockerClientFactory.java:203)Feb 03 19:14:14 at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.imageExists(FlinkContainer.java:316)Feb 03 19:14:14 at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.buildBaseImage(FlinkContainer.java:301)Feb 03 19:14:14 ... 21 more</description>
      <version>1.12.1,1.13.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-5 01:00:00" id="21294" opendate="2021-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support state access API for the map/flat_map operation of Python ConnectedStreams</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.PythonOperatorUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.TwoInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonKeyedProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonCoMapOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonCoFlatMapOperator.java</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-5 01:00:00" id="21297" opendate="2021-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support &amp;#39;LOAD/UNLOAD MODULE&amp;#39; syntax</summary>
      <description/>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-5 01:00:00" id="21298" opendate="2021-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support &amp;#39;USE MODULES&amp;#39; syntax</summary>
      <description/>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-5 01:00:00" id="21299" opendate="2021-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support &amp;#39;SHOW [FULL] MODULES&amp;#39; syntax</summary>
      <description/>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableResult.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-2 01:00:00" id="2130" opendate="2015-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RabbitMQ source does not fail when failing to retrieve elements</summary>
      <description>The RMQ source only logs when elements cannot be retrieved. Failures are not propagated.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-5 01:00:00" id="21300" opendate="2021-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update module documentation</summary>
      <description/>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.use.md</file>
      <file type="M">docs.content.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.docs.dev.table.sql.overview.md</file>
      <file type="M">docs.content.docs.dev.table.modules.md</file>
      <file type="M">docs.content.docs.dev.python.table.table.environment.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.use.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.modules.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.table.environment.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-5 01:00:00" id="21301" opendate="2021-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decouple window aggregate allow lateness with state ttl configuration</summary>
      <description>Currently, state retention time config will also effect state clean behavior of Window Aggregate, which is unexpected for most users.E.g for the following example,  User would set `MinIdleStateRetentionTime` to 1 Day to clean state in `deduplicate` . However, it will also effects clean behavior of window aggregate. For example, 2021-01-04 data would clean at 2021-01-06 instead of 2021-01-05. SELECT DATE_FORMAT(tumble_end(ROWTIME ,interval '1' DAY),'yyyy-MM-dd') as stat_time, count(1) first_phone_numFROM ( SELECT ROWTIME, user_id, row_number() over(partition by user_id, pdate order by ROWTIME ) as rn FROM source_kafka_biz_shuidi_sdb_crm_call_record ) cal where rn =1group by tumble(ROWTIME,interval '1' DAY);It's better to decouple window aggregate allow lateness with `MinIdleStateRetentionTime` .</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowEmitStrategy.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2021-2-9 01:00:00" id="21343" opendate="2021-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation with possible migration strategies</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.docs.ops.state.savepoints.md</file>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-3-10 01:00:00" id="21348" opendate="2021-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for StateWithExecutionGraph</summary>
      <description>This ticket is about adding dedicated tests for the StateWithExecutionGraph class.This is a follow up from https://github.com/apache/flink/pull/14879#discussion_r573707768</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StopWithSavepointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.RestartingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.FailingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.ExecutingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.CancelingTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.OperatorCoordinatorHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-3-10 01:00:00" id="21354" opendate="2021-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ChangelogStateBackend (proxy-everything)</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.StateConfigUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimeServiceManagerImpl.java</file>
      <file type="M">flink-state-backends.pom.xml</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateSnapshotTransformerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateBackendLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.CheckpointStorageLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CheckpointingOptions.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-2-12 01:00:00" id="21366" opendate="2021-2-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka connector documentation should mentions Maxwell as CDC mechanism</summary>
      <description>The current Kafka connector changelog section of the documentation mentions Debezium and Canal CDC tools but not the recently added Maxwell format.This PR linked to this ticket edits the text to add it.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kafka.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-3-12 01:00:00" id="21369" opendate="2021-2-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document Checkpoint Storage</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.docs.ops.state.savepoints.md</file>
      <file type="M">docs.content.docs.ops.state.large.state.tuning.md</file>
      <file type="M">docs.content.docs.ops.state.checkpoints.md</file>
      <file type="M">docs.content.docs.learn-flink.fault.tolerance.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.queryable.state.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.custom.serialization.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.tuning.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.oss.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.azure.md</file>
      <file type="M">docs.content.docs.concepts.glossary.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-3-16 01:00:00" id="21382" opendate="2021-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Standalone K8s documentation does not explain usage of standby JobManagers</summary>
      <description>Our standalone K8s documentation mentions how to configure K8s HA services. It does not mention that this only works with a single JobManager. When using standby JobManagers, then the given deployment yamls won't work because the jobmanager.rpc.address is configured to be the jobmanager service.Changing the configuration to work is surprisingly difficult because of a lack of documentation. Moreover, it is quite difficult to pass in custom configuration values when using a ConfigMap for sharing Flink's flink-conf.yaml. The problem is that mounted ConfigMaps are not writable from a pod perspective. See this answer for how one could achieve it.I think we could improve our documentation to explain our users how to configure a standalone HA cluster with standby JobManagers.</description>
      <version>1.12.1,1.13.0</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.kubernetes.md</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  <bug fixdate="2021-2-24 01:00:00" id="21481" opendate="2021-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move git-commit-id-plugin execution to flink-runtime</summary>
      <description>The properties set by the git-commit-id-plugin are only accessed in flink-runtime (see EnvironmentInformation), so we should use the execution of this plugin into flink-runtime to save some build time.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.PseudoRandomValueSelector.java</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-2-24 01:00:00" id="21489" opendate="2021-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hugo docs add two anchor links to headers</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.static.js.flink.js</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-25 01:00:00" id="21502" opendate="2021-2-25 00:00:00" resolution="Done">
    <buginformation>
      <summary>Reduce frequency of global re-allocate resources</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerDefaultResourceAllocationStrategyITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.AbstractFineGrainedSlotManagerITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServices.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-2-26 01:00:00" id="21517" opendate="2021-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test harnesses are bypassing serialization stack for events</summary>
      <description>Since FLINK-19297 (https://github.com/apache/flink/pull/13447/files#diff-e5c3ecec28e8d4c2f7f62bc8a4c9ed88c30141d44a570059849a3b2120e4d2b50) we accidentally removed a test coverage for event (de)serialization from a lot of the unit tests, that were/are using test harnesses. For example because of that I almost broke 1.12.2 release, since `stop-with-savepoint --drain` was only tested using test harnesses (didn't have an ITCases and/or end to end test).</description>
      <version>1.12.1,1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordOrEventCollectingResultPartitionWriter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-3 01:00:00" id="2152" opendate="2015-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide zipWithIndex utility in flink-contrib</summary>
      <description>We should provide a simple utility method for zipping elements in a data set with a dense index.its up for discussion whether we want it directly in the API or if we should provide it only as a utility from flink-contrib.I would put it in flink-contrib.See my answer on SO: http://stackoverflow.com/questions/30596556/zipwithindex-on-apache-flink</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-3-1 01:00:00" id="21542" opendate="2021-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for supporting INSERT INTO specific columns</summary>
      <description>We have supported INSERT INTO specific columns in FLINK-18726, but no add documentation yet.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.insert.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-1 01:00:00" id="21545" opendate="2021-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Running Kerberized YARN per-job on Docker test stalls on azure</summary>
      <description>For some reason the test started taking 10x more time than beforehttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13921&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13920&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13918&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13919&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529[PASS] 'Running Kerberized YARN per-job on Docker test (default input)' passed after 40 minutes and 34 seconds! Test exited with exit code 0.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-2 01:00:00" id="21561" opendate="2021-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce cache for binaries downloaded by bash tests</summary>
      <description>It seems that archive.apache.org is currently very slow, causing the bash e2e tests to timeout, because they need a lot of time downloading dependencies.This ticket is for tracking an improvement to the bash-based e2e tests to cache the binaries in Azure pipelines Caches &amp;#91;1&amp;#93;.&amp;#91;1&amp;#93; https://docs.microsoft.com/en-us/azure/devops/pipelines/release/caching?view=azure-devops</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.Dockerfile</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.yarn.docker.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-2 01:00:00" id="21571" opendate="2021-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Japicmp reference version does not exist</summary>
      <description>In FLINK-21570 the japicmp reference version was set to 1.13.0 . Since this version does not exist the japicmp checks have effectively been disabled.[INFO] --- japicmp-maven-plugin:0.11.0:cmp (default) @ flink-metrics-core ---[WARNING] Could not resolve org.apache.flink:flink-metrics-core:jar:1.13.0[WARNING] Could not resolve dependency with descriptor 'org.apache.flink:flink-metrics-core:1.13.0'.[WARNING] Please provide at least one resolvable old version using one of the configuration elements &lt;oldVersion/&gt; or &lt;oldVersions/&gt;.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-4 01:00:00" id="21611" opendate="2021-3-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Japicmp fails with "Could not resolve org.apache.flink:flink-core:jar:1.12.0"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14139&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=9b1a0f88-517b-5893-fc93-76f4670982b4[ERROR] Failed to execute goal com.github.siom79.japicmp:japicmp-maven-plugin:0.11.0:cmp (default) on project flink-core: Could not resolve org.apache.flink:flink-core:jar:1.12.0 -&gt; [Help 1]</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-3-5 01:00:00" id="21621" opendate="2021-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support TIMESTAMP_LTZ arithmetic</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-3-5 01:00:00" id="21623" opendate="2021-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce CURRENT_ROW_TIMESTAMP() function</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-3-5 01:00:00" id="21628" opendate="2021-3-5 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDAF in Tumbling Window</summary>
      <description>Support Python UDAF in Tumbling Window</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamTableAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamGroupWindowAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PassThroughPythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTableStatelessPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupTableAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonStatelessFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamDataStreamPythonFunctionRunner.java</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udaf.py</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.fast.pxd</file>
      <file type="M">flink-python.pyflink.common.types.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-5 01:00:00" id="21629" opendate="2021-3-5 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDAF in Sliding Window</summary>
      <description>Support Python UDAF in Sliding Window</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.udaf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.process.function.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.assigner.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.window.aggregate.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-5 01:00:00" id="21630" opendate="2021-3-5 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDAF in Session Window</summary>
      <description>Support Python UDAF in Session Window</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.udaf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.process.function.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.assigner.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-5 01:00:00" id="21632" opendate="2021-3-5 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add NamespacedStateView and PerWindowStateDataViewStore</summary>
      <description>Currently we only support KeyedStateView and PerKeyStateDataViewStore. We need to Add NamespacedStateView and PerWindowStateDataViewStore for support setting namespace.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-5 01:00:00" id="21633" opendate="2021-3-5 00:00:00" resolution="Done">
    <buginformation>
      <summary>Index pending task managers in TaskManagerTracker</summary>
      <description>This would accelerate the progress of finding matching pending task manager in the task manager registration.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TestingTaskManagerResourceInfoProvider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedTaskManagerTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskManagerResourceInfoProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedTaskManagerTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-3-9 01:00:00" id="21696" opendate="2021-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>scala code in "Writing WatermarkGenerators" documentation missing/deprecated</summary>
      <description>Hi all,I belive the scala code in the "Writing a Periodic WatermarkGenerator" and "Writing a Punctuated WatermarkGenerator" sections (Event Time -&gt; Generating Watermark) is outdated/wrong.And it's also missing in the previous section "Writing WatermarkGenerators".It uses undefined arguments (output) &amp;#8211; the onEvent and onPeriodicEmit methods have the wrong signatures.If it's alright I can put up a PR with the scala code on how to extend WatermarkGenerator class and some comment.Also sorry if I misplaced this Issue, feel free to improve/correctGiack</description>
      <version>1.12.2,1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.event-time.generating.watermarks.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-10 01:00:00" id="21698" opendate="2021-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable problematic cast conversion between NUMERIC type and TIMESTAMP type</summary>
      <description>The cast conversion  between NUMERIC type and TIMESTAMP type is problematic , we should disable it, NUMERIC type and TIMESTAMP_LTZ type cast conversion is valid.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.DecimalDataTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.data.DecimalDataUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-3-10 01:00:00" id="21715" opendate="2021-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support implicit cast conversion between timestamp and timestamp_ltz</summary>
      <description>The existed UDFs using `LocalDateTime` or `java.sql.Timestamp` as parameters type, if the data type of parameter column in SQL changed from TIMESTAMP to TIMESTAMP_LTZ, these UDFs must be rewritten, because TIMESTAMP_LTZ data type does not support `LocalDateTime` or `java.sql.Timestamp` as conversion class. The first approach is to support the two conversion classes  for TIMESTAMP_LTZ  data type, to obtain the correct result, the local time zone information is necessary when conversion happens between  SQL internal data structure `TimestampData` and external conversion class `LocalDateTime` or `java.sql.Timestamp`.In this approach, after a e2e POC, we found that there are more changes than we thought before. It is difficult to cover with tests and it is easy to introduce bugs. Thus, to resolve the UDF compatibility issue, we consider support the implicit cast conversion between timestamp and timestamp_ltz after some offline discuss with jark and ykt836,This way is clean and lightweight way to resolve the UDF compatibility issue and doesn't change any public interface as well.  BTW the  implicit cast conversion is supported in Oracle&amp;#91;1&amp;#93;  &amp;#91;1&amp;#93;https://docs.oracle.com/cd/B19306_01/server.102/b14225/ch4datetime.htm </description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeCasts.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-11 01:00:00" id="21728" opendate="2021-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DegreesWithExceptionITCase crash</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14422&amp;view=logs&amp;j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&amp;t=f266c805-9429-58ed-2f9e-482e7b82f58b</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.MemoryManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.MemoryManager.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemorySegment.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-12 01:00:00" id="21741" opendate="2021-3-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SHOW JARS statement in SQL Client</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.function.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-12 01:00:00" id="21742" opendate="2021-3-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support REMOVE JAR statement in SQL Client</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.function.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-15 01:00:00" id="21794" opendate="2021-3-15 00:00:00" resolution="Done">
    <buginformation>
      <summary>Support retrieving slot details via rest api</summary>
      <description>It would be helpful to allow retrieving detail information of slots via rest api. JobID that the slot is assigned to Slot resources (for dynamic slot allocation)Such information should be displayed on webui, once fine-grained resource management is enabled in future.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.TestingResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TestingSlotManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerDetailsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-15 01:00:00" id="21798" opendate="2021-3-15 00:00:00" resolution="Done">
    <buginformation>
      <summary>Guard MemorySegment against multiple frees.</summary>
      <description>As discussed in FLINK-21419, freeing a memory segment for multiple times usually indicates the ownership of the segment is unclear. It would be good to gradually getting rid of all such multiple-frees.This ticket serves as an umbrella for detected multiple-free cases.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.test.controller.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-4-18 01:00:00" id="21850" opendate="2021-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve document and config description of sort-merge blocking shuffle</summary>
      <description>After the improvement of FLINK-19614, some of the previous document description for sort-merge blocking shuffle is not accurate, we need to improve the corresponding document.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.NettyShuffleEnvironmentConfigurationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.network.section.html</file>
      <file type="M">docs.content.docs.deployment.memory.mem.tuning.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-18 01:00:00" id="21853" opendate="2021-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Running HA per-job cluster (rocks, non-incremental) end-to-end test could not finished in 900 seconds</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14921&amp;view=logs&amp;j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&amp;t=03dbd840-5430-533d-d1a7-05d0ebe03873&amp;l=7318Waiting for text Completed checkpoint [1-9]* for job 00000000000000000000000000000000 to appear 2 of times in logs...grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directorygrep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directorygrep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directoryStarting standalonejob daemon on host fv-az232-135.grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directoryKilled TM @ 15744Killed TM @ 19625Test (pid: 9232) did not finish after 900 seconds.</description>
      <version>1.11.3,1.13.0</version>
      <fixedVersion>1.13.3,1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.ha.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.ha.per.job.cluster.datastream.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.ha.datastream.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-19 01:00:00" id="21867" opendate="2021-3-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend UI to expose also concurrent task failures besides the root cause</summary>
      <description>The UI only exposes the root cause of failures caught and handled by Flink. This subtasks is about extending the UI to fully support the feature of concurrently captured task failures which is subject to the related FLINK-21189 subtask.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-exception.ts</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-3-22 01:00:00" id="21899" opendate="2021-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce SOURCE_WATERMARK built-infunction to preserve watermark from source</summary>
      <description>The SOURCE_WATERMARK function doesn't have concrete implementation. The eval() function should throw a meaningful exception message to indicate users the function should only be used in DDL to generate a watermark preserved from source system.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-23 01:00:00" id="21918" opendate="2021-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add execution.runtime-mode setter in StreamExecutionEnvironment</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-3-23 01:00:00" id="21930" opendate="2021-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix typo in Hive temporal join example</summary>
      <description>  fix hive dim doc replace order to o</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-23 01:00:00" id="21936" opendate="2021-3-23 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Rescale pointwise connections for unaligned checkpoints</summary>
      <description>We currently do not have any hard guarantees on pointwise connection regarding data consistency. However, since data was structured implicitly in the same way as any preceding source or keyby, some users relied on this behavior to divide compute-intensive tasks into smaller chunks while relying on ordering guarantees.As long as the parallelism does not change, unaligned checkpoints (UC) retains these properties. With the implementation of rescaling of UC (FLINK-19801), that has changed. For most exchanges, there is a meaningful way to reassign state from one channel to another (even in random order). For some exchanges, the mapping is ambiguous and requires post-filtering. However, for point-wise connections, it's impossible while retaining these properties.Consider, source -&gt; keyby -&gt; task1 -&gt; forward -&gt; task2. Now if we want to rescale from parallelism p = 1 to p = 2, suddenly the records inside the keyby channels need to be divided into two channels according to the keygroups. That is easily possible by using the keygroup ranges of the operators and a way to determine the key(group) of the record (independent of the actual approach). For the forward channel, we completely lack the key context. No record in the forward channel has any keygroup assigned; it's also not possible to calculate it as there is no guarantee that the key is still present.The root cause for this limitation is the conceptual mismatch between what we provide and what some users assume we provide (or we assume that the users assume). For example, it's impossible to use (keyed) state in task2 right now, because there is no key context, but we still want to guarantee orderness in respect to that key context.For 1.13, the easiest solution is to disable channel state in pointwise connections. For any non-trivial application with at least one shuffle, the number of pointwise channels (linear to p) is quickly dwarfed by all-to-all connections (quadratic to p). I'd add some alternative ideas to the discussion.</description>
      <version>1.13.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamEdge.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.RescalePartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.ForwardPartitioner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.SubtaskStateMapper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-24 01:00:00" id="21937" opendate="2021-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support batch mode in Python DataStream API for basic operations</summary>
      <description>This ticket is dedicated to support batch mode for basic operations such as map/flatmap/filter, etc in Python DataStream API. For the other operations such as reduce, etc, we will support them in separate tickets.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.pyflink.util.utils.py</file>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.testing.source.sink.utils.py</file>
      <file type="M">flink-python.pyflink.table.utils.py</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.schema.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.table.py</file>
      <file type="M">flink-python.pyflink.table.statement.set.py</file>
      <file type="M">flink-python.pyflink.table.sinks.py</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.state.backend.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.state.backend.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-python.pyflink.dataset.execution.environment.py</file>
      <file type="M">flink-python.pyflink.common.serialization.py</file>
      <file type="M">flink-python.pyflink.common.restart.strategy.py</file>
      <file type="M">flink-python.pyflink.common.execution.config.py</file>
      <file type="M">flink-python.pyflink.common.configuration.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-24 01:00:00" id="21938" opendate="2021-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about how to test Python UDFs</summary>
      <description>It should be similar to the Java UDFs:https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/testing.html#testing-user-defined-functions</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.python.table.udfs.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.overview.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-6-10 01:00:00" id="2194" opendate="2015-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type extractor does not support Writable type</summary>
      <description>The type extractor supports sub types of Writable but not Writable itself. The reason is that it wants to create a WritableTypeInfo which is not possible for the interface Writable, though. A solution would be to treat the Writable interface like a generic type.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-3-24 01:00:00" id="21947" opendate="2021-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support TIMESTAMP_LTZ type in CSV format</summary>
      <description>Currently CSV format does not support  TIMESTAMP_LTZ yet, and the code exists some bug, we should support the TIMESTAMP_LTZ type properly and correct the timestamp conversion.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.maxwell.MaxwellJsonSerDerTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.maxwell.MaxwellJsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.canal.CanalJsonSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.canal.CanalJsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.TimestampFormat.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.TimeFormats.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.RowDataToJsonConverters.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.maxwell.MaxwellJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.maxwell.MaxwellJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.maxwell.MaxwellJsonDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonToRowDataConverters.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonOptions.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonDecodingFormat.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonDecodingFormat.java</file>
      <file type="M">flink-formats.flink-json.pom.xml</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDeSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.RowDataToCsvConverters.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvToRowDataConverters.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowSerializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowSchemaConverter.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-25 01:00:00" id="21974" opendate="2021-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to match quoted values for &amp;#39;SET key=val&amp;#39; statement</summary>
      <description>When execute sql,SET key_placeholder='test name ';the value of the key_placeholder is test_name rather than 'test name'</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.parse.SetOperationParseStrategy.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.delegation.ParserImplTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.parse.SetOperationParseStrategy.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.command.SetOperation.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-25 01:00:00" id="21978" opendate="2021-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable cast conversion between Numeric type and TIMESTAMP_LTZ type</summary>
      <description>Currently we has supported the cast conversion between Numeric type and TIMESTAMP_LTZ type, we suppose the numeric value e.g `Long type 1000L` as epoch seconds and then cast  to TIMESTAMP_LTZ, but the java.lang.Long is a conversion class of `LocalZonedTimestampType`  and treats as milliseconds.To avoid the inconsistency, we should disable it and encourage user to use `TO_TIMESTAMP_LTZ(numeric, precisoon)` function.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.DecimalDataTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.data.DecimalDataUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-26 01:00:00" id="21984" opendate="2021-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change precision argument from optional to required in TO_TIMESTAMP_LTZ(numeric, precision)</summary>
      <description>To avoid the confusing the numeric value is in second or millisecond, we'd better set the precision argument to required. The background is that `LocalZonedTimestampType` always treats `Integer` conversion class as second, treats `Long` as millisecond. </description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlDateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.time.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-26 01:00:00" id="21986" opendate="2021-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>taskmanager native memory not release timely after restart</summary>
      <description>I run a regular join job with flink_1.12.1 , and find taskmanager native memory not release timely after restart cause by exceeded checkpoint tolerable failure threshold.problem job information： job first restart cause by exceeded checkpoint tolerable failure threshold. then taskmanager be killed by yarn many times in this case，tm heap is set to 7.68G，bug all tm heap size is under 4.2G nonheap size increase after restart，but still under 160M. taskmanager process memory increase 3-4G after restart（this figure show one of taskmanager）  my guess：RocksDB wiki mentioned ：Many of the Java Objects used in the RocksJava API will be backed by C++ objects for which the Java Objects have ownership. As C++ has no notion of automatic garbage collection for its heap in the way that Java does, we must explicitly free the memory used by the C++ objects when we are finished with them.So, is it possible that RocksDBStateBackend not call AbstractNativeReference#close() to release memory use by RocksDB C++ Object ?I make a change:        Actively call System.gc() and System.runFinalization() every minute. And run this test again: taskmanager process memory no obvious increase job run for several days，and restart many times，but no taskmanager killed by yarn like before Summary： first，there is some native memory can not release timely after restart in this situation I guess it maybe RocksDB C++ object，but I hive not check it from source code of RocksDBStateBackend </description>
      <version>1.11.3,1.12.1,1.13.0</version>
      <fixedVersion>1.11.4,1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-3-26 01:00:00" id="21994" opendate="2021-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support FLIP-27 based FileSource connector in PyFlink DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-python.pyflink.common.time.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-4-29 01:00:00" id="22006" opendate="2021-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Could not run more than 20 jobs in a native K8s session when K8s HA enabled</summary>
      <description>Currently, if we start a native K8s session cluster when K8s HA enabled, we could not run more than 20 streaming jobs.  The latest job is always initializing, and the previous one is created and waiting to be assigned. It seems that some internal resources have been exhausted, e.g. okhttp thread pool , tcp connections or something else.</description>
      <version>1.12.2,1.13.0</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactory.java</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-3-30 01:00:00" id="22051" opendate="2021-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better document the distinction between stop-with-savepoint and stop-with-savepoint-with-drain</summary>
      <description>The Flink documentation only contains very few details about the difference between stop-with-savepoint and stop-with-savepoint-with-drain. We should better explain the semantic differences.</description>
      <version>1.11.3,1.12.2,1.13.0</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.cli.md</file>
      <file type="M">docs.content.zh.docs.deployment.cli.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-31 01:00:00" id="22070" opendate="2021-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support FileSink in PyFlink DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-python.pyflink.common.serialization.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-1 01:00:00" id="22076" opendate="2021-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python Test failed with "OSError: [Errno 12] Cannot allocate memory"</summary>
      <description>https://dev.azure.com/sewen0794/Flink/_build/results?buildId=249&amp;view=logs&amp;j=fba17979-6d2e-591d-72f1-97cf42797c11&amp;t=443dc6bf-b240-56df-6acf-c882d4b238da&amp;l=21533Python Test failed with "OSError: &amp;#91;Errno 12&amp;#93; Cannot allocate memory" in Azure Pipeline. I am not sure if it is caused by insufficient machine memory on Azure.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.tox.ini</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-1 01:00:00" id="22094" opendate="2021-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update release scripts after documentation switch to Hugo</summary>
      <description>The release scripts has not been upgraded after migration to Hugo.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.update.branch.version.sh</file>
      <file type="M">tools.releasing.create.snapshot.branch.sh</file>
      <file type="M">tools.releasing.create.release.branch.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-2 01:00:00" id="22098" opendate="2021-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix bug for window join: plan is wrong if join condition contain &amp;#39;IS NOT DISTINCT FROM&amp;#39;</summary>
      <description>Plan test is wrong for window join if join condition contain 'IS NOT DISTINCT FROM'. </description>
      <version>None</version>
      <fixedVersion>1.13.0,1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalWindowJoinRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-2 01:00:00" id="22099" opendate="2021-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix bug for semi/anti window join.</summary>
      <description>Fix bug for Semi/Anti WindowJoin.//代码占位符@Testdef testSemiJoinIN(): Unit = { val sql = """ |SELECT * FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) L WHERE L.a IN ( |SELECT a FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable2, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) R |WHERE L.window_start = R.window_start AND L.window_end = R.window_end) """.stripMargin util.verifyRelPlan(sql)}@Testdef testSemiExist(): Unit = { val sql = """ |SELECT * FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) L WHERE EXISTS ( |SELECT * FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable2, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) R |WHERE L.window_start = R.window_start AND L.window_end = R.window_end AND L.a = R.a) """.stripMargin util.verifyRelPlan(sql)}@Testdef testAntiJoinNotExist(): Unit = { val sql = """ |SELECT * FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) L WHERE NOT EXISTS ( |SELECT * FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable2, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) R |WHERE L.window_start = R.window_start AND L.window_end = R.window_end AND L.a = R.a) """.stripMargin util.verifyRelPlan(sql)}@Testdef testAntiJoinNotIN(): Unit = { val sql = """ |SELECT * FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) L WHERE L.a NOT IN ( |SELECT a FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable2, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) R |WHERE L.window_start = R.window_start AND L.window_end = R.window_end) """.stripMargin util.verifyRelPlan(sql)}Now run the above sql, an `ArrayIndexOutOfBoundsException` would be thrown out.</description>
      <version>None</version>
      <fixedVersion>1.13.0,1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-2 01:00:00" id="22102" opendate="2021-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw meaningful exceptions for features we don&amp;#39;t support</summary>
      <description>As reported by user in this comment, we should error out in such cases.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-2 01:00:00" id="22107" opendate="2021-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include antlr into hive connector uber jars</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-6 01:00:00" id="22113" opendate="2021-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UniqueKey constraint is lost with multiple sources join in SQL</summary>
      <description>Hi team,   We have a use case to join multiple data sources to generate a continuous updated view. We defined primary key constraint on all the input sources and all the keys are the subsets in the join condition. All joins are left join.   In our case, the first two inputs can produce JoinKeyContainsUniqueKey input sepc, which is good and performant. While when it comes to the third input source, it's joined with the intermediate output table of the first two input tables, and the intermediate table does not carry key constraint information(although the thrid source input table does), so it results in a NoUniqueKey input sepc. Given NoUniqueKey inputs has dramatic performance implications per the Force Join Unique Key email thread, we want to know if there is any mitigation solution for this. Example:Take the example from https://github.com/ververica/flink-sql-cookbook/blob/master/joins/05/05_star_schema.mdCREATE TEMPORARY TABLE passengers ( passenger_key STRING, first_name STRING, last_name STRING, update_time TIMESTAMP(3), PRIMARY KEY (passenger_key) NOT ENFORCED) WITH ( 'connector' = 'upsert-kafka', 'topic' = 'passengers', 'properties.bootstrap.servers' = 'localhost:9092', 'key.format' = 'raw', 'value.format' = 'json');CREATE TEMPORARY TABLE stations ( station_key STRING, update_time TIMESTAMP(3), city STRING, PRIMARY KEY (station_key) NOT ENFORCED) WITH ( 'connector' = 'upsert-kafka', 'topic' = 'stations', 'properties.bootstrap.servers' = 'localhost:9092', 'key.format' = 'raw', 'value.format' = 'json');CREATE TEMPORARY TABLE booking_channels ( booking_channel_key STRING, update_time TIMESTAMP(3), channel STRING, PRIMARY KEY (booking_channel_key) NOT ENFORCED) WITH ( 'connector' = 'upsert-kafka', 'topic' = 'booking_channels', 'properties.bootstrap.servers' = 'localhost:9092', 'key.format' = 'raw', 'value.format' = 'json');CREATE TEMPORARY TABLE train_activities ( scheduled_departure_time TIMESTAMP(3), actual_departure_date TIMESTAMP(3), passenger_key STRING, origin_station_key STRING, destination_station_key STRING, booking_channel_key STRING, PRIMARY KEY (booking_channel_key, origin_station_key, destination_station_key) NOT ENFORCED) WITH ( 'connector' = 'upsert-kafka', 'topic' = 'train_activities', 'properties.bootstrap.servers' = 'localhost:9092', 'key.format' = 'json', 'value.format' = 'json');SELECT t.actual_departure_date, p.first_name, p.last_name, b.channel, os.city AS origin_station, ds.city AS destination_stationFROM train_activities_1 tLEFT JOIN booking_channels b ON t.booking_channel_key = b.booking_channel_keyLEFT JOIN passengers pON t.passenger_key = p.passenger_keyLEFT JOIN stations osON t.origin_station_key = os.station_keyLEFT JOIN stations dsON t.destination_station_key = ds.station_key  The query will generate exeuction plan of: Flink SQL&gt; explain&gt; SELECT&gt; t.actual_departure_date,&gt; p.first_name,&gt; p.last_name,&gt; b.channel,&gt; os.city AS origin_station,&gt; ds.city AS destination_station&gt; FROM train_activities_1 t&gt; LEFT JOIN booking_channels b&gt; ON t.booking_channel_key = b.booking_channel_key&gt; LEFT JOIN passengers p&gt; ON t.passenger_key = p.passenger_key&gt; LEFT JOIN stations os&gt; ON t.origin_station_key = os.station_key&gt; LEFT JOIN stations ds&gt; ON t.destination_station_key = ds.station_key;== Abstract Syntax Tree ==LogicalProject(actual_departure_date=[$1], first_name=[$10], last_name=[$11], channel=[$8], origin_station=[$15], destination_station=[$18])+- LogicalJoin(condition=[=($4, $16)], joinType=[left]) :- LogicalJoin(condition=[=($3, $13)], joinType=[left]) : :- LogicalJoin(condition=[=($2, $9)], joinType=[left]) : : :- LogicalJoin(condition=[=($5, $6)], joinType=[left]) : : : :- LogicalTableScan(table=[[default_catalog, default_database, train_activities_1]]) : : : +- LogicalWatermarkAssigner(rowtime=[update_time], watermark=[-($1, 10000:INTERVAL SECOND)]) : : : +- LogicalTableScan(table=[[default_catalog, default_database, booking_channels]]) : : +- LogicalTableScan(table=[[default_catalog, default_database, passengers]]) : +- LogicalTableScan(table=[[default_catalog, default_database, stations]]) +- LogicalTableScan(table=[[default_catalog, default_database, stations]])== Optimized Physical Plan ==Calc(select=[actual_departure_date, first_name, last_name, channel, city AS origin_station, city0 AS destination_station])+- Join(joinType=[LeftOuterJoin], where=[=(destination_station_key, station_key)], select=[actual_departure_date, destination_station_key, channel, first_name, last_name, city, station_key, city0], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) :- Exchange(distribution=[hash[destination_station_key]]) : +- Calc(select=[actual_departure_date, destination_station_key, channel, first_name, last_name, city]) : +- Join(joinType=[LeftOuterJoin], where=[=(origin_station_key, station_key)], select=[actual_departure_date, origin_station_key, destination_station_key, channel, first_name, last_name, station_key, city], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) : :- Exchange(distribution=[hash[origin_station_key]]) : : +- Calc(select=[actual_departure_date, origin_station_key, destination_station_key, channel, first_name, last_name]) : : +- Join(joinType=[LeftOuterJoin], where=[=(passenger_key, passenger_key0)], select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, channel, passenger_key0, first_name, last_name], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) : : :- Exchange(distribution=[hash[passenger_key]]) : : : +- Calc(select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, channel]) : : : +- Join(joinType=[LeftOuterJoin], where=[=(booking_channel_key, booking_channel_key0)], select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key, booking_channel_key0, channel], leftInputSpec=[HasUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) : : : :- Exchange(distribution=[hash[booking_channel_key]]) : : : : +- Calc(select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key]) : : : : +- ChangelogNormalize(key=[booking_channel_key, origin_station_key, destination_station_key]) : : : : +- Exchange(distribution=[hash[booking_channel_key, origin_station_key, destination_station_key]]) : : : : +- TableSourceScan(table=[[default_catalog, default_database, train_activities_1]], fields=[scheduled_departure_time, actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key]) : : : +- Exchange(distribution=[hash[booking_channel_key]]) : : : +- Calc(select=[booking_channel_key, channel]) : : : +- ChangelogNormalize(key=[booking_channel_key]) : : : +- Exchange(distribution=[hash[booking_channel_key]]) : : : +- TableSourceScan(table=[[default_catalog, default_database, booking_channels, watermark=[-($1, 10000:INTERVAL SECOND)]]], fields=[booking_channel_key, update_time, channel]) : : +- Exchange(distribution=[hash[passenger_key]]) : : +- Calc(select=[passenger_key, first_name, last_name]) : : +- ChangelogNormalize(key=[passenger_key]) : : +- Exchange(distribution=[hash[passenger_key]]) : : +- TableSourceScan(table=[[default_catalog, default_database, passengers]], fields=[passenger_key, first_name, last_name, update_time]) : +- Exchange(distribution=[hash[station_key]]) : +- Calc(select=[station_key, city]) : +- ChangelogNormalize(key=[station_key]) : +- Exchange(distribution=[hash[station_key]]) : +- TableSourceScan(table=[[default_catalog, default_database, stations]], fields=[station_key, update_time, city]) +- Exchange(distribution=[hash[station_key]]) +- Calc(select=[station_key, city]) +- ChangelogNormalize(key=[station_key]) +- Exchange(distribution=[hash[station_key]]) +- TableSourceScan(table=[[default_catalog, default_database, stations]], fields=[station_key, update_time, city])== Optimized Execution Plan ==Calc(select=[actual_departure_date, first_name, last_name, channel, city AS origin_station, city0 AS destination_station])+- Join(joinType=[LeftOuterJoin], where=[(destination_station_key = station_key)], select=[actual_departure_date, destination_station_key, channel, first_name, last_name, city, station_key, city0], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) :- Exchange(distribution=[hash[destination_station_key]]) : +- Calc(select=[actual_departure_date, destination_station_key, channel, first_name, last_name, city]) : +- Join(joinType=[LeftOuterJoin], where=[(origin_station_key = station_key)], select=[actual_departure_date, origin_station_key, destination_station_key, channel, first_name, last_name, station_key, city], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) : :- Exchange(distribution=[hash[origin_station_key]]) : : +- Calc(select=[actual_departure_date, origin_station_key, destination_station_key, channel, first_name, last_name]) : : +- Join(joinType=[LeftOuterJoin], where=[(passenger_key = passenger_key0)], select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, channel, passenger_key0, first_name, last_name], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) : : :- Exchange(distribution=[hash[passenger_key]]) : : : +- Calc(select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, channel]) : : : +- Join(joinType=[LeftOuterJoin], where=[(booking_channel_key = booking_channel_key0)], select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key, booking_channel_key0, channel], leftInputSpec=[HasUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) : : : :- Exchange(distribution=[hash[booking_channel_key]]) : : : : +- Calc(select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key]) : : : : +- ChangelogNormalize(key=[booking_channel_key, origin_station_key, destination_station_key]) : : : : +- Exchange(distribution=[hash[booking_channel_key, origin_station_key, destination_station_key]]) : : : : +- TableSourceScan(table=[[default_catalog, default_database, train_activities_1]], fields=[scheduled_departure_time, actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key]) : : : +- Exchange(distribution=[hash[booking_channel_key]]) : : : +- Calc(select=[booking_channel_key, channel]) : : : +- ChangelogNormalize(key=[booking_channel_key]) : : : +- Exchange(distribution=[hash[booking_channel_key]]) : : : +- TableSourceScan(table=[[default_catalog, default_database, booking_channels, watermark=[-($1, 10000:INTERVAL SECOND)]]], fields=[booking_channel_key, update_time, channel]) : : +- Exchange(distribution=[hash[passenger_key]]) : : +- Calc(select=[passenger_key, first_name, last_name]) : : +- ChangelogNormalize(key=[passenger_key]) : : +- Exchange(distribution=[hash[passenger_key]]) : : +- TableSourceScan(table=[[default_catalog, default_database, passengers]], fields=[passenger_key, first_name, last_name, update_time]) : +- Exchange(distribution=[hash[station_key]])(reuse_id=[1]) : +- Calc(select=[station_key, city]) : +- ChangelogNormalize(key=[station_key]) : +- Exchange(distribution=[hash[station_key]]) : +- TableSourceScan(table=[[default_catalog, default_database, stations]], fields=[station_key, update_time, city]) +- Reused(reference_id=[1])  </description>
      <version>1.13.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.MetadataTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnUniquenessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnUniqueness.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-4-6 01:00:00" id="22125" opendate="2021-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revisit the return value of MapState.get when a key doesn&amp;#39;t exist</summary>
      <description>Currently, it will thrown KeyError if the key doesn't exist for MapState in Python DataStream API. However, it returns null in the Java DataStream API. Maybe we should keep the behavior the same across Python DataStream API and Java DataStream API.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
      <file type="M">flink-python.pyflink.datastream.state.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-7 01:00:00" id="22127" opendate="2021-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enrich error message of read buffer request timeout exception</summary>
      <description>Enrich error message of read buffer request timeout exception to tell the user how to solve the timeout exception.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-7 01:00:00" id="22136" opendate="2021-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Device application for unaligned checkpoint test on cluster</summary>
      <description>To test unaligned checkpoints, we should use a few different applications that use different features: Mixing forward/rescale channels with keyby or other shuffle operations Unions 2 or n-ary operators Associated state ((keyed) process function) Correctness verificationsThe sinks should not be mocked but rather should be able to induce a fair amount of backpressure into the system. Quite possibly, it would be a good idea to have a way to add more backpressure to the sink by running the respective system on the cluster and be able to add/remove parallel instances.Things to check in the application Inflight data is restored to the correct keygroups -&gt; can be checked with keyed state in a process function Correctness: Completeness (no lost records) + no duplicates Orderness of data for keyed exchanges (we guarantee that records with the same key retain orderness across keyed operators) (To detect errors early, we can also use magic headers)</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-4-7 01:00:00" id="22138" opendate="2021-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow casting between row and structured type</summary>
      <description>There are still some minor barriers that prevent using toDataStream to its full extent.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastAvoidanceTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeCasts.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.common.ViewsExpandingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.schema.StructuredRelDataType.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.StructuredType.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-7 01:00:00" id="22142" opendate="2021-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove console logging for Kafka connector for AZP runs</summary>
      <description>For the Kafka connector we do log to the console. These logging statements clutter the AZP output considerably. I propose to remove this logic. Moreover, we still have some DEBUG logging for FLINK-16383 which has been fixed.</description>
      <version>1.12.2,1.13.0</version>
      <fixedVersion>1.11.4,1.13.0,1.12.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.log4j.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-4-8 01:00:00" id="22152" opendate="2021-4-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug of same timers are registered multiple times</summary>
      <description>The same timer will be registered multiple times. We need to deduplicate same timers</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.pyflink.fn.execution.utils.input.handler.py</file>
      <file type="M">flink-python.pyflink.fn.execution.timerservice.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.fast.pyx</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-4-8 01:00:00" id="22159" opendate="2021-4-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the new window TVF based operations</summary>
      <description>In this 1.13 version, we have supported window TVF based aggregation and TopN of FLIP-145. We should add documentation for them. We may also need to restructure the "Queries" page.</description>
      <version>None</version>
      <fixedVersion>1.13.0,1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-topn.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-agg.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-9 01:00:00" id="22171" opendate="2021-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the doc about SQL Client</summary>
      <description/>
      <version>1.13.0</version>
      <fixedVersion>1.13.0,1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-4-9 01:00:00" id="22177" opendate="2021-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add docs for consistent time functions and compatibility specification</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0,1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.concepts.time.attributes.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.time.attributes.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-4-11 01:00:00" id="22191" opendate="2021-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlinkStreamUserDefinedFunctionTests.test_udf_in_join_condition_2 fail due to NPE</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16326&amp;view=logs&amp;j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&amp;t=f5211ead-5e53-5af8-f827-4dbf08df26bb&amp;l=21130</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.PythonSharedResources.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-12 01:00:00" id="22217" opendate="2021-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add quotes around job names</summary>
      <description>Quotes could be neat here:Starting execution of job State machine job [..]</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-12 01:00:00" id="22227" opendate="2021-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job name, Job ID and receiving Dispatcher should be logged by the client</summary>
      <description>Surprisingly we don't log for job submission where we submit them to or what the job ID/name is.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-4-12 01:00:00" id="22236" opendate="2021-4-12 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document opt-in behavior of flamegraph</summary>
      <description>The flamegraph feature added in FLINK-13550 must be explicitly enabled, but this isn't documented.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.debugging.flame.graphs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-12 01:00:00" id="22243" opendate="2021-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reactive Mode parallelism changes are not shown in the job graph visualization in the UI</summary>
      <description>As reported here FLINK-22134, the parallelism in the visual job graph on top of the detail page is not in sync with the parallelism listed in the task list below, when reactive mode causes a parallelism change.</description>
      <version>1.13.0,1.14.0</version>
      <fixedVersion>1.16.0,1.17.0,1.15.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.ReactiveModeITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateTrackingMockExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.CreatingExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.jsonplan.JsonPlanGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-12 01:00:00" id="22244" opendate="2021-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify Reactive Mode documentation wrt to timeouts</summary>
      <description>In the release testing of Reactive Mode (FLINK-22134) we found that the documentation of the timeouts needs some clarification.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-1-13 01:00:00" id="22255" opendate="2021-4-13 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>AdaptiveScheduler improvements/bugs</summary>
      <description>This ticket collects the improvements/bugs for the AdaptiveScheduler.</description>
      <version>1.13.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-4-14 01:00:00" id="22273" opendate="2021-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for General Python Group Window Aggregation in Python Table API</summary>
      <description/>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.python.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-15 01:00:00" id="22286" opendate="2021-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix picklebytescoder doesn&amp;#39;t support custom python class</summary>
      <description>Currently picklebytescoder use pickle to serialize data, which won't work when the data is Python custom class.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-15 01:00:00" id="22289" opendate="2021-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update JDBC XA sink docs</summary>
      <description>According to https://issues.apache.org/jira/browse/FLINK-22141?focusedCommentId=17321934&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17321934(except code changes) cc:ym</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.jdbc.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-16 01:00:00" id="22297" opendate="2021-4-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Perform early check to ensure that the length of the result is the same as the input for Pandas UDF</summary>
      <description>For Pandas UDF, the input type for each input argument is Pandas.Series and the result type is also of type Pandas.Series. Besides, the length of the result should be the same as the inputs. If this is not the case, currently the behavior is unclear. We should perform early check for this and provide a clear error message.See http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-Vectorized-UDF-throws-NullPointerException-td42952.html and http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-called-already-closed-and-NullPointerException-td42997.html for more details.</description>
      <version>None</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.PythonOperatorUtils.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-16 01:00:00" id="22298" opendate="2021-4-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The ExecNode&amp;#39;s id should always start from 1 in json plan test</summary>
      <description>The ExecNode's id should always start from 1 in json plan test, which could make the test more stable.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testComplexCalc.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testSimpleProject.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ChangelogSourceJsonPlanTest.jsonplan.testUpsertSource.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoin.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoinOverrideParameters.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testLeftOuterJoinWithLiteralTrue.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testRegisterByClass.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testProcessingTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testRowTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoin.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoinWithEqualPk.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testLeftJoinNonEqui.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LimitJsonPlanTest.jsonplan.testLimit.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.RankJsonPlanTest.jsonplan.testRank.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.SortLimitJsonPlanTest.jsonplan.testSortLimit.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testPartitioning.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testWritingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testFilterPushDown.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testLimitPushDown.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testProjectPushDown.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testReadingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testWatermarkPushDown.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testJoinTemporalFunction.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testTemporalTableJoin.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortRowTime.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-16 01:00:00" id="22301" opendate="2021-4-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Statebackend and CheckpointStorage type is not shown in the Web UI</summary>
      <description>Currently we have already have the Rest API /jobs/:jobid/checkpoints/config that returns the statebackend name and checkpoint storage type:{ "mode":"exactly_once", "interval":20, "timeout":600000, "min_pause":0, "max_concurrent":1, "externalization":{ "enabled":false, "delete_on_cancellation":true }, "state_backend":"EmbeddedRocksDBStateBackend", "checkpoint_storage":"FileSystemCheckpointStorage", "unaligned_checkpoints":false, "tolerable_failed_checkpoints":0}But the two fields does not shown in the Web UI of checkpoint configuration:</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-checkpoint.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-16 01:00:00" id="22302" opendate="2021-4-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restructure SQL "Queries" documentation into one page per operation</summary>
      <description>Currently, the "Queries" page has been very large and it's getting longger when we supporting more features. We already have separate pages for Joins and CEP. I would propose to separate "Queries" into one page per operation. This way we can easily add more detailed informations for the operations and more examples.</description>
      <version>None</version>
      <fixedVersion>1.13.0,1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.insert.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.hints.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.drop.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.alter.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.versioned.tables.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.match.recognize.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.joins.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.md</file>
      <file type="M">docs.content.docs.dev.table.sql.overview.md</file>
      <file type="M">docs.content.docs.dev.table.sql.hints.md</file>
      <file type="M">docs.content.docs.dev.table.overview.md</file>
      <file type="M">docs.content.docs.dev.table.concepts.overview.md</file>
      <file type="M">docs.content.docs.dev.table.concepts.match.recognize.md</file>
      <file type="M">docs.content.docs.dev.table.concepts.joins.md</file>
      <file type="M">docs.content.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.docs.dev.table.tuning.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries..index.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-topn.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-agg.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.set-ops.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.group-agg.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.deduplication.md</file>
      <file type="M">docs.content.docs.dev.table.sql.insert.md</file>
      <file type="M">docs.content.docs.dev.table.sql.drop.md</file>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.docs.dev.table.sql.alter.md</file>
      <file type="M">docs.content.zh.docs.dev.table.tuning.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-topn.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-agg.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.set-ops.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.group-agg.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.deduplication.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-4-16 01:00:00" id="22307" opendate="2021-4-16 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Increase the default value of data writing cache size (not configurable) for sort-merge blocking shuffle</summary>
      <description>Currently, the data writing cache is 8M, which is not enough if data compression is enabled. By increasing the cache size to 16M, the performance of our benchmark job can be increased by about 20%. (We may make it configurable in the future)</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-16 01:00:00" id="2231" opendate="2015-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a Serializer for Scala Enumerations</summary>
      <description>Scala Enumerations are currently serialized with Kryo, but should be efficiently serialized by just writing the initial.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.ScalaSpecialTypesSerializerTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeInformationGen.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeDescriptors.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeAnalyzer.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-12-16 01:00:00" id="22316" opendate="2021-4-16 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support MODIFY column/constraint/watermark for ALTER TABLE statement</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.AlterSchemaConverter.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-16 01:00:00" id="22318" opendate="2021-4-16 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support RENAME column name for ALTER TABLE statement</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.AlterSchemaConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterTableRenameColumn.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.OperationConverterUtils.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-19 01:00:00" id="22335" opendate="2021-4-19 00:00:00" resolution="Done">
    <buginformation>
      <summary>Increase default resource wait timeout for the adaptive scheduler</summary>
      <description>As discussed in FLINK-22135, the current default value 10s for jobmanager.adaptive-scheduler.resource-wait-timeout is too short and can easily lead to job failures when working with active resource managers. We'd like to increase the default value to 5min, aligning with slot.request.timeout.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-4-19 01:00:00" id="22348" opendate="2021-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the Python operators of Python DataStream API doesn&amp;#39;t use managed memory in execute_and_collect</summary>
      <description/>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonConfig.java</file>
      <file type="M">flink-python.pyflink.util.java.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-4-19 01:00:00" id="22350" opendate="2021-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the doc of user-defined window in Python DataStream API</summary>
      <description/>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.python.datastream.operators.md</file>
      <file type="M">docs.content.zh.docs.dev.python.datastream.operators.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-9-19 01:00:00" id="22358" opendate="2021-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing stability annotation to Split Reader API classes</summary>
      <description>The Split Reader API currently has no stability annotations, it is unclear which classes are public API, which are internal, which are stable, and which are evolving.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.utils.SerdeUtils.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.splitreader.SplitsChange.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.splitreader.SplitsAddition.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.splitreader.SplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderOptions.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SingleThreadMultiplexSourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.RecordsWithSplitIds.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.RecordsBySplits.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.RecordEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTask.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SingleThreadFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.FetchTask.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-20 01:00:00" id="22368" opendate="2021-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UnalignedCheckpointITCase hangs on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16818&amp;view=logs&amp;j=b0a398c0-685b-599c-eb57-c8c2a771138e&amp;t=d13f554f-d4b9-50f8-30ee-d49c6fb0b3cc&amp;l=10144</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.1,1.12.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-4-21 01:00:00" id="22396" opendate="2021-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary entries in sql hbase-1.4 connector NOTICE file</summary>
      <description>The NOTICE file for flink-sql-connector-hbase-1.4 lists dependencies that it does not bundle: commons-configuration:commons-configuration:1.7 org.apache.hbase:hbase-prefix-tree:1.4.3 org.apache.hbase:hbase-procedure:1.4.3</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-18 01:00:00" id="2240" opendate="2015-6-18 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Use BloomFilter to minimize probe side records which are spilled to disk in Hybrid-Hash-Join</summary>
      <description>In Hybrid-Hash-Join, while small table does not fit into memory, part of the small table data would be spilled to disk, and the counterpart partition of big table data would be spilled to disk in probe phase as well. If we build a BloomFilter while spill small table to disk during build phase, and use it to filter the big table records which tend to be spilled to disk, this may greatly reduce the spilled big table file size, and saved the disk IO cost for writing and further reading.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.HashVsSortMiniBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.UnaryOperatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DriverTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReusingReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReusingHashMatchIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.NonReusingReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.NonReusingHashMatchIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.MutableHashTablePerformanceBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.TestTaskContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.RuntimeEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.RegularPactTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.PactTaskContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.PactDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReusingBuildSecondReOpenableHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReusingBuildSecondHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReusingBuildFirstReOpenableHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReusingBuildFirstHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReOpenableMutableHashTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.NonReusingBuildSecondReOpenableHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.NonReusingBuildSecondHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.NonReusingBuildFirstReOpenableHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.NonReusingBuildFirstHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.MutableHashTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.HashMatchIteratorBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AbstractCachedBuildSideJoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.Environment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-22 01:00:00" id="22407" opendate="2021-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump log4j to 2.14.1</summary>
      <description>Flink is currently relying on log4j 2.12.1 .Unfortunately, this vesrion has a bug related to json layout that prevents a user from adding additional log fields, as reported here https://issues.apache.org/jira/browse/LOG4J2-2652 as well as here: https://stackoverflow.com/questions/57003440/why-is-log4j2-jsonlayout-keyvaluepair-printing-empty-logevent-messagesThe problem is fixed in Log4j 2.13.1.Is there a good reason to keep Log4j 2.12.1, or can we upgrade?As an illustration, the presence of  additional1 in the snippet below: rootLogger.level = INFOrootLogger.appenderRef.console.ref = LogConsole appender.console.name = LogConsoleappender.console.type = CONSOLEappender.console.layout.type = JsonLayoutappender.console.layout.complete = falseappender.console.layout.compact = trueappender.console.layout.eventEol = trueappender.console.layout.properties = trueappender.console.layout.includeStacktrace=trueappender.console.layout.stacktraceAsString=true appender.console.layout.additional1.type=KeyValuePairappender.console.layout.additional1.key=timestampappender.console.layout.additional1.value=$${date:yyyy-MM-dd'T'HH:mm:ss.SSSZ}  leads to missing fields in the resulting logs, e.g.:{"logEvent":"Recover all persisted job graphs.","timestamp":"2021-04-21T16:50:31.722+0000"}{"logEvent":"Successfully recovered 0 persisted job graphs.","timestamp":"2021-04-21T16:50:31.723+0000"}{"logEvent":"Starting the SlotManager.","timestamp":"2021-04-21T16:50:31.732+0000"}{"logEvent":"Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .","timestamp":"2021-04-21T16:50:31.822+0000"}  Removing the additional1 resolves the issue and yield json logs containing all expected fields:{"thread":"cluster-io-thread-1","level":"INFO","loggerName":"org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess","message":"Successfully recovered 0 persisted job graphs.","endOfBatch":false,"loggerFqcn":"org.apache.logging.slf4j.Log4jLogger","instant":{"epochSecond":1619080838,"nanoOfSecond":216868000},"threadId":48,"contextMap":{},"threadPriority":5}{"thread":"flink-akka.actor.default-dispatcher-3","level":"INFO","loggerName":"org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl","message":"Starting the SlotManager.","endOfBatch":false,"loggerFqcn":"org.apache.logging.slf4j.Log4jLogger","instant":{"epochSecond":1619080838,"nanoOfSecond":313130000},"threadId":19,"contextMap":{},"threadPriority":5}{"thread":"cluster-io-thread-1","level":"INFO","loggerName":"org.apache.flink.runtime.rpc.akka.AkkaRpcService","message":"Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .","endOfBatch":false,"loggerFqcn":"org.apache.logging.slf4j.Log4jLogger","instant":{"epochSecond":1619080838,"nanoOfSecond":408714000},"threadId":48,"contextMap":{},"threadPriority":5}   </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-22 01:00:00" id="22413" opendate="2021-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hide Checkpointing page in the UI for batch jobs</summary>
      <description>This is a follow up to https://github.com/apache/flink/commit/8dca9fa852c72984ac873eae9a96bbd739e502f3#commitcomment-49744255Batch jobs don't need a Checkpointing page, because there is no information for these jobs available there.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.optimizer.jsonplan.JsonJobGraphGenerationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.jsonplan.JsonGeneratorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.jsonplan.JsonPlanGenerator.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-detail.ts</file>
      <file type="M">docs.content.docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.content.zh.docs.try-flink.flink-operations-playground.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-7-19 01:00:00" id="2248" opendate="2015-6-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow disabling of sdtout logging output</summary>
      <description>Currently when a job is submitted through the CLI we get in stdout all the log output about each stage in the job.It would useful to have an easy way to disable this output when submitting the job, as most of the time we are only interested in the log output if something goes wrong.</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.testjar.WordCount.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">docs.apis.cli.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-27 01:00:00" id="22489" opendate="2021-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>subtask backpressure indicator shows value for entire job</summary>
      <description>In the backpressure tab of the web UI, the OK/LOW/HIGH indication is displaying the job-level backpressure for every subtask, rather than the individual subtask values (effectively showing max back pressure from all of the subtasks of the given task for every subtask, instead of the individual values).</description>
      <version>1.9.3,1.10.3,1.11.3,1.12.2,1.13.0</version>
      <fixedVersion>1.11.4,1.14.0,1.13.1,1.12.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-28 01:00:00" id="22511" opendate="2021-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug of non-composite result type in Python TableAggregateFunction</summary>
      <description/>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.row.based.operation.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.fast.pxd</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-5-29 01:00:00" id="22523" opendate="2021-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TUMBLE TVF should throw helpful exception when specifying second interval parameter</summary>
      <description>Currently, the following query can run and no exception is thrown. However, the second interval parameter (i.e. the offset parameter) is not supported yet. We should throw a exception for this. select date_format(window_end, 'yyyy-MM-dd') as date_str, date_format(window_end, 'HH:mm') as time_str, count(distinct user_id) as uvfrom table(tumble(table user_behavior, descriptor(ts), interval '10' minute, interval '1' day))group by window_start, window_end;</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.WindowTableFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlTumbleTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlHopTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlCumulateTableFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-30 01:00:00" id="22537" opendate="2021-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation how to interact with DataStream API</summary>
      <description>Add documentation for FLIP-136.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-4 01:00:00" id="22556" opendate="2021-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend license checker to scan for traces of (L)GPL licensed code</summary>
      <description>This is a follow up to FLINK-22555.The goal is to catch this and similar cases automatically.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.java-ci-tools.src.test.java.org.apache.flink.tools.ci.licensecheck.JarFileCheckerTest.java</file>
      <file type="M">tools.ci.java-ci-tools.src.main.java.org.apache.flink.tools.ci.licensecheck.JarFileChecker.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-5 01:00:00" id="22566" opendate="2021-5-5 00:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>Running Kerberized YARN application on Docker test (default input) fails with no resources</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17558&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529&amp;l=8745May 05 01:29:04 Caused by: java.util.concurrent.TimeoutException: Timeout has occurred: 120000 msMay 05 01:29:04 at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.lambda$schedulePendingRequestBulkWithTimestampCheck$0(PhysicalSlotRequestBulkCheckerImpl.java:86) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_292]May 05 01:29:04 at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_292]May 05 01:29:04 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 ... 4 more</description>
      <version>1.13.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.yarn.docker.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-5-6 01:00:00" id="22581" opendate="2021-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Keyword &amp;#39;CATALOG&amp;#39; is missing in sql client doc</summary>
      <description>Excerpt from sql client doc:```CREATE CATALOG MyCatalog WITH ( 'type' = 'hive' );USE MyCatalog;```The statement `USE MyCatalog' should be 'USE CATALOG MyCatalog'</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-6 01:00:00" id="22586" opendate="2021-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve precision derivation for decimal arithmetics</summary>
      <description>Currently the precision and scale derivation is not properly for decimal data arithmetics, e.g,considering the following example:select cast('10.1' as decimal(38, 19)) * cast('10.2' as decimal(38, 19)) from Tthe result is `null`, which may confuses use a lot, because the result is actually not that big.The root cause is the precision derivation for the above multiplication is:(38, 19) * (38, 19) -&gt; (38, 38)So there is no space for integral digits, which leads to null results. </description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.DecimalTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.MathFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeSystem.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.SumWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.SumAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.Sum0AggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.AvgAggFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeMerging.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-22 01:00:00" id="2259" opendate="2015-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support training Estimators using a (train, validation, test) split of the available data</summary>
      <description>When there is an abundance of data available, a good way to train models is to split the available data into 3 parts: Train, Validation and Test.We use the Train data to train the model, the Validation part is used to estimate the test error and select hyperparameters, and the Test is used to evaluate the performance of the model, and assess its generalization &amp;#91;1&amp;#93;This is a common approach when training Artificial Neural Networks, and a good strategy to choose in data-rich environments. Therefore we should have some support of this data-analysis process in our Estimators.&amp;#91;1&amp;#93; Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. Vol. 1. Springer, Berlin: Springer series in statistics, 2001.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.libs.ml.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-7 01:00:00" id="22590" opendate="2021-5-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Scala implicit conversions for new API methods</summary>
      <description>FLINK-19980 should also be exposed through Scala's implicit conversions. To allow a fluent API such as `table.toDataStream(...)`</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DataStreamScalaITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.bridge.scala.TableConversions.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.bridge.scala.package.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.bridge.scala.DataStreamConversions.scala</file>
      <file type="M">docs.content.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.zh.docs.dev.table.data.stream.api.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-5-12 01:00:00" id="22640" opendate="2021-5-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataGen SQL Connector does not support defining fields min/max option of decimal type field</summary>
      <description>When defining fields' min/max option of decimal type field will fail to create DataGen SQL Connector.</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.DataGenTableSourceFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.RandomGeneratorVisitor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-13 01:00:00" id="22655" opendate="2021-5-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>When using -i &lt;init.sql&gt; option to initialize SQL Client session It should be possible to annotate the script with --</summary>
      <description/>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.SqlClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliStatementSplitterTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStatementSplitter.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-5-21 01:00:00" id="22733" opendate="2021-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type mismatch thrown in DataStream.union if parameter is KeyedStream for Python DataStream API</summary>
      <description>See http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-DataStream-union-type-mismatch-td43855.html for more details.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.13.1,1.12.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-5-25 01:00:00" id="22774" opendate="2021-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Kinesis SQL connector&amp;#39;s Guava to 27.0-jre</summary>
      <description>`flink-coonector-kinesis` uses a very old version (18.0) for compatibility reasons. However, since we don't expose Guava and relocate it in SQL connectors, we can use a newer version and avoid security concerns raised by vulnerability tools.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-26 01:00:00" id="22778" opendate="2021-5-26 00:00:00" resolution="Done">
    <buginformation>
      <summary>Upgrade to JUnit 4.13</summary>
      <description>The latest JUnit4 release has useful improvements and fixes: https://github.com/junit-team/junit4/blob/HEAD/doc/ReleaseNotes4.13.md</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-26 01:00:00" id="22780" opendate="2021-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression on 25.05</summary>
      <description>Tests such as: multiInputMapSink multiInputOneIdleMapSink readFileSplitshow regressions.Regression in run for range: 80ad5b3b51-bb597ea-1621977169It is most probably caused by: https://github.com/apache/flink/commit/ee9f9b227a7703c2688924070c4746a70bff3fd8</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSourceContexts.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-27 01:00:00" id="22789" opendate="2021-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 2.3.7</summary>
      <description>flink support for Hive version 2.3.7 releases.</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-7-28 01:00:00" id="22802" opendate="2021-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Fabric8 Kubernetes Client to &gt;= 5.X</summary>
      <description>Hi All, Currently, Flink is using version 4.9.2 of the Fabric8 Kubernetes Client which does not support new versions of the Kubernetes API such as 1.19 or 1.20 as pointed out by their Compatibility Matrix which is support in 5.4.0 or above.As far as I have seen in the Flink documentation, Flink supports `Kubernetes &gt;= 1.9.` but due to this dependency, it might not be the case. Is there a plan to update this dependency?What is the plan moving forwards when new versions of Kubernetes are released?I am raising this because I have been testing Flink HA Session Cluster on Kubernetes 1.19 and 1.20 and I have encountered some frequent errors that force the JM pods to restart or even result in an unrecoverable state.Thanks!</description>
      <version>1.13.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.MixedDispatcher.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClientTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.TestingFlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformerITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPodsWatcherTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionAndRetrievalITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase.java</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.AbstractKubernetesWatcher.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.KubernetesSharedWatcher.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback-session.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback-console.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-session.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-console.properties</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-6-31 01:00:00" id="22810" opendate="2021-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop usages of legacy planner in Elasticsearch modules</summary>
      <description>Remove references to flink-table-planner in all Elasticsearch modules.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchUpsertTableSinkFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchUpsertTableSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-31 01:00:00" id="22811" opendate="2021-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop usages of legacy planner in Avro module</summary>
      <description>Remove references to flink-table-planner in the Avro module.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.table.runtime.batch.AvroTypesITCase.java</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-6-31 01:00:00" id="22813" opendate="2021-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop usages of legacy planner in Hive module</summary>
      <description>Remove references to flink-table-planner in the Hive module.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-6-7 01:00:00" id="22906" opendate="2021-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add build time to Flink documentation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content..index.md</file>
      <file type="M">docs.content.zh..index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-9 01:00:00" id="22934" opendate="2021-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add instructions for using the " &amp;#39; " escape syntax of SQL client</summary>
      <description>FLINK-22921</description>
      <version>1.13.0,1.13.1</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-9 01:00:00" id="22942" opendate="2021-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable upsert into syntax in Flink SQL</summary>
      <description>I found we can write  insert into and upsert into in Flink SQL， but the later syntax's semantic and behavior is never discussed, currently they have same implementation.We should disable the later one util we support  `upsert into ` with correct behavior.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-6-15 01:00:00" id="23001" opendate="2021-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-avro-glue-schema-registry lacks scala suffix</summary>
      <description>The dependency on flink-streaming-java implies a need for a scala suffix.</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">tools.ci.java-ci-tools.src.main.java.org.apache.flink.tools.ci.suffixcheck.ScalaSuffixChecker.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-21 01:00:00" id="23054" opendate="2021-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct upsert optimization by upsert keys</summary>
      <description>After FLINK-22901.We can use upsert keys to fix upsert join, upsert rank, and upsert sink. For join and rank: if input has no upsert keys, do not use upsert optimization. For upsert sink: if input has unique keys but no upsert keys, we need add a materialize operator to produce upsert records.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.config.configuration.html</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.CalcRankTransposeRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.utils.RankProcessStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-21 01:00:00" id="23059" opendate="2021-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update playgrounds for Flink 1.13</summary>
      <description>The various playgrounds in apache/flink-playgrounds all need an update for the 1.13 release.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.content.zh.docs.try-flink.flink-operations-playground.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-6-22 01:00:00" id="23092" opendate="2021-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Built-in UDAFs could not be mixed use with Python UDAF in group window</summary>
      <description/>
      <version>1.13.0</version>
      <fixedVersion>1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalPythonGroupWindowAggregateRule.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udaf.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-6-24 01:00:00" id="23133" opendate="2021-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The dependencies are not handled properly when mixing use of Python Table API and Python DataStream API</summary>
      <description>The reason is that when converting from DataStream to Table, the dependencies should be handled and set correctly for the existing DataStream operators.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">docs.content.docs.dev.python.dependency.management.md</file>
      <file type="M">docs.content.zh.docs.dev.python.dependency.management.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-7-12 01:00:00" id="23356" opendate="2021-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase delegation token expired after 7 days</summary>
      <description>FLINK-6376 has solved the problem for HDFS but HBase still has the issue.The root cause of the issue is that HBase delegation token expires after 7 days and Flink is not re-obtaining any kind of token at the moment.</description>
      <version>1.13.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.modules.HadoopModule.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-26 01:00:00" id="23496" opendate="2021-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maven snapshot build does not use MAVEN_GLOBAL_OPTIONS</summary>
      <description>On CI we have a collection of useful settings for maven that we generally use everywhere, but the deployment of maven snapshot artifacts currently doesn't.This leads to some duplication and noisy logs.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.maven-utils.sh</file>
      <file type="M">tools.azure-pipelines.build-nightly-dist.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-31 01:00:00" id="23569" opendate="2021-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>There is a spelling error in a word in the document</summary>
      <description>I was browsing the documentation and found a word misspelled. This causes an exception when introducing libs in the pom.I checked the latest code and the problem still exists.</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.layouts.shortcodes.artifact.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-1 01:00:00" id="23570" opendate="2021-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation lists incorrect scala suffixes</summary>
      <description>Some of the maven dependencies in the documentation seem to have some problems and cannot be used directly.Page: DataStream Connectors -&gt; File Sink/Streaming FIle Sink </description>
      <version>1.13.0,1.14.0</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.layouts.shortcodes.artifact.html</file>
      <file type="M">docs.content.docs.dev.datastream.testing.md</file>
      <file type="M">docs.content.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.testing.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.streamfile.sink.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-24 01:00:00" id="23936" opendate="2021-8-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python UDFs instances are reinitialized if there is no input for more than 1 minute</summary>
      <description>We receive this feedback from some PyFlink users. After some investigation, we find out that the root case is that there is a mechanism in Beam that it will released the BundleProcessors which are inactive for more than 1 minute: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/worker/sdk_worker.py#L90</description>
      <version>1.10.0,1.11.0,1.12.0,1.13.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-27 01:00:00" id="24031" opendate="2021-8-27 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>I am trying to deploy Flink in kubernetes but when I launch the taskManager in other container I get a Exception</summary>
      <description> I explain here -&gt; https://github.com/apache/flink/pull/17020I have a problem when I try to run Flink in k8s with the follow manifestsI have the following exception JobManager :2021-08-27 09:16:57,917 ERROR akka.remote.EndpointWriter [] - dropping message &amp;#91;class akka.actor.ActorSelectionMessage&amp;#93; for non-local recipient [Actor&amp;#91;akka.tcp://flink@jobmanager-hs:6123/&amp;#93;] arriving at &amp;#91;akka.tcp://flink@jobmanager-hs:6123&amp;#93; inbound addresses are &amp;#91;akka.tcp://flink@cluster:6123&amp;#93; 2021-08-27 09:17:01,255 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request. 2021-08-27 09:17:01,284 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request. 2021-08-27 09:17:10,008 DEBUG akka.remote.transport.netty.NettyTransport [] - Remote connection to &amp;#91;/172.17.0.1:34827&amp;#93; was disconnected because of &amp;#91;id: 0x13ae1d03, /172.17.0.1:34827 :&gt; /172.17.0.23:6123&amp;#93; DISCONNECTED 2021-08-27 09:17:10,008 DEBUG akka.remote.transport.ProtocolStateActor [] - Association between local &amp;#91;tcp://flink@cluster:6123&amp;#93; and remote &amp;#91;tcp://flink@172.17.0.1:34827&amp;#93; was disassociated because the ProtocolStateActor failed: Unknown 2021-08-27 09:17:10,009 WARN akka.remote.ReliableDeliverySupervisor [] - Association with remote system &amp;#91;akka.tcp://flink@172.17.0.24:6122&amp;#93; has failed, address is now gated for &amp;#91;50&amp;#93; ms. Reason: &amp;#91;Disassociated&amp;#93;TaskManager:INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Could not resolve ResourceManager address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_. INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Could not resolve ResourceManager address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_.Best regards,Julio</description>
      <version>1.13.0,1.13.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.idea.vcs.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-30 01:00:00" id="24049" opendate="2021-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TupleTypeInfo doesn&amp;#39;t handle correctly for data types need conversion</summary>
      <description/>
      <version>1.12.0,1.13.0,1.14.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.common.typeinfo.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-17 01:00:00" id="24317" opendate="2021-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the test implementation in test_flat_aggregate</summary>
      <description/>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.row.based.operation.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-17 01:00:00" id="24318" opendate="2021-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Casting a number to boolean has different results between &amp;#39;select&amp;#39; fields and &amp;#39;where&amp;#39; condition</summary>
      <description>The same cast in the following two sql:// SQL 1SELECT cast(0.1 as boolean)// SQL 2SELECT * from test2 where cast(0.1 as boolean)has different results.The cast result in SQL 1 is true and the cast in SQL 2 is false.</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinDependentConditionDerivationRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionTypeCoerceRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionEqualityTransferRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceScanRule.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-14 01:00:00" id="24540" opendate="2021-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Resource leak due to Files.list</summary>
      <description>Files.list will open dir and we should close it</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.DirectoryBasedPluginFinder.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-27 01:00:00" id="24662" opendate="2021-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink sphinx check failed with "node class &amp;#39;meta&amp;#39; is already registered, its visitors will be overridden"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25481&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=8d78fe4f-d658-5c70-12f8-4921589024c3==========mypy checks... [SUCCESS]===========Oct 26 22:08:34 rm -rf _build/*Oct 26 22:08:34 /__w/1/s/flink-python/dev/.conda/bin/sphinx-build -b html -d _build/doctrees -a -W . _build/htmlOct 26 22:08:34 Running Sphinx v2.4.4Oct 26 22:08:34 Oct 26 22:08:34 Warning, treated as error:Oct 26 22:08:34 node class 'meta' is already registered, its visitors will be overriddenOct 26 22:08:34 Makefile:76: recipe for target 'html' failed</description>
      <version>1.12.0,1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-28 01:00:00" id="24676" opendate="2021-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema does not match if explain insert statement with partial column</summary>
      <description>create table MyTable (a int, b int) with ('connector' = 'datagen');create table MySink (c int, d int) with ('connector' = 'print');explain plan for insert into MySink(d) select a from MyTable where a &gt; 10;If execute the above statement, we will get the following exceptionorg.apache.flink.table.api.ValidationException: Column types of query result and sink for registered table 'default_catalog.default_database.MySink' do not match.Cause: Different number of columns.Query schema: &amp;#91;a: BIGINT&amp;#93;Sink schema: &amp;#91;d: BIGINT, e: INT&amp;#93;</description>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-3 01:00:00" id="2468" opendate="2015-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamCheckpointingITCase failure</summary>
      <description>Two of my recent builds failed on this case.https://travis-ci.org/sachingoel0101/flink/jobs/73851862https://travis-ci.org/apache/flink/jobs/73861390</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StateCheckpoinedITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-29 01:00:00" id="24708" opendate="2021-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>`ConvertToNotInOrInRule` has a bug which leads to wrong result</summary>
      <description>A user report this bug in maillist, I paste the content here.We are in the process of upgrading from Flink 1.9.3 to 1.13.3.  We have noticed that statements with either where UPPER(field) or LOWER(field) in combination with an IN do not always evaluate correctly.  The following test case highlights this problem.  import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.Schema;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;public class TestCase { public static void main(String[] args) throws Exception { final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); TestData testData = new TestData(); testData.setField1("bcd"); DataStream&lt;TestData&gt; stream = env.fromElements(testData); stream.print(); // To prevent 'No operators' error final StreamTableEnvironment tableEnvironment = StreamTableEnvironment.create(env); tableEnvironment.createTemporaryView("testTable", stream, Schema.newBuilder().build()); // Fails because abcd is larger than abc tableEnvironment.executeSql("select *, '1' as run from testTable WHERE lower(field1) IN ('abcd', 'abc', 'bcd', 'cde')").print(); // Succeeds because lower was removed tableEnvironment.executeSql("select *, '2' as run from testTable WHERE field1 IN ('abcd', 'abc', 'bcd', 'cde')").print(); // These 4 succeed because the smallest literal is before abcd tableEnvironment.executeSql("select *, '3' as run from testTable WHERE lower(field1) IN ('abc', 'abcd', 'bcd', 'cde')").print(); tableEnvironment.executeSql("select *, '4' as run from testTable WHERE lower(field1) IN ('abc', 'bcd', 'abhi', 'cde')").print(); tableEnvironment.executeSql("select *, '5' as run from testTable WHERE lower(field1) IN ('cde', 'abcd', 'abc', 'bcd')").print(); tableEnvironment.executeSql("select *, '6' as run from testTable WHERE lower(field1) IN ('cde', 'abc', 'abcd', 'bcd')").print(); // Fails because smallest is not first tableEnvironment.executeSql("select *, '7' as run from testTable WHERE lower(field1) IN ('cdef', 'abce', 'abcd', 'ab', 'bcd')").print(); // Succeeds tableEnvironment.executeSql("select *, '8' as run from testTable WHERE lower(field1) IN ('ab', 'cdef', 'abce', 'abcdefgh', 'bcd')").print(); env.execute("TestCase"); } public static class TestData { private String field1; public String getField1() { return field1; } public void setField1(String field1) { this.field1 = field1; } }} The job produces the following output:Empty set---------------------------------++-------------------------------op                         field1                            run---------------------------------++-------------------------------+I                            bcd                              2---------------------------------++-------------------------------1 row in set---------------------------------++-------------------------------op                         field1                            run---------------------------------++-------------------------------+I                            bcd                              3---------------------------------++-------------------------------1 row in set---------------------------------++-------------------------------op                         field1                            run---------------------------------++-------------------------------+I                            bcd                              4---------------------------------++-------------------------------1 row in set---------------------------------++-------------------------------op                         field1                            run---------------------------------++-------------------------------+I                            bcd                              5---------------------------------++-------------------------------1 row in set---------------------------------++-------------------------------op                         field1                            run---------------------------------++-------------------------------+I                            bcd                              6---------------------------------++-------------------------------1 row in setEmpty set---------------------------------++-------------------------------op                         field1                            run---------------------------------++-------------------------------+I                            bcd                              8---------------------------------++-------------------------------1 row in set  </description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.ConvertToNotInOrInRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ConvertToNotInOrInRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.calcite.FlinkRexBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-5 01:00:00" id="24798" opendate="2021-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump commons-cli to v1.5.0</summary>
      <description>Bump commons-cli:commons-cli:1.4 to commons-cli:commons-cli:1.5.0</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-12-29 01:00:00" id="25091" opendate="2021-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Official website document FileSink orc compression attribute reference error</summary>
      <description>I see the following version is like this &amp;#91;1.12、1.13、1.14 。。。&amp;#93; What should be quoted here is writerProperties Shouldn't be is writerProps docUrl</description>
      <version>1.12.0,1.13.0,1.14.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-2 01:00:00" id="25143" opendate="2021-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ITCase for Generalized incremental checkpoints</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.util.TestUtils.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointFormatITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.TestStreamEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-10 01:00:00" id="25240" opendate="2021-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update log4j2 version to 2.15.0</summary>
      <description>2.0 &lt;= Apache log4j2 &lt;= 2.14.1 have a RCE zero day.https://www.cyberkendra.com/2021/12/worst-log4j-rce-zeroday-dropped-on.htmlhttps://www.lunasec.io/docs/blog/log4j-zero-day/</description>
      <version>1.13.0</version>
      <fixedVersion>1.11.5,1.12.6,1.14.1,1.13.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  <bug fixdate="2022-4-9 01:00:00" id="26540" opendate="2022-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support handle join involving complex types in on condition</summary>
      <description>Now, the Hive dialect can't handle join involving complex types in on condition, which can be reproduced using the following code in HiveDialectITCase:tableEnv.executeSql("CREATE TABLE test2a (a ARRAY&lt;INT&gt;)");tableEnv.executeSql("CREATE TABLE test2b (a INT)");List&lt;Row&gt; results = CollectionUtil.iteratorToList( tableEnv.executeSql( "select * from test2b join test2a on test2b.a = test2a.a[1]") .collect());</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserJoinTypeCheckCtx.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-7 01:00:00" id="27108" opendate="2022-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>State cache clean up doesn&amp;#39;t work as expected</summary>
      <description>The test case test_session_window_late_merge failed when working on FLINK-26190. After digging into this problem, I found that the reason should be that the logic to determine whether a key &amp; namespace exists in state cache is wrong is wrong. It causes the state cache isn't clean up when it becomes invalidate.</description>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-8-8 01:00:00" id="28459" opendate="2022-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink SQL supports non-atomic CREATE TABLE AS SELECT(CTAS)</summary>
      <description>Non-atomic CTAS support in Streaming and Batch modes.Does not actively delete created target tables when a job fails or is cancelled</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlCreateTableConverter.java</file>
    </fixedFiles>
  </bug>
</bugrepository>