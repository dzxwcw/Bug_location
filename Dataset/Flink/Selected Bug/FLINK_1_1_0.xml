<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2015-5-9 01:00:00" id="3155" opendate="2015-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Flink docker version to latest stable Flink version</summary>
      <description>It would be nice to always set the Docker Flink binary URL to point to the latest Flink version. Until then, this JIRA keeps track of the updates for releases.</description>
      <version>1.0.0,1.1.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.docker-flink.Dockerfile</file>
      <file type="M">flink-contrib.docker-flink.docker-compose.yml</file>
      <file type="M">flink-contrib.docker-flink.flink.Dockerfile</file>
      <file type="M">flink-contrib.docker-flink.base.Dockerfile</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-29 01:00:00" id="3545" opendate="2016-2-29 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>ResourceManager: YARN integration</summary>
      <description>This integrates YARN support with the ResourceManager abstraction.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnTaskManager.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnProcessShutDownThread.java</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnMessages.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnJobManager.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMasterBase.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMaster.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationClient.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskManagerRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnCluster.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClientBase.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-yarn-tests.src.main.scala.org.apache.flink.yarn.TestingYarnTaskManager.scala</file>
      <file type="M">flink-yarn-tests.src.main.scala.org.apache.flink.yarn.TestingYarnJobManager.scala</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.TestingApplicationMaster.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-29 01:00:00" id="3547" opendate="2016-2-29 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add support for streaming projection, selection, and union</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.SelectITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.TranslationContext.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.DataSetTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.TableConversions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.package.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-3 01:00:00" id="3573" opendate="2016-3-3 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement more String functions for Table API</summary>
      <description>Implement the remaining string functions:CHARACTER_LENGTHCONCATUPPERLOWERINITCAPLIKESIMILAR</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.test.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.RexNodeTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.call.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.TrimCallGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.MethodCallGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.CallGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-3 01:00:00" id="3574" opendate="2016-3-3 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement math functions for Table API</summary>
      <description>MODEXPPOWERLNLOG10ABS</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.test.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.RexNodeTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.parser.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.call.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.arithmetic.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-4 01:00:00" id="3577" opendate="2016-3-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Display anchor links when hovering over headers.</summary>
      <description>This is useful to share the document url if display anchor links when hovering over headers. Currently we must scroll up to the TOC, find the section,click it, then copy the url.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE</file>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs.page.js.codetabs.js</file>
      <file type="M">docs.page.css.flink.css</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-7 01:00:00" id="3585" opendate="2016-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deploy scripts don&amp;#39;t support spaces in paths</summary>
      <description/>
      <version>1.0.0,1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.generate.specific.pom.sh</file>
      <file type="M">tools.deploy.to.maven.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-8 01:00:00" id="3586" opendate="2016-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Risk of data overflow while use sum/count to calculate AVG value</summary>
      <description>Now, we use (sum: Long, count: Long to store AVG partial aggregate data, which may have data overflow risk, we should use unbounded data type(such as BigInteger) to store them for necessary data types.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.SumAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.CountAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AvgAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.Aggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-8 01:00:00" id="3587" opendate="2016-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Calcite version to 1.7.0</summary>
      <description>We currently depend on Calcite 1.5.0. The latest stable release is 1.6.0, but I propose we bump the version to 1.7.0-SNAPSHOT to benefit from latest features. If we do that, we can also get rid of the custom FlinkJoinUnionTransposeRule.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.FlinkFilterAggregateTransposeRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetCalcRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-9 01:00:00" id="3596" opendate="2016-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataSet RelNode refactoring</summary>
      <description>After discussion with fhueske, chengxiang li, and twalthr, we have decided to make the following refactoring: Make the DataSet RelNodes correspond to logical relational operators. Move the code generation from the rules into the DataSet RelNodes. Remove the Flink RelNode layer and have a 1-pass translation instead of a 2-pass translation as currently.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggregationsITCase.scala.orig</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.logical.FlinkUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.logical.FlinkScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.logical.FlinkJoinUnionTransposeRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.logical.FlinkJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.logical.FlinkCalcRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.logical.FlinkAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetCalcRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkConvention.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetReduce.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetMap.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetGroupReduce.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetFlatMap.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetExchange.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.JavaBatchTranslator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-9 01:00:00" id="3597" opendate="2016-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API operator names should reflect relational expression</summary>
      <description>The names of the DataSet operators generated by the Table API do not reflect the relational expression they represent.This should be changed to make the Flink plan (which is visualized in the web dashboard) more readable.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-16 01:00:00" id="3622" opendate="2016-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error messages for invalid joins</summary>
      <description>Invalid Table API joins do not show descriptive error messages: Joins without a equality join condition Joins with non-matching equality join predicates</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-17 01:00:00" id="3632" opendate="2016-3-17 00:00:00" resolution="Done">
    <buginformation>
      <summary>Clean up Table API exceptions</summary>
      <description>The Table API throws many different exception types including: IllegalArgumentException TableException CodeGenException PlanGenException ExpressionParserExceptionfrom various places of the query translation code. This needs to be cleaned up.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.SelectITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.ToTableITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.FromDataSetITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.RowTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.trees.TreeNode.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.sources.CsvTableSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.FlinkTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.TableConversions.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-17 01:00:00" id="3634" opendate="2016-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix documentation for DataSetUtils.zipWithUniqueId()</summary>
      <description>Under FLINK-2590 the assignment and testing of unique IDs was improved but the documentation looks to still reference the old implementation.With parallelism=1 there is no difference between zipWithUniqueID and zipWithIndex. With greater parallelism the results of zipWithUniqueID are dependent on the partitioning.The documentation should demonstrate a possible result that is different from the incremental sequence of zipWithIndex while noting that results are dependent on the parallelism and partitioning.</description>
      <version>1.1.0</version>
      <fixedVersion>1.0.1,1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.zip.elements.guide.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-18 01:00:00" id="3636" opendate="2016-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NoClassDefFoundError while running WindowJoin example</summary>
      <description>Example ran with:bin/flink run examples/streaming/WindowJoin.jarorbin/flink run examples/streaming/WindowJoin.jar --windowSize 1000 --rate 2------------------------------------------------------------ The program finished with the following exception:java.lang.NoClassDefFoundError: org/apache/flink/streaming/examples/utils/ThrottledIterator at org.apache.flink.streaming.examples.join.WindowJoinSampleData$GradeSource.getSource(WindowJoinSampleData.java:65) at org.apache.flink.streaming.examples.join.WindowJoin.main(WindowJoin.java:67) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:505) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:403) at org.apache.flink.client.program.Client.runBlocking(Client.java:248) at org.apache.flink.client.CliFrontend.executeProgramBlocking(CliFrontend.java:866) at org.apache.flink.client.CliFrontend.run(CliFrontend.java:333) at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:1189) at org.apache.flink.client.CliFrontend.main(CliFrontend.java:1239)Caused by: java.lang.ClassNotFoundException: org.apache.flink.streaming.examples.utils.ThrottledIterator at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 13 moreEncountered after hand-building the current master.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-21 01:00:00" id="3640" opendate="2016-3-21 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add support for SQL queries in DataSet programs</summary>
      <description>This issue covers the task of supporting SQL queries embedded in DataSet programs. In this mode, the input and output of a SQL query is a Table. For this issue, we need to make the following additions to the Table API: add a tEnv.sql(query: String): Table method for converting a query result into a Table integrate Calcite's SQL parser into the batch Table API translation process.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.libs.table.md</file>
      <file type="M">docs.apis.batch.libs.index.md</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.FilterITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.TranslationContext.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.PlanTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.AbstractTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.JavaBatchTranslator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-21 01:00:00" id="3641" opendate="2016-3-21 00:00:00" resolution="Done">
    <buginformation>
      <summary>Document registerCachedFile API call</summary>
      <description>Flink's stable API supports the registerCachedFile API call at the ExecutionEnvironment. However, it is nowhere mentioned in the online documentation. Furthermore, the DistributedCache is also not explained.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-22 01:00:00" id="3649" opendate="2016-3-22 00:00:00" resolution="Done">
    <buginformation>
      <summary>Document stable API methods maxBy/minBy</summary>
      <description>The Java DataSet API contains the stable API methods maxBy and minBy which are nowhere mentioned in our documentation.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.index.md</file>
      <file type="M">docs.apis.batch.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-29 01:00:00" id="3676" opendate="2016-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebClient hasn&amp;#39;t been removed from the docs</summary>
      <description/>
      <version>1.0.0,1.1.0</version>
      <fixedVersion>1.0.0,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.docker-flink.flink.conf.flink-conf.yaml</file>
      <file type="M">flink-contrib.docker-flink.flink.config-flink.sh</file>
      <file type="M">docs.apis.common.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-4 01:00:00" id="3695" opendate="2016-4-4 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>ValueArray types</summary>
      <description>Flink provides mutable Value type implementations of Java primitives along with efficient serializers and comparators. It would be useful to have corresponding ValueArray implementations backed by primitive rather than object arrays, along with an ArrayableValue interface tying a Value to its ValueArray.</description>
      <version>1.1.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-4-6 01:00:00" id="3708" opendate="2016-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala API for CEP</summary>
      <description>Currently, The CEP library does not support Scala case classes, because the TypeExtractor cannot handle them. In order to support them, it would be necessary to offer a Scala API for the CEP library.</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.completeness.ScalaAPICompletenessTestBase.scala</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-libraries.pom.xml</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Pattern.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.AndFilterFunction.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.PatternStream.java</file>
      <file type="M">flink-libraries.flink-cep.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-6 01:00:00" id="3709" opendate="2016-4-6 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>[streaming] Graph event rates over time</summary>
      <description>The streaming server job page displays bytes and records sent and received, which answers the question "is data moving?" The next obvious question is "is data moving over time?" That could be answered by a chart displaying bytes/events rates. This would be a great chart to add to this display.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.ExecutionGraphHolder.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-11 01:00:00" id="3727" opendate="2016-4-11 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add support for embedded streaming SQL (projection, filter, union)</summary>
      <description>Similar to the support for SQL embedded in batch Table API programs, this issue tracks the support for SQL embedded in stream Table API programs. The only currently supported operations on streaming Tables are projection, filtering, and union.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.streaming.test.utils.StreamTestData.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.streaming.test.utils.StreamITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.streaming.test.SelectITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.streaming.test.FilterITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.DataStreamTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.EnumerableToLogicalTableScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.StreamTableEnvironment.scala</file>
      <file type="M">docs.apis.batch.libs.table.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-11 01:00:00" id="3728" opendate="2016-4-11 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Throw meaningful exceptions for unsupported SQL features</summary>
      <description>We must explicitly exclude unsupported SQL features such as Grouping Sets from being translated to Flink programs. Otherwise, the resulting program will compute invalid results.For that we must restrict the Calcite rules that translate Logical RelNodes into DataSetRel or DataStreamRel nodes.We may only translate to DataSetRel or DataStreamRel nodes if these support the semantics of the RelNode.Not translating a RelNode will yield a Calcite CannotPlanException that we should catch and enrich with a meaningful error message.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.PlanGenException.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.BatchTableEnvironment.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-11 01:00:00" id="3735" opendate="2016-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Embedded SQL union should fail during translation</summary>
      <description>The Table API currently only supports union all and embedded SQL queries translate both union and union all to union all. We should only generate a valid plan for union all.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetUnionRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-12 01:00:00" id="3739" opendate="2016-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a null literal to Table API</summary>
      <description>The Table API needs support for the null literal e.g. for passing null values to functions.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.sql.test.FilterITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.package.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.literals.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.apis.batch.libs.table.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-12 01:00:00" id="3743" opendate="2016-4-12 00:00:00" resolution="Done">
    <buginformation>
      <summary>Upgrade breeze from 0.11.2 to 0.12</summary>
      <description>Upgrade to the a new version of breeze that is available.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-ml.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-4-13 01:00:00" id="3748" opendate="2016-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CASE function to Table API</summary>
      <description>Add a CASE/WHEN functionality to Java/Scala Table API and add support for SQL API.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.sql.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.RexNodeTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.logic.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.call.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-4-14 01:00:00" id="3759" opendate="2016-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API should throw exception is null value is encountered in non-null mode.</summary>
      <description>The Table API can be configured to omit null-checks in generated code to speed up processing. Currently, the generated code replaces a null value with a data type specific default value if it is encountered in non-null-check mode. This can silently cause wrong results and should be changed such that an exception is thrown if a null value is encountered in non-null-check mode.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.sql.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-15 01:00:00" id="3768" opendate="2016-4-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Clustering Coefficient</summary>
      <description>The local clustering coefficient measures the connectedness of each vertex's neighborhood. Values range from 0.0 (no edges between neighbors) to 1.0 (neighborhood is a clique).</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-16 01:00:00" id="3772" opendate="2016-4-16 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Graph algorithms for vertex and edge degree</summary>
      <description>Many graph algorithms require vertices or edges to be marked with the degree. This ticket provides algorithms for annotating vertex degree for undirected graphs vertex out-, in-, and out- and in-degree for directed graphs edge source, target, and source and target degree for undirected graphs</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-18 01:00:00" id="3775" opendate="2016-4-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Scala shell does not respect Flink configuration</summary>
      <description>The Flink Scala shell does not load Flink's configuration properly. This makes it impossible to connect to a remote HA cluster, for example. The reason is that the GlobalConfiguration is never properly loaded. I think the Scala shell should respect the Flink settings specified in flink-conf.yaml if available.</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellLocalStartupITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-18 01:00:00" id="3776" opendate="2016-4-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Scala shell does not allow to set configuration for local execution</summary>
      <description>Flink's Scala shell starts a LocalFlinkMiniCluster with an empty configuration when the shell is started in local mode. In order to allow the user to configure the mini cluster, e.g., number of slots, size of memory, it would be good to forward a user specified configuration.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-22 01:00:00" id="3804" opendate="2016-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update YARN documentation</summary>
      <description>The YARN documentation should be reviewed and updated so that it reflects the current state. It contains parts which are no longer valid. E.g., the statement "The ports Flink is using for its services are the standard ports configured by the user + the application id as an offset" is outdated.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.yarn.setup.md</file>
      <file type="M">docs.apis.streaming.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-26 01:00:00" id="3824" opendate="2016-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ResourceManager may repeatedly connect to outdated JobManager in HA mode</summary>
      <description>When the ResourceManager receives a new leading JobManager via the LeaderRetrievalService it tries to register with this JobManager until connected. If during registration a new leader gets elected, the ResourceManager may still repeatedly try to register with the old one. This doesn't affect the registration with the new JobManager but leaves error messages in the log file and may process unnecessary messages.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.FlinkResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-26 01:00:00" id="3825" opendate="2016-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update CEP documentation to include Scala API</summary>
      <description>After adding the Scala CEP API FLINK-3708, we should update the online documentation to also contain the Scala API. This can be done similarly to the DataSet and DataStream API by providing Java and Scala code for all examples.</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.streaming.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-27 01:00:00" id="3835" opendate="2016-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JSON execution plan not helpful to debug plans with KeySelectors</summary>
      <description>The JSON execution plans are not helpful when debugging plans that include join operators with key selectors. For joins with hash join strategy, the driver strategy shows: "Hybrid Hash (build: Key Extractor)" where (build: Key Extractor) shall help to identify the build side of the join. However, if both inputs use KeySelectors, the build side cannot be identified.I propose to add the operator id to the build side information. The same issue applied for cross driver strategies.</description>
      <version>1.0.2,1.1.0</version>
      <fixedVersion>1.0.3,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plandump.PlanJSONDumpGenerator.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testJoin1.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testJoin0.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-28 01:00:00" id="3840" opendate="2016-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDB local parent dir is polluted with empty folders with random names</summary>
      <description>For some reason when the job starts the rocksdb root dir filled with hundreds of empty folders with random names like:041da1c-5fec-42ed-a69c-298240f1a065 4e6061aa-0c69-4755-a1ad-5ac4dec1d3f0 a7004bd1-778c-4a0f-96d4-9941208d188800db8406-6cb4-46ad-aac9-beeaa3247d16</description>
      <version>1.0.0,1.0.1,1.0.2,1.1.0</version>
      <fixedVersion>1.0.3,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-6-2 01:00:00" id="3859" opendate="2016-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add BigDecimal/BigInteger support to Table API</summary>
      <description>Since FLINK-3786 has been solved, we can now start integrating BigDecimal/BigInteger into the Table API.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.SumAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.AvgAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.AggregateTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.utils.ExpressionEvaluator.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeCoercion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.SumAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AvgAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.literals.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.comparison.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.arithmetic.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.FloorCeilCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-6-4 01:00:00" id="3872" opendate="2016-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Kafka TableSource with JSON serialization</summary>
      <description>Add a Kafka TableSource which reads JSON serialized data.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09ITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08ITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">docs.apis.table.md</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-4 01:00:00" id="3875" opendate="2016-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a TableSink for Elasticsearch</summary>
      <description>Add a TableSink that writes data to Elasticsearch</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.sinks.UpsertStreamTableSink.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.StreamTableDescriptorValidator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.StreamTableDescriptor.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.DescriptorProperties.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.elasticsearch.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-6 01:00:00" id="3879" opendate="2016-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Native implementation of HITS algorithm</summary>
      <description>Hyperlink-Induced Topic Search (HITS, also "hubs and authorities") is presented in &amp;#91;0&amp;#93; and described in &amp;#91;1&amp;#93;."&amp;#91;HITS&amp;#93; is a very popular and effective algorithm to rank documents based on the link information among a set of documents. The algorithm presumes that a good hub is a document that points to many others, and a good authority is a document that many documents point to." https://pdfs.semanticscholar.org/a8d7/c7a4c53a9102c4239356f9072ec62ca5e62f.pdfThis implementation differs from FLINK-2044 by providing for convergence, outputting both hub and authority scores, and completing in half the number of iterations.&amp;#91;0&amp;#93; http://www.cs.cornell.edu/home/kleinber/auth.pdf&amp;#91;1&amp;#93; https://en.wikipedia.org/wiki/HITS_algorithm</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.library.clustering.undirected.LocalClusteringCoefficientTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.library.clustering.directed.LocalClusteringCoefficientTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.HITSAlgorithm.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.test.java.org.apache.flink.graph.library.HITSAlgorithmITCase.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.examples.JaccardIndex.java</file>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-8 01:00:00" id="3883" opendate="2016-5-8 00:00:00" resolution="Abandoned">
    <buginformation>
      <summary>Rename flink clients for inclusion on system path</summary>
      <description>I ran into some trouble in preparing a Homebrew formula to install Flink on Mac (homebrew-core#968). Homebrew can install bin scripts into the system path, e.g. `/usr/local/bin/flink`:$ cat /usr/local/bin/flink#!/bin/bashexec "/usr/local/Cellar/apache-flink/1.0.2/libexec/bin/flink" "$@"It would be nice to install the various Flink shells too, but their names don't seem ideal for inclusion on the system path. I propose that they be renamed or aliased to have the following names (old -&gt; new):"pyflink2.sh" -&gt; "pyflink2""pyflink3.sh" -&gt; "pyflink3""start-scala-shell.sh" -&gt; "flink-scala-shell" (note: updated based on feedback)A related issue is, the shell scripts don't correctly determine their install location when called via a symlink.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.deploy.to.maven.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-11 01:00:00" id="3898" opendate="2016-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adamic-Adar Similarity</summary>
      <description>The implementation of Adamic-Adar Similarity &amp;#91;0&amp;#93; is very close to Jaccard Similarity. Whereas Jaccard Similarity counts common neighbors, Adamic-Adar Similarity sums the inverse logarithm of the degree of common neighbors.Consideration will be given to the computation of the inverse logarithm, in particular whether to pre-compute a small array of values.&amp;#91;0&amp;#93; http://social.cs.uiuc.edu/class/cs591kgk/friendsadamic.pdf</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.utils.Murmur3.32.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.similarity.JaccardIndex.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.clustering.directed.LocalClusteringCoefficient.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.generator.SingletonEdgeGraph.java</file>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-13 01:00:00" id="3909" opendate="2016-5-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maven Failsafe plugin may report SUCCESS on failed tests</summary>
      <description>The following build completed successfully on Travis but there are actually test failures: https://travis-ci.org/apache/flink/jobs/129943398#L5402</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-nifi.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-elasticsearch.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-java8.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-15 01:00:00" id="3913" opendate="2016-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up documentation typos</summary>
      <description>While reading through the docs I noticed a few spelling mistakes. We should do a quick review of the docs and fix up any mistakes found.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.yarn.setup.md</file>
      <file type="M">docs.setup.local.setup.md</file>
      <file type="M">docs.setup.config.md</file>
      <file type="M">docs.internals.monitoring.rest.api.md</file>
      <file type="M">docs.internals.job.scheduling.md</file>
      <file type="M">docs.internals.general.arch.md</file>
      <file type="M">docs.internals.back.pressure.monitoring.md</file>
      <file type="M">docs.apis.table.md</file>
      <file type="M">docs.apis.streaming.storm.compatibility.md</file>
      <file type="M">docs.apis.streaming.index.md</file>
      <file type="M">docs.apis.streaming.fault.tolerance.md</file>
      <file type="M">docs.apis.streaming.event.timestamps.watermarks.md</file>
      <file type="M">docs.apis.streaming.event.time.md</file>
      <file type="M">docs.apis.streaming.connectors.rabbitmq.md</file>
      <file type="M">docs.apis.scala.shell.md</file>
      <file type="M">docs.apis.common.index.md</file>
      <file type="M">docs.apis.best.practices.md</file>
      <file type="M">docs.apis.batch.python.md</file>
      <file type="M">docs.apis.batch.libs.ml.pipelines.md</file>
      <file type="M">docs.apis.batch.libs.ml.multiple.linear.regression.md</file>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
      <file type="M">docs.apis.batch.iterations.md</file>
      <file type="M">docs.apis.batch.index.md</file>
      <file type="M">docs.apis.batch.examples.md</file>
      <file type="M">docs.apis.batch.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-18 01:00:00" id="3925" opendate="2016-5-18 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>GraphAlgorithm to filter by maximum degree</summary>
      <description>Filtering by minimum degree is K-Core which is iterative. Filtering by maximum degree can be performed in constant time by filtering the set of high-degree vertices then doing an anti-join against the original vertex set and two anti-joins against the original edge set.Two reasons to remove high-degree vertices: 1) they may simply be noise in the input data, and 2) speedup algorithms such as Adamic-Adar and Jaccard Index which run quadratic in the vertex degree.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-11-18 01:00:00" id="3932" opendate="2016-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement State Backend Security</summary>
      <description>This issue is part of a series of improvements detailed in the Secure Data Access design doc.Flink should protect its HA, checkpoint, and savepoint state against unauthorized access.As described in the design doc, implement: ZooKeeper authentication w/ Kerberos ZooKeeper authorization (i.e. znode ACLs) Checkpoint/savepoint data protection</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-19 01:00:00" id="3936" opendate="2016-5-19 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add MIN / MAX aggregations function for BOOLEAN types</summary>
      <description>When executing TPC-H Q4, I observed that Calcite generates a MIN aggregate on Boolean literals to translate a decorrelate subquery in an EXIST predicate.MIN and MAX aggregates on Boolean data types are currently not supported and should be added.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateUtil.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-19 01:00:00" id="3938" opendate="2016-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn tests don&amp;#39;t run on the current master</summary>
      <description>Independently of FLINK-3909, I just discovered that the Yarn tests don't run on the current master (09b428b).[INFO] ------------------------------------------------------------------------[INFO] Building flink-yarn-tests 1.1-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ flink-yarn-tests_2.10 ---[INFO] [INFO] --- maven-checkstyle-plugin:2.16:check (validate) @ flink-yarn-tests_2.10 ---[INFO] [INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-maven) @ flink-yarn-tests_2.10 ---[INFO] [INFO] --- build-helper-maven-plugin:1.7:add-source (add-source) @ flink-yarn-tests_2.10 ---[INFO] Source directory: /home/travis/build/apache/flink/flink-yarn-tests/src/main/scala added.[INFO] [INFO] --- maven-remote-resources-plugin:1.5:process (default) @ flink-yarn-tests_2.10 ---[INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ flink-yarn-tests_2.10 ---[INFO] Using 'UTF-8' encoding to copy filtered resources.[INFO] skip non existing resourceDirectory /home/travis/build/apache/flink/flink-yarn-tests/src/main/resources[INFO] Copying 3 resources[INFO] [INFO] --- scala-maven-plugin:3.1.4:compile (scala-compile-first) @ flink-yarn-tests_2.10 ---[INFO] No sources to compile[INFO] [INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ flink-yarn-tests_2.10 ---[INFO] No sources to compile[INFO] [INFO] --- build-helper-maven-plugin:1.7:add-test-source (add-test-source) @ flink-yarn-tests_2.10 ---[INFO] Test Source directory: /home/travis/build/apache/flink/flink-yarn-tests/src/test/scala added.[INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ flink-yarn-tests_2.10 ---[INFO] Using 'UTF-8' encoding to copy filtered resources.[INFO] Copying 1 resource[INFO] Copying 3 resources[INFO] [INFO] --- scala-maven-plugin:3.1.4:testCompile (scala-test-compile) @ flink-yarn-tests_2.10 ---[INFO] /home/travis/build/apache/flink/flink-yarn-tests/src/test/scala:-1: info: compiling[INFO] Compiling 2 source files to /home/travis/build/apache/flink/flink-yarn-tests/target/test-classes at 1463615798796[INFO] prepare-compile in 0 s[INFO] compile in 9 s[INFO] [INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ flink-yarn-tests_2.10 ---[INFO] Nothing to compile - all classes are up to date[INFO] [INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ flink-yarn-tests_2.10 ---[INFO] Surefire report directory: /home/travis/build/apache/flink/flink-yarn-tests/target/surefire-reports[WARNING] The system property log4j.configuration is configured twice! The property appears in &lt;argLine/&gt; and any of &lt;systemPropertyVariables/&gt;, &lt;systemProperties/&gt; or user property.------------------------------------------------------- T E S T S-------------------------------------------------------Results :Tests run: 0, Failures: 0, Errors: 0, Skipped: 0</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-20 01:00:00" id="3942" opendate="2016-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for INTERSECT</summary>
      <description>Currently, the Table API and SQL do not support INTERSECT.INTERSECT can be executed as join on all fields.In order to add support for INTERSECT to the Table API and SQL we need to: Implement a DataSetIntersect class that translates an INTERSECT into a DataSet API program using a join on all fields. Implement a DataSetIntersectRule that translates a Calcite LogicalIntersect into a DataSetIntersect. Extend the Table API (and validation phase) to provide an intersect() method.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-20 01:00:00" id="3943" opendate="2016-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for EXCEPT (set minus)</summary>
      <description>Currently, the Table API and SQL do not support EXCEPT.EXCEPT can be executed as a coGroup on all fields that forwards records of the first input if the second input is empty.In order to add support for EXCEPT to the Table API and SQL we need to: Implement a DataSetMinus class that translates an EXCEPT into a DataSet API program using a coGroup on all fields. Implement a DataSetMinusRule that translates a Calcite LogicalMinus into a DataSetMinus. Extend the Table API (and validation phase) to provide an except() method.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.util.CollectionDataSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.UnsupportedOpsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.IntersectCoGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetIntersectRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetIntersect.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-21 01:00:00" id="3948" opendate="2016-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>EventTimeWindowCheckpointingITCase Fails with Core Dump</summary>
      <description>It fails because of a core dump in RocksDB.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-22 01:00:00" id="3949" opendate="2016-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Collect Metrics in Runtime Operators</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.UnregisteredTaskMetricsGroup.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamIterationTail.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSourceTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.source.InputFormatSourceFunctionTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.IOMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.BufferReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.ReaderBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.AdaptiveSpanningRecordDeserializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.RecordDeserializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnknownInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AbstractCachedBuildSideJoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AbstractOuterJoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AllGroupCombineDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AllReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.BatchTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedAllReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedFlatMapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedMapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedTerminationCriterionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.GroupCombineChainedDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.SynchronousChainedCombineDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CoGroupDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CrossDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSinkTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.FlatMapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.GroupReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.MapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.MapPartitionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.NoOpChainedDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.NoOpDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.ReduceCombineDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.ReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.UnionWithTempOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.reader.AbstractReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.TestSingleInputGate.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.UnionInputGateTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-22 01:00:00" id="3950" opendate="2016-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Meter Metric Type</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.ProxyMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-statsd.src.test.java.org.apache.flink.metrics.statsd.StatsDReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-statsd.src.main.java.org.apache.flink.metrics.statsd.StatsDReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.metrics.jmx.JMXReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.main.java.org.apache.flink.metrics.jmx.JMXReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.ScheduledDropwizardReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.ScheduledDropwizardReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.metrics.DropwizardHistogramWrapper.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.reporter.AbstractReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.MetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.groups.UnregisteredMetricsGroup.java</file>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-22 01:00:00" id="3951" opendate="2016-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Histogram Metric Type</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-metric-reporters.pom.xml</file>
      <file type="M">flink-metric-reporters.flink-metrics-statsd.src.main.java.org.apache.flink.metrics.statsd.StatsDReporter.java</file>
      <file type="M">flink-metric-reporters.flink-metrics-statsd.pom.xml</file>
      <file type="M">flink-metric-reporters.flink-metrics-graphite.src.main.java.org.apache.flink.metrics.graphite.GraphiteReporter.java</file>
      <file type="M">flink-metric-reporters.flink-metrics-graphite.pom.xml</file>
      <file type="M">flink-metric-reporters.flink-metrics-ganglia.src.main.java.org.apache.flink.metrics.ganglia.GangliaReporter.java</file>
      <file type="M">flink-metric-reporters.flink-metrics-ganglia.pom.xml</file>
      <file type="M">flink-metric-reporters.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.ScheduledDropwizardReporter.java</file>
      <file type="M">flink-metric-reporters.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.metrics.GaugeWrapper.java</file>
      <file type="M">flink-metric-reporters.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.metrics.CounterWrapper.java</file>
      <file type="M">flink-metric-reporters.flink-metrics-dropwizard.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.reporter.JMXReporterTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.MetricRegistryTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.MetricGroupRegistrationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.reporter.JMXReporter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.reporter.AbstractReporter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.MetricGroup.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.UnregisteredMetricsGroup.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.AbstractMetricGroup.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-22 01:00:00" id="3952" opendate="2016-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Netty to 4.1</summary>
      <description>Netty 4.1 is about to release final. This release has a number of significant enhancements, and in particular I find HTTP/2 codecs to be incredibly desirable to have. Additionally, hopefully, the Hadoop patches for Netty 4.1 get some tests and get merged, &amp; I believe if/when that happens it'll be important for Flink to also be using the new Netty minor version.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.minicluster.LocalFlinkMiniClusterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyMessageSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedBufferTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.AbstractByteBufTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.router.RouterHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.router.RoutedRequest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.AbstractHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedNetworkBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.NetworkBuffer.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.NettyBufferPool.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.ChunkedByteBuf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-23 01:00:00" id="3953" opendate="2016-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Surefire plugin executes unit tests twice</summary>
      <description>After FLINK-3909 the unit tests are executed twice. There are now two executions defined for the Surefire plugin: unit-tests and integration-tests. In addition, there is a default execution called default-test. This leads to the unit tests to be executed twice. Either renaming unit-tests to default-test or skipping default-test would fix the problem.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-23 01:00:00" id="3955" opendate="2016-5-23 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Change Table.toSink() to Table.writeToSink()</summary>
      <description>Currently, a Table can be emitted to a TableSink using the Table.toSink() method.However, the name of the method indicates that the Table is converted into a Sink.Therefore, I propose to change the method to Table.writeToSink().</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.TableSinkITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.TableSinkITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.BatchTableEnvironment.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-25 01:00:00" id="3971" opendate="2016-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aggregates handle null values incorrectly.</summary>
      <description>Table API and SQL aggregates are supposed to ignore null values, e.g., sum(1,2,null,4) is supposed to return 7. There current implementation is correct if at least one valid value is present however, is incorrect if only null values are aggregated. sum(null, null, null) should return null instead of 0Currently only the Count aggregate handles the case of null-values-only correctly.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.SumAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.CountAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.AvgAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.SumAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AvgAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-25 01:00:00" id="3973" opendate="2016-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API documentation is "hidden" in Programming Guide menu list</summary>
      <description>The Table API / SQL documentation is hard to find in the drop down list of the "Programming Guide" menu entry.We should either move it into the "Libraries" menu or move it up in the "Programming Guide" entry and bold like the other top entries.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.libs.table.md</file>
      <file type="M">docs.apis.table.md</file>
      <file type="M">docs.apis.best.practices.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-27 01:00:00" id="3981" opendate="2016-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t log duplicate TaskManager registrations as exceptions</summary>
      <description>Duplicate TaskManager registrations shouldn't be logged with Exceptions in the ResourceManager. Duplicate registrations can happen if the TaskManager sends out registration messages too fast when the actual reply is not lost but still in transit.The ResourceManager should simply acknowledge the duplicate registrations, leaving it up to the JobManager to decide how to treat the duplicate registrations (currently it will send an AlreadyRegistered to the TaskManager).This change also affects our test stability because the Yarn tests check the logs for exceptions.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.FlinkResourceManager.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-6-2 01:00:00" id="4009" opendate="2016-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala Shell fails to find library for inclusion in test</summary>
      <description>The Scala Shell test fails to find the flink-ml library jar in the target folder when executing with Intellij. This is due to its working directory being expected in "flink-scala-shell/target" when it is in fact "flink-scala-shell". When executed with Maven, this works fine because the Shade plugin changes the basedir from the project root to the /target folder*. As per till.rohrmann and greghogan suggestions we could simply add flink-ml as a test dependency and look for the jar path in the classpath.&amp;#42; Because we have the dependencyReducedPomLocation set to /target/.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.resources.log4j-test.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-8 01:00:00" id="4030" opendate="2016-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ScalaShellITCase gets stuck</summary>
      <description>The ScalaShellITCase fails regularly on Travis:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (integration-tests) on project flink-scala-shell_2.10: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?[ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-scala-shell/target &amp;&amp; /usr/lib/jvm/java-8-oracle/jre/bin/java -Xms256m -Xmx800m -Dmvn.forkNumber=1 -XX:-UseGCOverheadLimit -jar /home/travis/build/apache/flink/flink-scala-shell/target/surefire/surefirebooter5669599672364114854.jar /home/travis/build/apache/flink/flink-scala-shell/target/surefire/surefire854521958557782961tmp /home/travis/build/apache/flink/flink-scala-shell/target/surefire/surefire_7186137661441589930637tmp</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-9 01:00:00" id="4038" opendate="2016-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Impossible to set more than 1 JVM argument in env.java.opts</summary>
      <description>The Taskmanager start scripts fail when env.java.opts contains more than 1 jvm opts due to:if [[ $FLINK_TM_MEM_PRE_ALLOCATE == "false" ]] &amp;&amp; [ -z $FLINK_ENV_JAVA_OPTS ]; then-z checks the length of the first argument but it fails if it has more than 1 argument</description>
      <version>1.0.0,1.1.0</version>
      <fixedVersion>1.0.4,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-13 01:00:00" id="4062" opendate="2016-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Windowing Documentation</summary>
      <description>The window documentation could be a bit more principled and also needs updating with the new allowed lateness setting.There is also essentially no documentation about how to write a custom trigger.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.streaming.windows.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-13 01:00:00" id="4068" opendate="2016-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move constant computations out of code-generated `flatMap` functions.</summary>
      <description>The generated functions for expressions of the Table API or SQL include constant computations.For instance the code generated for a predicate like:myInt &lt; (10 + 20)looks roughly like:public void flatMap(Row in, Collector&lt;Row&gt; out) { Integer in1 = in.productElement(1); int temp = 10 + 20; if (in1 &lt; temp) { out.collect(in) }}In this example the computation of temp is constant and could be moved out of the flatMap() method.The same might apply for generated function other than FlatMap as well.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.BatchTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkTypeSystem.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testJoin1.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testJoin0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.ExplainTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.FlinkCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkRelBuilder.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-15 01:00:00" id="4074" opendate="2016-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reporter can block TaskManager shutdown</summary>
      <description>If a report is being submitted while a TaskManager is shutting down the reporter can cause the shutdown to be delayed since it submits the complete report.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metric-reporters.flink-metrics-statsd.src.main.java.org.apache.flink.metrics.statsd.StatsDReporter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.MetricRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-15 01:00:00" id="4077" opendate="2016-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Register Pojo DataSet/DataStream as Table requires alias expression.</summary>
      <description>Registering a Pojo DataSet / DataStream as Table requires alias expressions and does not work with simple field references. However, alias expressions would only be necessary if the fields of the Pojo should be renamed. DataStream&lt;Person&gt; persons = ...// DOES NOT WORKtEnv.registerDataStream( "Persons", persons, "name, age, address");// DOES WORKtEnv.registerDataStream( "Persons", persons, "name AS name, age AS age, address AS address");We should also allow simple field name references in addition to alias expressions to rename fields.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-17 01:00:00" id="4085" opendate="2016-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set Kinesis Consumer Agent to Flink</summary>
      <description>Currently, we use the default Kinesis Agent name.I was asked by Amazon to set it to something containing Flink.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-17 01:00:00" id="4091" opendate="2016-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-connector-cassandra has conflicting guava version</summary>
      <description>The newly merged cassandra streaming connector has an issue with its guava dependency.The build-process for flink-connector-cassandra creates shaded JAR file which contains the connector, the datastax cassandra driver plus in org.apache.flink.shaded a shaded copy of guava. The datastax cassandra driver calls into Futures.withFallback (&amp;#91;1&amp;#93;) which is present in this guava version. This also works inside the flink-connector-cassandra jar.Now the actual build-process for Flink happens and builds another shaded JAR and creates the flink-dist.jar. Inside this JAR, there is also a shaded version of guava inside org.apache.flink.shaded.Now the issue: The guava version which is in the flink-dist.jar is not compatible and doesn't contain the Futures.withFallback which the datastax driver is using.This leads into the following issue: You can without any problems launch a flink task which uses the casandra driver locally (so through the mini-cluster) because that is never using the flink-dist.jar. BUT: As soon as you are trying to start this job on a flink cluster (which uses the flink-dist.jar), the job breaks with the following exception:https://gist.github.com/theomega/5ab9b14ffb516b15814de28e499b040dYou can inspect this by opening the flink-connector-cassandra_2.11-1.1-SNAPSHOT.jar and the flink-dist_2.11-1.1-SNAPSHOT.jar in a java decompiler.I don't know a good solution here: Perhaps it would be one solution to shade the guava for the cassandra-driver somewhere else than at org.apache.flink.shaded.&amp;#91;1&amp;#93;: https://google.github.io/guava/releases/19.0/api/docs/com/google/common/util/concurrent/Futures.html#withFallback(com.google.common.util.concurrent.ListenableFuture, com.google.common.util.concurrent.FutureFallback, java.util.concurrent.Executor)</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-20 01:00:00" id="4093" opendate="2016-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose metric interfaces</summary>
      <description/>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.MetricGroup.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.UnregisteredMetricsGroup.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.AbstractMetricGroup.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.Gauge.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.Counter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-22 01:00:00" id="4104" opendate="2016-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restructure Gelly docs</summary>
      <description>The Gelly documentation has grown sufficiently long to suggest dividing into sub-pages. Leave "Using Gelly" on the main page and link to the following topics as sub-pages: Graph API Iterative Graph Processing Library Methods Graph Algorithms Graph Generators</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.libs.index.md</file>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-22 01:00:00" id="4108" opendate="2016-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE in Row.productArity</summary>
      <description>&amp;#91;this is my first issue request here, please apologize if something is missing&amp;#93; JDBCInputFormat of flink 1.1-SNAPSHOT fails with an NPE in Row.productArity:java.io.IOException: Couldn't access resultSet at org.apache.flink.api.java.io.jdbc.JDBCInputFormat.nextRecord(JDBCInputFormat.java:288) at org.apache.flink.api.java.io.jdbc.JDBCInputFormat.nextRecord(JDBCInputFormat.java:98) at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:162) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:588) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.NullPointerException at org.apache.flink.api.table.Row.productArity(Row.scala:28) at org.apache.flink.api.java.io.jdbc.JDBCInputFormat.nextRecord(JDBCInputFormat.java:279) ... 4 moreThe code reproduce this can be found in this gist: https://gist.github.com/zeitgeist/b91a60460661618ca4585e082895c616The reason for the NPE, I believe, is the way through which Flink creates Row instances through Kryo. As Row expects the number of fields to allocate as a parameter, which Kryo does not provide, the fields member of Row ends up being null. As Im neither a reflection nor a Kryo expert, I rather leave a true analysis to more knowledgable programmers.Part of the aforementioned example is a not very elegant workaround though a custom type and a cast (function jdbcNoIssue + custom Row type MyRow) to serve as a further hint towards my theory.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.TypeInfoFactoryTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.package.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-23 01:00:00" id="4109" opendate="2016-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the name of ternary condition operator &amp;#39;eval&amp;#39; to &amp;#39;?&amp;#39;</summary>
      <description>The ternary condition operator in Table API is named eval, for example: (42 &gt; 5).eval("A", "B") leads to "A". IMO, the eval function is not well understood. Instead the "?" is a better choice I think, which is used in Java for condition operator. It will be clearer and more literal understood, e.g.(42 &gt; 5).?("A", "B") or (42 &gt; 5) ? ("A", "B")If it make sense, I will pull a request.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.logic.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-7-27 01:00:00" id="4122" opendate="2016-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cassandra jar contains 2 guava versions</summary>
      <description/>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-28 01:00:00" id="4128" opendate="2016-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>compile error about git-commit-id-plugin</summary>
      <description>When I build with latest flink code, I got following error:&amp;#91;INFO&amp;#93; ------------------------------------------------------------------------&amp;#91;INFO&amp;#93; BUILD FAILURE&amp;#91;INFO&amp;#93; ------------------------------------------------------------------------&amp;#91;INFO&amp;#93; Total time: 01:06 h&amp;#91;INFO&amp;#93; Finished at: 2016-06-28T22:11:58+08:00&amp;#91;INFO&amp;#93; Final Memory: 104M/3186M&amp;#91;INFO&amp;#93; ------------------------------------------------------------------------&amp;#91;ERROR&amp;#93; Failed to execute goal pl.project13.maven:git-commit-id-plugin:2.1.5:revision (default) on project flink-runtime_2.11: Execution default of goal pl.project13.maven:git-commit-id-plugin:2.1.5:revision failed. NullPointerException -&gt; &amp;#91;Help 1&amp;#93;&amp;#91;ERROR&amp;#93;&amp;#91;ERROR&amp;#93; To see the full stack trace of the errors, re-run Maven with the -e switch.&amp;#91;ERROR&amp;#93; Re-run Maven using the -X switch to enable full debug logging.&amp;#91;ERROR&amp;#93;&amp;#91;ERROR&amp;#93; For more information about the errors and possible solutions, please read the following articles:&amp;#91;ERROR&amp;#93; &amp;#91;Help 1&amp;#93; http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException&amp;#91;ERROR&amp;#93;&amp;#91;ERROR&amp;#93; After correcting the problems, you can resume the build with the command&amp;#91;ERROR&amp;#93; mvn &lt;goals&gt; -rf :flink-runtime_2.11I think it's because wrong `doGetDirectory` value is provided.And another question is if we should upgrade the version of this plugin, so that we can got more meaningful error message instead of NPE. Eg:Could not get HEAD Ref, are you sure you have some commits in the dotGitDirectory?Current stable version is 2.2.1, but the disadvantage is that Java 1.6 is no longer supported with new version.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-30 01:00:00" id="4137" opendate="2016-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobManager web frontend does not shut down on OOM exception on JM</summary>
      <description>After the following Exception on the JobManager.2016-06-30 14:45:06,642 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Completed checkpoint 379 (in 7017 ms)2016-06-30 14:45:06,642 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Triggering checkpoint 380 @ 14672979066422016-06-30 14:45:17,902 ERROR akka.actor.ActorSystemImpl - Uncaught fatal error from thread [flink-akka.remote.default-remote-dispatcher-6] shutting down ActorSystem [flink]java.lang.OutOfMemoryError: Java heap space at com.google.protobuf.ByteString.copyFrom(ByteString.java:192) at com.google.protobuf.CodedInputStream.readBytes(CodedInputStream.java:324) at akka.remote.WireFormats$SerializedMessage.&lt;init&gt;(WireFormats.java:3030) at akka.remote.WireFormats$SerializedMessage.&lt;init&gt;(WireFormats.java:2980) at akka.remote.WireFormats$SerializedMessage$1.parsePartialFrom(WireFormats.java:3073) at akka.remote.WireFormats$SerializedMessage$1.parsePartialFrom(WireFormats.java:3068) at com.google.protobuf.CodedInputStream.readMessage(CodedInputStream.java:309) at akka.remote.WireFormats$RemoteEnvelope.&lt;init&gt;(WireFormats.java:993) at akka.remote.WireFormats$RemoteEnvelope.&lt;init&gt;(WireFormats.java:927) at akka.remote.WireFormats$RemoteEnvelope$1.parsePartialFrom(WireFormats.java:1049) at akka.remote.WireFormats$RemoteEnvelope$1.parsePartialFrom(WireFormats.java:1044) at com.google.protobuf.CodedInputStream.readMessage(CodedInputStream.java:309) at akka.remote.WireFormats$AckAndEnvelopeContainer.&lt;init&gt;(WireFormats.java:241) at akka.remote.WireFormats$AckAndEnvelopeContainer.&lt;init&gt;(WireFormats.java:175) at akka.remote.WireFormats$AckAndEnvelopeContainer$1.parsePartialFrom(WireFormats.java:279) at akka.remote.WireFormats$AckAndEnvelopeContainer$1.parsePartialFrom(WireFormats.java:274) at com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:141) at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:176) at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:188) at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:193) at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:49) at akka.remote.WireFormats$AckAndEnvelopeContainer.parseFrom(WireFormats.java:409) at akka.remote.transport.AkkaPduProtobufCodec$.decodeMessage(AkkaPduCodec.scala:181) at akka.remote.EndpointReader.akka$remote$EndpointReader$$tryDecodeMessageAndAck(Endpoint.scala:995) at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:928) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231)2016-06-30 14:45:18,502 INFO org.apache.flink.yarn.YarnJobManager - Stopping JobManager akka.tcp://flink@172.31.23.121:45569/user/jobmanager.2016-06-30 14:45:18,533 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom File Source (1/1) (5f2a1062c796ec6098a0a88227b9eab4) switched from RUNNING to CANCELINGThe JobManager JVM keeps running (keeping the YARN session alive) because the web monitor is not stopped on such errors.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-1 01:00:00" id="4139" opendate="2016-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn: Adjust parallelism and task slots correctly</summary>
      <description>The Yarn CLI should handle the following situations correctly: The user specifies no parallelism -&gt; parallelism is adjusted to #taskSlots * #nodes. The user specifies parallelism but no #taskSlots or too few slots -&gt; #taskSlots are set such that they meet the parallelismThese functionality has been present in Flink 1.0.x but there were some glitches in the implementation.</description>
      <version>1.0.3,1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.CliFrontendYarnAddressConfigurationTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendAddressConfigurationTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-1 01:00:00" id="4143" opendate="2016-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configurable delimiter for metric identifier</summary>
      <description>The metric identifier is currently hard-coded to separate components with a dot.We should make this configurable.</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-statsd.src.test.java.org.apache.flink.metrics.statsd.StatsDReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.metrics.DropwizardFlinkHistogramWrapperTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.ScheduledDropwizardReporter.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.MetricRegistryTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.TaskManagerJobGroupTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.TaskManagerGroupTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.TaskGroupTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.OperatorGroupTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.JobManagerJobGroupTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.JobManagerGroupTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.reporter.AbstractReporter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.MetricRegistry.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.scope.ScopeFormat.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.AbstractMetricGroup.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.apis.metrics.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-5 01:00:00" id="4151" opendate="2016-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address Travis CI build time: We are exceeding the 2 hours limit</summary>
      <description>We've recently started hitting the two hours limit for Travis CI.I'll look into some approaches to get our build stable again.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2016-11-7 01:00:00" id="4173" opendate="2016-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace maven-assembly-plugin by maven-shade-plugin in flink-metrics</summary>
      <description>The modules flink-metrics-dropwizard, flink-metrics-ganglia and flink-metrics-graphite use the maven-assembly-plugin to build a fat jar. The resulting fat jar has the suffix jar-with-dependencies. In order to make the naming consistent with the rest of the system we should create a fat-jar without this suffix.Additionally we could replace the maven-assembly-plugin with the maven-shade-plugin to make it consistent with the rest of the system.</description>
      <version>1.1.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-graphite.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-8-11 01:00:00" id="4189" opendate="2016-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce symbols for internal use</summary>
      <description>Currently we are using integer values to pass Calcite SQL symbols down to code generation. This causes problems like in FLINK-4068. We should support symbols internally.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.RexNodeTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.GeneratedFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.GeneratedExpression.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.TrimCallGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.MethodCallGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-12 01:00:00" id="4200" opendate="2016-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka consumers should log the offset from which they restore</summary>
      <description>Kafka consumers should log the offset from which they restore so that it is easier to investigate problems with recovery.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-12 01:00:00" id="4203" opendate="2016-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Table API documentation</summary>
      <description>Some ideas: Add a list of all supported scalar functions and a description Add a more advanced example Describe supported data types</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-13 01:00:00" id="4212" opendate="2016-7-13 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Lock PID file when starting daemons</summary>
      <description>As noted on the mailing list (0), when multiple TaskManagers are started in parallel (using pdsh) there is a race condition on updating the pid: 1) the pid file is first read to parse the process' index, 2) the process is started, and 3) on success the daemon pid is appended to the pid file.We could use a tool such as flock to lock on the pid file while starting the Flink daemon.0: http://mail-archives.apache.org/mod_mbox/flink-user/201607.mbox/%3CCA%2BssbKXw954Bz_sBRwP6db0FntWyGWzTyP7wJZ5nhOeQnof3kg%40mail.gmail.com%3E</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-daemon.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-21 01:00:00" id="4242" opendate="2016-7-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve validation exception messages</summary>
      <description>The Table API's validation exceptions could be improved to be more meaningful for users. For example, the following code snippet:Table inputTable = tableEnv.fromDataStream(env.fromElements( Tuple3.of(1, "a", 1.0), Tuple3.of(2, "b", 2.0), Tuple3.of(3, "c", 3.0)), "a, b, c");inputTable.select("a").where("!a");fails correctly. However, the validation exception message says "Expression !('a) failed on input check: Not only accepts child of Boolean Type, get Integer". I think it could be changed such that it says: "The not operator requires a boolean input but "a" is of type integer." or something similar.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.logic.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.arithmetic.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-8-21 01:00:00" id="4251" opendate="2016-7-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add possiblity for the RMQ Streaming Sink to customize the queue</summary>
      <description>This patch adds the possibilty for the user of the RabbitMQStreaming Sink to customize the queue which is used. This adopts the behavior of FLINK-4025 for the sink.The commit doesn't change the actual behaviour but makes itpossible for users to override the `setupQueue`method and customize their implementation. This was only possible for the RMQSource before. The Sink and the Source offer now both the same functionality, so this should increase usability. FLINK-4025 = https://issues.apache.org/jira/browse/FLINK-4025</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.1,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSink.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-11-25 01:00:00" id="4260" opendate="2016-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow SQL&amp;#39;s LIKE ESCAPE</summary>
      <description>Currently, the SQL API does not support specifying an ESCAPE character in a LIKE expression. The SIMILAR TO should also support that.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-29 01:00:00" id="4279" opendate="2016-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Set flink dependencies to provided</summary>
      <description/>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-1 01:00:00" id="4291" opendate="2016-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>No log entry for unscheduled reporters</summary>
      <description>When a non-Scheduled reporter is configured no log message is printed. It would be nice if we would print a log message for every instantiated reporter, not just Scheduled ones.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-1 01:00:00" id="4296" opendate="2016-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scheduler accepts more tasks than it has task slots available</summary>
      <description>Flink's scheduler doesn't support queued scheduling but expects to find all necessary task slots upon scheduling. If it does not it throws an error. Due to some changes in the latest master, this seems to be broken.Flink accepts jobs with parallelism &gt; total number of task slots, schedules and deploys tasks in all available task slots, and leaves the remaining tasks lingering forever.Easy to reproduce: ./bin/flink run -p TASK_SLOTS+n where TASK_SLOTS is the number of total task slots of the cluster and n&gt;=1.Here, p=11, TASK_SLOTS=10:bin/flink run -p 11 examples/batch/EnumTriangles.jarCluster configuration: Standalone cluster with JobManager at localhost/127.0.0.1:6123Using address localhost:6123 to connect to JobManager.JobManager web interface address http://localhost:8081Starting execution of programExecuting EnumTriangles example with default edges data set.Use --edges to specify file input.Printing result to stdout. Use --output to specify output path.Submitting job with JobID: cd0c0b4cbe25643d8d92558168cfc045. Waiting for job completion.08/01/2016 12:12:12 Job execution switched to status RUNNING.08/01/2016 12:12:12 CHAIN DataSource (at getDefaultEdgeDataSet(EnumTrianglesData.java:57) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; Map (Map at main(EnumTriangles.java:108))(1/1) switched to SCHEDULED08/01/2016 12:12:12 CHAIN DataSource (at getDefaultEdgeDataSet(EnumTrianglesData.java:57) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; Map (Map at main(EnumTriangles.java:108))(1/1) switched to DEPLOYING08/01/2016 12:12:12 CHAIN DataSource (at getDefaultEdgeDataSet(EnumTrianglesData.java:57) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; Map (Map at main(EnumTriangles.java:108))(1/1) switched to RUNNING08/01/2016 12:12:12 CHAIN DataSource (at getDefaultEdgeDataSet(EnumTrianglesData.java:57) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; Map (Map at main(EnumTriangles.java:108))(1/1) switched to FINISHED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(1/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(3/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(2/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(7/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(7/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(6/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(4/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(5/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(4/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(3/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(9/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(9/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(5/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(1/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(1/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(1/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(2/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(2/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(3/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(3/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(4/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(4/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(5/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(5/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(6/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(6/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(7/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(7/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(8/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(8/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(9/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(9/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(10/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(10/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(11/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(10/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(11/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(10/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(8/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(6/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(2/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(3/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(11/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(1/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(1/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(2/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(3/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(9/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(4/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(5/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(7/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(6/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(8/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(9/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(10/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(10/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(11/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(4/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(5/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(7/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(2/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(6/11) switched to RUNNING08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(1/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(2/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(7/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(6/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(3/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(9/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(11/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(5/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(10/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(4/11) switched to FINISHEDFor 8/11, the Join task switches to RUNNING, but the GroupReduce does not:08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(8/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(8/11) switched to DEPLOYING....08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(8/11) switched to SCHEDULED....{08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(8/11) switched to RUNNING}}</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-8-2 01:00:00" id="4306" opendate="2016-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Flink and Storm dependencies in flink-storm and flink-storm-examples</summary>
      <description>Flink dependencies should be in scope provided, like in the other libraries. flink-storm-examples should not draw storm-core directly, but only via flink-storm, so it gets the proper transitive dependency exclusions flink-storm-examples should have the clojure jar repository as an additional maven repository</description>
      <version>1.0.3,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-2 01:00:00" id="4307" opendate="2016-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Broken user-facing API for ListState</summary>
      <description>The user-facing ListState is supposed to return an empty list when no element is contained in the state.A previous change altered that behavior to make it in the runtime classes accessible whether a ListState is empty.To not break the user-facing API, we need to restore the behavior for ListState exposed to the users via the RuntimeContext.</description>
      <version>None</version>
      <fixedVersion>1.1.0,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContextTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-3 01:00:00" id="4310" opendate="2016-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move BinaryCompatibility Check plugin to relevant projects</summary>
      <description>The Maven plugin that checks binary compatibility is currently run for every project, rather than only the once where we have public API classes.Since the plugin contributes to build instability in some cases, we can improve stability by running it only in the relevant projects.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-shaded-curator.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-core.pom.xml</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-batch-connectors.flink-hadoop-compatibility.pom.xml</file>
      <file type="M">flink-annotations.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-5 01:00:00" id="4318" opendate="2016-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make master docs build target version-specific</summary>
      <description>Snapshots docs are found at .../flink-docs-master. This is relative to when the link is posted. We often do this on the mailing lists, but the docs evolve over time. This can lead to dead links or page content which don't match the original content (when the link was posted).I would like to move the snapshot docs to their respective version, e.g. .../flink-docs-release-1.2 before we do the actual release. This way, we will hopefully have less broken links and archived emails will make more sense. Furthermore, we will have more freedom when re-organizing pages.The current master could point to .../flink-docs-release-1.1.mxm what do you think?</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..config.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-9 01:00:00" id="4337" opendate="2016-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary Scala suffix from Hadoop1 artifact</summary>
      <description>The hadoop1 artifacts have a "_2.10" Scala suffix, but have no Scala dependency.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop1.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-9 01:00:00" id="4339" opendate="2016-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Slot Pool Core</summary>
      <description>Impements the core slot structures and behavior of the SlotPool: pool of available slots request slots and response if slot is available in pool return / deallocate slotsDetail design in here: https://docs.google.com/document/d/1y4D-0KGiMNDFYOLRkJy-C04nl8fwJNdm9hoUfxce6zY/</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.SlotID.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-9 01:00:00" id="4342" opendate="2016-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix dependencies of flink-connector-filesystem</summary>
      <description>The flink-connector-filesystem has inconsistent dependencies The Guava dependency is unused and can be removed The hadoop-shaded dependency is in 'compile' scope, but should be in 'provided' scope, because it must not go into the user code jar.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-9 01:00:00" id="4346" opendate="2016-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement basic RPC abstraction</summary>
      <description>As part of refactoring of the cluster management, we can introduce a new RPC abstraction on top of our Akka-based distributed coordination.It should address the following issues: Add type safety to the sender and receiver of messages. We want proper types methods to be called, rather than haveing generic message types and pattern matching everywhere. This is similar to typed actors. Make the message receivers testable without involving actors, i.e. the methods should be callable directly. When used with other component, the receiver will be wrapped in an actor that calls the methods based on received messages. We want to keep the paradigm of single-threaded execution per "actor"There is some basic code layout in the following branch and commit:https://github.com/apache/flink/tree/flip-6/flink-runtime/src/main/java/org/apache/flink/runtime/rpc</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-10 01:00:00" id="4351" opendate="2016-8-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobManager handle TaskManager&amp;#39;s registration</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.JobLeaderService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-10 01:00:00" id="4359" opendate="2016-8-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add INTERVAL type</summary>
      <description>In order to start with StreamSQL windows we need a way to define intervals in time.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.expression.TimeTypesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeCoercion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.Types.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.literals.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.cast.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.arithmetic.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-10 01:00:00" id="4363" opendate="2016-8-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement TaskManager basic startup of all components in java</summary>
      <description>Similar with current TaskManager,but implement initialization and startup all components in java instead of scala.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.taskexecutor.TaskExecutor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-10 01:00:00" id="4368" opendate="2016-8-10 00:00:00" resolution="Done">
    <buginformation>
      <summary>Eagerly initialize RrcProtocol members</summary>
      <description>The members of the RPC endpoint (RpcProtocol) are lazily created upon the start() call.I suggest to initialize them eagerly as they seem to be integral parts without which several functions cannot work properly.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.MainThreadExecutor.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-8-11 01:00:00" id="4385" opendate="2016-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Union on Timestamp fields does not work</summary>
      <description>The following does not work:public static class SDF { public Timestamp t = Timestamp.valueOf("1990-10-10 12:10:10");}ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();DataSet&lt;SDF&gt; dataSet1 = env.fromElements(new SDF());DataSet&lt;SDF&gt; dataSet2 = env.fromElements(new SDF());BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);tableEnv.registerDataSet( "table0", dataSet1 );tableEnv.registerDataSet( "table1", dataSet2 );Table table = tableEnv.sql( "select t from table0 union select t from table1" );DataSet&lt;Row&gt; d = tableEnv.toDataSet(table, Row.class);d.print();</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.IntervalTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-8-13 01:00:00" id="4392" opendate="2016-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make RPC Service Thread Safe</summary>
      <description>The RPC Service should not fail in the presence of concurrent requests to start endpoints</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaGateway.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-17 01:00:00" id="4409" opendate="2016-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>class conflict between jsr305-1.3.9.jar and flink-shaded-hadoop2-1.1.1.jar</summary>
      <description>It seems all classes in jsr305-1.3.9.jar can be found in flink-shaded-hadoop2-1.1.1.jar,too.I can exclude these jars for a success assembly and run when I was using sbtlibraryDependencies ++= Seq( "com.typesafe.play" %% "play-json" % "2.3.8", "org.apache.flink" %% "flink-scala" % "1.1.1" exclude("com.google.code.findbugs", "jsr305"), "org.apache.flink" %% "flink-connector-kafka-0.8" % "1.1.1" exclude("com.google.code.findbugs", "jsr305"), "org.apache.flink" %% "flink-streaming-scala" % "1.1.1" exclude("com.google.code.findbugs", "jsr305"), "org.apache.flink" %% "flink-clients" % "1.1.1" exclude("com.google.code.findbugs", "jsr305"), "joda-time" % "joda-time" % "2.9.4", "org.scalikejdbc" %% "scalikejdbc" % "2.2.7", "mysql" % "mysql-connector-java" % "5.1.15", "io.spray" %% "spray-caching" % "1.3.3")But I think it might be better to remove jsr305 dependency from Flink.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.2,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-17 01:00:00" id="4411" opendate="2016-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Chained dual input children are not properly propagated</summary>
      <description/>
      <version>1.1.0</version>
      <fixedVersion>1.1.2,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main2.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-18 01:00:00" id="4422" opendate="2016-8-18 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Convert all time interval measurements to System.nanoTime()</summary>
      <description>In contrast to System.currentTimeMillis(), System.nanoTime() is monotonous. To measure delays and time intervals, System.nanoTime() is hence reliable, while System.currentTimeMillis() is not.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientConnectionTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KillerWatchDog.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.ClosableBlockingQueue.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-24 01:00:00" id="4478" opendate="2016-8-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement heartbeat logic</summary>
      <description>With the Flip-6 refactoring, we'll have the need for a dedicated heartbeat component. The heartbeat component is used to check the liveliness of the distributed components among each other. Furthermore, heartbeats are used to regularly transmit status updates to another component. For example, the TaskManager informs the ResourceManager with each heartbeat about the current slot allocation.The heartbeat is initiated from one component. This component sends a heartbeat request to another component which answers with an heartbeat response. Thus, one can differentiate between a sending and a receiving side. Apart from the triggering of the heartbeat request, the logic of treating heartbeats, marking components dead and payload delivery are the same and should be reusable by different distributed components (JM, TM, RM).Different models for the heartbeat reporting are conceivable. First of all, the heartbeat request could be sent as an ask operation where the heartbeat response is returned as a future on the sending side. Alternatively, the sending side could request a heartbeat response by sending a tell message. The heartbeat response is then delivered by an RPC back to the heartbeat sender. The latter model has the advantage that a heartbeat response is not tightly coupled to a heartbeat request. Such a tight coupling could cause that heartbeat response are ignored after the future has timed out even though they might still contain valuable information (receiver is still alive).Furthermore, different strategies for the heartbeat triggering and marking heartbeat targets as dead are conceivable. For example, we could periodically (with a fixed period) trigger a heartbeat request and mark all targets as dead if we didn't receive a heartbeat response in a given time period. Furthermore, we could adapt the heartbeat interval and heartbeat timeouts with respect to the latency of previous heartbeat responses. This would reflect the current load and network conditions better.For the first version, I would propose to use a fixed period heartbeat with a maximum heartbeat timeout before a target is marked dead. Furthermore, I would propose to use tell messages (fire and forget) to request and report heartbeats because they are the more flexible model imho.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotProtocolTest.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-5-25 01:00:00" id="4497" opendate="2016-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for Scala tuples and case classes to Cassandra sink</summary>
      <description>The new Cassandra sink only supports streams of Flink Java tuples and Java POJOs that have been annotated for use by Datastax Mapper. The sink should be extended to support Scala types and case classes.</description>
      <version>1.1.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraTupleSink.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraSink.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-25 01:00:00" id="4499" opendate="2016-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce findbugs maven plugin</summary>
      <description>As suggested by Stephan in FLINK-4482, this issue is to add findbugs-maven-plugin into the build process so that we can detect lack of proper locking and other defects automatically.We can begin with small set of rules.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-26 01:00:00" id="4514" opendate="2016-8-26 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>ExpiredIteratorException in Kinesis Consumer on long catch-ups to head of stream</summary>
      <description>Original mailing thread for the reported issue:http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Kinesis-connector-Iterator-expired-exception-td8711.htmlNormally, the exception is thrown when the consumer uses the same shard iterator after 5 minutes since it was retrieved. I've still yet to clarify &amp; reproduce the root cause of the ExpiredIteratorException, because from the code this seems to be impossible. I'm leaning towards suspecting this is a Kinesis-side issue (from the description in the ML, the behaviour also seems indeterminate).Either way, the exception can be fairly easily handled so that the consumer doesn't just fail. When caught, we request a new shard iterator from Kinesis with the last sequence number.</description>
      <version>1.1.0,1.1.1</version>
      <fixedVersion>1.1.3,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.FakeKinesisBehavioursFactory.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ConsumerConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-31 01:00:00" id="4538" opendate="2016-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement slot allocation protocol with JobMaster</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.TestingSerialRpcService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.TestingRpcService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.SlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.SlotStatus.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.SlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.SlotAssignment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.RegistrationResponse.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.JobMasterRegistration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-1 01:00:00" id="4555" opendate="2016-9-1 00:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Explicitly kill TaskManager on YARN when ApplicationMaster is shutting down</summary>
      <description>It seems that Flink is not explicitly destroying the TaskManager JVM when the ApplicationMaster is shutting down (when the YARN application is stopping).Since this was once in Flink (in 1.0.x) we should add a test case to ensure this feature stays in the code.</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.FlinkResourceManager.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-9-2 01:00:00" id="4560" opendate="2016-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>enforcer java version as 1.7</summary>
      <description>1. maven-enforcer-plugin add java version enforce2. maven-enforcer-plugin version upgrade to 1.4.1explicit require java version</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-3-4 01:00:00" id="4577" opendate="2016-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-enable transparent reshard handling in Kinesis Consumer</summary>
      <description>In FLINK-4341, we disabled transparent reshard handling in the Kinesis consumer as a short-term workaround before FLINK-4576 comes around.This ticket tracks the progress of re-enabling it again, by implementing a LowWatermarkListener interface as described in FLINK-4576.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-30 01:00:00" id="4718" opendate="2016-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Confusing label in Parallel Streams Diagram</summary>
      <description>The Event &amp;#91;id|timestamp&amp;#93; label in the Parallel Streams Diagram is confusing. The 'id' is in fact the key of the stream and not the id of the event record. Hence we have B35 and B33.The 'id' label should be changed to key or key_id to better represent its nature.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.fig.parallel.streams.watermarks.svg</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-14 01:00:00" id="4827" opendate="2016-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The scala example of SQL on Streaming Tables with wrong variable name in flink document</summary>
      <description>val env = StreamExecutionEnvironment.getExecutionEnvironmentval tEnv = TableEnvironment.getTableEnvironment(env)// read a DataStream from an external sourceval ds: DataStream&amp;#91;(Long, String, Integer)&amp;#93; = env.addSource(...)// register the DataStream under the name "Orders"tableEnv.registerDataStream("Orders", ds, 'user, 'product, 'amount)// run a SQL query on the Table and retrieve the result as a new Tableval result = tableEnv.sql( "SELECT product, amount FROM Orders WHERE product LIKE '%Rubber%'")There is no variable named tableEnv had defined ,Only tEnv defined here</description>
      <version>1.1.0,1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-12-22 01:00:00" id="5128" opendate="2016-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get Kafka partitions in FlinkKafkaProducer only if a partitioner is set</summary>
      <description>The fetched partitions list is only used when calling open(...) for a user supplied custom partitioner in FlinkKafkaProducer.Therefore, we can actually only fetch the partition list if the user used a partitioner (right now we always do the partition fetching).</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
    </fixedFiles>
  </bug>
  
  
</bugrepository>