<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  <bug fixdate="2020-3-10 01:00:00" id="16516" opendate="2020-3-10 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Avoid codegen user-defined function for Python UDF</summary>
      <description>Currently we make use of codegen to generate PythonScalarFunction and PythonTableFunction, but it is unnecessary. We can directly create a static PythonScalarFunction and PythonTableFunction.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.utils.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.SimplePythonFunction.java</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.catalog.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-30 01:00:00" id="1799" opendate="2015-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala API does not support generic arrays</summary>
      <description>The Scala API does not support generic arrays at the moment. It throws a rather unhelpful error message ```InvalidTypesException: The given type is not a valid object array```.Code to reproduce the problem is given below:def main(args: Array[String]) { foobar[Double]}def foobar[T: ClassTag: TypeInformation]: DataSet[Block[T]] = { val tpe = createTypeInformation[Array[T]] null}</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.types.TypeInformationGenTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeInformationGen.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-8 01:00:00" id="18182" opendate="2020-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade AWS SDK in flink-connector-kinesis to include new region af-south-1</summary>
      <description>Current (1.11.1) version of flink-connector-kinesis is compiled against version 1.11.754 of the AWS SDK, which does not include the new af-south-1 (Cape Town) AWS region.Looking at the release notes for AWS SDK - this region is included from version 1.11.768 onwards.I'd be happy to try and create a PR for this.</description>
      <version>1.11.1</version>
      <fixedVersion>1.11.4,1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-17 01:00:00" id="18343" opendate="2020-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable DEBUG logging for java e2e tests</summary>
      <description>Java e2e tests run with the default logging configuration, which only logs on INFO.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.main.java.org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkDistribution.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-7-20 01:00:00" id="18644" opendate="2020-7-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove obsolete doc for hive connector</summary>
      <description/>
      <version>1.10.1,1.11.1</version>
      <fixedVersion>1.10.2,1.11.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.connectors.zh.md</file>
      <file type="M">docs.dev.batch.connectors.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-21 01:00:00" id="18660" opendate="2020-7-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump netty to 4.1.49</summary>
      <description>Bump netty to 4.1.49 for some security fixes.This also entails bumping netty-tcnative to 2.30.0 .</description>
      <version>None</version>
      <fixedVersion>shaded-12.0,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-8-23 01:00:00" id="18678" opendate="2020-7-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector fails to create vector orc reader if user specifies incorrect hive version</summary>
      <description>Issue reported by user. User's Hive deployment is 2.1.1 and uses flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar in Flink lib. If user specifies Hive version as 2.1.1, then creating vectorized orc reader fails with exception:java.lang.ClassCastException: org.apache.hadoop.hive.ql.io.orc.ReaderImpl cannot be cast to org.apache.orc.Reader at org.apache.flink.orc.shim.OrcShimV200.createReader(OrcShimV200.java:63) ~[flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar:1.11.0] at org.apache.flink.orc.shim.OrcShimV200.createRecordReader(OrcShimV200.java:98) ~[flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar:1.11.0] at org.apache.flink.orc.OrcSplitReader.&lt;init&gt;(OrcSplitReader.java:73) ~[flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar:1.11.0] at org.apache.flink.orc.OrcColumnarRowSplitReader.&lt;init&gt;(OrcColumnarRowSplitReader.java:54) ~[flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar:1.11.0] at org.apache.flink.orc.OrcSplitReaderUtil.genPartColumnarRowReader(OrcSplitReaderUtil.java:91) ~[flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar:1.11.0]......</description>
      <version>1.11.1</version>
      <fixedVersion>1.11.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
      <file type="M">docs.dev.table.catalogs.zh.md</file>
      <file type="M">docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-7-24 01:00:00" id="18708" opendate="2020-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The links of the connector sql jar of Kafka 0.10 and 0.11 are extinct</summary>
      <description>The links of the connector sql jar of Kafka 0.10 and 0.11 are extinct. I will fix it as soon as possible.</description>
      <version>1.11.1,1.12.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-27 01:00:00" id="18725" opendate="2020-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Run Kubernetes test" failed with "30025: provided port is already allocated"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4901&amp;view=logs&amp;j=08866332-78f7-59e4-4f7e-49a56faa3179&amp;t=3e8647c1-5a28-5917-dd93-bf78594ea994The Service "flink-job-cluster" is invalid: spec.ports[2].nodePort: Invalid value: 30025: provided port is already allocated</description>
      <version>1.11.0,1.11.1</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.container-scripts.job-cluster-service.yaml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-27 01:00:00" id="18726" opendate="2020-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support INSERT INTO specific columns</summary>
      <description>Currently Flink only supports insert into a table without specifying columns, but most database systems support insert into specific columns byINSERT INTO table_name(column1, column2, ...) ...The columns not specified will be filled with default values or NULL if no default value is given when creating the table.As Flink currently does not support default values when creating tables, we can fill the unspecified columns with NULL and throw exceptions if there are columns with NOT NULL constraints.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-27 01:00:00" id="18730" opendate="2020-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Beta tag from SQL Client docs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-27 01:00:00" id="18731" opendate="2020-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The monotonicity of UNIX_TIMESTAMP function is not correct</summary>
      <description>Currently, the monotonicity of UNIX_TIMESTAMP function is always INCREASING, actually, when it has empty function arguments (UNIX_TIMESTAMP(), is equivalent to NOW()), its monotonicity is INCREASING. otherwise its monotonicity should be NOT_MONOTONIC. (e.g. UNIX_TIMESTAMP(string))</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-8-9 01:00:00" id="18861" opendate="2020-8-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support add_source() to get a DataStream for Python StreamExecutionEnvironment</summary>
      <description>Support add_source() to get a DataStream for Python StreamExecutionEnvironment. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-9 01:00:00" id="18862" opendate="2020-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix LISTAGG throws BinaryRawValueData cannot be cast to StringData exception in runtime</summary>
      <description>1. Env：flinksql、 version 1.11.1，perjob mode2. Error：org.apache.flink.table.data.binary.BinaryRawValueData cannot be cast to org.apache.flink.table.data.StringData3、Job：(1) create a kafka table CREATE TABLE kafka( x String, y String )with( 'connector' = 'kafka', ...... )(2)create a view: CREATE VIEW view1 AS SELECT x, y, CAST(COUNT(1) AS VARCHAR) AS ct FROM kafka GROUP BY x, y(3) aggregate on the view: select x, LISTAGG(CONCAT_WS('=', y, ct), ',') AS lists FROM view1 GROUP BY xAnd then the exception is thrown：org.apache.flink.table.data.binary.BinaryRawValueData cannot be cast to org.apache.flink.table.data.StringDataThe problem is that, there is no RawValueData in the query. The result type of count(1) should be bigint, not RawValueData. (4) If there is no aggregation, the job can run succefully. select x, CONCAT_WS('=', y, ct) from view1The detailed exception:java.lang.ClassCastException: org.apache.flink.table.data.binary.BinaryRawValueData cannot be cast to org.apache.flink.table.data.StringData at org.apache.flink.table.data.GenericRowData.getString(GenericRowData.java:169) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.table.data.JoinedRowData.getString(JoinedRowData.java:139) ~[flink-table-blink_2.11-1.11.1.jar:?] at org.apache.flink.table.data.RowData.get(RowData.java:273) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copyRowData(RowDataSerializer.java:156) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1] at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:123) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1] at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:50) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1] at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:715) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:205) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1] at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1] at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:85) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:161) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:178) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:153) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:345) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:191) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:181) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:558) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:530) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) [ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) [ad_features_auto-1.0-SNAPSHOT.jar:?]</description>
      <version>1.11.1</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-9 01:00:00" id="18863" opendate="2020-8-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support read_text_file() and print() interface for Python DataStream API.</summary>
      <description>Support print() and read_text_file() interface for Python DataStream API.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-9 01:00:00" id="18864" opendate="2020-8-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support key_by() operation for Python DataStream API</summary>
      <description>Support key_by() operation for Python DataStream API.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.util.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-8-10 01:00:00" id="18878" opendate="2020-8-10 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support dependency management for Python StreamExecutionEnvironment.</summary>
      <description>Add dependency management for StreamExecutionEnvironment when Users need to specified third party dependencies in their DataStream Job. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.util.PythonConfigUtilTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonConfig.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-11 01:00:00" id="18883" opendate="2020-8-11 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support reduce() operation for Python KeyedStream.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.datastream.runtime.operators.python.DataStreamPythonStatelessFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-11 01:00:00" id="18884" opendate="2020-8-11 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add chaining strategy and slot sharing group interfaces for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-11 01:00:00" id="18885" opendate="2020-8-11 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add partitioning interfaces for Python DataStream API.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-11 01:00:00" id="18886" opendate="2020-8-11 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Kafka connectors for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.pyflink.gateway.server.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-11 01:00:00" id="18888" opendate="2020-8-11 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support execute_async for StreamExecutionEnvironment.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.util.DataStreamTestCollectSink.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.util.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-13 01:00:00" id="18916" opendate="2020-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Operations" link(linked to dev/table/tableApi.md) under the "Python API" -&gt; "User Guide" -&gt; "Table API" section</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.user-guide.table.sql.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.sql.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.index.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-13 01:00:00" id="18917" opendate="2020-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Built-in Functions" link (linked to dev/table/functions/systemFunctions.md) under the "Python API" -&gt; "User Guide" -&gt; "Table API" section</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs.page.js.flink.js</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-16 01:00:00" id="1892" opendate="2015-4-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local job execution does not exit.</summary>
      <description>When using the LocalTezEnvironment to run a job from the IDE the job fails to exit after producing data. The following thread seems to run and not allow the job to exit:"Thread-31" #46 prio=5 os_prio=31 tid=0x00007fb5d2c43000 nid=0x5507 runnable &amp;#91;0x0000000127319000&amp;#93; java.lang.Thread.State: RUNNABLE at java.lang.Throwable.fillInStackTrace(Native Method) at java.lang.Throwable.fillInStackTrace(Throwable.java:783) locked &lt;0x000000076dfda130&gt; (a java.lang.InterruptedException) at java.lang.Throwable.&lt;init&gt;(Throwable.java:250) at java.lang.Exception.&lt;init&gt;(Exception.java:54) at java.lang.InterruptedException.&lt;init&gt;(InterruptedException.java:57) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1220) at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335) at java.util.concurrent.PriorityBlockingQueue.take(PriorityBlockingQueue.java:545) at org.apache.tez.dag.app.rm.LocalTaskSchedulerService$AsyncDelegateRequestHandler.processRequest(LocalTaskSchedulerService.java:322) at org.apache.tez.dag.app.rm.LocalTaskSchedulerService$AsyncDelegateRequestHandler.run(LocalTaskSchedulerService.java:316) at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-13 01:00:00" id="18926" opendate="2020-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Environment Variables" document under the "Python API" -&gt; "User Guide" -&gt; "Table API" section</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-13 01:00:00" id="18930" opendate="2020-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Hive Dialect" page of "Hive Integration" into Chinese</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.hive.dialect.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-13 01:00:00" id="18936" opendate="2020-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation about user-defined aggregate functions</summary>
      <description>The documentation needs an update because all functions support the new type inference now.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-13 01:00:00" id="18937" opendate="2020-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Environment Setup" section to the "Installation" document</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.installation.zh.md</file>
      <file type="M">docs.dev.python.installation.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-14 01:00:00" id="18943" opendate="2020-8-14 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support CoMapFunction for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.datastream.runtime.operators.python.DataStreamPythonStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.dev.glibc.version.fix.h</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-14 01:00:00" id="18944" opendate="2020-8-14 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support JDBC connector for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-14 01:00:00" id="18945" opendate="2020-8-14 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support CoFlatMap for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-14 01:00:00" id="18947" opendate="2020-8-14 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support partitionCustom() operation for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperatorBase.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.beam.ProcessPythonEnvironmentManager.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-14 01:00:00" id="18948" opendate="2020-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end to end test for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-14 01:00:00" id="18949" opendate="2020-8-14 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Streaming File Sink for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-python.pyflink.common.tests.test.serialization.schemas.py</file>
      <file type="M">flink-python.pyflink.common.serialization.schemas.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-14 01:00:00" id="18950" opendate="2020-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Operations in Python DataStream API.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.index.zh.md</file>
      <file type="M">docs.dev.stream.operators.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-14 01:00:00" id="18951" opendate="2020-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for dependency management in Python DataStream API.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-14 01:00:00" id="18952" opendate="2020-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 10 minutes to DataStream API documentation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1,1.12.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.python.datastream.data.types.md</file>
      <file type="M">docs.content.zh.docs.dev.python.datastream.data.types.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-18 01:00:00" id="18988" opendate="2020-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Continuous query with LATERAL and LIMIT produces wrong result</summary>
      <description>I was trying out the example queries provided in this blog post: https://materialize.io/lateral-joins-and-demand-driven-queries/ to check if Flink supports the same and found that the queries were translated and executed but produced the wrong result.I used the SQL Client and Kafka (running at kafka:9092) to store the table data. I executed the following statements:-- create cities tableCREATE TABLE cities ( name STRING NOT NULL, state STRING NOT NULL, pop INT NOT NULL) WITH ( 'connector' = 'kafka', 'topic' = 'cities', 'properties.bootstrap.servers' = 'kafka:9092', 'properties.group.id' = 'mygroup', 'scan.startup.mode' = 'earliest-offset', 'format' = 'json');-- fill cities tableINSERT INTO cities VALUES ('Los_Angeles', 'CA', 3979576), ('Phoenix', 'AZ', 1680992), ('Houston', 'TX', 2320268), ('San_Diego', 'CA', 1423851), ('San_Francisco', 'CA', 881549), ('New_York', 'NY', 8336817), ('Dallas', 'TX', 1343573), ('San_Antonio', 'TX', 1547253), ('San_Jose', 'CA', 1021795), ('Chicago', 'IL', 2695598), ('Austin', 'TX', 978908);-- execute querySELECT state, name FROM (SELECT DISTINCT state FROM cities) states, LATERAL ( SELECT name, pop FROM cities WHERE state = states.state ORDER BY pop DESC LIMIT 3 );-- resultstate name CA Los_Angeles NY New_York IL Chicago-- expected resultstate | name------+-------------TX    | DallasAZ    | PhoenixIL    | ChicagoTX    | HoustonCA    | San_JoseNY    | New_YorkCA    | San_DiegoCA    | Los_AngelesTX    | San_AntonioAs you can see from the query result, Flink computes the top3 cities over all states, not for every state individually. Hence, I assume that this is a bug in the query optimizer or one of the rewriting rules.There are two valid ways to solve this issue: Fixing the rewriting rules / optimizer (obviously preferred) Disabling this feature and throwing an exception</description>
      <version>1.11.1</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-19 01:00:00" id="18998" opendate="2020-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>No watermark is shown on Flink UI when ProcessingTime is used</summary>
      <description>As stated in the subject, no watermark is shown on Flink UI when ProcessingTime is used, see the attached screenshot. It is better to be more specific, like "Watermarks are only available if EventTime is used." </description>
      <version>1.11.1,1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.humanize-watermark.pipe.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-20 01:00:00" id="19005" opendate="2020-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document JDBC drivers as source of Metaspace leaks</summary>
      <description>Hi !Im running a 1.11.1 flink cluster, where I execute batch jobs made with DataSet API.I submit these jobs every day to calculate daily data.In every execution, cluster's used metaspace increase by 7MB and its never released.This ends up with an OutOfMemoryError caused by Metaspace every 15 days and i need to restart the cluster to clean the metaspacetaskmanager.memory.jvm-metaspace.size is set to 512mbAny idea of what could be causing this metaspace grow and why is it not released ? =================================================== Summary ======================================================================================Case 1, reported by gestevez: Flink 1.11.1 Java 11 Maximum Metaspace size set to 512mb Custom Batch job, submitted daily Requires restart every 15 days after an OOM Case 2, reported by Echo Lee: Flink 1.11.0 Java 11 G1GC WordCount Batch job, submitted every second / every 5 minutes eventually fails TaskExecutor with OOMCase 3, reported by DaDaShen Flink 1.11.0 Java 11 WordCount Batch job, submitted every 5 seconds growing Metaspace, eventually OOM </description>
      <version>1.11.1</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.debugging.classloading.zh.md</file>
      <file type="M">docs.monitoring.debugging.classloading.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-17 01:00:00" id="1902" opendate="2015-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web interface reports false (the default) jobmanager.rpc.port on YARN</summary>
      <description>Running Flink as YARN session mode I was completely confused by the web interface reporting a false jobmanager.rpc.port (the default).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMaster.scala</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-9-1 01:00:00" id="19109" opendate="2020-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split Reader eats chained periodic watermarks</summary>
      <description>Attempting to generate watermarks chained to the Split Reader / ContinuousFileReaderOperator, as inSingleOutputStreamOperator&lt;Event&gt; results = env .readTextFile(...) .map(...) .assignTimestampsAndWatermarks(bounded) .keyBy(...) .process(...);leads to the Watermarks failing to be produced. Breaking the chain, via disableOperatorChaining() or a rebalance, works around the bug. Using punctuated watermarks also avoids the issue.Looking at this in the debugger reveals that timer service is being prematurely quiesced.In many respects this is FLINK-7666 brought back to life.The problem is not present in 1.9.3.There's a minimal reproducible example in https://github.com/alpinegizmo/flink-question-001/tree/bug.</description>
      <version>1.10.0,1.10.1,1.10.2,1.11.0,1.11.1</version>
      <fixedVersion>1.10.3,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-1 01:00:00" id="19110" opendate="2020-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flatten current PyFlink documentation structure</summary>
      <description>The navigation for this entire section is overly complex. I would much rather see something flatter, like this: Python API Installation Table API Tutorial Table API User's Guide DataStream API User's Guide FAQ</description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.python.table.api.zh.md</file>
      <file type="M">docs.try-flink.python.table.api.md</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.sql.alter.zh.md</file>
      <file type="M">docs.dev.table.sql.alter.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.index.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.types.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.types.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.config.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.config.md</file>
      <file type="M">docs.dev.python.user-guide.table.metrics.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.metrics.md</file>
      <file type="M">docs.dev.python.user-guide.table.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.index.md</file>
      <file type="M">docs.dev.python.user-guide.table.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.dependency.management.md</file>
      <file type="M">docs.dev.python.user-guide.table.conversion.of.pandas.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.conversion.of.pandas.md</file>
      <file type="M">docs.dev.python.user-guide.table.built.in.functions.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.built.in.functions.md</file>
      <file type="M">docs.dev.python.user-guide.table.10.minutes.to.table.api.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.10.minutes.to.table.api.md</file>
      <file type="M">docs.dev.python.user-guide.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.index.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.index.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.data.types.zh.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.data.types.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.table.api.tutorial.zh.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.table.api.tutorial.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.index.zh.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.index.md</file>
      <file type="M">docs.dev.python.getting-started.installation.zh.md</file>
      <file type="M">docs.dev.python.getting-started.installation.md</file>
      <file type="M">docs.dev.python.getting-started.index.zh.md</file>
      <file type="M">docs.dev.python.getting-started.index.md</file>
      <file type="M">docs.dev.python.faq.zh.md</file>
      <file type="M">docs.dev.python.faq.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-2 01:00:00" id="19121" opendate="2020-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid accessing HDFS frequently in HiveBulkWriterFactory</summary>
      <description>In HadoopPathBasedBulkWriter, getSize will invoke `FileSystem.exists` and `FileSystem.getFileStatus`, but it is invoked per record.There will be lots of visits to HDFS, may make HDFS pressure too high.</description>
      <version>1.11.1,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-9-9 01:00:00" id="19170" opendate="2020-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parameter naming error</summary>
      <description/>
      <version>1.11.1</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.CsvTableSinkFactoryBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-14 01:00:00" id="19213" opendate="2020-9-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the Chinese documentation</summary>
      <description>We should update the Chinese documentation with the changes introduced in FLINK-18802</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.avro-confluent.zh.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-9-15 01:00:00" id="19229" opendate="2020-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce the PythonStreamGroupAggregateOperator for Python UDAF</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTableStatelessPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.RowDataPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.AbstractPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractRowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractRowDataPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonTableFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonStatelessFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamStatelessPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamDataStreamStatelessPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-15 01:00:00" id="19243" opendate="2020-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Snakeyaml to 1.27</summary>
      <description>CVE-2017-18640</description>
      <version>None</version>
      <fixedVersion>shaded-12.0,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch7-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch6-test.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-18 01:00:00" id="19284" opendate="2020-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about how to use Python UDF in the Java Table API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.md</file>
      <file type="M">docs.dev.python.faq.zh.md</file>
      <file type="M">docs.dev.python.faq.md</file>
      <file type="M">docs.dev.python.datastream-api-users-guide.dependency.management.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-26 01:00:00" id="19421" opendate="2020-9-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Python UDAF in streaming mode</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.PythonAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedAggFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecGroupAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecPythonAggregateRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.PythonFunctionInfo.java</file>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.aggregate.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
      <file type="M">flink-python.pyflink.common.types.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-10-29 01:00:00" id="19453" opendate="2020-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate old source and sink interfaces</summary>
      <description>Deprecate all interfaces and classes that are not necessary anymore with FLIP-95.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.tsextractors.TimestampExtractor.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.TableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.RowtimeAttributeDescriptor.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.ProjectableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.PartitionableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.NestedFieldsProjectableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.LookupableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.LimitableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.FilterableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.FieldComputer.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.DefinedRowtimeAttributes.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.DefinedProctimeAttribute.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.DefinedFieldMapping.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sinks.TableSink.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sinks.PartitionableTableSink.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sinks.OverwritableTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.StreamTableSource.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.InputFormatTableSource.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.UpsertStreamTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.StreamTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.RetractStreamTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.OutputFormatTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.AppendStreamTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.StreamTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.StreamTableSinkFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.BatchTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.BatchTableSinkFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-10-30 01:00:00" id="19475" opendate="2020-9-30 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement a timer service that holds a single key at a time</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.StreamOperatorSnapshotRestoreTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimeServiceManagerImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-15 01:00:00" id="19667" opendate="2020-10-15 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add integration with AWS Glue</summary>
      <description>AWS Glue is releasing new features for the AWS Glue Data Catalog. This request is to add a new format to launch an integration for Apache Flink with AWS Glue Data Catalog</description>
      <version>1.11.0,1.11.1,1.11.2,1.11.3</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-20 01:00:00" id="19736" opendate="2020-10-20 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement the `SinkTransformation`</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.TestSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.SimpleTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.TypeInformationTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-20 01:00:00" id="19737" opendate="2020-10-20 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce TableOperatorWrapperGenerator to translate transformation DAG in a multiple-input node to TableOperatorWrapper DAG</summary>
      <description>Transformation is not serializable, while StreamOperatorFactory is. We need to introduce another class (named TableOperatorWrapper) to store the information of a Transformation, and introduce a utility class (named TableOperatorWrapper) to convert the Transformation DAG to TableOperatorWrapper DAG.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.batch.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testExplainMultipleInput.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.MultipleInputRel.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-21 01:00:00" id="19750" opendate="2020-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deserializer is not opened in Kafka consumer when restoring from state</summary>
      <description>When a job using Kafka consumer is recovered from a checkpoint or savepoint, the open method of the record deserializer is not called. This is possibly because this.deserializer.open is put into the else clause by mistake, which will only be called if the job has a clean start. </description>
      <version>1.11.0,1.11.1,1.11.2</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-17 01:00:00" id="20642" opendate="2020-12-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce InternalRow to optimize Row used in Python UDAF</summary>
      <description>InternalRow is a cython classcdef enum InternalRowKind: INSERT = 0 UPDATE_BEFORE = 1 UPDATE_AFTER = 2 DELETE = 3cdef class InternalRow: cdef list values cdef InternalRowKind row_kind</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.fast.pxd</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-22 01:00:00" id="2083" opendate="2015-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure high quality docs for FlinkML in 0.9</summary>
      <description>As defined in our vision for FlinkML, providing high-quality documentation is a primary goal for us.This issue concerns the docs that will be included in 0.9, and will track improvements and additions for the release.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs..includes.latex.commands.html</file>
      <file type="M">docs.libs.ml.optimization.md</file>
      <file type="M">docs.libs.ml.multiple.linear.regression.md</file>
      <file type="M">docs.libs.ml.distance.metrics.md</file>
      <file type="M">docs.libs.ml.als.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-6-5 01:00:00" id="2174" opendate="2015-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow comments in &amp;#39;slaves&amp;#39; file</summary>
      <description>Currently, each line in slaves in interpreded as a host name. Scripts should skip lines starting with '#'. Also allow for comments at the end of a line and skip empty lines.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster-streaming.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-4-16 01:00:00" id="22327" opendate="2021-4-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE exception happens if it throws exception in finishBundle during job shutdown</summary>
      <description>Currently, if it throws exceptions in finishBundle during job shutdown, NPE exception may happen if time-based finish bundle is scheduled. It caused the actual exception isn't propagate. This makes users very difficult to trouble shot the problem.See http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-called-already-closed-and-NullPointerException-td42997.html for more details.</description>
      <version>None</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-6-19 01:00:00" id="22712" opendate="2021-5-19 00:00:00" resolution="Done">
    <buginformation>
      <summary>Support accessing Row fields by attribute name in PyFlink Row-based Operation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonMapMergeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PythonMapMergeRule.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.AbstractPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.RowDataPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.row.based.operation.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">docs.content.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.docs.dev.python.table.operations.row.based.operations.md</file>
      <file type="M">docs.content.zh.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.operations.row.based.operations.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-6 01:00:00" id="22893" opendate="2021-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Leader retrieval fails with NoNodeException</summary>
      <description>The NodeCache used by the LeaderElection-/-RetrievalDrivers ensures that parents to the observed node exists by regularly issuing mkdir calls. This operation can fail if concurrently the HA data is being cleaned up, which results in curator throwing an unhandled exception which crashes the TM.https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18700&amp;view=logs&amp;j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&amp;t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc&amp;l=4382</description>
      <version>1.11.1,1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.java</file>
    </fixedFiles>
  </bug>
</bugrepository>