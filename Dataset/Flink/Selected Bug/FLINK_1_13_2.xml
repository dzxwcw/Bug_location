<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2021-3-19 01:00:00" id="21885" opendate="2021-3-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minor Typo Fix in JDBC Connector Documentation</summary>
      <description>There currently exists a minor typo within the JDBC Datastream documentation:A JDBC batch is executed as soon as one of the following condition is true: the configured batch interval time is elapsed the maximum batch size is reached a Flink checkpoint has startedSince it's plural, condition should be changed to conditions. </description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.jdbc.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-20 01:00:00" id="21887" opendate="2021-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add show views test in CliClientITCase</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.catalog.database.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutorBuilder.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-20 01:00:00" id="21888" opendate="2021-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maintain our own ASTNode class</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.SelectClauseASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.IdentifiersASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserStorageFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserBaseSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTParseUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTParseDriver.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTLexer.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTHintParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.FromClauseASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserContext.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserAuthorizationParseUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserASTBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.desc.HiveParserCreateViewDesc.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.desc.CreateTableASDesc.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-6-1 01:00:00" id="22822" opendate="2021-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop usages of legacy planner in JDBC module</summary>
      <description>Remove references to flink-table-planner in the JDBC module.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-7-1 01:00:00" id="2305" opendate="2015-7-1 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add documenation about Storm compatibility layer</summary>
      <description>Storm compatibility layer is currently no documented at the project web site.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.StormBoltFileSink.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.SimpleOutputFormatter.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.excamation.ExclamationTopology.java</file>
      <file type="M">docs..includes.navbar.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-21 01:00:00" id="23073" opendate="2021-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix space handling in Row CSV timestamp parser</summary>
      <description>FLINK-21947 Added support for TIMESTAMP_LTZ in the CSV format by replacing java.sql.Timestamp.valueOf with java.time.LocalDateTime.parse. Timestamp.valueOf internally calls `trim()` on the string before parsing while LocalDateTime.parse does not. This caused a breaking change where the CSV format can no longer parse timestamps of CSV's with spaces after the delimiter. We should manually re-add the call to trim to revert the behavior.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDeSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvToRowDataConverters.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-9-21 01:00:00" id="23462" opendate="2021-7-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the abfs documentation to chinese</summary>
      <description>Translate the documentation changes that were made in this PR to chinese https://github.com/apache/flink/pull/16559/</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.deployment.filesystems.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.azure.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-9 01:00:00" id="23686" opendate="2021-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaSource metric "commitsSucceeded" should count per-commit instead of per-partition</summary>
      <description>Currently if a successful offset commit includes multiple topic partition (let's say 4), the counter will increase by 4 instead of 1</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.metrics.KafkaSourceReaderMetricsTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.metrics.KafkaSourceReaderMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-16 01:00:00" id="23789" opendate="2021-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary setTotalOrderForSeek for Rocks iterator</summary>
      <description>From FLINK-17800, we have to explicitly add setTotalOrderForSeek to ensure data correctness if user choose to set OptimizeForPointLookup.However, this is unnecessary since we upgraded the RocksDB version (please refer to rocksDB PR) which removes the prefix extractor and hash-based indexing.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateMisuseOptionTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResource.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBIncrementalCheckpointUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-8-18 01:00:00" id="23845" opendate="2021-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify metric deletion guarantees for Prometheus PushGateway reporter on shutdown</summary>
      <description>see https://issues.apache.org/jira/browse/FLINK-20691 .  whatever the problem has always existed, we should avoid other guys met it</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.main.java.org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.prometheus.push.gateway.reporter.configuration.html</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-8-19 01:00:00" id="23871" opendate="2021-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dispatcher should handle finishing job exception when recover</summary>
      <description>The exception during run recovery job will trigger fatal error which is introduced in https://issues.apache.org/jira/browse/FLINK-9097.  If a job have reached a finished status. But crash at clean up phase or any other post phase. When recover job, it may recover a job in RunningJobsRegistry.JobSchedulingStatus.DONE status, this may lead to the dispatcher fatal again. I think we should deal with the  RunningJobsRegistry.JobSchedulingStatus.DONE with special exception like JobFinishingException, which represents the job/master crashed in job finishing phase. And only do the clean up work for this exception</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.JobMasterBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcessTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.OnCompletionActions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-21 01:00:00" id="23906" opendate="2021-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase akka.ask.timeout for tests using the MiniCluster</summary>
      <description>We have seen over the last couple of weeks/months an increased number of test failures because of TimeoutException that were triggered because the akka.ask.timeout was exceeded. The reason for this was that on our CI infrastructure it can happen that there are pauses of more than 10s (not sure about the exact reason) or our infrastructure simply being slow. In order to harden all tests relying on the MiniCluster I propose to increase the akka.ask.timeout to 5 minutes if nothing else has been configured.</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-22 01:00:00" id="23909" opendate="2021-8-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove redundant variables and improve some style format in coding.</summary>
      <description/>
      <version>1.13.2</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.JarHelper.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcDataTypeTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-23 01:00:00" id="23912" opendate="2021-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary "Clearing resource requirements of job"</summary>
      <description>The ResourceManager will print the log each time it receives an empty resource declaration. For deduplication, we need to: At JM side, skip decreasing empty resources. At SlotManager side, does not log if it is already empty.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.ResourceTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-24 01:00:00" id="23929" opendate="2021-8-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chaining optimization doesn&amp;#39;t handle properly for transformations with multiple outputs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizerTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-25 01:00:00" id="23962" opendate="2021-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UpdateKind trait is not propagated properly in changeLog inference for DAG optimizing</summary>
      <description>For sql jobs with multi-sinks, the plan is divided into relNode blocks, changeLog mode should be also inferred among blocks. Currently, updateKind trait is not propagated properly from parent block to child blocks for the following pattern.                              -&gt; block3 block0 -&gt; block1 -&gt; block4             -&gt; block2 In the above example, if block3 requires UB and block2, block4 do not require UB, block1 only contains Calc node.For Agg in block0, UB should be emitted, but the updateKind for block0 is inferred as ONLY_UPDATE_AFTER.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-25 01:00:00" id="23965" opendate="2021-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>E2E do not execute locally on MacOS</summary>
      <description>After FLINK-21346, the e2e tests are no longer executing locally on MacOS. The problem seems to be that the e2e configure a log directory that does not exist and this fails starting a Flink cluster.I suggest to change the directory to the old directory FLINK_DIR/log instead of FLINK_DIR/logs.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-single-test.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-8-26 01:00:00" id="24010" opendate="2021-8-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HybridSource needs to forward checkpoint notifications</summary>
      <description>Since the reader currently swallows notifyCheckpointComplete, offset commit in contained Kafka consumer doesn't happen.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.hybrid.HybridSourceSplitEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.hybrid.HybridSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.hybrid.HybridSourceSplitEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.hybrid.HybridSourceReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-10-27 01:00:00" id="24019" opendate="2021-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separately package Scala-reliant modules</summary>
      <description>Bundle all Scala-reliant modules (flink-scala, flink-streaming-scala, flink-scala-shell) into a separate jar, containing Scala-exclusive dependencies (e.g., Scala itself, Scala extension of Chill). This jar will be added to lib/ by default, but can be removed by users to get a Scala-free experience.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.licenses.LICENSE.scala</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-27 01:00:00" id="24029" opendate="2021-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix "dept_id" typo in SQL "Getting Started" page</summary>
      <description>Current sql in doc of branch release-1.13 is still dep_id and we should fix it to dept_id</description>
      <version>1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.gettingStarted.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.gettingStarted.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-27 01:00:00" id="24031" opendate="2021-8-27 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>I am trying to deploy Flink in kubernetes but when I launch the taskManager in other container I get a Exception</summary>
      <description> I explain here -&gt; https://github.com/apache/flink/pull/17020I have a problem when I try to run Flink in k8s with the follow manifestsI have the following exception JobManager :2021-08-27 09:16:57,917 ERROR akka.remote.EndpointWriter [] - dropping message &amp;#91;class akka.actor.ActorSelectionMessage&amp;#93; for non-local recipient [Actor&amp;#91;akka.tcp://flink@jobmanager-hs:6123/&amp;#93;] arriving at &amp;#91;akka.tcp://flink@jobmanager-hs:6123&amp;#93; inbound addresses are &amp;#91;akka.tcp://flink@cluster:6123&amp;#93; 2021-08-27 09:17:01,255 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request. 2021-08-27 09:17:01,284 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request. 2021-08-27 09:17:10,008 DEBUG akka.remote.transport.netty.NettyTransport [] - Remote connection to &amp;#91;/172.17.0.1:34827&amp;#93; was disconnected because of &amp;#91;id: 0x13ae1d03, /172.17.0.1:34827 :&gt; /172.17.0.23:6123&amp;#93; DISCONNECTED 2021-08-27 09:17:10,008 DEBUG akka.remote.transport.ProtocolStateActor [] - Association between local &amp;#91;tcp://flink@cluster:6123&amp;#93; and remote &amp;#91;tcp://flink@172.17.0.1:34827&amp;#93; was disassociated because the ProtocolStateActor failed: Unknown 2021-08-27 09:17:10,009 WARN akka.remote.ReliableDeliverySupervisor [] - Association with remote system &amp;#91;akka.tcp://flink@172.17.0.24:6122&amp;#93; has failed, address is now gated for &amp;#91;50&amp;#93; ms. Reason: &amp;#91;Disassociated&amp;#93;TaskManager:INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Could not resolve ResourceManager address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_. INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Could not resolve ResourceManager address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_.Best regards,Julio</description>
      <version>1.13.0,1.13.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.idea.vcs.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-27 01:00:00" id="24033" opendate="2021-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Propagate unique keys for fromChangelogStream</summary>
      <description>Similar to FLINK-23915, we are not propagating unique keys for fromChangelogStream because it is not written into statistics.</description>
      <version>1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.DataStreamJavaITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.stats.FlinkStatistic.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.java</file>
      <file type="M">docs.content.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.zh.docs.dev.table.data.stream.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-27 01:00:00" id="24034" opendate="2021-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons-compress to 1.21</summary>
      <description>CVE-2021-35517</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-28 01:00:00" id="24036" opendate="2021-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSL cannot be installed on CI</summary>
      <description># install libssl1.0.0 for netty tcnativewget http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.debsudo apt install ./libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.deb--2021-08-27 20:48:49-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 91.189.91.38, 2001:67c:1562::15, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2021-08-27 20:48:49 ERROR 404: Not Found.</description>
      <version>1.14.0,1.12.5,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-2 01:00:00" id="24120" opendate="2021-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document MALLOC_ARENA_MAX as workaround for glibc memory leak</summary>
      <description>My task will do a savepoint every hour, so every hour will do a savepoint. From the memory monitoring, it can be seen that the memory of each hour will soar up, although the memory will drop a little later, but from every hour From the point of view of the memory peak on the whole point, the memory continues to rise little by little, and eventually rises to the limited memory, which will lead to being killed by k8S  </description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.trouble.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.trouble.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-3 01:00:00" id="24148" opendate="2021-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add bloom filter policy option in RocksDBConfiguredOptions</summary>
      <description>Bloom filter can efficiently enhance the read on RocksDB, especially for the reading among L0 files. (more details see https://github.com/facebook/rocksdb/wiki/RocksDB-Bloom-Filter)</description>
      <version>1.13.2,1.14.1</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.DefaultConfigurableOptionsFactory.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-3 01:00:00" id="24155" opendate="2021-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate documentation for how to configure the CheckpointFailureManager</summary>
      <description>Documentation added in FLINK-23916 should be translated to it's Chinese counterpart. Note that this applies to three separate commits:merged to master as cd01d4c0279merged to release-1.14 as 2e769746bf2merged to release-1.13 as e1a71219454</description>
      <version>1.14.0,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-9-10 01:00:00" id="24248" opendate="2021-9-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-clients dependency missing in Gradle Example</summary>
      <description>The Gradle example on the "Project Configuration" page misses ``` compile "org.apache.flink:flink-clients_${scalaBinaryVersion}:${flinkVersion}"```in order to be able to run the program locally.</description>
      <version>1.13.2</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-13 01:00:00" id="24274" opendate="2021-9-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong parameter order in documentation of State Processor API</summary>
      <description>Wrong order of parameters path and stateBackend in example code of State Processor Api # modifying-savepoints  </description>
      <version>None</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.libs.state.processor.api.md</file>
      <file type="M">docs.content.zh.docs.libs.state.processor.api.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-16 01:00:00" id="24305" opendate="2021-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BatchPandasUDAFITTests.test_over_window_aggregate_function fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24170&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901&amp;l=23011Sep 15 20:40:43 cls = &lt;class 'pyflink.table.tests.test_pandas_udaf.BatchPandasUDAFITTests'&gt;Sep 15 20:40:43 actual = JavaObject id=o8666Sep 15 20:40:43 expected = ['+I[1, 4.3333335, 13, 5.5, 3.0, 3.0, 4.3333335, 8.0, 5.0, 5.0]', '+I[1, 4.3333335, 5, 4.3333335, 3.0, 3.0, 2.5, 4.333....0, 4.0, 2.0]', '+I[2, 2.0, 9, 2.0, 4.0, 4.0, 2.0, 2.0, 4.0, 4.0]', '+I[3, 2.0, 3, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0]']Sep 15 20:40:43 Sep 15 20:40:43 @classmethodSep 15 20:40:43 def assert_equals(cls, actual, expected):Sep 15 20:40:43 if isinstance(actual, JavaObject):Sep 15 20:40:43 actual_py_list = cls.to_py_list(actual)Sep 15 20:40:43 else:Sep 15 20:40:43 actual_py_list = actualSep 15 20:40:43 actual_py_list.sort()Sep 15 20:40:43 expected.sort()Sep 15 20:40:43 assert len(actual_py_list) == len(expected)Sep 15 20:40:43 &gt; assert all(x == y for x, y in zip(actual_py_list, expected))Sep 15 20:40:43 E AssertionError: assert FalseSep 15 20:40:43 E + where False = all(&lt;generator object PyFlinkTestCase.assert_equals.&lt;locals&gt;.&lt;genexpr&gt; at 0x7f792d98b900&gt;)</description>
      <version>1.14.0,1.12.5,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-17 01:00:00" id="24315" opendate="2021-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot rebuild watcher thread while the K8S API server is unavailable</summary>
      <description>In native k8s integration, Flink will try to rebuild the watcher thread if the API server is temporarily unavailable. However, if the jitter is longer than the web socket timeout, the rebuilding of the watcher will timeout and Flink cannot handle the pod event correctly.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.13.3,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-29 01:00:00" id="2432" opendate="2015-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Provide support for custom serialization</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Constants.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.functions.ReduceFunction.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.functions.GroupReduceFunction.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.functions.Function.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.functions.CoGroupFunction.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.connection.Iterator.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.connection.Collector.py</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.Sender.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.Receiver.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-9-18 01:00:00" id="24336" opendate="2021-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink TableEnvironment executes the SQL randomly MalformedURLException with the configuration for &amp;#39;pipeline.classpaths&amp;#39;</summary>
      <description>When I run flink client to submit a python based workflow, I got the MalformedURLException like this:https://gist.github.com/is/faabafc7f8750f3f3161fbb6517ed6ffAfter some debug work, I found the problem is related with TableEvneriontment.execute_sql. The root cause is TableEvenriontment._add_jars_to_j_env_config in pyflink/table/TableEnverionment.py.```if j_configuration.containsKey(config_key): for url in j_configuration.getString(config_key, "").split(";"): jar_urls_set.add(url)```In our case, pipeline.classpaths was set by empty list valuefrom FromProgramOption, so the upper code block willintroduce a empty string ("") into pipeline.classpaths, for example"a.jar;b.jar;;c.jar", and it will cause the according exception.Another problem, the order of string set in python is notdeterminate, so ";".join(jar_urls_set) does NOT keep theclasspaths order. The list is more suiteable in this case.</description>
      <version>1.14.0,1.13.2,1.14.1</version>
      <fixedVersion>1.13.3,1.12.8,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-23 01:00:00" id="24366" opendate="2021-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary/misleading error message about failing restores when tasks are already canceled.</summary>
      <description>The following line is logged in all cases where the restore operation fails. The check whether the task is canceled comes only after that line.The fix would be to move the log line to after the check.Exception while restoring my-stateful-task from alternative (1/1), will retry while more alternatives are available.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.BackendRestorerProcedure.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-26 01:00:00" id="24376" opendate="2021-9-26 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Operator name in OperatorCoordinator should not use chained name</summary>
      <description>Currently the operator name passed to CoordinatedOperatorFactory#getCoordinatorProvider is a chained operator name (e.g. Source -&gt; Map) instead of the name of coordinating operator, which might be misleading. </description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.7,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-27 01:00:00" id="24380" opendate="2021-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink should handle the state transition of the pod from Pending to Failed</summary>
      <description>In K8s, there is five phases in pod's lifecycle: Pending, Running, Secceeded, Failed and Unknown. Currently, Flink does not handle the state transition of the pod from Pending to Failed. If a pod failed from Pending by `OutOfCPU` or `OutOfMem`, it will never be released and Flink keep waiting for it.To fix this issue, Flink should terminate the pod in Failed phase proactively.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.13.3,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPod.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-10-30 01:00:00" id="24431" opendate="2021-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Kinesis][EFO] EAGER registration strategy does not work when job fails over</summary>
      <description>BackgroundThe EFO Kinesis connector will register and de-register stream consumers based on the configured registration strategy. When EAGER is used, the client (usually job manager) will register the consumer and then the task managers will de-register the consumer when job stops/fails. If the job is configured to restart on fail, then the consumer will not exist and the job will continuously fail over.SolutionThe proposal is to not deregister the stream consumer when EAGER is used. The documentation should be updated to reflect this.</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.13.3,1.12.8,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.StreamConsumerRegistrarUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.StreamConsumerRegistrarUtil.java</file>
      <file type="M">docs.content.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kinesis.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-10-12 01:00:00" id="24516" opendate="2021-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modernize Maven Archetype</summary>
      <description>The maven archetypes used by many to start their first Flink application do not reflect the project's current state. Issues: They still bundle the DataSet API and recommend it for batch processing The JavaDoc recommends deprecated APIs </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.StreamingJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.BatchJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.StreamingJob.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.BatchJob.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-13 01:00:00" id="25268" opendate="2021-12-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support task manager node-label in Yarn deployment</summary>
      <description>Now Flink only support application level node label, it's necessary to introduce task manager level node-label on Yarn deployment.Why we need it?Sometimes we will implement Flink to support deep learning payload using GPU, so if having this feature, job manager and task managers could use different nodes.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerDriverTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManagerDriver.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-21 01:00:00" id="2558" opendate="2015-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Streaming Connector for Elasticsearch</summary>
      <description>We should add a sink that can write to Elasticsearch. A source does not seem necessary because Elasticsearch would mostly be used for accessing results, for example using a dashboard.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.pom.xml</file>
      <file type="M">docs.apis.streaming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-4 01:00:00" id="26482" opendate="2022-3-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support WindowedStream.reduce in Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.window.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">docs.content.docs.dev.datastream.operators.windows.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.windows.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-7 01:00:00" id="26506" opendate="2022-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support StreamExecutionEnvironment.registerCachedFile in Python DataStream API</summary>
      <description>This API is missed in Python DataStream API.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-8-8 01:00:00" id="27951" opendate="2022-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the "Debugging Classloading" page into Chinese</summary>
      <description>The page "Debugging Classloading" needs to be translated into Chinese.I'm willing to work on this issue.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.ops.debugging.debugging.classloading.md</file>
    </fixedFiles>
  </bug>
  
</bugrepository>