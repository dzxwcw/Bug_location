<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2021-9-11 01:00:00" id="22198" opendate="2021-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaTableITCase hang.</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16287&amp;view=logs&amp;j=c5f0071e-1851-543e-9a45-9ac140befc32&amp;t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&amp;l=6625There is no any artifacts.</description>
      <version>1.14.0,1.12.4</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-5-21 01:00:00" id="22747" opendate="2021-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update commons-io to 2.8</summary>
      <description>commons-io 2.7 has known vulnerabilities that are detected in Flink by some tools. Even though it is unlikely that we use the mentioned class. We should upgrade it to make the tools happy. Context:VULNDB-239195"Vendor Specific News/Changelog Entryhttps://commons.apache.org/proper/commons-io/changes-report.html#a2.8.0Vendor Specific Solution URLhttps://github.com/apache/commons-io/commit/0de91c048fb575b9e7906e966a4428574fd03695Vendor Specific Solution URLhttps://github.com/apache/commons-io/commit/97ae01c95837f50a2e9be34c370b271c4d8fc88bBug Trackerhttps://issues.apache.org/jira/browse/IO-675"</description>
      <version>1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-28 01:00:00" id="22802" opendate="2021-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Fabric8 Kubernetes Client to &gt;= 5.X</summary>
      <description>Hi All, Currently, Flink is using version 4.9.2 of the Fabric8 Kubernetes Client which does not support new versions of the Kubernetes API such as 1.19 or 1.20 as pointed out by their Compatibility Matrix which is support in 5.4.0 or above.As far as I have seen in the Flink documentation, Flink supports `Kubernetes &gt;= 1.9.` but due to this dependency, it might not be the case. Is there a plan to update this dependency?What is the plan moving forwards when new versions of Kubernetes are released?I am raising this because I have been testing Flink HA Session Cluster on Kubernetes 1.19 and 1.20 and I have encountered some frequent errors that force the JM pods to restart or even result in an unrecoverable state.Thanks!</description>
      <version>1.13.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.MixedDispatcher.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClientTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.TestingFlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformerITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPodsWatcherTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionAndRetrievalITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase.java</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.AbstractKubernetesWatcher.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.KubernetesSharedWatcher.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback-session.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback-console.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-session.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-console.properties</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-6-31 01:00:00" id="22815" opendate="2021-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable unaligned checkpoints for broadcast partitioning</summary>
      <description>Broadcast partitioning can not work with unaligned checkpointing. Thereare no guarantees that records are consumed at the same rate in allchannels. This can result in some tasks applying state changescorresponding to a certain broadcasted event while others don't. In turnupon restore, it may lead to an inconsistent state.</description>
      <version>1.13.1,1.12.4</version>
      <fixedVersion>1.11.4,1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.BroadcastPartitioner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.SubtaskStateMapperTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.SubtaskStateMapper.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-6-6 01:00:00" id="22886" opendate="2021-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thread leak in RocksDBStateUploader</summary>
      <description>ExecutorService in RocksDBStateUploader is not shut down, which may leak thread when tasks fail.BTW, we should name the thread group in ExecutorService, otherwise what we see in the stack, is a lot of threads named with pool-m-thread-n like this: </description>
      <version>1.11.3,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateDataTransfer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-9 01:00:00" id="22939" opendate="2021-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generalize JDK switch in azure setup</summary>
      <description>Our current azure setup makes it a bit difficult to switch to a different JDK because the "jdk" parameter is only evaluated if it is set to "jdk11".Instead, we could generalize this a bit so that it is always evaluated, such that if the image contains the JDK (under some expected location) one can just specify jdk: 14.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-10 01:00:00" id="22952" opendate="2021-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>docs_404_check fail on azure due to ruby version not available</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18852&amp;view=logs&amp;j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&amp;t=404fcc1b-71ae-54f6-61c8-430a6aeff2b5Starting: UseRubyVersion==============================================================================Task : Use Ruby versionDescription : Use the specified version of Ruby from the tool cache, optionally adding it to the PATHVersion : 0.186.0Author : Microsoft CorporationHelp : https://docs.microsoft.com/azure/devops/pipelines/tasks/tool/use-ruby-version==============================================================================##[error]Version spec = 2.4 for architecture %25s did not match any version in Agent.ToolsDirectory.Available versions: /opt/hostedtoolcache2.5.9,2.6.7,2.7.3,3.0.1If this is a Microsoft-hosted agent, check that this image supports side-by-side versions of Ruby at https://aka.ms/hosted-agent-software.If this is a self-hosted agent, see how to configure side-by-side Ruby versions at https://go.microsoft.com/fwlink/?linkid=2005989.Finishing: UseRubyVersion</description>
      <version>1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-10 01:00:00" id="22957" opendate="2021-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rank TTL should use enableTimeToLive of state instead of timer</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.TopNFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyFirstNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyFirstNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AbstractTopNFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-10 01:00:00" id="22963" opendate="2021-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The description of taskmanager.memory.task.heap.size in the official document is incorrect</summary>
      <description>When I studied the memory model of TaskManager, I found that there is a problem in the official document, which is the description of taskmanager.memory.task.heap.size is incorrect.According to the official memory model, I think the correct description should be that task Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for tasks. If not specified, it will be derived as Total Flink Memory minus Framework Heap Memory, Framework Off-Heap Heap Memory, Task Off-Heap Memory, Managed Memory and Network Memory.However, in the official document, the Framework Off-Heap Heap Memory should be subtracted.</description>
      <version>1.14.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.memory.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.memory.section.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-10 01:00:00" id="22964" opendate="2021-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Connector-base exposes dependency to flink-core.</summary>
      <description>Connectors get shaded into the user jar and as such should contain no unnecessary dependencies to flink. However, connector-base is exposing `flink-core` which then by default gets shaded into the user jar. Except for 6MB of extra size, the dependency also causes class loading issues, when `classloader.parent-first-patterns` does not include `o.a.f`.Fix is to make `flink-core` provided in `connector-base`.</description>
      <version>1.14.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-15 01:00:00" id="22994" opendate="2021-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the performance of nesting udf invoking</summary>
      <description>BackGroundIn some nesting udf invoking cases, Flink convert the udf result to external object and then convert to internalOrNull object as params for next udf invoking. The performance of some converter is poor.   Test ParamsSource = Kafka, Schema = PB with snappy; Flink Slot = 1; taskmanager.memory.process.size=4g; Linux Core = Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz UDF Introduction: ipip:  input: int ip, output: map ip_info, map size = 14. ip_2_country: input map ip_info, output: string country. ip_2_region: input  map ip_info, output: string region. ip_2_isp_domain: input  map ip_info, output: string isp. ip_2_timezone: input map ip_info, output: string timezone.Performance Compare with MapMapConverterThe throughput of nesting udf invoking with MapMapConverter(5 times): 41.42 k/sGoalFor some cases, skip toInternalOrNull &amp; toExternal, Use the udf result directly.Performance Compare without MapMapConverterThe throughput of nesting udf invoking without MapMapConverter: 174.41 k/s </description>
      <version>1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-16 01:00:00" id="23004" opendate="2021-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix misleading log</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-22 01:00:00" id="23074" opendate="2021-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>There is a class conflict between flink-connector-hive and flink-parquet</summary>
      <description>flink-connector-hive 2.3.6 include parquet-hadoop 1.8.1 version but flink-parquet include 1.11.1.org.apache.parquet.hadoop.example.GroupWriteSupport is different.</description>
      <version>1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-6-23 01:00:00" id="23119" opendate="2021-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the issue that the exception that General Python UDAF is unsupported is not thrown in Compile Stage.</summary>
      <description/>
      <version>1.13.1,1.12.4</version>
      <fixedVersion>1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.PythonOverWindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.PythonGroupWindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalOverAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalPythonWindowAggregateRule.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2021-7-7 01:00:00" id="23297" opendate="2021-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Protobuf to 3.17.3</summary>
      <description>In order to support compilation with ARM (e.g. Apple M1 chip), we need to bump our Protobuf dependency to version 3.17.3.</description>
      <version>1.14.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-26 01:00:00" id="23493" opendate="2021-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>python tests hang on Azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20898&amp;view=logs&amp;j=821b528f-1eed-5598-a3b4-7f748b13f261&amp;t=4fad9527-b9a5-5015-1b70-8356e5c91490&amp;l=22829</description>
      <version>1.14.0,1.13.1,1.12.4,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.boot.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-12-15 01:00:00" id="24290" opendate="2021-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support STRUCTURED_TYPE for JSON_OBJECT / JSON_ARRAY</summary>
      <description>In FLINK-16203 we excluded this because we run into a limitation with fromValues when testing it. I will apply the patch that should implement this, but we need to find a way to make the test work.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.JsonFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.JsonGenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.SpecificInputTypeStrategies.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-3-7 01:00:00" id="24474" opendate="2021-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Standalone clusters should bind to localhost by default</summary>
      <description>By default the REST endpoints bind to 0.0.0.0.This is fine for docker use-cases as it simplifies the setup and the API isn't reachable unless the user explicitly enables that via docker.However, for standalone clusters this is a different story, and it is currently too easy for users to accidentally expose their clusters to the outside world.We should set the bind address by default to localhost, and change the docker-scripts to set this to 0.0.0.0 .</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.ConnectionUtils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnEntrypointUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.FlinkConfMountDecorator.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.container.FlinkContainersBuilder.java</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-21 01:00:00" id="26270" opendate="2022-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink SQL write data to kafka by CSV format , decimal type was converted to scientific notation</summary>
      <description>Source：Oraclefield type：decimal Sink:：kafkafield type：decimalformat：CSV Cannot set not to convert to scientific notation </description>
      <version>1.12.4</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatOptions.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvCommons.java</file>
      <file type="M">docs.content.docs.connectors.table.formats.csv.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.formats.csv.md</file>
    </fixedFiles>
  </bug>
</bugrepository>