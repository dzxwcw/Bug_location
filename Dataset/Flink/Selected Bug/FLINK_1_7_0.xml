<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2018-11-31 01:00:00" id="10009" opendate="2018-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the casting problem for function TIMESTAMPADD in Table</summary>
      <description>There seems to be a bug in TIMESTAMPADD function. For example, TIMESTAMPADD(MINUTE, 1, DATE '2016-06-15') throws a ClassCastException ( java.lang.Integer cannot be cast to java.lang.Long). Actually, it tries to cast an integer date to a long timestamp in RexBuilder.java:1524 - return TimestampString.fromMillisSinceEpoch((Long) o).</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-3-6 01:00:00" id="10076" opendate="2018-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite dependency to 1.18</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.QueryDecorrelationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.TimeIndicatorRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.SqlValidatorImpl.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-9 01:00:00" id="10115" opendate="2018-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Content-length limit is also applied to FileUploads</summary>
      <description>Uploading jar files via WEB UI not working. After initializing upload... it only shows saving... and file never shows up on UI to be able to submit it</description>
      <version>1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.MultipartUploadResource.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-11-14 01:00:00" id="10142" opendate="2018-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce synchronization overhead for credit notifications</summary>
      <description>When credit-based flow control was introduced, we also added some checks and optimisations for uncommon code paths that make common code paths unnecessarily more expensive, e.g. checking whether a channel was released before forwarding a credit notification to Netty. Such checks would have to be confirmed by the Netty thread anyway and thus only add additional load for something that happens only once (per channel).</description>
      <version>1.5.0,1.5.1,1.5.2,1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClient.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-16 01:00:00" id="10164" opendate="2018-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for resuming from savepoints to StandaloneJobClusterEntrypoint</summary>
      <description>The StandaloneJobClusterEntrypoint should support to resume from a savepoint/checkpoint. I suggest to introduce an optional command line parameter for specifying the savepoint/checkpoint path.</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPointTest.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfigurationParserFactoryTest.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfigurationParserFactory.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfiguration.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-10-24 01:00:00" id="10209" opendate="2018-8-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude jdk.tools dependency from hadoop when running with java 9</summary>
      <description>hadoop-common has a jdk.tools dependency which cannot be resolved on java 9. At least for compiling we have to exclude this dependency.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-27 01:00:00" id="10227" opendate="2018-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove usage of javax.xml.bind.DatatypeConverter</summary>
      <description>In java 9 javax.xml.bind.DatatypeConverter is no longer accessible by default. Since this calss is only used in 3 instances (and only the single method parseHexBinary) we should replace it with another implementation.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.TriggerId.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobVertexID.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.JobID.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-10-29 01:00:00" id="10253" opendate="2018-8-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run MetricQueryService with lower priority</summary>
      <description>We should run the MetricQueryService with a lower priority than the main Flink components. An idea would be to start the underlying threads with a lower priority.</description>
      <version>1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.util.MetricUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.akka.AkkaUtilsTest.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">docs..includes.generated.metric.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-9-30 01:00:00" id="10263" opendate="2018-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>User-defined function with LITERAL paramters yields CompileException</summary>
      <description>When using a user-defined scalar function only with literal parameters, a CompileException is thrown. For exampleSELECT myFunc(CAST(40.750444 AS FLOAT), CAST(-73.993475 AS FLOAT))public class MyFunc extends ScalarFunction { public int eval(float lon, float lat) { // do something }}results in [ERROR] Could not execute SQL statement. Reason:org.codehaus.commons.compiler.CompileException: Line 5, Column 10: Cannot determine simple type name "com"The problem is probably caused by the expression reducer because it disappears if a regular attribute is added to a parameter expression.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-5 01:00:00" id="10282" opendate="2018-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide separate thread-pool for REST endpoint</summary>
      <description>The REST endpoints currently share their thread-pools with the RPC system, which can cause the Dispatcher to become unresponsive if the REST parts are overloaded.</description>
      <version>1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ExecutorThreadFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.SessionRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.JobRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
      <file type="M">docs..includes.generated.rest.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-6 01:00:00" id="10291" opendate="2018-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate JobGraph with fixed/configurable JobID in StandaloneJobClusterEntrypoint</summary>
      <description>The StandaloneJobClusterEntrypoint currently generates the JobGraph from the user code when being started. Due to the nature of how the JobGraph is generated, it will get a random JobID assigned. This is problematic in case of a failover because then, the JobMaster won't be able to detect the checkpoints. In order to solve this problem, we need to either fix the JobID assignment or make it configurable.</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plan.StreamingPlan.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.ClassPathJobGraphRetrieverTest.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.ClassPathJobGraphRetriever.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgramUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-6 01:00:00" id="10295" opendate="2018-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tokenisation of Program Args resulting in unexpected results</summary>
      <description>We were upgrading from Flink 1.4 to 1.6. At present we have a jar which takes all the details to run the job as program args against a jarid, including sql query and kafka details. In version 1.5 the program args are tokenised as a result single quote (') and double quote(") are stripped from the arguments. This results in malformed args.Attached a sample request for reference.</description>
      <version>1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRequestUtils.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ProgramArgsQueryParameter.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunMessageParameters.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanMessageParameters.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-11-17 01:00:00" id="10357" opendate="2018-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming File Sink end-to-end test failed with mismatch</summary>
      <description>The Streaming File Sink end-to-end test failed on an Amazon instance with the following result: FAIL File Streaming Sink: Output hash mismatch. Got f2000bbc18a889dc8ec4b6f2b47bf9f5, expected 6727342fdd3aae2129e61fc8f433fb6f.head hexdump of actual:0000000 0 \n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n0000010 8 \n 9 \n0000014</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.file.sink.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-9-22 01:00:00" id="10393" opendate="2018-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy entrypoints from startup scripts</summary>
      <description>Remove the legacy entrypoints from the startup scripts.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.jobmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-daemon.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-console.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-22 01:00:00" id="10394" opendate="2018-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy mode testing profiles from Travis config</summary>
      <description>Remove the legacy mode testing profiles from Travis config.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-25 01:00:00" id="10414" opendate="2018-9-25 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add skip to next strategy</summary>
      <description>Add skip to next strategy, that should discard all partial matches that started with the same element as found match.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.AfterMatchSkipITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.aftermatch.SkipToLastStrategy.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.aftermatch.SkipToFirstStrategy.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.aftermatch.SkipPastLastStrategy.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.aftermatch.AfterMatchSkipStrategy.java</file>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-25 01:00:00" id="10416" opendate="2018-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add files generated by jepsen tests to rat excludes</summary>
      <description>Currently jepsen generates some files that results in rat plugin failures.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-25 01:00:00" id="10421" opendate="2018-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shaded Hadoop S3A end-to-end test failed on Travis</summary>
      <description>https://api.travis-ci.org/v3/job/432916761/log.txt</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-26 01:00:00" id="10440" opendate="2018-9-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CassandraPojoOutputFormat</summary>
      <description>Add a CassandraPojoOutputFormat to write Pojo in Cassandra from batch API like the CassandraPojoSink for streaming API</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.batch.connectors.cassandra.example.BatchPojoExample.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-28 01:00:00" id="10457" opendate="2018-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SequenceFile for StreamingFileSink</summary>
      <description>SequenceFile is major file format in Hadoop eco system.It is simple to manage file and easy to combine with other tools.So we are still needed SequenceFile format, even if the file format supports Parquet and ORC.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.2,1.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.FiniteTestSource.java</file>
      <file type="M">flink-formats.flink-sequencefile.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-formats.flink-sequencefile.src.test.java.org.apache.flink.formats.sequencefile.SequenceFileSinkITCase.java</file>
      <file type="M">flink-formats.flink-sequencefile.src.main.java.org.apache.flink.formats.sequencefile.SequenceFileWriterFactory.java</file>
      <file type="M">flink-formats.flink-sequencefile.src.main.java.org.apache.flink.formats.sequencefile.SequenceFileWriter.java</file>
      <file type="M">flink-formats.flink-sequencefile.pom.xml</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.testutils.FiniteTestSource.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.avro.ParquetStreamingFileSinkITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-29 01:00:00" id="10465" opendate="2018-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen: runit supervised sshd is stopped on tear down</summary>
      <description>When tearing down the DB, we tear down all services supervised by runit. However when running the tests in Docker, sshd is under supervision by runit. When sshd is stopped, the tests cannot be continued because the control node cannot interact with the DB nodes anymore.How to reproduceRun command below in control-node container:./docker/run-tests.sh 1 [...]/flink/flink-1.6.1/flink-1.6.1-bin-hadoop28-scala_2.11.tgzExpected behaviorsshd should never be stopped</description>
      <version>1.6.2,1.7.0</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.utils.clj</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-1 01:00:00" id="10474" opendate="2018-10-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t translate IN with Literals to JOIN with VALUES for streaming queries</summary>
      <description>IN predicates with literals are translated to JOIN with VALUES if the number of elements in the IN clause exceeds a certain threshold. This should not be done, because a streaming join is very heavy and materializes both inputs (which is fine for the VALUES) input but not for the other.There are two ways to solve this: don't translate IN to a JOIN at all translate it to a JOIN but have a special join strategy if one input is bound and final (non-updating)Option 1. should be easy to do, option 2. requires much more effort.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.calcite.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.CalcTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.CalcTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.CalciteConfig.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-8 01:00:00" id="10512" opendate="2018-10-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy REST API docs</summary>
      <description/>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-8 01:00:00" id="10514" opendate="2018-10-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>change Tachyon to Alluxio</summary>
      <description>Since the Tachyon renamed to Alluxio, we should change doc as well.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-fs-tests.src.test.resources.log4j-test.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-11 01:00:00" id="10532" opendate="2018-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Broken links in documentation</summary>
      <description>https://travis-ci.org/apache/flink/builds/440115490#L599http://localhost:4000/dev/stream/operators.html:Remote file does not exist -- broken link!!!--http://localhost:4000/dev/table/streaming/sql.html:Remote file does not exist -- broken link!!!http://localhost:4000/dev/table/streaming.html:Remote file does not exist -- broken link!!!</description>
      <version>1.7.0</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.connect.md</file>
      <file type="M">docs.dev.stream.python.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-12 01:00:00" id="10537" opendate="2018-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Network throughput performance regression after broadcast changes</summary>
      <description>There is a slight network throughput regression introduced in: https://issues.apache.org/jira/browse/FLINK-9913It is visible in the following benchmark:http://codespeed.dak8s.net:8000/timeline/#/?exe=1&amp;ben=networkThroughput.1,100ms&amp;env=2&amp;revs=200&amp;equid=off&amp;quarts=on&amp;extr=on(drop in the chart that happened since 21st September.)</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-15 01:00:00" id="10554" opendate="2018-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump flink-shaded dependency version</summary>
      <description/>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-17 01:00:00" id="1056" opendate="2014-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build jar in generated archetypes using the maven-assembly-plugin</summary>
      <description>By using the "maven-assembly-plugin" with the "jar-with-dependencies" descriptor, we can ensure that the user's dependencies are always bundled into the jar file of the user's job.We may have to define some exclusion rules to avoid flink dependencies being packed as well (jar size) but that should be doable.</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.META-INF.maven.archetype-metadata.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.META-INF.maven.archetype-metadata.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">docs.scala.api.quickstart.md</file>
      <file type="M">docs.java.api.quickstart.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-16 01:00:00" id="10569" opendate="2018-10-16 00:00:00" resolution="Staged">
    <buginformation>
      <summary>Clean up uses of Scheduler and Instance in valid tests</summary>
      <description>Legacy class Scheduler and Instance are still used in some valid tests like ExecutionGraphRestartTest. We should replace them with FLIP-6 schedule mode. The best way I can find is use SimpleSlotProvider.Note that we need not to remove all use points among all files since most of them stay in legacy codebase like JobManager.scala and would be removed later.</description>
      <version>1.7.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.FailoverRegionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.CoLocationConstraintTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.RescalePartitionerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerSlotSharingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerIsolatedTasksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.VertexSlotSharingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.RestartPipelinedRegionStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.PointwisePatternTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexSchedulingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexCancelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRescalingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphMetricsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphConstructionTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-17 01:00:00" id="10583" opendate="2018-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for state retention to the Processing Time versioned joins.</summary>
      <description/>
      <version>1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.TemporalJoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.TemporalProcessTimeJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-17 01:00:00" id="10584" opendate="2018-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for state retention to the Event Time versioned joins.</summary>
      <description>Open PR here</description>
      <version>1.7.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TwoInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.TemporalJoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.TemporalRowtimeJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.BaseTwoInputStreamOperatorWithStateRetention.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-18 01:00:00" id="10591" opendate="2018-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add functions to return TimeIndicators from MATCH_RECOGNIZE</summary>
      <description>In order to be able to apply windowing on top of results from MATCH_RECOGNIZE clause we have to provide a way to return proper TimeIndicator. We cannot simply return rowtime/proctime for any row of the match, cause match can be finalized e.g. by the first non matching row (in case of greedy), so the TimeIndicator will be equal to the timestamp of that non-matching row.The suggestion is to provide functions: MATCH_ROWTIME() MATCH_PROCTIME()that will output rowtime/proctime indicators.</description>
      <version>1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-18 01:00:00" id="10596" opendate="2018-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add access to timerService in Pattern(Flat)SelectFunction</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.utils.NFATestHarness.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAStatusChangeITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.conditions.IterativeCondition.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.functions.PatternProcessFunction.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.context.TimerContext.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.utils.OutputAsserter.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CepOperatorTestUtilities.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.PatternStream.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.TimestampedSideOutputCollector.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.SelectTimeoutCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.SelectCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.FlatSelectTimeoutCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.FlatSelectCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CEPOperatorUtils.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
      <file type="M">flink-libraries.flink-cep-scala.src.test.scala.org.apache.flink.cep.scala.PatternStreamScalaJavaAPIInteroperabilityTest.scala</file>
      <file type="M">flink-libraries.flink-cep-scala.src.main.scala.org.apache.flink.cep.scala.PatternStream.scala</file>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-18 01:00:00" id="10597" opendate="2018-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable UDFs support in MATCH_RECOGNIZE</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.match.MatchOperatorValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.PatternSelectFunctionRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.MatchUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.IterativeConditionRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.MatchCodeGenerator.scala</file>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-18 01:00:00" id="10600" opendate="2018-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide End-to-end test cases for modern Kafka connectors</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.kafka.Kafka010Example.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.kafka.KafkaEventSchema.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.kafka.KafkaEvent.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.kafka.Kafka010Example.java</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka010.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-10-19 01:00:00" id="10608" opendate="2018-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add avro files generated by datastream-allround-test to RAT exclusions</summary>
      <description>With the 1.5.5 and 1.6.2 release-candidates it was discovered that files generated during the build are not covered by the apache-rat-plugin license-header check.As a result the first compilation succeeds but a subsequent one may fail.This is a bit surprising considering that the plugin is executed in the verify phase, which is executed after the compilation and execution of tests.This is because the plugin is only run in the flink-parent project before anything is compiled. The plugin-configuration is explicitly disables inheritance.I'm re-purposing this Jira to add the avro classes to the exclusion list.However, in the long term I'd suggest to enable inheritance for the plugin and define the module-specific exclusions in each module respectively. This will allows to run the plugin in the verify phase of each module which would've caught this error.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-21 01:00:00" id="10623" opendate="2018-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend streaming SQL end-to-end test to test MATCH_RECOGNIZE</summary>
      <description>We should extend the existing test_streaming_sql.sh to test the newly added MATCH_RECOGNIZE functionality.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-21 01:00:00" id="10625" opendate="2018-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add MATCH_RECOGNIZE documentation</summary>
      <description>The newly added MATCH_RECOGNIZE functionality needs to be documented.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
      <file type="M">docs.dev.table.streaming.query.configuration.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-21 01:00:00" id="10628" opendate="2018-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end-to-end test for REST communication encryption</summary>
      <description>With FLINK-10371 we added support for mutual authentication for the REST communication. We should adapt one of the existing end-to-end tests to require this feature for the REST communication.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.file.sink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.allround.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">docs.ops.security-ssl.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-21 01:00:00" id="10631" opendate="2018-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update jepsen tests to run with multiple slots</summary>
      <description>After fixing FLINK-9455, we should update the Jepsen tests to run with multiple slots per TaskExecutor.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-21 01:00:00" id="10633" opendate="2018-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Prometheus reporter</summary>
      <description>Add an end-to-end test using the PrometheusReporter to verify that all metrics are properly reported. Additionally verify that the newly introduce RocksDB metrics are accessible (see FLINK-10423).</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-22 01:00:00" id="10638" opendate="2018-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid table scan resolution for temporal join queries</summary>
      <description>Registered tables that contain a temporal join are not properly resolved when performing a table scan.A planning error occurs when registering a table with a temporal join and reading from it again.LogicalProject(amount=[*($0, $4)]) LogicalFilter(condition=[=($3, $1)]) LogicalCorrelate(correlation=[$cor0], joinType=[inner], requiredColumns=[{2}]) LogicalTableScan(table=[[_DataStreamTable_0]]) LogicalTableFunctionScan(invocation=[Rates(CAST($cor0.rowtime):TIMESTAMP(3) NOT NULL)], rowType=[RecordType(VARCHAR(65536) currency, BIGINT rate, TIME ATTRIBUTE(ROWTIME) rowtime)], elementType=[class [Ljava.lang.Object;])</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.TemporalTableJoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-22 01:00:00" id="10639" opendate="2018-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix java syntax error in document</summary>
      <description>Due to the  StreamTableSourceFactory  is a trait. So the java example in the document should using "implements" keyword.    </description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.types.serialization.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-24 01:00:00" id="10669" opendate="2018-10-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exceptions &amp; errors are not properly checked in logs in e2e tests</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-26 01:00:00" id="1067" opendate="2014-8-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Stratosphere" occurance in docs FAQ</summary>
      <description>The name Stratosphere occurs in the answer to the fault-tolerance question of the FAQ.</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.faq.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-24 01:00:00" id="10670" opendate="2018-10-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Correlate codegen error</summary>
      <description>TableFunctionCollector should handle reuseInitCode and reuseMemberCode</description>
      <version>None</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CollectorCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-25 01:00:00" id="10676" opendate="2018-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add &amp;#39;as&amp;#39; method for OverWindowWithOrderBy in Java API</summary>
      <description>The preceding clause of OVER Window in the traditional database is optional. The default is UNBOUNDED. So we can add the "as" method to OverWindowWithOrderBy. This way OVERWindow is written more easily. e.g.:.window(Over partitionBy 'c orderBy 'proctime preceding UNBOUNDED_ROW as 'w)Can be simplified as follows:.window(Over partitionBy 'c orderBy 'proctime as 'w)What do you think? </description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.OverWindowStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.OverWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.windows.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.java.windows.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-25 01:00:00" id="10678" opendate="2018-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a switch to run_test to configure if logs should be checked for errors/excepions</summary>
      <description>After adding the switch, we should disable log checking for nightly-tests that currently fail (or fix the test).</description>
      <version>1.7.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.queryable.state.restart.tm.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.run-single-test.sh</file>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-25 01:00:00" id="10681" opendate="2018-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>elasticsearch6.ElasticsearchSinkITCase fails if wrong JNA library installed</summary>
      <description>The elasticsearch6.ElasticsearchSinkITCase fails on systems where a wrong JNA library is installed.There is an incompatible JNA native library installed on this systemExpected: 5.2.0Found: 4.0.0/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib.To resolve this issue you may do one of the following: - remove or uninstall the offending library - set the system property jna.nosys=true - set jna.boot.library.path to include the path to the version of the jnidispatch library included with the JNA jar file you are using at com.sun.jna.Native.&lt;clinit&gt;(Native.java:199) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:264) at org.elasticsearch.bootstrap.Natives.&lt;clinit&gt;(Natives.java:45) at org.elasticsearch.bootstrap.BootstrapInfo.isMemoryLocked(BootstrapInfo.java:50) at org.elasticsearch.monitor.process.ProcessProbe.processInfo(ProcessProbe.java:130) at org.elasticsearch.monitor.process.ProcessService.&lt;init&gt;(ProcessService.java:44) at org.elasticsearch.monitor.MonitorService.&lt;init&gt;(MonitorService.java:48) at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:363) at org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl$PluginNode.&lt;init&gt;(EmbeddedElasticsearchNodeEnvironmentImpl.java:85) at org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl.start(EmbeddedElasticsearchNodeEnvironmentImpl.java:53) at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.prepare(ElasticsearchSinkTestBase.java:73) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:113) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)I propose to solve the problem by setting the system property jna.nosys=true to prefer the bundled JNA library.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-10-26 01:00:00" id="10692" opendate="2018-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden Confluent schema E2E test</summary>
      <description>The Confluent schema E2E test starts a Confluent schema registry to run against. If the schema registry cannot be started, the test proceeds and simply dead locks. In order to improve the situation I suggest to fail if we cannot start the Confluent schema registry.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-29 01:00:00" id="10711" opendate="2018-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-end-to-end-tests can fail silently</summary>
      <description>Because they are written in bash and they are not settingset -eat the beginning, errors can be swallowed silently.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka.common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka010.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.cli.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.allround.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.README.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-29 01:00:00" id="10717" opendate="2018-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce SimpleSerializerSnapshot as replacement for ParameterlessTypeSerializerConfig</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SerializationProxiesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.OperatorStateBackendTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.TypeSerializerSerializationUtilTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.VoidSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.StringValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.StringSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.SqlTimestampSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.SqlTimeSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.SqlDateSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ShortValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ShortSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.NullValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.LongValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.LongSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.IntValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.IntSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.InstantSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.FloatValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.FloatSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.DoubleValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.DoubleSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.DateSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.CharValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.CharSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ByteValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ByteSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.BooleanValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.BooleanSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.BigIntSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.BigDecSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.StringArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.ShortPrimitiveArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.LongPrimitiveArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.IntPrimitiveArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.FloatPrimitiveArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.DoublePrimitiveArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.CharPrimitiveArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.BooleanPrimitiveArraySerializer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-27 01:00:00" id="1072" opendate="2014-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend Travis testing to more Hadoop versions.</summary>
      <description>http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/Extend-Travis-CI-build-matrix-td1516.html</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-30 01:00:00" id="10725" opendate="2018-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for Java 11 (LTS)</summary>
      <description>Java 8 is over 5 years old and will be end of life in 2019/2020. Java 11, the latest long-term support release, became GA in September 2018. Given that FLINK-8033 still hasn't been resolved and that Java 9 was end of life (discontinued / no longer publically available or supported) since March 2018, it doesn't make sense to continue trying to add Java 9 support when both Java 9 and Java 10 are end-of-life.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-30 01:00:00" id="10726" opendate="2018-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming SQL end-to-end test fails due to recent flink-table-common changes</summary>
      <description>The streaming SQL end-to-end test fails with:java.lang.NoClassDefFoundError: org/apache/flink/table/api/TableException at org.apache.flink.sql.tests.StreamSQLTestProgram.main(StreamSQLTestProgram.java:85) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:529) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:421) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:427) at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:813) at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:287) at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1050) at org.apache.flink.client.cli.CliFrontend.lambda$main$11(CliFrontend.java:1126) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1126)Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.api.TableException at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 18 moreThis has to do with the recent dependency changes for introducing a flink-table-common module.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-1 01:00:00" id="10754" opendate="2018-11-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Jepsen tests to run with activated local recovery</summary>
      <description>With FLINK-9635 Flink now supports to run properly with local recovery activated. We should update the Jepsen tests to run with this feature in order to give it more test exposure.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-2 01:00:00" id="10765" opendate="2018-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify one of the s3 tests to use the Presto S3 s3p schema</summary>
      <description>We should modify one end-to-end test which uses S3 to explicitly use the Presto S3 filesystem by specifying the s3p schema. This was introduced with FLINK-10563.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-5 01:00:00" id="10797" opendate="2018-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"IntelliJ Setup" link is broken in Readme.md</summary>
      <description>The link points to https://github.com/apache/flink/blob/master/docs/internals/ide_setup.md#intellij-idea which is a 404 not found.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.md</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-6 01:00:00" id="10803" opendate="2018-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about S3 support by the StreamingFileSink</summary>
      <description/>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.streamfile.sink.md</file>
      <file type="M">docs.ops.filesystems.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-7 01:00:00" id="10811" opendate="2018-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hcatalog modules needs scala suffix</summary>
      <description>The hcatalog connector has a compile dependency on flink-hadoop-compatibility which has a scala suffix, and thus also requires a suffix.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hcatalog.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-7 01:00:00" id="10812" opendate="2018-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>javadoc-plugin fails for flink-e2e-test-utils</summary>
      <description>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.9.1:jar (attach-javadocs) on project flink-e2e-test-utils: MavenReportException: Error while creating archive:[ERROR] Exit code: 1 - javadoc: error - No public or protected classes found to document.[ERROR] [ERROR] Command line was: /usr/local/asfpackages/java/jdk1.8.0_191/jre/../bin/javadoc @options @packages[ERROR] [ERROR] Refer to the generated Javadoc files in '/home/jenkins/jenkins-slave/workspace/flink-snapshot-deployment/flink-end-to-end-tests/flink-e2e-test-utils/target/apidocs' dir.[ERROR] -&gt; [Help 1]</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-e2e-test-utils.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-7 01:00:00" id="10814" opendate="2018-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka examples modules need scala suffix</summary>
      <description>The kafka examples need scala suffixes just like flink-examples-batch and flink-examples-streaming.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming-kafka.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-base.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.11.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.10.src.main.scala.org.apache.flink.streaming.scala.examples.kafka.Kafka010Example.scala</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-8 01:00:00" id="10823" opendate="2018-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing scala suffixes</summary>
      <description>The jdbc connector and jmx/prometheus reporter have provided dependencies to scala-infected modules (streaming-java/runtime) and thus also require a scala sufix.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.pom.xml</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">flink-connectors.flink-jdbc.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-jmx.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-13 01:00:00" id="10863" opendate="2018-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Assign uids to all operators</summary>
      <description>We should assign uids to operators in the test so that we can also properly test removing operators.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-13 01:00:00" id="10869" opendate="2018-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update S3 testing settings</summary>
      <description>Currently S3 tests go against a bucket hosted by 'data Artisans'.As part of reworking the AWS permission setup, we need to adapt the credentials and buckets for these tests.Future tests should refer to the following environment variables for S3 tests: `IT_CASE_S3_BUCKET` `IT_CASE_S3_ACCESS_KEY` `IT_CASE_S3_SECRET_KEY`</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.s3.S3Credentials.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3RecoverableWriterTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemBehaviorITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnFileStageTestS3ITCase.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.file.sink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.shaded.presto.s3.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.shaded.hadoop.s3a.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.s3.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-14 01:00:00" id="10872" opendate="2018-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend SQL client end-to-end to test KafkaTableSink for kafka connector 0.11</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-14 01:00:00" id="10885" opendate="2018-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro Confluent Schema Registry E2E test failed on Travis</summary>
      <description>https://travis-ci.org/zentol/flink/jobs/454943551Waiting for schema registry...[2018-11-14 12:20:59,394] ERROR Server died unexpectedly: (io.confluent.kafka.schemaregistry.rest.SchemaRegistryMain:51)org.apache.kafka.common.config.ConfigException: No supported Kafka endpoints are configured. Either kafkastore.bootstrap.servers must have at least one endpoint matching kafkastore.security.protocol or broker endpoints loaded from ZooKeeper must have at least one endpoint matching kafkastore.security.protocol. at io.confluent.kafka.schemaregistry.storage.KafkaStore.endpointsToBootstrapServers(KafkaStore.java:313) at io.confluent.kafka.schemaregistry.storage.KafkaStore.&lt;init&gt;(KafkaStore.java:130) at io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry.&lt;init&gt;(KafkaSchemaRegistry.java:144) at io.confluent.kafka.schemaregistry.rest.SchemaRegistryRestApplication.setupResources(SchemaRegistryRestApplication.java:53) at io.confluent.kafka.schemaregistry.rest.SchemaRegistryRestApplication.setupResources(SchemaRegistryRestApplication.java:37) at io.confluent.rest.Application.createServer(Application.java:149) at io.confluent.kafka.schemaregistry.rest.SchemaRegistryMain.main(SchemaRegistryMain.java:43)</description>
      <version>1.7.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-15 01:00:00" id="10900" opendate="2018-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mark Kafka 2.0 connector as beta feature</summary>
      <description>Given the test problems with the Kafka 2.0 connector we should mark this connector as a beta feature until we have fully understood why so many tests deadlock.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-16 01:00:00" id="10911" opendate="2018-11-16 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Enable flink-scala-shell with Scala 2.12</summary>
      <description>Flink's flink-scala-shell module is not working with Scala 2.12. Therefore, it is currently excluded from the Scala 2.12 builds.</description>
      <version>1.7.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-20 01:00:00" id="10944" opendate="2018-11-20 00:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>EventTimeWindowCheckpointingITCase.testTumblingTimeWindow failed on Travis</summary>
      <description>The EventTimeWindowCheckpointingITCase.testTumblingTimeWindow failed on Travis: https://api.travis-ci.org/v3/job/457177641/log.txthttps://travis-ci.org/apache/flink/builds/457177636?utm_source=github_status&amp;utm_medium=notification</description>
      <version>1.7.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-21 01:00:00" id="10958" opendate="2018-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add overload support for user defined function</summary>
      <description>Currently overload is not supported in user defined function and given the following UDFclass Func21 extends ScalarFunction { def eval(p: People): String = { p.name } def eval(p: Student): String = { "student#" + p.name }}class People(val name: String)class Student(name: String) extends People(name)class GraduatedStudent(name: String) extends Student(name)Queries such as the following will compile failed with error msg "Found multiple 'eval' methods which match the signature." val udf = new Func21val table = ...table.select(udf(new GraduatedStudent("test"))) That's because overload is not supported in user defined function currently. I think it will make sense to support overload following the java language specification in section 15.2. </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.userDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.UserDefinedScalarFunctionTest.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-22 01:00:00" id="10992" opendate="2018-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen: Do not use /tmp as HDFS Data Directory</summary>
      <description>dfs.name.dir and dfs.data.dir should not be located in /tmp. The directories might get deleted unintentionally, which can cause test failures.</description>
      <version>1.6.2,1.7.0,1.8.0</version>
      <fixedVersion>1.6.3,1.7.0,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.utils.clj</file>
      <file type="M">flink-jepsen.src.jepsen.flink.hadoop.clj</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-23 01:00:00" id="10997" opendate="2018-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro-confluent-registry does not bundle any dependency</summary>
      <description>The flink-avro-confluent-registry is not bundling any dependencies, yet defines a relocation for the transitive jackson dependency pulled in by kafka-schema-registry-client.It is like that the registry-client should be included in the jar.</description>
      <version>1.6.2,1.7.0</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-26 01:00:00" id="11003" opendate="2018-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document of Java Lambda Expressions has a mistake</summary>
      <description>Documentation of Java Lambda Expressions has a mistake which may cause confusion.In the last code block, it presents some way to solve type missing problem.In 15 line:public static class MyTuple2Mapper extends MapFunction&lt;Integer, Integer&gt; {    @Override    public Tuple2&lt;Integer, Integer&gt; map(Integer i){         return Tuple2.of(i, i);     }}The second generic type in MapFunction should be Tuple2&lt;Integer, Integer&gt;</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.java.lambdas.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-26 01:00:00" id="11005" opendate="2018-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define flink-sql-client uber-jar dependencies via artifactSet</summary>
      <description>The module flink-sql-client defines the content of its uber jar via filtering files from the set of all dependencies. I think this is not ideal because it misses for example the NOTICE files from down stream dependencies. A solution could be to define an &lt;artifactSet&gt;&lt;includes&gt;&lt;include&gt;&lt;/include&gt;&lt;/includes&gt;&lt;/artifactSet&gt; and exclude files via the filter.</description>
      <version>1.7.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-27 01:00:00" id="11008" opendate="2018-11-27 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Speed up RocksDB upload file procedure</summary>
      <description>As FLINK-10461  did, we could speed up upload checkpoint files by using multi-thread.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateDataTransferTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksFullSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDbStateDataTransfer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.AsyncSnapshotCallableTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AsyncSnapshotCallable.java</file>
      <file type="M">docs..includes.generated.rocks.db.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-28 01:00:00" id="11017" opendate="2018-11-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Time interval for window aggregations in SQL is wrongly translated if specified with YEAR_MONTH resolution</summary>
      <description>If a time interval was specified with YEAR TO MONTH resolution like e.g.:SELECT * FROM MytableGROUP BY TUMBLE(rowtime, INTERVAL '1-2' YEAR TO MONTH)it will be wrongly translated to 14 milliseconds window. We should allow for only DAY TO SECOND resolution.</description>
      <version>1.6.2,1.7.0</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.validation.WindowAggregateValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamLogicalWindowAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-28 01:00:00" id="11023" opendate="2018-11-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update LICENSE and NOTICE files for flink-connectors</summary>
      <description>Similar to FLINK-10987 we should also update the LICENSE and NOTICE files for flink-connectors.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka-0.10.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-2-22 01:00:00" id="1105" opendate="2014-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for locally sorted output</summary>
      <description>This feature will make it possible to sort the output which is sent to an OutputFormat to obtain a locally sorted result.This feature was available in the "old" Java API and has not be ported to the new Java API yet. Hence optimizer and runtime should already have support for this feature. However, the API and job generation part is missing.It is also a subfeature of FLINK-598 which will provide also globally sorted results.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.programming.guide.md</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.util.CollectionDataSets.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DataSink.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.GenericDataSinkBase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-3-4 01:00:00" id="11068" opendate="2018-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert the API classes *Table, *Window to interfaces</summary>
      <description>A more detailed description can be found in FLIP-32.This includes: Table, GroupedTable, WindowedTable, WindowGroupedTable, OverWindowedTable, Window, OverWindowWe can keep the "Table" Scala implementation in a planner module until it has been converted to Java.We can add a method to the planner later to give us a concrete instance. This is one possibility to have a smooth transition period instead of changing all classes at once.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.UpdatingPlanCheckerTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RetractionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalCorrelateToTemporalTableJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.TemporalTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.subquery.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.table.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.TableConversions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.package.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.CollectStreamResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.ChangelogCollectStreamResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ResultStore.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.CollectStreamTableSink.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.CollectBatchTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-5 01:00:00" id="11074" opendate="2018-12-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the harness test to make it possible test with state backend</summary>
      <description>Currently, the harness test can only test without state backend. If you use a DataView in the accumulator of the aggregate function, the DataView is a java object and held in heap, not replaced with StateMapView/StateListView which values are actually held in the state backend. We should improve the harness test to make it possible to test with state backend. Otherwise, issues such as FLINK-10674 could have never been found. With this harness test available, we could test the built-in aggregate functions which use the DataView more fine grained.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.CollectAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-5 01:00:00" id="11079" opendate="2018-12-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip deployment for flnk-storm-examples</summary>
      <description>Similar to FLINK-10987 we should also update the LICENSE and NOTICE for flink-storm-examples. This project creates several fat example jars that are deployed to maven central.Alternatively we could about dropping these examples.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ListSerializerSnapshot.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ListSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.CollectionSerializerConfigSnapshot.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-5 01:00:00" id="11080" opendate="2018-12-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define flink-connector-elasticsearch6 uber-jar dependencies via artifactSet</summary>
      <description>Similar to FLINK-11005.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.1,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-6 01:00:00" id="11085" opendate="2018-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NoClassDefFoundError in presto-s3 filesystem</summary>
      <description>A user has reporter an issue on the ML where using the presto-s3 filesystem fails with an exception due to a missing class. The missing class is indeed filtered out in the shade-plugin configuration.java.lang.NoClassDefFoundError: org/apache/flink/fs/s3presto/shaded/com/facebook/presto/hadoop/HadoopFileStatus at org.apache.flink.fs.s3presto.shaded.com.facebook.presto.hive.PrestoS3FileSystem.directory(PrestoS3FileSystem.java:446) at org.apache.flink.fs.s3presto.shaded.com.facebook.presto.hive.PrestoS3FileSystem.delete(PrestoS3FileSystem.java:423) at org.apache.flink.fs.s3.common.hadoop.HadoopFileSystem.delete(HadoopFileSystem.java:147) at org.apache.flink.runtime.state.filesystem.FileStateHandle.discardState(FileStateHandle.java:80) at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.doDiscard(CompletedCheckpoint.java:250) at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.discardOnSubsume(CompletedCheckpoint.java:219) at org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.addCheckpoint(StandaloneCompletedCheckpointStore.java:72) at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:844) at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:756) at org.apache.flink.runtime.jobmaster.JobMaster.lambda$acknowledgeCheckpoint$8(JobMaster.java:680) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-12-7 01:00:00" id="11101" opendate="2018-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use enforcer-plugin to ban openjdk dependency in Presto S3 FileSystem module</summary>
      <description>The presto-s3-fs module defines he following exclusion in the shade-plugin:&lt;excludes&gt; &lt;exclude&gt;org.openjdk.jol&lt;/exclude&gt;&lt;/excludes&gt;This exclusion has no effect on the resulting artifact. We could think about removing it.</description>
      <version>1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-1-22 01:00:00" id="1112" opendate="2014-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add GroupSorting with KeySelectors</summary>
      <description>Group sorting is currently only supported for field-index keys and not for KeySelectors.This feature was requested.</description>
      <version>None</version>
      <fixedVersion>0.9,0.8.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.GroupReduceITCase.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.GroupReduceOperator.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.translation.CustomPartitioningGroupingKeySelectorTest.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.GroupReduceITCase.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.GroupedDataSet.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnsortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SortedGrouping.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.custompartition.GroupingKeySelectorTranslationTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-10 01:00:00" id="11120" opendate="2018-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TIMESTAMPADD function handles TIME incorrectly</summary>
      <description>The error occur when timestampadd(MINUTE, 1, time '01:00:00') is executed:java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Longat org.apache.calcite.rex.RexBuilder.clean(RexBuilder.java:1520)at org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:1318)at org.apache.flink.table.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:135)at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressionsInternal(ReduceExpressionsRule.java:620)at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:540)at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:288)I think it should meet the following conditions:expressionExpect the resulttimestampadd(MINUTE, -1, time '00:00:00')23:59:00timestampadd(MINUTE, 1, time '00:00:00')00:01:00timestampadd(MINUTE, 1, time '23:59:59')00:00:59timestampadd(SECOND, 1, time '23:59:59')00:00:00timestampadd(HOUR, 1, time '23:59:59')00:59:59This problem seems to be a bug in calcite. I have submitted isuse to calcite. The following is the link.CALCITE-2699</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-11 01:00:00" id="11125" opendate="2018-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused imports</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.BlockingShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.BloomFilterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.InPlaceMutableHashTableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTablePerformanceComparison.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeJobRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.AsynchronousFileIOChannelTest.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.ValueTypeInfoTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.MissingTypeInfoTest.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-12-12 01:00:00" id="11136" opendate="2018-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the logical of merge for DISTINCT aggregates</summary>
      <description>The logic of merge for DISTINCT aggregates has bug. For the following query:SELECT c, COUNT(DISTINCT b), SUM(DISTINCT b), SESSION_END(rowtime, INTERVAL '0.005' SECOND)FROM MyTableGROUP BY SESSION(rowtime, INTERVAL '0.005' SECOND), cthe following exception will be thrown:Caused by: java.lang.ClassCastException: org.apache.flink.types.Row cannot be cast to java.lang.Integerat scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:101)at scala.math.Numeric$IntIsIntegral$.plus(Numeric.scala:58)at org.apache.flink.table.functions.aggfunctions.SumAggFunction.accumulate(SumAggFunction.scala:50)at GroupingWindowAggregateHelper$18.mergeAccumulatorsPair(Unknown Source)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:66)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:33)at org.apache.flink.runtime.state.heap.HeapAggregatingState.mergeState(HeapAggregatingState.java:117)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState$MergeTransformation.apply(AbstractHeapMergingState.java:102)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:463)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:341)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState.mergeNamespaces(AbstractHeapMergingState.java:91)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:341)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:311)at org.apache.flink.streaming.runtime.operators.windowing.MergingWindowSet.addWindow(MergingWindowSet.java:212)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.processElement(WindowOperator.java:311)at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:202)at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:105)at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)at org.apache.flink.runtime.taskmanager.Task.run(Task.java:704)at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-22 01:00:00" id="1114" opendate="2014-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ScalaStyle prevents projects from being built individually</summary>
      <description>Maven project building and verification fails, unless all are built together.Message:Failed to execute goal org.scalastyle:scalastyle-maven-plugin:0.5.0:check (default) on project flink-spargel: Failed during scalastyle execution: Unable to find configuration file at location tools/maven/scalastyle-config.xml</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-examples.flink-scala-examples.pom.xml</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.graph.TransitiveClosureNaive.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-12 01:00:00" id="11142" opendate="2018-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Undefined behavior in the conversion from DataStream/DataSet to Table</summary>
      <description>When we try to convert DataStream/DataSet to Table. There are two ways of adding schema information, ByName or ByPosition.This feature first proposed in this pr.In ByPosition mode, the current code does not check if the number of fields less than its in  DataStream/DataSet. This may cause undefined behavior, e.g. make a projection in ByPosition mode.We can either fix it by adding some checking or regard this as a feature and just improve the doc to clarify it. In my opinion, the latter way seems better.twalthr Could you take a look at it when you free?</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-9-23 01:00:00" id="1118" opendate="2014-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude Java Crash Log Files from License Header Check</summary>
      <description>I from time to time my builds fail with JVM crashes.If I try to run the build a second time, it fails due to the Header check on the Java crash log filles hs_err_pidxxxx.log.It would be nice to exclude these files from being checked as they are not part of the project ressources.</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-1-18 01:00:00" id="11189" opendate="2018-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove documentation for deprecated readSequenceFile method</summary>
      <description>The function readSequenceFile is no longer in class ExecutionEnvironment since FLINK-4315.But on the website example, the readSequenceFile function is still called from this class.https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/batch/Currently, this method is in flink-hadoop-compatibility, so the documents should be revised.</description>
      <version>1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-18 01:00:00" id="11191" opendate="2018-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exception in code generation when ambiguous columns in MATCH_RECOGNIZE</summary>
      <description>Query:SELECT *FROM TickerMATCH_RECOGNIZE ( PARTITION BY symbol, price ORDER BY proctime MEASURES A.symbol AS symbol, A.price AS price PATTERN (A) DEFINE A AS symbol = 'a') AS Tthrows a cryptic exception from the code generation stack that the output arity is wrong. We should add early validation and throw a meaningful exception. I've also created a calcite ticket to fix it on calcite's side: CALCITE-2747</description>
      <version>1.7.0,1.8.0</version>
      <fixedVersion>1.7.2,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.match.MatchRecognizeValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamMatchRule.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-1-20 01:00:00" id="11207" opendate="2018-12-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Apache commons-compress from 1.4.1 to 1.18</summary>
      <description>There is at least one security vulnerability in the current version that we should address by upgrading to 1.18+:https://app.snyk.io/vuln/SNYK-JAVA-ORGAPACHECOMMONS-32473</description>
      <version>1.5.5,1.6.2,1.7.0,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2-uber.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-25 01:00:00" id="11217" opendate="2018-12-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add back-to-top buttons</summary>
      <description/>
      <version>1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
      <file type="M">docs.ops.deployment.hadoop.md</file>
      <file type="M">docs.examples.index.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.streaming.index.md</file>
      <file type="M">docs.dev.stream.state.schema.evolution.md</file>
      <file type="M">docs.dev.stream.state.broadcast.state.md</file>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
      <file type="M">docs.dev.stream.operators.joining.md</file>
      <file type="M">docs.dev.stream.experimental.md</file>
      <file type="M">docs.dev.libs.ml.index.md</file>
      <file type="M">docs.dev.java.lambdas.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-29 01:00:00" id="11232" opendate="2018-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Empty Start Time of sub-task on web dashboard</summary>
      <description/>
      <version>1.5.5,1.6.2,1.6.3,1.7.0,1.7.1</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexDetailsInfo.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-2-17 01:00:00" id="11382" opendate="2019-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable MetricFetcher if interval is configured to 0</summary>
      <description>Follow-up for FLINK-10822 to disable the MetricFetcher completely if the interval is configured to 0.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">docs..includes.generated.metric.configuration.html</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-22 01:00:00" id="11404" opendate="2019-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI: Add Load More Button to Exception Page</summary>
      <description>Add a load more button in the exception history page, when there the exception count is more than 20 (the truncated flag is true), clicking the load more button the frontend will add 20 to the MAX_NUMBER_EXCEPTION_TO_REPORT request params.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-1-30 01:00:00" id="11469" opendate="2019-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix example in "Tuning Checkpoints and Large State" documentation</summary>
      <description>Sample code for subtitle Tuning RocksDB in Tuning Checkpoints and Large State is wrong  Affects Version：All versions after 1.1  </description>
      <version>1.6.2,1.6.3,1.6.4,1.7.0,1.7.1,1.7.2,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-5 01:00:00" id="11535" opendate="2019-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client jar does not contain table-api-java</summary>
      <description>The SQL Client jar does not package flink-table-api-java.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-7 01:00:00" id="11544" opendate="2019-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to manually set job ID for job submissions via REST API</summary>
      <description>Add an option to specify the job ID during job submissions via the REST API. </description>
      <version>1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanRequestBody.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgramUtils.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-15 01:00:00" id="11626" opendate="2019-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump flink-shaded version to 6.0</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-14 01:00:00" id="5340" opendate="2016-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a metric exposing jobs uptimes</summary>
      <description>I would like the job manager to expose a metric indicating how long each job has been up. This way I can grab this number and measure the health of my job.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-18 01:00:00" id="9885" opendate="2018-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Elasticsearch 6.x connector</summary>
      <description>We have decided to try and merge the pending Elasticsearch 6.x PRs. This should also come with an end-to-end test that covers this.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.elasticsearch.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch6-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch6SinkExample.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.Elasticsearch1ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.examples.ElasticsearchSinkExample.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.Elasticsearch5ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.test.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.Elasticsearch2ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchApiCallBridge.java</file>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-18 01:00:00" id="9886" opendate="2018-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build SQL jars with every build</summary>
      <description>Currently, the shaded fat jars for SQL are only built in the -Prelease profile. However, end-to-end tests require those jars and should also be able to test them. E.g. existing META-INF entry and proper shading. We should build them with every release. If a build should happen quicker one can use the -Pfast profile.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.pom.xml</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-24 01:00:00" id="9935" opendate="2018-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batch Table API: grouping by window and attribute causes java.lang.ClassCastException:</summary>
      <description> Grouping by window AND some other attribute(s) seems broken. Test case attached:class BatchStatisticsIntegrationTest extends FlatSpec with Matchers { trait BatchContext { implicit lazy val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment implicit val tableEnv: BatchTableEnvironment = TableEnvironment.getTableEnvironment(env) val data = Seq( (1532424567000L, "id1", "location1"), (1532424567000L, "id2", "location1"), (1532424567000L, "id3", "location1"), (1532424568000L, "id1", "location2"), (1532424568000L, "id2", "location3") ) val rawDataSet: DataSet[(Long, String, String)] = env.fromCollection(data) val table: Table = tableEnv.fromDataSet(rawDataSet, 'rowtime, 'id, 'location) } it should "be possible to run Table API queries with grouping by tumble window and column(s) on batch data" in new BatchContext { val results = table .window(Tumble over 1.second on 'rowtime as 'w) .groupBy('w, 'location) .select( 'w.start.cast(Types.LONG), 'w.end.cast(Types.LONG), 'location, 'id.count ) .toDataSet[(Long, Long, String, Long)] .collect() results should contain theSameElementsAs Seq( (1532424567000L, 1532424568000L, "location1", 3L), (1532424568000L, 1532424569000L, "location2", 1L), (1532424568000L, 1532424569000L, "location3", 1L) ) }}It seems like during execution time, the 'rowtime attribute replaces 'location and that causes ClassCastException.[info] Cause: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.String[info] at org.apache.flink.api.common.typeutils.base.StringSerializer.serialize(StringSerializer.java:28)[info] at org.apache.flink.api.java.typeutils.runtime.RowSerializer.serialize(RowSerializer.java:160)[info] at org.apache.flink.api.java.typeutils.runtime.RowSerializer.serialize(RowSerializer.java:46)[info] at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:54)[info] at org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.addRecord(SpanningRecordSerializer.java:88)[info] at org.apache.flink.runtime.io.network.api.writer.RecordWriter.sendToTarget(RecordWriter.java:129)[info] at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:105)[info] at org.apache.flink.runtime.operators.shipping.OutputCollector.collect(OutputCollector.java:65)[info] at org.apache.flink.runtime.operators.util.metrics.CountingCollector.collect(CountingCollector.java:35)[info] at org.apache.flink.api.java.operators.translation.RichCombineToGroupCombineWrapper.combine(RichCombineToGroupCombineWrapper.java:52)Here is some debug information that I was able to get. So, field serializers don't match the type of Row fields:this.instance = {Row@68451} "1532424567000,(3),1532424567000" fields = {Object[3]@68461} 0 = {Long@68462} 1532424567000 1 = {CountAccumulator@68463} "(3)" 2 = {Long@68462} 1532424567000this.serializer = {RowSerializer@68452} fieldSerializers = {TypeSerializer[3]@68455} 0 = {StringSerializer@68458} 1 = {TupleSerializer@68459} 2 = {LongSerializer@68460} arity = 3 nullMask = {boolean[3]@68457}  </description>
      <version>1.4.2,1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.0,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.GroupWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-25 01:00:00" id="9946" opendate="2018-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstart E2E test archetype version is hard-coded</summary>
      <description>mvn archetype:generate \ -DarchetypeGroupId=org.apache.flink \ -DarchetypeArtifactId=flink-quickstart-${TEST_TYPE} \ -DarchetypeVersion=1.6-SNAPSHOT \ -DgroupId=org.apache.flink.quickstart \ -DartifactId=${ARTIFACT_ID} \ -Dversion=${ARTIFACT_VERSION} \ -Dpackage=org.apache.flink.quickstart \ -DinteractiveMode=false</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-25 01:00:00" id="9951" opendate="2018-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update scm developerConnection</summary>
      <description>The developer connection must be updated to point to the update remote.</description>
      <version>1.3.3,1.4.2,1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.0,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>