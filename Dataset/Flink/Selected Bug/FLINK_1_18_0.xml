<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2022-6-7 01:00:00" id="27925" opendate="2022-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid to create watcher without the resourceVersion</summary>
      <description>Currently, we create the watcher in KubernetesResourceManager. But it do not pass the resourceVersion parameter, it will trigger a request to etcd. It will bring the burden to the etcd in large scale cluster (which have been seen in our internal k8s cluster). More detail can be found here I think we could use the informer to improve it (which will spawn a list-watch and maintain the resourceVersion internally)</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.NoOpWatchCallbackHandler.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformerITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPodsWatcherTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClientTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-2-9 01:00:00" id="30972" opendate="2023-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>E2e tests always fail in phase "Prepare E2E run"</summary>
      <description>Installing required softwareReading package lists...Building dependency tree...Reading state information...bc is already the newest version (1.07.1-2build1).bc set to manually installed.libapr1 is already the newest version (1.6.5-1ubuntu1).libapr1 set to manually installed.0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.--2023-02-09 04:38:47-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2023-02-09 04:38:47 ERROR 404: Not Found.WARNING: apt does not have a stable CLI interface. Use with caution in scripts.Reading package lists...E: Unsupported file ./libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.deb given on commandline##[error]Bash exited with code '100'.Finishing: Prepare E2E run</description>
      <version>1.17.0,1.15.4,1.16.2,1.18.0</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-3-21 01:00:00" id="31166" opendate="2023-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>array_contains does NOT work when haystack elements are not nullable and needle is nullable</summary>
      <description>ARRAY_CONTAINS works ok for the case when both haystack elements and needle are not nullable e.g.SELECT array_contains(ARRAY[0, 1], 0);it works ok when both haystack elements and needle are nullable e.g.SELECT array_contains(ARRAY[0, 1, NULL], CAST(NULL AS INT));it works ok when haystack elements are nullable and needle is not nullable e.g.SELECT array_contains(ARRAY[0, 1, NULL], 1);and it does NOT work when haystack elements are not nullable and needle is nullable e.g.SELECT array_contains(ARRAY[0, 1], CAST(NULL AS INT));  </description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CollectionFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.ArrayElementArgumentTypeStrategy.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-2-28 01:00:00" id="31250" opendate="2023-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet format supports MULTISET type</summary>
      <description>Parquet format supports ARRAY, MAP and ROW type, doesn't support MULTISET type. Parquet format should support MULTISET type.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReaderTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.utils.ParquetSchemaConverter.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.row.ParquetRowDataWriter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-6 01:00:00" id="3126" opendate="2015-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove accumulator type from "value" in web frontend</summary>
      <description>The web frontend shows the type of the counter twice:http://i.imgur.com/yBWT8GR.pngIt would be nicer to just show the value "42" instead of "LongValue 42" there.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.accumulators.StringifiedAccumulatorResult.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-3-9 01:00:00" id="31378" opendate="2023-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation fails to build due to lack of package</summary>
      <description>In Project Configuration Section it shows that "If you want to run your job by simply executing the main class, you will need flink-runtime in your classpath". However, when I just add flink-runtime in my classPath, an error is thrown like this:"No ExecutorFactory found to execute the application".It seems that flink-clients is also needed to supply an excutor through Java Service Load.Could you please add this in official article for beginners like me? </description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.configuration.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.configuration.overview.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-3-9 01:00:00" id="31385" opendate="2023-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce extended Assertj Matchers for completable futures</summary>
      <description>Introduce extended Assertj Matchers for completable futures that don't rely on timeouts.In general, we want to avoid relying on timeouts in the Flink test suite to get additional context (thread dump) in case something gets stuck.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.FlinkAssertions.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-8 01:00:00" id="3140" opendate="2015-12-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NULL value data layout in Row Serializer/Comparator</summary>
      <description>To store/materialize NULL value in Row objects, we should need new Row Serializer/Comparator which is aware of NULL value fields.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.CaseClassTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggreagationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeinfo.RowTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeinfo.RowSerializer.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorTTT2Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorTTT1Test.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.ComparatorTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-3-11 01:00:00" id="31405" opendate="2023-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor tests to git rid of timeout of CompletableFuture assertions.</summary>
      <description>In general, we want to avoid relying on local timeouts in the Flink test suite to get additional context (thread dump) in case something gets stuck(see Code Style and Quality Guide).Some of timeout in tests are introduced by assertions for CompleteFuture. After FLINK-31385, we can refactor these tests to git rid of timeout for CompletableFuture assertions.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.test.java.org.apache.flink.core.testutils.FlinkAssertionsTest.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.FlinkCompletableFutureAssert.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.testsuites.SinkTestSuiteBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.ThreadInfoSampleServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataSpillerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.cleanup.DefaultResourceCleanerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.cleanup.CheckpointResourcesCleanupRunnerTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.PerJobMiniClusterFactoryTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.ClientHeartbeatTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-4-14 01:00:00" id="31447" opendate="2023-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aligning unit tests of FineGrainedSlotManager with DeclarativeSlotManager</summary>
      <description>There's the DeclarativeSlotManagerTest that covers some specific issues that should be ported to the fine grained slot manager.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.MockResourceManagerRuntimeServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TestingResourceAllocatorBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TestingResourceAllocator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerDefaultResourceAllocationStrategyITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.DefaultSlotStatusSyncerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.AbstractFineGrainedSlotManagerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-5-14 01:00:00" id="31448" opendate="2023-3-14 00:00:00" resolution="Done">
    <buginformation>
      <summary>Use FineGrainedSlotManager as the default SlotManager</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.test.controller.sh</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.cluster.configuration.html</file>
      <file type="M">docs.content.docs.deployment.finegrained.resource.md</file>
      <file type="M">docs.content.zh.docs.deployment.finegrained.resource.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-5-17 01:00:00" id="31498" opendate="2023-3-17 00:00:00" resolution="Done">
    <buginformation>
      <summary>DeclartiveSlotManager always request redundant task manager when resource is not enough</summary>
      <description>Currently redundant task manager check in DeclarativeSlotManager only compare free slots with required redundant slots. when there are no enough resources in YARN/Kubernetes, this mechanism will always try to request new task manager. there are two way to address this.1. maintain the state of redundant workers to avoid request twice2. only try to request redundant workers when there is no pending workerThe first way will make the logic of redundant worker too complicated, I would like to choose the second wayLooking forward to any suggestion.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskExecutorManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskExecutorManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-1-17 01:00:00" id="31501" opendate="2023-3-17 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Move SqlCreateView conversion logic to SqlCreateViewConverter</summary>
      <description>Introduce SqlCreateViewConverter and move the conversion logic of SqlCreateView -&gt; CreateViewOperation to it.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-4-18 01:00:00" id="31510" opendate="2023-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use getMemorySize instead of getMemory</summary>
      <description>In YARN-4844, use getMemorySize instead of getMemory, because using int to represent memory may exceed the bounds in some cases and produce negative numbers.This change was merged in HADOOP-2.8.0, we should use getMemorySize instead of getMemory.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.TaskExecutorProcessSpecContainerResourcePriorityAdapterTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.TaskExecutorProcessSpecContainerResourcePriorityAdapter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-3-21 01:00:00" id="31538" opendate="2023-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supports parse catalog/database and properties for uri</summary>
      <description>Supports parse catalog/database and properties for uri</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkDriver.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-3-21 01:00:00" id="31547" opendate="2023-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce FlinkResultSetMetaData for jdbc driver</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-jdbc-driver-bundle.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-21 01:00:00" id="31549" opendate="2023-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add doc for jdbc driver</summary>
      <description>1. How to use jdbc driver in java code2. How to use jdbc driver in tools</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.overview.md</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2023-5-28 01:00:00" id="31639" opendate="2023-3-28 00:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce tiered storage memory manager</summary>
      <description/>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.LocalBufferPool.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2023-7-28 01:00:00" id="31646" opendate="2023-3-28 00:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce remote storage tier</summary>
      <description/>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.common.TieredStorageTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.common.TieredStorageUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.common.TieredStorageConfiguration.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-4-3 01:00:00" id="31693" opendate="2023-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump http-cache-semantics from 4.1.0 to 4.1.1</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-4-3 01:00:00" id="31694" opendate="2023-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump ua-parser-js from 0.7.31 to 0.7.33</summary>
      <description>Dependabot PR: https://github.com/apache/flink/pull/21767</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-4-11 01:00:00" id="31767" opendate="2023-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the implementation for "analyze table" execution on partitioned table</summary>
      <description>Currently, for partitioned table, the "analyze table" command will generate a separate SQL statement for each partition. When there are too many partitions, the compilation/submission/execution time will be very long. This issue aims to improve it: we can combine the sql statements for each partition into one with "union all", and just need to execution one sql.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.AnalyzeTableUtil.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-7-17 01:00:00" id="31819" opendate="2023-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for using watermark advanced functions in sql</summary>
      <description>Add document for using watermark advanced functions in sql  </description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.concepts.time.attributes.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.time.attributes.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-4-18 01:00:00" id="31834" opendate="2023-4-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Azure Warning: no space left on device</summary>
      <description>In this CI run: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48213&amp;view=logs&amp;j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&amp;t=841082b6-1a93-5908-4d37-a071f4387a5f&amp;l=21There was this warning:Loaded image: confluentinc/cp-kafka:6.2.2Loaded image: testcontainers/ryuk:0.3.3ApplyLayer exit status 1 stdout: stderr: write /opt/jdk-15.0.1+9/lib/modules: no space left on device##[error]Bash exited with code '1'.Finishing: Restore docker images</description>
      <version>None</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-4-19 01:00:00" id="31859" opendate="2023-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update maven cyclonedx plugin to 2.7.7</summary>
      <description>there are at least 2 related improvements1. current version depends on jackson-databind 2.14.0 and has a memory issue described at https://github.com/FasterXML/jackson-databind/issues/3665 which is fixed in later versions2. current version leads to lots of traces in logs (e.g. mvn clean verify for flink-core) which is fixed in later versions[ERROR] An error occurred attempting to read POMorg.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen &lt;?xml version="1.0" encoding="ISO-8859-1"... @1:42) at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423) at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345) at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197) at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828) at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757) at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375) at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940) at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612) at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627) at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759) at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746) at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694) at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524) at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481) at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137) at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:370) at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:351) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:171) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:163) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:294) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192) at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105) at org.apache.maven.cli.MavenCli.execute (MavenCli.java:960) at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:293) at org.apache.maven.cli.MavenCli.main (MavenCli.java:196) at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke (Method.java:498) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282) at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406) at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)</description>
      <version>1.17.0,1.18.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-4-23 01:00:00" id="31888" opendate="2023-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce interfaces and utility classes related to enrichment/labelling of failures leading to job restart</summary>
      <description>We need to introduce new interfaces/implementations for FailureEnricher / Context / FailureEnricherUtils</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-6-27 01:00:00" id="31957" opendate="2023-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the user story</summary>
      <description>Add documentation on how to use compiled plan to configure operator-level state TTL.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.concepts.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-4-27 01:00:00" id="31962" opendate="2023-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>libssl not found when running CI</summary>
      <description>Installed Maven 3.2.5 to /home/vsts/maven_cache/apache-maven-3.2.5Installing required softwareReading package lists...Building dependency tree...Reading state information...bc is already the newest version (1.07.1-2build1).bc set to manually installed.libapr1 is already the newest version (1.6.5-1ubuntu1).libapr1 set to manually installed.0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.--2023-04-27 11:42:53-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.11_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2023-04-27 11:42:53 ERROR 404: Not Found.</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-7-4 01:00:00" id="31996" opendate="2023-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chaining operators with different max parallelism prevents rescaling</summary>
      <description>We might chain operators with different max parallelism together if they are set to have the same parallelism initially.When we decide to rescale the JobGraph vertices (using AdaptiveScheduler), we're gapped by the lowest maxParallelism of the operator chain. This is especially visible with things like CollectSink, TwoPhaseCommitSink, CDC, and a GlobalCommiter with maxParallelism set to 1. An obvious solution would be to prevent the chaining of operators with different maxParallelism, but we need to double-check this doesn't introduce a breaking change.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.StreamingScalaAPICompletenessTest.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.PipelineOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pipeline.configuration.html</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-5-11 01:00:00" id="32058" opendate="2023-5-11 00:00:00" resolution="Done">
    <buginformation>
      <summary>Migrate subclasses of BatchAbstractTestBase in runtime.batch.sql to JUnit5</summary>
      <description>Migrate subclasses of BatchAbstractTestBase in runtime.batch.sql to JUnit5.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.junit.extensions.parameterized.ParameterizedTestExtension.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.WindowTableFunctionITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.ValuesITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.UnnestITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.UnionITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableScanITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.SortLimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OverAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.MultipleInputITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.MiscITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.Limit0RemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LegacyTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LegacyLimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CorrelateITCase2.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CodeSplitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-11 01:00:00" id="32059" opendate="2023-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate subclasses of BatchAbstractTestBase in batch.sql.agg and batch.sql.join to JUnit5</summary>
      <description>Migrate subclasses of BatchAbstractTestBase in batch.sql.agg and batch.sql.join to JUnit5.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.SemiJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.ScalarQueryITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.OuterJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinWithoutKeyITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinConditionTypeCoerceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.InnerJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortDistinctAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.PruneAggregateCallITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.HashAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.GroupingSetsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.DistinctAggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateRemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateJoinTransposeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-5-11 01:00:00" id="32060" opendate="2023-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate subclasses of BatchAbstractTestBase in table and other modules to JUnit5</summary>
      <description/>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.StreamFileSystemTestCsvITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.StreamFileSystemITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.FileSystemITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.SortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.OverAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.LimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.LegacyTableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.LegacyLimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.AggregationITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.FileSystemTestCsvITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase.scala</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonBatchFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFilesystemStreamITCase.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFilesystemBatchITCase.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroFilesystemITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-6 01:00:00" id="3208" opendate="2016-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename Gelly vertex-centric model to scatter-gather</summary>
      <description>The idea is to have the following naming: Pregel model: vertex-centric iteration Spargel model: scatter-gather iteration GSA model: as isOpen to suggestions!</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.test.VertexCentricConfigurationITCase.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.test.example.IncrementalSSSPITCase.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.test.CollectionModeSuperstepITCase.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.spargel.SpargelTranslationTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.spargel.SpargelCompilerTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.spargel.VertexUpdateFunction.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.spargel.VertexCentricIteration.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.spargel.VertexCentricConfiguration.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.spargel.MessagingFunction.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.SingleSourceShortestPaths.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.PageRank.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.LabelPropagation.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.ConnectedComponents.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.CommunityDetection.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.GraphCsvReader.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.Graph.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.example.SingleSourceShortestPaths.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.example.IncrementalSSSP.java</file>
      <file type="M">flink-libraries.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.Graph.scala</file>
      <file type="M">flink-libraries.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.example.SingleSourceShortestPaths.scala</file>
      <file type="M">docs.libs.gelly.guide.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-5-14 01:00:00" id="32094" opendate="2023-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>startScheduling.BATCH performance regression since May 11th</summary>
      <description>http://codespeed.dak8s.net:8000/timeline/#/?exe=5&amp;ben=startScheduling.BATCH&amp;extr=on&amp;quarts=on&amp;equid=off&amp;env=2&amp;revs=200</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.NettyShuffleMaster.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-5-17 01:00:00" id="32123" opendate="2023-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro Confluent Schema Registry nightly end-to-end test failed due to timeout</summary>
      <description>For the past few hours, E2E tests fail with: 'Avro Confluent Schema Registry nightly end-to-end test' failed after 9 minutes and 53 seconds! Test exited with exit code 1Looks like https://archive.apache.org/dist/kafka/  mirror is overloaded – download locally took more than 30minLets switch to  https://downloads.apache.org mirror </description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-11 01:00:00" id="3216" opendate="2016-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define pattern specification</summary>
      <description>In order to detect event patterns we first have to define the pattern. This issue tracks the progress of implementing a user facing API to define event patterns. Patterns should support the following operations next(): The given event has to follow directly after the preceding eventfollowedBy(): The given event has to follow the preceding event. There might occur other events in-between every(): In a follow-by relationship a starting event can be matched with multiple successive events. Consider the pattern a → b where → denotes the follow-by relationship. The event sequence a, b, b can be matched as a, b or a, (b), b where the first b is left out. The essential question is whether a is allowed to match multiple times or only the first time. The method every specifies exactly that. Every events in a pattern can match with multiple successive events. This makes only sense in a follow-by relationship, though. followedByEvery(): Similar to followedBy just that the specified element can be matched with multiple successive events or(): Alternative event which can be matched instead of the original event: every(“e1”).where().or(“e2”).where() within(): Defines a time interval in which the pattern has to be completed, otherwise an incomplete pattern can be emitted (timeout case)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-libraries.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-5-23 01:00:00" id="32162" opendate="2023-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Misleading log message due to missing null check</summary>
      <description>Updating the job requirements always logs "Failed to update requirements for job {}." because we don't check whether the error is not null.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-5-23 01:00:00" id="32166" opendate="2023-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show unassigned/total TM resources in web ui</summary>
      <description>It is important to know how many resources of a TM are currently assigned to jobs.This is different to what resources currently used, since you can have assigned 1gb memory to a job with it only using 10mb at this time.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.task-manager.ts</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-7-23 01:00:00" id="32168" opendate="2023-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log required/available resources in RM</summary>
      <description>When matching requirements against available resource the RM currently doesn't log anything apart from whether it could fulfill the resources or not.We can make the system easier to audit by logging the current requirements, available resources, and how many resources are left after the matching.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-23 01:00:00" id="32169" opendate="2023-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show allocated slots on TM page</summary>
      <description>Show the allocated slogs on the TM page, so that you can better understand which job is consuming what resources.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.task-manager.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-25 01:00:00" id="32186" opendate="2023-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support subtask stack auto-search when redirecting from subtask backpressure tab</summary>
      <description>Note that we have introduced a dump link on the backpressure page in FLINK-29996(Figure 1), which helps to check what are the corresponding subtask doing more easily.But we still have to search for the corresponding call stack of the back-pressured subtask from the whole TaskManager thread dumps, it's not convenient enough.Therefore, I would like to trigger the search for the editor automatically after redirecting from the backpressure tab, which will help to scroll the thread dumps to the corresponding call stack of the back-pressured subtask (As shown in Figure 2).Figure 1. ThreadDump Link in Backpressure TabFigure 2. Trigger Auto-search after Redirecting from Backpressure Tab</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.thread-dump.task-manager-thread-dump.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.thread-dump.task-manager-thread-dump.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-6-31 01:00:00" id="32231" opendate="2023-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>libssl not found when running CI</summary>
      <description>--2023-05-31 19:10:13-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.12_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 185.125.190.39, 91.189.91.38, 91.189.91.39, ...Connecting to security.ubuntu.com (security.ubuntu.com)|185.125.190.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2023-05-31 19:10:13 ERROR 404: Not Found.e.g.https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49523&amp;view=logs&amp;j=bea52777-eaf8-5663-8482-18fbc3630e81&amp;t=d6e79740-7cf7-5407-2e69-ca34c9be0efb&amp;l=265</description>
      <version>1.18.0,1.16.3,1.17.2</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-1 01:00:00" id="32232" opendate="2023-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supports parse truncate table statement</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-12 01:00:00" id="3226" opendate="2016-1-12 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Translate optimized logical Table API plans into physical plans representing DataSet programs</summary>
      <description>This issue is about translating an (optimized) logical Table API (see FLINK-3225) query plan into a physical plan. The physical plan is a 1-to-1 representation of the DataSet program that will be executed. This means: Each Flink RelNode refers to exactly one Flink DataSet or DataStream operator. All (join and grouping) keys of Flink operators are correctly specified. The expressions which are to be executed in user-code are identified. All fields are referenced with their physical execution-time index. Flink type information is available. Optional: Add physical execution hints for joinsThe translation should be the final part of Calcite's optimization process.For this task we need to: implement a set of Flink DataSet RelNodes. Each RelNode corresponds to one Flink DataSet operator (Map, Reduce, Join, ...). The RelNodes must hold all relevant operator information (keys, user-code expression, strategy hints, parallelism). implement rules to translate optimized Calcite RelNodes into Flink RelNodes. We start with a straight-forward mapping and later add rules that merge several relational operators into a single Flink operator, e.g., merge a join followed by a filter. Timo implemented some rules for the first SQL implementation which can be used as a starting point. Integrate the translation rules into the Calcite optimization process</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetProjectRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.typeinfo.RowSerializerTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.typeinfo.RowComparatorTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.typeinfo.RenamingProxyTypeInfoTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.PojoGroupingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.JoinITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeinfo.RowTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeinfo.RenamingProxyTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeinfo.RenameOperator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableConfig.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.package.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.ExpressionSelectFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.ExpressionJoinFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.ExpressionFilterFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.ExpressionAggregateFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetMap.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetExchange.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.literals.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.Indenter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.GenerateSelect.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.GenerateResultAssembler.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.GenerateJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.GenerateFilter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.ExpressionCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.TableConversions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetReduce.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetFlatMap.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.CastingITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.JavaBatchTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.ScalaBatchTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.PlanTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.DataSetTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.TypeConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.AsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.FilterITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.SelectITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.AsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.SelectITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetGroupReduce.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.RexNodeTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.TranslationContext.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.AggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.OperatorCodeGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.parser.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.CastingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.StringExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.FilterITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.StringExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.test.TableProgramsTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.AggregateFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.aggregate.Aggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.aggregate.AggregateFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.aggregate.AvgAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.aggregate.CountAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.aggregate.MaxAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.aggregate.MinAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.aggregate.SumAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.FunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.GroupedAggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.UnionITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetFilterRule.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-6-6 01:00:00" id="32271" opendate="2023-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Report RECOMMENDED_PARALLELISM as an autoscaler metric</summary>
      <description>It is beneficial to report the recommended parallelism and overlay it with the current parallelism on the same chart when auto scaler is running in advisor mode.</description>
      <version>None</version>
      <fixedVersion>kubernetes-operator-1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-8 01:00:00" id="32288" opendate="2023-6-8 00:00:00" resolution="Done">
    <buginformation>
      <summary>Improve the scheduling performance of AdaptiveBatchScheduler</summary>
      <description>After adding the benchmark of AdaptiveBatchScheduler in FLINK-30480, we noticed a regression in the performance of SchedulingDownstreamTasksInBatchJobBenchmark#SchedulingDownstreamTasks. When scheduling a batch job with a parallelism of 4000*4000, the time spent increased from 32ms to 1336ms on my local PC.To improve the performance, we can optimize the traversal by checking if the consumedPartitionGroups have finished all its partitions.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingInputConsumableDecider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.DefaultInputConsumableDeciderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.VertexwiseSchedulingStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.PartialFinishedInputConsumableDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.InputConsumableDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.DefaultInputConsumableDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.AllFinishedInputConsumableDecider.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-9 01:00:00" id="32294" opendate="2023-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The CI fails due to HiveITCase</summary>
      <description>2 ITCases fail: HiveITCase.testHiveDialect HiveITCase.testReadWriteHivehttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49766&amp;view=logs&amp;j=ef799394-2d67-5ff4-b2e5-410b80c9c0af&amp;t=9e5768bc-daae-5f5f-1861-e58617922c7a&amp;l=14346 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49766&amp;view=logs&amp;j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&amp;s=ae4f8708-9994-57d3-c2d7-b892156e7812&amp;t=0f3adb59-eefa-51c6-2858-3654d9e0749d&amp;l=14652  </description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hive.src.test.java.org.apache.flink.tests.hive.containers.HiveContainer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-13 01:00:00" id="3230" opendate="2016-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis streaming producer</summary>
      <description>Add a FlinkKinesisProducer for the Flink Kinesis streaming connector. We will be using AWS SDK implementation for code consistency with the FlinkKinesisConsumer.The features of FlinkKinesisProducer is rather straightforward:1. Partition put records based on partition key.2. Configurable put mode: Bulk put for higher throughput vs. sequential single record puts. Size of bulk should also be configurable.3. For bulk put, user can also choose to enforce strict ordering of the result with the tradeoff of higher put latency. Ref: https://brandur.org/kinesis-order</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.create.release.files.sh</file>
      <file type="M">flink-streaming-connectors.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-twitter.pom.xml</file>
      <file type="M">docs.apis.streaming.connectors.hdfs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-9 01:00:00" id="32300" opendate="2023-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support get object for result set</summary>
      <description>Support get object for result set</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.test.java.org.apache.flink.table.jdbc.FlinkStatementTest.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.test.java.org.apache.flink.table.jdbc.FlinkResultSetTest.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkStatement.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkResultSet.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.BaseStatement.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver-bundle.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-9 01:00:00" id="32302" opendate="2023-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable Hbase 2.x tests on Java 17</summary>
      <description>Lacking support on the HBase side. Version bumps may solve it, but that's out of scope of this issue since the connector is being externalized.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-9 01:00:00" id="32304" opendate="2023-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce rpc-akka jar size</summary>
      <description>We bundle unnecessary dependencies in the rpc-akka jar; we can easily shave of 15mb of dependencies.</description>
      <version>None</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-rpc.flink-rpc-akka.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-6-14 01:00:00" id="32337" opendate="2023-6-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL array_union could return wrong result</summary>
      <description>This is was mentioned at https://github.com/apache/flink/pull/22717#issuecomment-1587333488 how to reproduceSELECT array_union(ARRAY[CAST(NULL AS INT)], ARRAY[1]); -- returns [NULL, 1], this is OKSELECT array_union(ARRAY[1], ARRAY[CAST(NULL AS INT)]); -- returns [1, 0], this is NOT OK</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.scalar.ArrayUnionFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CollectionFunctionsITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-14 01:00:00" id="32338" opendate="2023-6-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add FailsOnJava17 annotation</summary>
      <description>Add an annotation for disabling specific tests on Java 17, similar to FailsOnJava11.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hbase.src.test.java.org.apache.flink.tests.util.hbase.SQLClientHBaseITCase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-6-16 01:00:00" id="32369" opendate="2023-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setup cron build</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-6-20 01:00:00" id="32389" opendate="2023-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Dependabot] Bump guava from 27.0.1-jre to 32.0.0-jre</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-20 01:00:00" id="32390" opendate="2023-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Dependabot] Bump socket.io-parser and socket.io and engine.io</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-6-20 01:00:00" id="32392" opendate="2023-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Several jobs failed on AZP with No space left on device</summary>
      <description>This Build failed with no space left https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50162&amp;view=logs&amp;j=585d8b77-fa33-51bc-8163-03e54ba9ce5b##[error]Unhandled exception. System.IO.IOException: No space left on device : '/home/vsts/agents/3.220.5/_diag/Worker_20230619-021757-utc.log' at System.IO.RandomAccess.WriteAtOffset(SafeFileHandle handle, ReadOnlySpan`1 buffer, Int64 fileOffset) at System.IO.Strategies.BufferedFileStreamStrategy.FlushWrite() at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder) at System.Diagnostics.TextWriterTraceListener.Flush() at Microsoft.VisualStudio.Services.Agent.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id) in /home/vsts/work/1/s/src/Microsoft.VisualStudio.Services.Agent/HostTraceListener.cs:line 151 at Microsoft.VisualStudio.Services.Agent.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message) in /home/vsts/work/1/s/src/Microsoft.VisualStudio.Services.Agent/HostTraceListener.cs:line 81 at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message) at Microsoft.VisualStudio.Services.Agent.Util.ProcessInvoker.ProcessExitedHandler(Object sender, EventArgs e) in /home/vsts/work/1/s/src/Agent.Sdk/ProcessInvoker.cs:line 496 at System.Diagnostics.Process.OnExited() at System.Diagnostics.Process.RaiseOnExited() at System.Diagnostics.Process.CompletionCallback(Object waitHandleContext, Boolean wasSignaled) at System.Threading._ThreadPoolWaitOrTimerCallback.WaitOrTimerCallback_Context_f(Object state) at System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state)--- End of stack trace from previous location --- at System.Threading._ThreadPoolWaitOrTimerCallback.PerformWaitOrTimerCallback(_ThreadPoolWaitOrTimerCallback helper, Boolean timedOut) at System.Threading.PortableThreadPool.CompleteWait(RegisteredWaitHandle handle, Boolean timedOut) at System.Threading.ThreadPoolWorkQueue.Dispatch() at System.Threading.PortableThreadPool.WorkerThread.WorkerThreadStart(),##[error]The hosted runner encountered an error while running your job. (Error Type: Failure).for 1.16, 1.17 it happens while 'Upload artifacts to S3'for 1.18 while 'Deploy maven snapshot'</description>
      <version>1.18.0,1.16.3,1.17.2</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
      <file type="M">tools.azure-pipelines.build-nightly-dist.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2023-6-25 01:00:00" id="32426" opendate="2023-6-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix adaptive local hash agg can&amp;#39;t work when auxGrouping exist</summary>
      <description>For the following case, the field `a` is primary key,  we select from `AuxGroupingTable` and group by a, b. Since a is primary key, it also guarantee the unique, so planner will extract b as auxGrouping field.registerCollection( "AuxGroupingTable", data2, type2, "a, b, c, d, e", nullablesOfData2, FlinkStatistic.builder().uniqueKeys(Set(Set("a").asJava).asJava).build())checkResult( "SELECT a, b, COUNT(c) FROM AuxGroupingTable GROUP BY a, b", Seq( row(1, 1, 1), row(2, 3, 2), row(3, 4, 3), row(4, 10, 4), row(5, 11, 5) ))  Due to the generated code doesn't get auxGrouping fields from input RowData and then setting it to aggBuffer, the aggBuffer RowData loses some fields, and it will throw an index Exception when get the field from it. As following:Caused by: java.lang.AssertionError: index (1) should &lt; 1    at org.apache.flink.table.data.binary.BinaryRowData.assertIndexIsValid(BinaryRowData.java:127)    at org.apache.flink.table.data.binary.BinaryRowData.isNullAt(BinaryRowData.java:156)    at org.apache.flink.table.data.utils.JoinedRowData.isNullAt(JoinedRowData.java:113)    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.toBinaryRow(RowDataSerializer.java:201)    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:103)    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:48)    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:165)    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:43)    at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:54)    at org.apache.flink.runtime.io.network.api.writer.RecordWriter.serializeRecord(RecordWriter.java:141)    at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:107)    at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:55)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:134)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collectAndCheckIfChained(RecordWriterOutput.java:114)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:95)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:48)    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:59)    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:31)    at LocalHashAggregateWithKeys$39.processElement_split2(Unknown Source)    at LocalHashAggregateWithKeys$39.processElement(Unknown Source)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)    at BatchExecCalc$10.processElement(Unknown Source)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)    at SourceConversion$6.processElement(Unknown Source)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)    at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)    at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)    at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)    at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)</description>
      <version>1.18.0,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.HashAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-25 01:00:00" id="32428" opendate="2023-6-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce base interfaces for CatalogStore</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-17 01:00:00" id="3246" opendate="2016-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consolidate maven project names with *-parent suffix</summary>
      <description>The projects flink-streaming-connectors-parent and flink-contrib parent carry the unnecessary -parent suffix.I suspect that was mistakenly added when looking at the root project called flink-parent. The suffix was added there to not have a project with an unqualified name flink. However, for the projects mentioned here, that suffix is not necessary and can be dropped.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-twitter.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-rabbitmq.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-nifi.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-flume.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-elasticsearch.pom.xml</file>
      <file type="M">flink-contrib.pom.xml</file>
      <file type="M">flink-contrib.flink-tweet-inputformat.pom.xml</file>
      <file type="M">flink-contrib.flink-streaming-contrib.pom.xml</file>
      <file type="M">flink-contrib.flink-storm.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
      <file type="M">flink-contrib.flink-operator-stats.pom.xml</file>
      <file type="M">flink-contrib.flink-connector-wikiedits.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-28 01:00:00" id="32460" opendate="2023-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add doc for list procedures</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.show.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-7-29 01:00:00" id="32491" opendate="2023-6-29 00:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce RuntimeFilterOperator to support runtime filter which can reduce the shuffle data size before shuffle join</summary>
      <description/>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.runtimefilter.LocalRuntimeFilterBuilderOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.runtimefilter.GlobalRuntimeFilterBuilderOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.runtimefilter.util.RuntimeFilterUtils.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.runtimefilter.LocalRuntimeFilterBuilderOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-30 01:00:00" id="32492" opendate="2023-6-30 00:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce FlinkRuntimeFilterProgram to inject runtime filter</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.DynamicFilteringITCase.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.optimizer.config.configuration.html</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.JoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalJoinRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-30 01:00:00" id="32498" opendate="2023-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>array_max return type should always nullable</summary>
      <description/>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CollectionFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.strategies.ArrayElementOutputTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.SpecificTypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.ArrayElementOutputTypeStrategy.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-18 01:00:00" id="3250" opendate="2016-1-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Savepoint coordinator requires too strict parallelism match</summary>
      <description>The savepoint coordinator requires that the number of collected states of an operator and the parallelism of the new job (the one to be restored) match exactly. This is too strict.For example a Kafka source with parallelism 2 and a single Kafka partition does not collect state for one of the two sources (hence it is not part of the savepoint state). Currently, restoring the same job with the same parallelism fails, which should not happen.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.SavepointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SavepointCoordinator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-7-4 01:00:00" id="32526" opendate="2023-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update apache parquet to 1.13.1</summary>
      <description>Now 1.13.1 is availablehttps://parquet.apache.org/blog/</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-formats.flink-sql-parquet.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-4 01:00:00" id="32536" opendate="2023-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python tests fail with Arrow DirectBuffer exception</summary>
      <description>https://dev.azure.com/chesnay/flink/_build/results?buildId=3674&amp;view=logs&amp;j=fba17979-6d2e-591d-72f1-97cf42797c11&amp;t=727942b6-6137-54f7-1ef9-e66e706ea0682023-07-04T12:54:15.5296754Z Jul 04 12:54:15 E py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame.2023-07-04T12:54:15.5299579Z Jul 04 12:54:15 E : java.lang.RuntimeException: Arrow depends on DirectByteBuffer.&lt;init&gt;(long, int) which is not available. Please set the system property 'io.netty.tryReflectionSetAccessible' to 'true'.2023-07-04T12:54:15.5302307Z Jul 04 12:54:15 E at org.apache.flink.table.runtime.arrow.ArrowUtils.checkArrowUsable(ArrowUtils.java:184)2023-07-04T12:54:15.5302859Z Jul 04 12:54:15 E at org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame(ArrowUtils.java:546)2023-07-04T12:54:15.5303177Z Jul 04 12:54:15 E at jdk.internal.reflect.GeneratedMethodAccessor287.invoke(Unknown Source)2023-07-04T12:54:15.5303515Z Jul 04 12:54:15 E at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2023-07-04T12:54:15.5303929Z Jul 04 12:54:15 E at java.base/java.lang.reflect.Method.invoke(Method.java:568)2023-07-04T12:54:15.5307338Z Jul 04 12:54:15 E at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)2023-07-04T12:54:15.5309888Z Jul 04 12:54:15 E at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)2023-07-04T12:54:15.5310306Z Jul 04 12:54:15 E at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)2023-07-04T12:54:15.5337220Z Jul 04 12:54:15 E at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)2023-07-04T12:54:15.5341859Z Jul 04 12:54:15 E at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)2023-07-04T12:54:15.5342363Z Jul 04 12:54:15 E at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)2023-07-04T12:54:15.5344866Z Jul 04 12:54:15 E at java.base/java.lang.Thread.run(Thread.java:833)2023-07-04T12:54:15.5663559Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::BatchPandasConversionTests::test_empty_to_pandas2023-07-04T12:54:15.5663891Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::BatchPandasConversionTests::test_from_pandas2023-07-04T12:54:15.5664299Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::BatchPandasConversionTests::test_to_pandas2023-07-04T12:54:15.5664655Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::BatchPandasConversionTests::test_to_pandas_for_retract_table2023-07-04T12:54:15.5665003Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::StreamPandasConversionTests::test_empty_to_pandas2023-07-04T12:54:15.5665360Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::StreamPandasConversionTests::test_from_pandas2023-07-04T12:54:15.5665704Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::StreamPandasConversionTests::test_to_pandas2023-07-04T12:54:15.5666045Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::StreamPandasConversionTests::test_to_pandas_for_retract_table2023-07-04T12:54:15.5666415Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::StreamPandasConversionTests::test_to_pandas_with_event_time2023-07-04T12:54:15.5666840Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_group_aggregate_function2023-07-04T12:54:15.5667189Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_group_aggregate_with_aux_group2023-07-04T12:54:15.5667526Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_group_aggregate_without_keys2023-07-04T12:54:15.5667882Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_over_window_aggregate_function2023-07-04T12:54:15.5668242Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_slide_group_window_aggregate_function2023-07-04T12:54:15.5668607Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_tumble_group_window_aggregate_function2023-07-04T12:54:15.5668961Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_execute_over_aggregate_from_json_plan2023-07-04T12:54:15.5669334Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_proc_time_over_rows_window_aggregate_function2023-07-04T12:54:15.5669714Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_row_time_over_range_window_aggregate_function2023-07-04T12:54:15.5670085Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_row_time_over_rows_window_aggregate_function2023-07-04T12:54:15.5670450Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_sliding_group_window_over_count2023-07-04T12:54:15.5670804Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_sliding_group_window_over_time2023-07-04T12:54:15.5671168Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_tumbling_group_window_over_count2023-07-04T12:54:15.5671510Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_tumbling_group_window_over_time2023-07-04T12:54:15.5671847Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::BatchPandasUDFITTests::test_all_data_types2023-07-04T12:54:15.5672171Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::BatchPandasUDFITTests::test_basic_functionality2023-07-04T12:54:15.5672544Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::BatchPandasUDFITTests::test_data_types2023-07-04T12:54:15.5672864Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::BatchPandasUDFITTests::test_invalid_pandas_udf2023-07-04T12:54:15.5673188Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::StreamPandasUDFITTests::test_all_data_types2023-07-04T12:54:15.5673501Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::StreamPandasUDFITTests::test_basic_functionality2023-07-04T12:54:15.5673881Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::StreamPandasUDFITTests::test_data_types2023-07-04T12:54:15.5674201Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::StreamPandasUDFITTests::test_invalid_pandas_udf2023-07-04T12:54:15.5674544Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::BatchRowBasedOperationITTests::test_aggregate_with_pandas_udaf2023-07-04T12:54:15.5674941Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::BatchRowBasedOperationITTests::test_aggregate_with_pandas_udaf_without_keys2023-07-04T12:54:15.5675324Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::BatchRowBasedOperationITTests::test_map_with_pandas_udf2023-07-04T12:54:15.5675684Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_aggregate2023-07-04T12:54:15.5676033Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_flat_aggregate2023-07-04T12:54:15.5676401Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_flat_aggregate_list_view2023-07-04T12:54:15.5676771Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_map_with_pandas_udf2023-07-04T12:54:15.5677224Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_sql.py::JavaSqlTests::test_java_sql_ddl - sub...2023-07-04T12:54:15.5677536Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_data_view_clear2023-07-04T12:54:15.5677856Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_distinct_and_filter2023-07-04T12:54:15.5678162Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_double_aggregate2023-07-04T12:54:15.5678476Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_list_view2023-07-04T12:54:15.5678784Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_map_view2023-07-04T12:54:15.5679082Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_map_view_iterate</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-5 01:00:00" id="32539" opendate="2023-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Archunit violations started to fail in test_misc</summary>
      <description>blocker since now it fails on every buildto reproduce jdk 8 is requiredmvn clean install -DskipTestsmvn verify -pl flink-architecture-tests/flink-architecture-tests-production/ -Darchunit.freeze.store.default.allowStoreUpdate=falseIt seems the reason is FLINK-27415where it was removed line checkArgument(fileLength &gt; 0L);at the same time it was mentioned in achunit violations and now should be removed as wellexample of failurehttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50946&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=995c650b-6573-581c-9ce6-7ad4cc038461&amp;l=23064</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.f7a4e6fa-e7de-48c9-a61e-c13e83f0c72e</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-7-5 01:00:00" id="32544" opendate="2023-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PythonFunctionFactoryTest fails on Java 17</summary>
      <description>https://dev.azure.com/chesnay/flink/_build/results?buildId=3676&amp;view=logs&amp;j=fba17979-6d2e-591d-72f1-97cf42797c11&amp;t=727942b6-6137-54f7-1ef9-e66e706ea068Jul 05 10:17:23 Exception in thread "main" java.lang.reflect.InaccessibleObjectException: Unable to make field private static java.util.IdentityHashMap java.lang.ApplicationShutdownHooks.hooks accessible: module java.base does not "opens java.lang" to unnamed module @1880a322Jul 05 10:17:23 at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)Jul 05 10:17:23 at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)Jul 05 10:17:23 at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)Jul 05 10:17:23 at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)Jul 05 10:17:23 at org.apache.flink.client.python.PythonFunctionFactoryTest.closeStartedPythonProcess(PythonFunctionFactoryTest.java:115)Jul 05 10:17:23 at org.apache.flink.client.python.PythonFunctionFactoryTest.cleanEnvironment(PythonFunctionFactoryTest.java:79)Jul 05 10:17:23 at org.apache.flink.client.python.PythonFunctionFactoryTest.main(PythonFunctionFactoryTest.java:52)Side-notes: maybe re-evaluate if the test could be run through maven now The shutdown hooks business is quite sketchy, and AFAICT would be unnecessary if the test were an ITCase</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.sql.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-7-10 01:00:00" id="32568" opendate="2023-7-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure that all subtasks are sorted by busy ratio at the backpressure tab by default</summary>
      <description>After FLINK-29998 and FLINK-30468, all subtasks are sorted by busy ratio at the backpressure tab by default.FLINK-30829 makes the backpressure tab could be sort by busy/backpressure/idle seperately. However, the default sort rule is changed.Following is the picture about it, all subtask are sorted by 3 columns be default:</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-7-12 01:00:00" id="32585" opendate="2023-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filter javax.xml.bind:jaxb-api false positive for Pulsar connector</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.licensecheck.JarFileChecker.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-7-18 01:00:00" id="32623" opendate="2023-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rest api doesn&amp;#39;t return minimum resource requirements correctly</summary>
      <description>The resource requirements returned by the rest api always return a hardcoded 1 lower bound for each vertex.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-8-24 01:00:00" id="32656" opendate="2023-7-24 00:00:00" resolution="Done">
    <buginformation>
      <summary>Deprecate ManagedTable related APIs</summary>
      <description>Please refer to FLIP-346: Deprecate ManagedTable related APIs for more details.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.ManagedTableFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.RequireCatalogLock.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.CatalogLock.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.ManagedTableListener.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableDescriptor.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalogLock.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-8-25 01:00:00" id="32670" opendate="2023-7-25 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Cascade deprecation to classes that implement SourceFunction</summary>
      <description/>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.windowing.GroupedProcessingTimeWindowExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.statemachine.StateMachineExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.statemachine.KafkaEventsGeneratorJob.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.statemachine.generator.EventsGeneratorSource.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.gpu.MatrixVectorMul.java</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-common.src.main.java.org.apache.flink.walkthrough.common.source.TransactionSource.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.FiniteTestSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.StatefulSequenceSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.SocketTextStreamFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.RichSourceFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ParallelSourceFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.MultipleIdsMessageAcknowledgingSourceBase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.MessageAcknowledgingSourceBase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.FromSplittableIteratorFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.FromIteratorFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.FromElementsFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.FileMonitoringFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.checkpoint.ExternallyInducedSource.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.sources.ArrowSourceFunction.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.TtlStateUpdateSource.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-sql-test.src.main.java.org.apache.flink.sql.tests.StreamSQLTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-state-evolution-test.src.main.java.org.apache.flink.test.StatefulStreamingJob.java</file>
      <file type="M">flink-end-to-end-tests.flink-queryable-state-test.src.main.java.org.apache.flink.streaming.tests.queryablestate.QsStateProducer.java</file>
      <file type="M">flink-end-to-end-tests.flink-netty-shuffle-memory-control-test.src.main.java.org.apache.flink.streaming.tests.NettyShuffleMemoryControlTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-local-recovery-and-allocation-test.src.main.java.org.apache.flink.streaming.tests.StickyAllocationAndLocalRecoveryTestJob.java</file>
      <file type="M">flink-end-to-end-tests.flink-heavy-deployment-stress-test.src.main.java.org.apache.flink.deployment.HeavyDeploymentStressTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-file-sink-test.src.main.java.org.apache.flink.connector.file.sink.FileSinkProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.SequenceGeneratorSource.java</file>
      <file type="M">flink-end-to-end-tests.flink-cli-test.src.main.java.org.apache.flink.streaming.tests.PeriodicStreamingJob.java</file>
      <file type="M">flink-contrib.flink-connector-wikiedits.src.main.java.org.apache.flink.streaming.connectors.wikiedits.WikipediaEditsSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-26 01:00:00" id="32674" opendate="2023-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the new Context.getTargetColumns</summary>
      <description/>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.insert.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.insert.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-8-26 01:00:00" id="32676" opendate="2023-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add doc for catalog modification listener</summary>
      <description>Add doc for catalog modification listener</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.catalogs.md</file>
      <file type="M">docs.content.zh.docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-8-26 01:00:00" id="32683" opendate="2023-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Pekko from 1.0.0 to 1.0.1</summary>
      <description>Updates Pekko dependency to 1.0.1 which contains the following bugfix https://github.com/apache/incubator-pekko/pull/492 . See https://github.com/apache/incubator-pekko/issues/491 for more info</description>
      <version>1.18.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-rpc.flink-rpc-akka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-8-26 01:00:00" id="32685" opendate="2023-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression on sortedMultiInput and sortedTwoInput since 2023-07-18</summary>
      <description>http://codespeed.dak8s.net:8000/timeline/#/?exe=1&amp;ben=sortedMultiInput&amp;extr=on&amp;quarts=on&amp;equid=off&amp;env=2&amp;revs=200http://codespeed.dak8s.net:8000/timeline/#/?exe=1&amp;ben=sortedTwoInput&amp;extr=on&amp;quarts=on&amp;equid=off&amp;env=2&amp;revs=200</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-8-26 01:00:00" id="32686" opendate="2023-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression on startScheduling.BATCH and startScheduling.STREAMING since 2023-07-24</summary>
      <description>http://codespeed.dak8s.net:8000/timeline/#/?exe=5&amp;ben=startScheduling.STREAMING&amp;extr=on&amp;quarts=on&amp;equid=off&amp;env=2&amp;revs=200http://codespeed.dak8s.net:8000/timeline/#/?exe=5&amp;ben=startScheduling.BATCH&amp;extr=on&amp;quarts=on&amp;equid=off&amp;env=2&amp;revs=200</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.NettyShuffleMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.common.TieredStoragePartitionId.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.common.TieredStorageIdMappingUtils.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-7-27 01:00:00" id="32703" opendate="2023-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hotfix] flink-python POM has a typo for protobuf-java in shading config</summary>
      <description>Fix typo. `inculde` -&gt; `include`                                  &lt;includes combine.children="append"&gt;                                    &lt;include&gt;net.razorvine:*&lt;/include&gt;                                    &lt;include&gt;net.sf.py4j:*&lt;/include&gt;                                    &lt;include&gt;org.apache.beam:*&lt;/include&gt;                                    &lt;include&gt;com.fasterxml.jackson.core:*&lt;/include&gt;                                    &lt;include&gt;joda-time:*&lt;/include&gt;                                    &lt;inculde&gt;com.google.protobuf:*&lt;/inculde&gt;                                    &lt;include&gt;org.apache.arrow:*&lt;/include&gt;                                    &lt;include&gt;io.netty:*&lt;/include&gt;                                    &lt;include&gt;com.google.flatbuffers:*&lt;/include&gt;                                    &lt;include&gt;com.alibaba:pemja&lt;/include&gt;                                &lt;/includes&gt;</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-8-28 01:00:00" id="32710" opendate="2023-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The LeaderElection component IDs for running is only the JobID which might be confusing in the log output</summary>
      <description>I noticed that the leader log messages for the jobs use the plain job ID as the component ID. That might be confusing when reading the logs since it's a UUID with no additional context.We might want to add a prefix (e.g. job- to these component IDs.)</description>
      <version>1.18.0</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperLeaderElectionHaServices.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionHaServices.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-8-30 01:00:00" id="32713" opendate="2023-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cascade deprecation to non-private methods that reference SourceFunction</summary>
      <description/>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.connector.source.SourceFunctionProvider.java</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSourceContexts.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.7602816f-5c01-4b7a-9e3e-235dfedec245</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-8-2 01:00:00" id="32730" opendate="2023-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BatchExecMultipleInput may contain unsupported SourceTransformation</summary>
      <description>When testing 10T TPC-DS on cluster, Q6 always reports an error as shown in the figure during compile time.</description>
      <version>1.18.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.program.DynamicPartitionPruningProgramTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DynamicFilteringTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.optimize.program.DynamicPartitionPruningProgramTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.reuse.ScanReuserUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.reuse.ReusableScanVisitor.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-8-7 01:00:00" id="32770" opendate="2023-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the inaccurate backlog number of Hybrid Shuffle</summary>
      <description>The backlog is inaccurate in both memory and disk tier. We should fix it to prevent redundant memory usage in reader side.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TieredStorageResultSubpartitionViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.NettyConnectionWriterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TieredStorageResultSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TieredStorageNettyServiceImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.NettyConnectionWriterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-22 01:00:00" id="3278" opendate="2016-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Partitioned State Backend Based on RocksDB</summary>
      <description>This would allow users to have state that can grow larger than the available memory. Once WindowOperator is based on the partitioned state abstraction (FLINK-3200) this will also allow windows to grow larger than the available memory.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-contrib.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-8-8 01:00:00" id="32788" opendate="2023-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SpeculativeScheduler do not handle allocate task errors when schedule speculative tasks may causes resource leakage.</summary>
      <description>When the SpeculativeScheduler allocates slots for speculative tasks, exceptions may occur, but there is currently no exception handling mechanism in place. This can lead to resource leakage (such as FLINK-32768) when errors occur. In such cases, a fatalError should be triggered.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.slowtaskdetector.ExecutionTimeBasedSlowTaskDetectorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.slowtaskdetector.ExecutionTimeBasedSlowTaskDetector.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-8-10 01:00:00" id="32827" opendate="2023-8-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Operator fusion codegen may not take effect when enable runtime filter</summary>
      <description>Currently, the RuntimeFilterOperator does not support operator fusion codegen(OFCG), which means the Runtime Filter and OFCG can not take affect together, we should fix it.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.RuntimeFilterITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.runtimefilter.BatchExecRuntimeFilter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-8-11 01:00:00" id="32844" opendate="2023-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Runtime Filter should not be applied if the field is already filtered by DPP</summary>
      <description>Currently, the runtime filter and DPP may take effect on the same key. In this case, the runtime filter may be redundant because the data may have been filtered out by the DPP. We should avoid this because redundant runtime filters can have negative effects.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.program.FlinkRuntimeFilterProgramTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.optimize.program.FlinkRuntimeFilterProgramTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalDynamicFilteringTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.optimize.program.FlinkRuntimeFilterProgram.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-1-25 01:00:00" id="3285" opendate="2016-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip Maven deployment of flink-java8</summary>
      <description>flink-java8 has a Scala dependency due to its dependency on flink-streaming-java. It deploys some examples and a test jar. However, the deployed jars are not necessary to develop or test Java 8 Lambda code.I propose to disable deployment of the flink-java8 module and its test jar.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java8.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  <bug fixdate="2023-8-16 01:00:00" id="32879" opendate="2023-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>slotmanager.max-total-resource.cpu/memory should be ignored in standalone mode.</summary>
      <description>Just as slotmanager.number-of-slots.max, we should also ignore the cpu and memory limitation in standalone mode.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerDisconnectOnShutdownITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-9-28 01:00:00" id="32968" opendate="2023-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix doc for customized catalog listener</summary>
      <description>Refer to https://issues.apache.org/jira/browse/FLINK-32798 for more details</description>
      <version>1.18.0,1.19.0</version>
      <fixedVersion>1.18.0,1.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.catalogs.md</file>
      <file type="M">docs.content.zh.docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-1-4 01:00:00" id="33026" opendate="2023-9-4 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>The chinese doc of sql &amp;#39;Performance Tuning&amp;#39; has a wrong title in the index page</summary>
      <description>The chinese doc of sql 'Performance Tuning' has a wrong title in the index page</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.table.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-9-6 01:00:00" id="33042" opendate="2023-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow trigger flamegraph when task is initializing</summary>
      <description>Currently, the flamegraph can be triggered when task is running.After FLINK-17012 and FLINK-22215, flink split the running to running and initializing. We should allow trigger flamegraph when task is initializing. For example, the initialization is very slow, we need to troubleshoot. Here is a stack example, task is rebuilding the rocksdb after the parallelism is changed.  </description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.VertexThreadInfoTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.VertexThreadInfoTracker.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-1-6 01:00:00" id="33050" opendate="2023-9-6 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Prompts user to close when atomicity implementation is not supported</summary>
      <description>When atomicity is enabled, an exception may occur when creating a DynamicTableSink, and we need to prompt the user to disable atomicity. When we use InMemoryCatalog, RTAS drop table will only delete the metadata, not clean up the underlying data files, RTAS write data does not use overwrite semantics by default, so it looks like the data is duplicated, this problem needs to be clarified in the documentation.</description>
      <version>1.18.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-9-7 01:00:00" id="33053" opendate="2023-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Watcher leak in Zookeeper HA mode</summary>
      <description>We observe a watcher leak in our OLAP stress test when enabling Zookeeper HA mode. TM's watches on the leader of JobMaster has not been stopped after job finished.Here is how we re-produce this issue: Start a session cluster and enable Zookeeper HA mode. Continuously and concurrently submit short queries, e.g. WordCount to the cluster. echo -n wchp | nc {zk host} {zk port} to get current watches.We can see a lot of watches on /flink/{cluster_name}/leader/{job_id}/connection_info.</description>
      <version>1.17.0,1.18.0,1.17.1</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-9-7 01:00:00" id="33055" opendate="2023-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the error value about &amp;#39;state.backend.type&amp;#39; in the document</summary>
      <description> state.backend.type: The state backend to use. This defines the data structure mechanism for taking snapshots. Common values are filesystem or rocksdbfilesystem should be replaced with hashmap after FLINK-16444.</description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.config.md</file>
      <file type="M">docs.content.zh.docs.deployment.config.md</file>
    </fixedFiles>
  </bug>
  
  
  
  
</bugrepository>