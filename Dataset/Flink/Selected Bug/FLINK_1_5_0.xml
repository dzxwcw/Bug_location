<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2018-8-1 01:00:00" id="10016" opendate="2018-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make YARN/Kerberos end-to-end test stricter</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.yarn.kerberos.docker.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.config.yarn-site.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-8 01:00:00" id="10101" opendate="2018-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mesos web ui url is missing.</summary>
      <description>Mesos web ui url is missing in new deploy mode.</description>
      <version>1.5.0,1.5.1,1.5.2</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosSessionClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosJobClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-14 01:00:00" id="10142" opendate="2018-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce synchronization overhead for credit notifications</summary>
      <description>When credit-based flow control was introduced, we also added some checks and optimisations for uncommon code paths that make common code paths unnecessarily more expensive, e.g. checking whether a channel was released before forwarding a credit notification to Netty. Such checks would have to be confirmed by the Netty thread anyway and thus only add additional load for something that happens only once (per channel).</description>
      <version>1.5.0,1.5.1,1.5.2,1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClient.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-22 01:00:00" id="10195" opendate="2018-8-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RabbitMQ Source With Checkpointing Doesn&amp;#39;t Backpressure Correctly</summary>
      <description>The connection between the RabbitMQ server and the client does not appropriately back pressure when auto acking is disabled. This becomes very problematic when a downstream process throttles the data processing to slower then RabbitMQ sends the data to the client.The difference in records ends up being stored in the flink's heap space, which grows indefinitely (or technically to "Integer Max" Deliveries). Looking at RabbitMQ's metrics the number of unacked messages looks like steadily rising saw tooth shape.Upon further invesitgation it looks like this is due to how the QueueingConsumer works, messages are added to the BlockingQueue faster then they are being removed and processed, resulting in the previously described behavior.This may be intended behavior, however this isn't explicitly obvious in the documentation or any of the examples I have seen.</description>
      <version>1.4.0,1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfigTest.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSource.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfig.java</file>
      <file type="M">docs.dev.connectors.rabbitmq.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-6 01:00:00" id="10295" opendate="2018-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tokenisation of Program Args resulting in unexpected results</summary>
      <description>We were upgrading from Flink 1.4 to 1.6. At present we have a jar which takes all the details to run the job as program args against a jarid, including sql query and kafka details. In version 1.5 the program args are tokenised as a result single quote (') and double quote(") are stripped from the arguments. This results in malformed args.Attached a sample request for reference.</description>
      <version>1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRequestUtils.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ProgramArgsQueryParameter.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunMessageParameters.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanMessageParameters.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-7-9 01:00:00" id="5750" opendate="2017-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect translation of n-ary Union</summary>
      <description>Calcite's union operator is supports more than two input relations. However, Flink's translation rules only consider the first two relations because we assumed that Calcite's union is binary. This problem exists for batch and streaming queries.It seems that Calcite only generates non-binary Unions in rare cases ((SELECT * FROM t) UNION ALL (SELECT * FROM t) UNION ALL (SELECT * FROM t) results in two binary union operators) but the problem definitely needs to be fixed.The following query can be used to validate the problem. @Test public void testValuesWithCast() throws Exception { ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config()); String sqlQuery = "VALUES (1, cast(1 as BIGINT) )," + "(2, cast(2 as BIGINT))," + "(3, cast(3 as BIGINT))"; String sqlQuery2 = "VALUES (1,1)," + "(2, 2)," + "(3, 3)"; Table result = tableEnv.sql(sqlQuery); DataSet&lt;Row&gt; resultSet = tableEnv.toDataSet(result, Row.class); List&lt;Row&gt; results = resultSet.collect(); Table result2 = tableEnv.sql(sqlQuery2); DataSet&lt;Row&gt; resultSet2 = tableEnv.toDataSet(result2, Row.class); List&lt;Row&gt; results2 = resultSet2.collect(); String expected = "1,1\n2,2\n3,3"; compareResultAsText(results2, expected); compareResultAsText(results, expected); }AR for results variablejava.lang.AssertionError: Different elements in arrays: expected 3 elements and received 2 expected: [1,1, 2,2, 3,3] received: [1,1, 2,2] Expected :3Actual :2</description>
      <version>1.2.0,1.3.4,1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.0,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnionStream0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnion1.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnion0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.UnionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.GroupingSetsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2017-1-9 01:00:00" id="7777" opendate="2017-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump japicmp to 0.11.0</summary>
      <description>Currently, flink used japicmp-maven-plugin version is 0.7.0, I'm getting these warnings from the maven plugin during a mvn clean verify:[INFO] Written file '.../target/japicmp/japicmp.diff'.[INFO] Written file '.../target/japicmp/japicmp.xml'.[INFO] Written file '.../target/japicmp/japicmp.html'.Warning: org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.Compiler warnings: WARNING: 'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'Warning: org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.Warning: org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.Warning: org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.japicmp fixed in version 0.7.1 : _Excluded xerces vom maven-reporting dependency in order to prevent warnings from SAXParserImpl. _The current stable version is 0.11.0, we can consider upgrading to this version.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-17 01:00:00" id="7857" opendate="2017-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port JobVertexDetails to REST endpoint</summary>
      <description>Port JobVertexDetails to REST endpoint</description>
      <version>1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-6 01:00:00" id="7992" opendate="2017-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>extend PR template with S3 question</summary>
      <description>S3 file system tests are only run if AWS credentials are specified, i.e. ARTIFACTS_AWS_BUCKET, ARTIFACTS_AWS_ACCESS_KEY, and ARTIFACTS_AWS_SECRET_KEY. Since these must remain secret, they are only set in Apache Flink's Travis CI configuration and not available in the Travis runs on pull requests (PR) to not leak them in any way. This however means that if a contributor changes something S3-related, the PR's test results will not reflect the actual changes and if something breaks there, we will only see it once merged.Therefore, I propose to add one more question to the PR template so that the committer is aware of this fact and the need to run the tests in his own Travis CI configuration first with proper AWS credentials set up.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.github.PULL.REQUEST.TEMPLATE.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-7 01:00:00" id="8010" opendate="2017-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump remaining flink-shaded dependencies</summary>
      <description/>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-15 01:00:00" id="8079" opendate="2017-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip remaining E2E tests if one failed</summary>
      <description>I propose that if one end-to-end tests fails the remaining tests are skipped.aljoscha What do you think?</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-15 01:00:00" id="8082" opendate="2017-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump version compatibility check to 1.4</summary>
      <description>Similar to FLINK-7977, we must bump the version of the compatibility check to compare 1.5 against 1.4, once it is released.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-17 01:00:00" id="8095" opendate="2017-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce ProjectSetOpTransposeRule to Flink</summary>
      <description>ProjectSetOpTransposeRule is similar to FilterSetOpTransposeRule, adding ProjectSetOpTransposeRule is necessary.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-17 01:00:00" id="8097" opendate="2017-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add built-in support for min/max aggregation for Date/Time</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MinWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MinAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MaxWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MaxAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.Ordering.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunctionWithRetract.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunctionWithRetract.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-22 01:00:00" id="8130" opendate="2017-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadocs link for snapshot release is not correct</summary>
      <description>See last comments on FLINK-7702.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..config.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-22 01:00:00" id="8131" opendate="2017-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update to Kafka 0.11.0.2</summary>
      <description>This update fixes some critical bugs, for example: KAFKA-6119 KAFKA-6131</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-22 01:00:00" id="8133" opendate="2017-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate documentation for new REST API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">docs.monitoring.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-23 01:00:00" id="8141" opendate="2017-11-23 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Add AppendStreamTableSink for bucketed ORC files</summary>
      <description>It would be good to have an AppendStreamTableSink that writes to bucketed ORC files.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.JobDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobDetailsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobDetailsHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-26 01:00:00" id="8151" opendate="2017-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Table] Map equality check to use entrySet equality</summary>
      <description>Following up with FLINK-8038. The equality check currently is broken. Plan to support element-wise equality check by always using the base class: "java.util.Map.equals" method.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.MapTypeValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.MapTypeTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.MapTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.collection.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-27 01:00:00" id="8161" opendate="2017-11-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flakey YARNSessionCapacitySchedulerITCase on Travis</summary>
      <description>The YARNSessionCapacitySchedulerITCase spuriously fails on Travis because it now contains 2017-11-25 22:49:49,204 WARN akka.remote.transport.netty.NettyTransport - Remote connection to &amp;#91;null&amp;#93; failed with java.nio.channels.NotYetConnectedException from time to time in the logs. I suspect that this is due to switching from Flakka to Akka 2.4.0. In order to solve this problem I propose to add this log statement to the whitelisted log statements.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-12-4 01:00:00" id="8190" opendate="2017-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add extra FlinkKafkaConsumer constructors to expose pattern-based topic subscription functionality</summary>
      <description>The required internals for pattern-based topic discovery was implemented as part of FLINK-4022 (along with partition discovery). However, the functionality for pattern-based topic discovery was not yet exposed via any visible user API on the version-specific subclasses of FlinkKafkaConsumerBase.I propose to add two more constructors for this:public FlinkKafkaConsumerXX(java.util.regex.Pattern subscriptionPattern, DeserializationSchema&lt;T&gt; schema, Properties props);public FlinkKafkaConsumerXX(java.util.regex.Pattern subscriptionPattern, KeyedDeserializationSchema&lt;T&gt; schema, Properties props);This allows the consumer to pick up all matching topics on startup.To continuously pick up matching topics on the fly when they are created after the job has already started running, users should additionally set the KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS property, as they would do for partition discovery.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer08.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010.java</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-4 01:00:00" id="8194" opendate="2017-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable akka.actor.warn-about-java-serializer-usage to suppress akka warnings when using the Java serializer</summary>
      <description>With Akka 2.4, Akka is logging warnings when using the Java serializer for message serialization. We should turn this off via akka.actor.warn-about-java-serializer-usage since we used Java serialization before and it is only cluttering the logs making the users worry.</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-4 01:00:00" id="8196" opendate="2017-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Hadoop Servlet Dependency Exclusion</summary>
      <description>We currently exclude the `javax.servlet` API dependency, which is unfortunately needed as a core dependency by Hadoop 2.7.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-12-7 01:00:00" id="8215" opendate="2017-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support implicit type widening for array/map constructors in SQL</summary>
      <description>TableAPI goes through `LogicalNode.validate()`, which brings up the collection validation and rejects inconsistent type, this will throw `ValidationExcpetion` for something like `array(1.0, 2.0f)`.SqlAPI uses `FlinkPlannerImpl.validator(SqlNode)`, which uses calcite SqlNode validation, which supports resolving leastRestrictive type. `ARRAY&amp;#91;CAST(1 AS DOUBLE), CAST(2 AS FLOAT)&amp;#93;` throws codegen exception.Root cause is the CodeGeneration for these collection value constructors does not cast or resolve leastRestrictive type correctly. I see 2 options:1. Strengthen validation to not allow resolving leastRestrictive type on SQL.2. Making codegen support leastRestrictive type cast, such as using `generateCast` instead of direct casting like `(ClassType) element`.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.MapTypeValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.ArrayTypeValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.MapTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-12-11 01:00:00" id="8235" opendate="2017-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot run spotbugs for single module</summary>
      <description>When running the spotbugs plugin (-Dspotbugs) in a sub-module of Flink the build will fail because it cannot find the exclusion file.[ERROR] Could not find resource 'tools/maven/spotbugs-exclude.xml'. -&gt; [Help 1]The problem is that the configured relative path is resolved against the sub-module directory, and not the parent one.</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-4-13 01:00:00" id="8254" opendate="2017-12-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>REST API documentation wonky due to shading</summary>
      <description>The REST API documentation isn't quite correct as all jackson annotations are being ignored. Our annotations come from flink-shaded-jackson, but the tool we use (jackson-module-jsonSchema) checks against vanilla jackson.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.generated.rest.dispatcher.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-18 01:00:00" id="8274" opendate="2017-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Java 64K method compiling limitation for CommonCalc</summary>
      <description>For complex SQL Queries, the generated code for DataStreamCalc, DataSetCalc may exceed Java's method length limitation 64kb.This issue will split long method to several sub method calls.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.utils.StreamTestData.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.utils.StreamingWithStateTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.CorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.InputFormatCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.FunctionCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CollectorCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableConfig.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-18 01:00:00" id="8278" opendate="2017-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala examples in Metric documentation do not compile</summary>
      <description>The Scala examples in the Metrics documentation do not compile.The line @transient private var counter: Counterneeds to be extended to@transient private var counter: Counter = _</description>
      <version>1.3.2,1.4.0,1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">docs.dev.stream.state.state.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-19 01:00:00" id="8288" opendate="2017-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Register the web interface url to yarn for yarn job mode</summary>
      <description>For flip-6 job mode， the resource manager is created before the web monitor, so the web interface url is not set to resource manager, and the resource manager can not register the url to yarn.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.JobClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-21 01:00:00" id="8301" opendate="2017-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Unicode in codegen for SQL &amp;&amp; TableAPI</summary>
      <description>The current code generation do not support Unicode, "\u0001" will be generated to "u0001", function call like concat(str, "\u0001") will lead to wrong result.This issue intend to handle char/varchar literal correctly, some examples followed as below.literal: '\u0001abc' -&gt; codegen: "\u0001abc"literal: '\u0022\' -&gt; codegen: "\""</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.userDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.literals.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-27 01:00:00" id="8323" opendate="2017-12-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Mod scala function bug</summary>
      <description>As we know mod(1514356320000,60000)=0, but currently we get `-15488` when call `MOD(1514356320000,60000)`.</description>
      <version>1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ScalarOperatorsTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-28 01:00:00" id="8325" opendate="2017-12-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add COUNT AGG support constant parameter, i.e. COUNT(*), COUNT(1)</summary>
      <description>COUNT(1) with Group Window, always output 0. e.g.DATA:val data = List( (1L, 1, "Hi"), (2L, 2, "Hello"), (4L, 2, "Hello"), (8L, 3, "Hello world"), (16L, 3, "Hello world"))SQL:SELECT b, COUNT(1) FROM MyTable GROUP BY Hop(proctime, interval '0.001' SECOND, interval '0.002' SECOND),bOUTPUT:1,0,1, 1,0,1, 2,0,1,2,0,1, 2,0,2, 3,0,1,3,0,1</description>
      <version>1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.CountAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.CountAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-2-6 01:00:00" id="8383" opendate="2018-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-mesos build failing: duplicate Jackson relocation in shaded jar</summary>
      <description>Example: https://travis-ci.org/apache/flink/jobs/325604587The build for flink-mesos is failing with:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade (shade-flink) on project flink-mesos_2.11: Error creating shaded jar: duplicate entry: META-INF/services/org.apache.flink.mesos.shaded.com.fasterxml.jackson.core.JsonFactory -&gt; [Help 1]Seems to be caused by https://github.com/apache/flink/commit/9ae4c5447a2f5aae2b65d5860f822d452a9d5af1.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-2-10 01:00:00" id="8401" opendate="2018-1-10 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow subclass to override write-failure behavior in CassandraOutputFormat</summary>
      <description>Currently it will throw an exception and fail the entire job, we would like to keep the current default behavior, but refactor the code to allow subclass to override and customize the failure handling.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.batch.connectors.cassandra.CassandraOutputFormatBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-10 01:00:00" id="8404" opendate="2018-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mark Flip-6 tests with Flip6 annotation</summary>
      <description>After introducing the Flip6 marker interface, we should update all existing Flip-6 tests with this annotation. That way they will only be executed if the Flip-6 profile is active.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.NetworkBufferCalculationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerIdPathParameterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.TaskManagerMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.SubtaskMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.MetricsFilterParameterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.JobVertexMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.JobMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.JobManagerMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.AbstractMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.MetricsHandlerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AbstractMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolSchedulingTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolRpcTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.DualKeyMapTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.AvailableSlotsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlotsTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-10 01:00:00" id="8407" opendate="2018-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setting the parallelism after a partitioning operation should be forbidden</summary>
      <description>Partitioning operations (shuffle, rescale, etc.) for a DataStream create new DataStreams, which allow the users to set parallelisms for them. However, the PartitionTransformations in these returned DataStreams will only add virtual nodes, whose parallelisms could not be specified, in the execution graph. We should forbid users to set the parallelism after a partitioning operation since they won't actually work. Also the corresponding documents should be updated.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.DataStreamTest.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-13 01:00:00" id="8429" opendate="2018-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement stream-stream non-window right outer join</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.RetractionRulesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamJoin.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-13 01:00:00" id="8430" opendate="2018-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement stream-stream non-window full outer join</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.RetractionRulesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowOuterJoinWithNonEquiPredicates.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowOuterJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamJoin.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-2-18 01:00:00" id="8456" opendate="2018-1-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Scala API for Connected Streams with Broadcast State.</summary>
      <description/>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.package.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-1-23 01:00:00" id="8492" opendate="2018-1-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve cost estimation for Calcs</summary>
      <description>Considering the following test, unsupported exception will be thrown due to multi calc existing between correlate and TableFunctionScan.// code placeholder@Testdef testCrossJoinWithMultiFilter(): Unit = { val t = testData(env).toTable(tEnv).as('a, 'b, 'c) val func0 = new TableFunc0 val result = t .join(func0('c) as('d, 'e)) .select('c, 'd, 'e) .where('e &gt; 10) .where('e &gt; 20) .select('c, 'd) .toAppendStream[Row] result.addSink(new StreamITCase.StringSink[Row]) env.execute() val expected = mutable.MutableList("Jack#22,Jack,22", "Anna#44,Anna,44") assertEquals(expected.sorted, StreamITCase.testResults.sorted)}I can see two options to fix this problem: Adapt calcite OptRule to merge the continuous calc. Merge multi calc in correlate convert rule.I prefer the second one, not only it is easy to implement but also i think with or without an optimize rule should not influence flink functionality. </description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.CalcTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.CalcTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamCorrelateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetCorrelateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonCalc.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-25 01:00:00" id="8507" opendate="2018-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite dependency to 1.16</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-26 01:00:00" id="8518" opendate="2018-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support DOW, EPOCH, DECADE for EXTRACT</summary>
      <description>We upgraded Calcite to version 1.15 in FLINK-7934. The EXTRACT method supports more conversion targets. The targets DOW, EPOCH, DECADE should be implemented and tested for different datatypes.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ExtractCallGen.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.expressions.symbols.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ExtractCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.typeutils.SymbolUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.DateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.TimePointUnit.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.TimeIntervalUnit.java</file>
      <file type="M">docs.content.docs.dev.table.functions.systemFunctions.md</file>
      <file type="M">docs.content.zh.docs.dev.table.functions.systemFunctions.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-29 01:00:00" id="8520" opendate="2018-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CassandraConnectorITCase.testCassandraTableSink unstable on Travis</summary>
      <description>The CassandraConnectorITCase.testCassandraTableSink fails on Travis with a timeout. https://travis-ci.org/tillrohrmann/flink/jobs/333711342</description>
      <version>1.5.0</version>
      <fixedVersion>1.4.2,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraSinkBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-30 01:00:00" id="8529" opendate="2018-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Let Yarn entry points use YarnConfigOptions#APPLICATION_MASTER_PORT</summary>
      <description>The Yarn cluster entry points should use `YarnConfigOptions#APPLICATION_MASTER_PORT` in order to select the common RpcService port.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-2 01:00:00" id="8548" opendate="2018-2-2 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add Streaming State Machine Example</summary>
      <description>Add the example from https://github.com/StephanEwen/flink-demos/tree/master/streaming-state-machine to the Flink examples.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-5 01:00:00" id="8559" opendate="2018-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exceptions in RocksDBIncrementalSnapshotOperation#takeSnapshot cause job to get stuck</summary>
      <description>In the RocksDBKeyedStatebackend#snapshotIncrementally we can find this code final RocksDBIncrementalSnapshotOperation&lt;K&gt; snapshotOperation = new RocksDBIncrementalSnapshotOperation&lt;&gt;( this, checkpointStreamFactory, checkpointId, checkpointTimestamp);snapshotOperation.takeSnapshot();return new FutureTask&lt;KeyedStateHandle&gt;( new Callable&lt;KeyedStateHandle&gt;() { @Override public KeyedStateHandle call() throws Exception { return snapshotOperation.materializeSnapshot(); } }) { @Override public boolean cancel(boolean mayInterruptIfRunning) { snapshotOperation.stop(); return super.cancel(mayInterruptIfRunning); } @Override protected void done() { snapshotOperation.releaseResources(isCancelled()); }};In the constructor of RocksDBIncrementalSnapshotOperation we call aquireResource() on the RocksDB ResourceGuard. If snapshotOperation.takeSnapshot() fails with an exception these resources are never released. When the task is shutdown due to the exception it will get stuck on releasing RocksDB.</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-6 01:00:00" id="8563" opendate="2018-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support consecutive DOT operators</summary>
      <description>We added support for accessing fields of arrays of composite types in FLINK-7923. However, accessing another nested subfield is not supported by Calcite. See CALCITE-2162. We should fix this once we upgrade to Calcite 1.16.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.CompositeTypeTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.CompositeAccessTest.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-2-7 01:00:00" id="8593" opendate="2018-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Latency metric docs are outdated</summary>
      <description>I missed to update the latency metric documentation while working on FLINK-7608. The docs should be updated to contain the new naming scheme and that it is a job-level metric.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-7 01:00:00" id="8595" opendate="2018-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include table source factory services in flink-table jar</summary>
      <description>The flink-table jar does not include the table source factory services.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-12 01:00:00" id="8640" opendate="2018-2-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Japicmp fails on java 9</summary>
      <description>The japicmp plugin does not work out-of-the-box with java 9 as per https://github.com/siom79/japicmp/issues/177.We have to add the following to the plugins dependency section: &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;javax.xml.bind&lt;/groupId&gt; &lt;artifactId&gt;jaxb-api&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.sun.xml.bind&lt;/groupId&gt; &lt;artifactId&gt;jaxb-core&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.sun.xml.bind&lt;/groupId&gt; &lt;artifactId&gt;jaxb-impl&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.activation&lt;/groupId&gt; &lt;artifactId&gt;javax.activation-api&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;</description>
      <version>1.5.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-13 01:00:00" id="8644" opendate="2018-2-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shut down AkkaRpcActors with PoisonPill</summary>
      <description>In order to ensure that all messages get processed before shutting down an AkkaRpcActor we should stop it by sending a PoisonPill. Otherwise we risk that we have some dangling futures which will time out.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-4-15 01:00:00" id="8661" opendate="2018-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Collections.EMPTY_MAP with Collections.emptyMap()</summary>
      <description>The use of Collections.EMPTY_SET and Collections.EMPTY_MAP often causes unchecked assignment. It should be replaced with Collections.emptySet() and Collections.emptyMap() .</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobAccumulatorsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.ResourceProfileTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-4-18 01:00:00" id="8689" opendate="2018-2-18 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add runtime support of distinct filter using MapView</summary>
      <description>This ticket should cover distinct aggregate function support to codegen for AggregateCall, where isDistinct fields is set to true.This can be verified using the following SQL, which is not currently producing correct results.SELECT a, SUM(b) OVER (PARTITION BY a ORDER BY proctime ROWS BETWEEN 5 PRECEDING AND CURRENT ROW)FROM MyTable  </description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.OverWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.OverAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-18 01:00:00" id="8692" opendate="2018-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mistake in MyMapFunction code snippet</summary>
      <description>The MyMapFunction code snippet on the Basic API Concepts page has an extra parenthesis. Just remove the last parenthesis. See the attached screenshot. Thanks. </description>
      <version>1.4.1,1.5.0</version>
      <fixedVersion>1.4.2,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.api.concepts.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-19 01:00:00" id="8696" opendate="2018-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove JobManager local mode from the Unix Shell Scripts</summary>
      <description>In order to work towards removing the local JobManager mode, the shell scripts need to be changed to not use/assume that mode any more</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">test-infra.end-to-end-test.common.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-local.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-local.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.jobmanager.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-2-19 01:00:00" id="8705" opendate="2018-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate Remote(Stream)Environment with Flip-6 cluster</summary>
      <description>Allow the Remote(Stream)Environment to submit jobs to a Flip-6 cluster. This entails that we create the correct ClusterClient to communicate with the respective cluster.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerProcessFailureStreamingRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureBatchRecoveryITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.RemoteStreamEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.operators.RemoteEnvironmentITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.MiniClusterResource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.Flip6LocalStreamEnvironment.java</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellLocalStartupITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.MiniClusterITCase.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.FlinkMiniCluster.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.StandaloneMiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterJobDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.JobExecutorService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobResult.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.RemoteExecutor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.LocalExecutor.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-2-21 01:00:00" id="8736" opendate="2018-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory segment offsets for slices of slices are wrong</summary>
      <description>FLINK-8588 introduced memory segment offsets but the offsets of slices of slices do not account for their parent's slice offset.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedBufferTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedNetworkBuffer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-21 01:00:00" id="8738" opendate="2018-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Converge runtime dependency versions for &amp;#39;scala-lang&amp;#39; and for &amp;#39;com.typesafe:config&amp;#39;</summary>
      <description>These dependencies are currently diverged:Dependency convergence error for com.typesafe:config:1.3.0 paths to dependency are:+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-actor_2.11:2.4.20 +-com.typesafe:config:1.3.0and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-stream_2.11:2.4.20 +-com.typesafe:ssl-config-core_2.11:0.2.1 +-com.typesafe:config:1.2.0andDependency convergence error for org.scala-lang:scala-library:2.11.12 paths to dependency are:+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-org.scala-lang:scala-library:2.11.12and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-actor_2.11:2.4.20 +-org.scala-lang:scala-library:2.11.11and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-actor_2.11:2.4.20 +-org.scala-lang.modules:scala-java8-compat_2.11:0.7.0 +-org.scala-lang:scala-library:2.11.7and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-stream_2.11:2.4.20 +-org.scala-lang:scala-library:2.11.11and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-stream_2.11:2.4.20 +-com.typesafe:ssl-config-core_2.11:0.2.1 +-org.scala-lang:scala-library:2.11.8and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-stream_2.11:2.4.20 +-com.typesafe:ssl-config-core_2.11:0.2.1 +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 +-org.scala-lang:scala-library:2.11.6and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-protobuf_2.11:2.4.20 +-org.scala-lang:scala-library:2.11.11and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-slf4j_2.11:2.4.20 +-org.scala-lang:scala-library:2.11.11and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-org.clapper:grizzled-slf4j_2.11:1.0.2 +-org.scala-lang:scala-library:2.11.0and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.twitter:chill_2.11:0.7.4 +-org.scala-lang:scala-library:2.11.7</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-22 01:00:00" id="8741" opendate="2018-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaFetcher09/010/011 uses wrong user code classloader</summary>
      <description>This commit https://github.com/apache/flink/commit/0a1ce0060ef3af29b196ab6ad58f97e49a40a4cf#diff-51fb939365cf758a89794a2599344702R98 caused the wrong classloader to be used.The user code classloader should be used directly, not its parent. That change seems to be irrelevant to the issue, and seems to have been changed by accident.</description>
      <version>1.4.1,1.5.0</version>
      <fixedVersion>1.4.2,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test-infra.end-to-end-test.test.streaming.kafka010.sh</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.kafka.Kafka010Example.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-6-23 01:00:00" id="8759" opendate="2018-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prepare LengthFieldBasedFrameDecoder for Netty upgrade</summary>
      <description>For a bug in Netty's shutdown sequence and overall improvements in Netty, I'd like to bump the version (and stay within the 4.0 series for now). The problem we faced in the past should not be relevant for credit-based flow control anymore and can be worked around (for the fallback code path) by restoring LengthFieldBasedFrameDecoder's old behaviour of copying contents to new buffers instead of slicing the existing one (please refer to FLINK-7428 for the inverse direction).</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyMessageSerializationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyProtocol.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyMessage.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-23 01:00:00" id="8765" opendate="2018-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify quickstart properties</summary>
      <description>This does not pull out the slf4j and log4j version into properties any more, making the quickstarts a bit simpler.Given that both versions are used only once, and only for the feature to have convenience logging in the IDE, the versions might as well be defined directly in the dependencies.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-23 01:00:00" id="8766" opendate="2018-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pin scala runtime version for Java Quickstart</summary>
      <description>Followup to FLINK-7414, which pinned the scala version for the Scala Quickstart</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-23 01:00:00" id="8767" opendate="2018-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set the maven.compiler.source and .target properties for Java Quickstart</summary>
      <description>Setting these properties helps properly pinning the Java version in IntelliJ.Without these properties, Java version keeps switching back to 1.5 in some setups.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-23 01:00:00" id="8771" opendate="2018-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade scalastyle to 1.0.0</summary>
      <description>scalastyle 1.0.0 fixes issue with import order, explicit type for public methods, line length limitation and comment validation.Also a few scala class header is not correctly formatted before. scalastyle 1.0.0 detected that. We should upgrade to scalastyle 1.0.0</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.taskmanager.TaskManagerFailsITCase.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.TaskManagerFailsWithSlotSharingITCase.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.SlotSharingITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-26 01:00:00" id="8778" opendate="2018-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate queryable state ITCases to use MiniClusterResource</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.NonHAQueryableStateRocksDBBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.NonHAQueryableStateFsBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.NonHAAbstractQueryableStateTestBase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.HAQueryableStateRocksDBBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.HAQueryableStateFsBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.HAAbstractQueryableStateTestBase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.AbstractQueryableStateTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-26 01:00:00" id="8780" opendate="2018-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Broadcast State documentation.</summary>
      <description/>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.state.queryable.state.md</file>
      <file type="M">docs.dev.stream.state.index.md</file>
      <file type="M">docs.dev.stream.state.custom.serialization.md</file>
      <file type="M">docs.dev.stream.state.checkpointing.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-2-26 01:00:00" id="8791" opendate="2018-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix documentation on how to link dependencies</summary>
      <description>The documentation in "Linking with Flink" and "Linking with Optional Dependencies" is very outdated and gives wrong advise to users.</description>
      <version>None</version>
      <fixedVersion>1.4.2,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.linking.with.flink.md</file>
      <file type="M">docs.dev.linking.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-27 01:00:00" id="8795" opendate="2018-2-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala shell broken for Flip6</summary>
      <description>I am trying to run the simple code below after building everything from Flink's github master branch for various reasons. I get an exception below and I wonder what runs on port 9065? and How to fix this exception?I followed the instructions from the Flink master branch so I did the following.git clone https://github.com/apache/flink.git cd flink mvn clean package -DskipTests cd build-target ./bin/start-scala-shell.sh localAnd Here is the code I ranval dataStream = senv.fromElements(1, 2, 3, 4)dataStream.countWindowAll(2).sum(0).print()senv.execute("My streaming program")And I finally get this exceptionCaused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$18(RestClusterClient.java:306) at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870) at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$222(RestClient.java:196) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745) Caused by: java.util.concurrent.CompletionException: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926) ... 16 more Caused by: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281) </description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellLocalStartupITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-4-28 01:00:00" id="8804" opendate="2018-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump flink-shaded-jackson dependency to 3.0</summary>
      <description/>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.ConfigUtil.java</file>
      <file type="M">flink-libraries.flink-sql-client.pom.xml</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-28 01:00:00" id="8810" opendate="2018-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move end-to-end test scripts to end-to-end module</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">test-infra.end-to-end-test.test.streaming.python.wordcount.sh</file>
      <file type="M">test-infra.end-to-end-test.test.streaming.kafka010.sh</file>
      <file type="M">test-infra.end-to-end-test.test.streaming.classloader.sh</file>
      <file type="M">test-infra.end-to-end-test.test.shaded.presto.s3.sh</file>
      <file type="M">test-infra.end-to-end-test.test.shaded.hadoop.s3a.sh</file>
      <file type="M">test-infra.end-to-end-test.test.hadoop.free.sh</file>
      <file type="M">test-infra.end-to-end-test.test.batch.wordcount.sh</file>
      <file type="M">test-infra.end-to-end-test.test-data.words</file>
      <file type="M">test-infra.end-to-end-test.common.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-3-1 01:00:00" id="8824" opendate="2018-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>In Kafka Consumers, replace &amp;#39;getCanonicalName()&amp;#39; with &amp;#39;getClassName()&amp;#39;</summary>
      <description>The connector uses getCanonicalName() in all places, gather than getClassName().getCanonicalName()'s intention is to normalize class names for arrays, etc, but is problematic when instantiating classes from class names.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-3-2 01:00:00" id="8832" opendate="2018-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a SQL Client Kafka fat-jar</summary>
      <description>Create fat-jars for Apache Kafka.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-2 01:00:00" id="8833" opendate="2018-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a SQL Client JSON format fat-jar</summary>
      <description>Create a fat-jar for flink-json.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-2 01:00:00" id="8838" opendate="2018-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Support for UNNEST a MultiSet type field</summary>
      <description>MultiSetTypeInfo was introduced by  FLINK-7491, and UNNEST support Array type only,  so it would be nice to support UNNEST a MultiSet type field.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.util.ExplodeFunctionUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalUnnestRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-2 01:00:00" id="8839" opendate="2018-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table source factory discovery is broken in SQL Client</summary>
      <description>Table source factories cannot not be discovered if they were added using a jar file.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.descriptors.RowtimeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.sources.TableSourceFactoryService.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.Rowtime.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.SessionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-libraries.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-2 01:00:00" id="8842" opendate="2018-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default REST port to 8081</summary>
      <description>In order to avoid confusion, we should set the default REST port to 8081.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-3-3 01:00:00" id="8847" opendate="2018-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modules containing package-info.java are always recompiled</summary>
      <description>All modules that contain a package-info.java file (that do not contain annotations which applies to all instances in Flink) will always be recompiled by the maven-compiler-plugin.To detect modified files the compiler compares timestamps of the source and .class file. In the case of package-info.java no .class file is created if it doesn't contain annotations, which the compiler interprets as a missing .class file.We can add -Xpkginfo:always to the compiler configuration to force the generation of these files to prevent this from happening.</description>
      <version>1.3.2,1.4.1,1.5.0</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-5-5 01:00:00" id="8859" opendate="2018-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDB backend should pass WriteOption to Rocks.put() when restoring</summary>
      <description>We should pass `WriteOption` to Rocks.put() when restoring from handle (Both in full &amp; incremental checkpoint). Because of `WriteOption.setDisableWAL(true)`, the performance can be increased by about 2 times.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-5 01:00:00" id="8861" opendate="2018-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for batch queries in SQL Client</summary>
      <description>This issue is a subtask of part two "Full Embedded SQL Client" of the implementation plan mentioned in FLIP-24.Similar to streaming queries, it should be possible to execute batch queries in the SQL Client and collect the results using DataSet.collect() for debugging purposes.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ResultStore.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.DynamicResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.CollectStreamTableSink.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.CollectStreamResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Execution.java</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-3-6 01:00:00" id="8877" opendate="2018-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configure Kryo&amp;#39;s log level based on Flink&amp;#39;s log level</summary>
      <description>Kryo uses its embedded MinLog for logging.When Flink is set to trace, Kryo should be set to trace as well. Other log levels should not be uses, as even debug logging in Kryo results in excessive logging.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-6 01:00:00" id="8888" opendate="2018-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade AWS SDK in flink-connector-kinesis</summary>
      <description>Bump up the java aws sdk version to 1.11.272. Evaluate also the impact of this version upgrade for KCL and KPL versions.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-8 01:00:00" id="8897" opendate="2018-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rowtime materialization causes "mismatched type" AssertionError</summary>
      <description>As raised in this thread, the query created by the following code will throw a calcite "mismatch type" (Timestamp(3) and TimeIndicator) exception.String sql1 = "select id, eventTs as t1, count(*) over (partition by id order by eventTs rows between 100 preceding and current row) as cnt1 from myTable1";String sql2 = "select distinct id as r_id, eventTs as t2, count(*) over (partition by id order by eventTs rows between 50 preceding and current row) as cnt2 from myTable2";Table left = tableEnv.sqlQuery(sql1);Table right = tableEnv.sqlQuery(sql2);left.join(right).where("id === r_id &amp;&amp; t1 === t2").select("id, t1").writeToSink(...)The logical plan is as follows.LogicalProject(id=[$0], t1=[$1]) LogicalFilter(condition=[AND(=($0, $3), =($1, $4))]) LogicalJoin(condition=[true], joinType=[inner]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]])That is because the the rowtime field after an aggregation will be materialized while the RexInputRef type for the filter's operands (t1 === t2) is still TimeIndicator. We should make them unified.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamWindowJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-5-9 01:00:00" id="8910" opendate="2018-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce automated end-to-end test for local recovery (including sticky scheduling)</summary>
      <description>We should have an automated end-to-end test that can run nightly to check that sticky allocation and local recovery work as expected.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0,1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.BackendRestorerProcedure.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-9 01:00:00" id="8912" opendate="2018-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI does not render error messages correctly in FLIP-6 mode</summary>
      <description>DescriptionThe Web UI renders error messages returned by the REST API incorrectly, e.g., on the job submission page. The JSON returned by the REST API is rendered as a whole. However, the UI should only render the contents of the errors field.Steps to reproduceSubmit examples/streaming/SocketWindowWordCount.jar without specifying program arguments. Error message will be rendered as follows:{"errors":["org.apache.flink.client.program.ProgramInvocationException: The program plan could not be fetched - the program aborted pre-maturely.\n\nSystem.err: (none)\n\nSystem.out: No port specified. Please run 'SocketWindowWordCount --hostname &lt;hostname&gt; --port &lt;port&gt;', where hostname (localhost by default) and port is the address of the text server\nTo start a simple text server, run 'netcat -l &lt;port&gt;' and type the input text into the command line\n"]}Note that flip6 mode must be enabled.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.submit.submit.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.submit.submit.ctrl.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-11 01:00:00" id="8916" opendate="2018-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpointing Mode is always shown to be "At Least Once" in Web UI</summary>
      <description>This only happens in flip6 mode. The CheckpointConfigHandler returns the checkpoint mode uppercased. For example:{"mode":"EXACTLY_ONCE","interval":5000,"timeout":600000,"min_pause":0,"max_concurrent":1,"externalization":{"enabled":false,"delete_on_cancellation":true}}However, the Web UI expects the value to be lower cased: &lt;tr&gt; &lt;td&gt;Checkpointing Mode&lt;/td&gt; &lt;td ng-if="checkpointConfig['mode'] == 'exactly_once'"&gt;Exactly Once&lt;/td&gt; &lt;td ng-if="checkpointConfig['mode'] != 'exactly_once'"&gt;At Least Once&lt;/td&gt; &lt;/tr&gt;</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfo.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-12 01:00:00" id="8922" opendate="2018-3-12 00:00:00" resolution="Feedback Received">
    <buginformation>
      <summary>Revert FLINK-8859 because it causes segfaults in testing</summary>
      <description>We need to revert FLINK-8859 because it causes problems with RocksDB that make our automated tests fail on Travis. The change looks actually good and it is currently unclear why this can introduce such a problem. This might also be a Rocks in RocksDB. Nevertheless, for the sake of a proper release testing, we should revert the change for now.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-3-12 01:00:00" id="8927" opendate="2018-3-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eagerly release the checkpoint object created from RocksDB</summary>
      <description>We should eagerly release the checkpoint object that is created from RocksDB, because it's a RocksObject (a native resource).</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-12 01:00:00" id="8928" opendate="2018-3-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error message on server binding error.</summary>
      <description/>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.AbstractServerBase.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-12-15 01:00:00" id="8949" opendate="2018-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rest API failure with long URL</summary>
      <description>When you have jobs with high parallelism, the URL for a REST request can get very long. When the URL is longer than 4096 bytes, the  REST API will return errorFailure: 404 Not Found This can easily be seen in the Web UI, when Flink queries for the watermark using the REST API:GET /jobs/:jobId/vertices/:vertexId/metrics?get=0.currentLowWatermark,1.currentLowWatermark,2.currentLo...The request will fail with more than 170 subtasks and the watermark will not be displayed in the Web UI.</description>
      <version>1.4.2,1.5.0,1.6.4,1.7.2,1.8.2</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.metrics.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-4-16 01:00:00" id="8973" opendate="2018-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Run general purpose job with failures in standalone mode</summary>
      <description>We should set up an end-to-end test which runs the general purpose job (FLINK-8971) in a standalone setting with HA enabled (ZooKeeper). When running the job, the job failures should be activated. Additionally, we should randomly kill Flink processes (cluster entrypoint and TaskExecutors). When killing them, we should also spawn new processes to make up for the loss.This end-to-end test case should run with all different state backend settings: RocksDB (full/incremental, async/sync), FsStateBackend (sync/async)We should then verify that the general purpose job is successfully recovered without data loss or other failures.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.statemachine.StateMachineExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-16 01:00:00" id="8974" opendate="2018-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Run general purpose DataSet job with failures in standalone mode</summary>
      <description>Similar to FLINK-8973, we should setup an end-to-end test where we run the general purpose DataSet job from FLINK-8972 in a HA standalone setting with failures and process kills.</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.ha.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-dataset-allround-test.src.main.java.org.apache.flink.batch.tests.DataSetAllroundTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-16 01:00:00" id="8975" opendate="2018-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Resume from savepoint</summary>
      <description>User usually take a savepoint and want to resume from it. In order to verify that Flink supports this feature, we should add an end-to-end test which scripts this behavior. We should use the general purpose testing job FLINK-8971 with failures disabled for that.The end-to-end test should do the following: Submit FLINK-8971 job Verify that the savepoint is there Cancel job and resume from savepoint Verify that job could be resumed Use different StateBackends: RocksDB incremental async/sync, RocksDB full async/sync, FsStateBackend aysnc/sync</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.statemachine.StateMachineExample.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-16 01:00:00" id="8977" opendate="2018-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Manually resume job after terminal failure</summary>
      <description>We should add an end-to-end test which verifies that a job can be resumed manually after a terminal job failure if there is a checkpoint. In order to do that we should run the general purpose testing job FLINK-8971 wait for the completion of a checkpoint Trigger a failure which leads to a terminal failure Resume the job from the retained checkpointThis end-to-end test should run with all state backend combinations: RocksDB (incremental/full, async/sync), FsStateBackend (async/sync).</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.externalized.checkpoints.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-16 01:00:00" id="8979" opendate="2018-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend Kafka end-to-end tests to run with different versions</summary>
      <description>The current Kafka end-to-end test only runs with Kafka 0.10. We should extend the test to also run with Kafka 0.8 Kafka 0.9 Kafka 0.11Additionally we should change the test job to not be embarrassingly parallel by introducing a shuffle.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.kafka.Kafka010Example.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka010.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-16 01:00:00" id="8980" opendate="2018-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: BucketingSink</summary>
      <description>In order to verify the BucketingSink, we should add an end-to-end test which verifies that the BucketingSink does not lose data under failures.An idea would be to have a CountUp job which simply counts up a counter which is persisted. The emitted values will be written to disk by the BucketingSink. Now we should kill randomly Flink processes (cluster entrypoint and TaskExecutors) to simulate failures. Even after these failures, the written files should contain the correct sequence of numbers.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-stream-sql-test.src.main.java.org.apache.flink.sql.tests.StreamSQLTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-parent-child-classloading-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-dataset-allround-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-16 01:00:00" id="8981" opendate="2018-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end-to-end test for running on YARN with Kerberos</summary>
      <description>We should add an end-to-end test which verifies Flink's integration with Kerberos security. In order to do this, we should start a Kerberos secured Hadoop, ZooKeeper and Kafka cluster. Then we should start a Flink cluster with HA enabled and run a job which reads from and writes to Kafka. We could use a simple pipe job for that purpose which has some state for checkpointing to HDFS.See security docs for how more information about Flink's Kerberos integration.</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-16 01:00:00" id="8982" opendate="2018-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Queryable state</summary>
      <description>We should add an end-to-end test which verifies that Queryable State is working.florianschmidt and kkl0u could you please provide more details for the description.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-11-16 01:00:00" id="8985" opendate="2018-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: CLI</summary>
      <description>We should add end-to-end test which verifies that all client commands are working correctly.</description>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.cli.api.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-api-test.src.main.java.org.apache.flink.runtime.tests.PeriodicStreamingJob.java</file>
      <file type="M">flink-end-to-end-tests.flink-api-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-16 01:00:00" id="8987" opendate="2018-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Avro state evolution</summary>
      <description>We should add an end-to-end test which verifies that we can upgrade Avro types by adding and removing fields. We can use the general purpose job (FLINK-8971) after it added Avro types for that.What should happen is Start general purpose job Take savpoint Change Avro type to have different fields Resume from savepointCheck for different state backends: RocksDB, FsStateBackend</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-16 01:00:00" id="8990" opendate="2018-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Dynamic Kafka partition discovery</summary>
      <description>We should add an end-to-end test which verifies the dynamic partition discovery of Flink's Kafka connector. We can simulate it by reading from a Kafka topic to which we add partitions after the job started. By writing to these new partitions it should be verifiable whether Flink noticed them by checking the output for completeness.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka010.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-16 01:00:00" id="8992" opendate="2018-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement source and operator that validate exactly-once</summary>
      <description>We can build this with sources that emit sequences per key and a stateful (keyed) operator that validate for the update of each key that the new value is the old value + 1. This can help to easily detect if events/state were lost or duplicates.</description>
      <version>1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.SequenceGeneratorSource.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.SemanticsCheckMapper.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.GeneralPurposeJobTest.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.Event.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.artificialstate.eventpayload.ComplexPayload.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.artificialstate.eventpayload.ArtificialValueStateBuilder.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.artificialstate.eventpayload.ArtificialMapStateBuilder.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.artificialstate.ArtificialKeyedStateMapper.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.artificialstate.ArtificialKeyedStateBuilder.java</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-16 01:00:00" id="8993" opendate="2018-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a test operator with keyed state that uses Kryo serializer (registered/unregistered/custom)</summary>
      <description>Add an operator with keyed state that uses Kryo serializer (registered/unregistered/custom).</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-stream-stateful-job-upgrade-test.src.main.java.org.apache.flink.streaming.tests.StatefulStreamJobUpgradeTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.artificialstate.builder.ArtificialValueStateBuilder.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.artificialstate.builder.ArtificialListStateBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-16 01:00:00" id="8995" opendate="2018-3-16 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add a test operator with keyed state that uses custom, stateful serializer</summary>
      <description>This test should figure out problems in places where multiple threads would share the same serializer instead of properly duplicating it.</description>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-16 01:00:00" id="8997" opendate="2018-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add sliding window aggregation to the job</summary>
      <description>The test job should also test windowing. Sliding windows are probably the most demanding form, so this would be a good pick for the test.</description>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.SemanticsCheckMapper.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.Event.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-3-17 01:00:00" id="9016" opendate="2018-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unregister jobs from JobMetricGroup after termination</summary>
      <description>In order to free resources and unregister metrics, jobs should be properly unregistered from the JobMetricGroup once they have reached terminal state.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobManagerRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.MiniDispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.SessionClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.JobClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.StandaloneDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.MiniDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-4-20 01:00:00" id="9033" opendate="2018-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace usages of deprecated TASK_MANAGER_NUM_TASK_SLOTS</summary>
      <description>The deprecated ConfigConstants#TASK_MANAGER_NUM_TASK_SLOTS is still used a lot.We should replace these usages with TaskManagerOptions#NUM_TASK_SLOTS.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.taskmanager.TaskManagerFailsITCase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.jobmanager.JobManagerFailsITCase.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.minicluster.LocalFlinkMiniClusterITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerFailureRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureBatchRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHACheckpointRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.operators.RemoteEnvironmentITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.operators.ExecutionEnvironmentITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.SavepointMigrationTestBase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.MiniClusterResource.java</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingUtils.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.RecoveryITCase.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.akka.AkkaSslITCase.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.backpressure.StackTraceSampleCoordinatorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.backpressure.BackPressureStatsTrackerImplITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeStateCleanupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeJobRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerCleanupITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartialConsumePipelinedResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CoordinatorShutdownTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkLocalCluster.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualExactlyOnceWithStreamReshardingTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualExactlyOnceTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.LocalExecutor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.ClusterSpecification.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-22 01:00:00" id="9059" opendate="2018-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for unified table source and sink declaration in environment file</summary>
      <description>1) Add a common property called "type" with single value 'source'.2) in yaml file, replace "sources" with "tables".</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.TestTableSourceFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.TableSourceFactoryServiceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.TableSourceDescriptor.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-factory.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Environment.java</file>
      <file type="M">flink-libraries.flink-sql-client.conf.sql-client-defaults.yaml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-23 01:00:00" id="9064" opendate="2018-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Scaladocs link to documentation</summary>
      <description>Browse to the Apache Flink Documentation page.On the sidebar, under the Javadocs link, I recommend that you add a Scaladocs link.Thanks! </description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.update.branch.version.sh</file>
      <file type="M">tools.releasing.create.release.branch.sh</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs..config.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-24 01:00:00" id="9068" opendate="2018-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Website documentation issue - html tag visible on screen</summary>
      <description>In the documentation at the following urlhttps://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/#physical-partitioningIn the section which explains the 'Reduce' operator (ReduceKeyedStream → DataStream), an html tag (&lt;/p&gt;) is visible.</description>
      <version>None</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-26 01:00:00" id="9088" opendate="2018-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Nifi connector dependency to 1.6.0</summary>
      <description>Currently dependency of Nifi is 0.6.1We should upgrade to 1.6.0</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-nifi.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-26 01:00:00" id="9089" opendate="2018-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Orc dependency to 1.4.3</summary>
      <description>Currently flink-orc uses Orc 1.4.1 release.This issue upgrades to Orc 1.4.3</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-orc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-26 01:00:00" id="9091" opendate="2018-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failure while enforcing releasability in building flink-json module</summary>
      <description>Got the following when building flink-json module:[WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:Failed while enforcing releasability. See above detailed error message....[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M1:enforce (dependency-convergence) on project flink-json: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -&gt; [Help 1]</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-27 01:00:00" id="9093" opendate="2018-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>If Google can&amp;#39;t be accessed,the document can&amp;#39;t be use</summary>
      <description>these links can't be visited.</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.3,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">docs..layouts.base.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-29 01:00:00" id="9104" opendate="2018-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-generate REST API documentation for FLIP-6</summary>
      <description>The API documentation is missing for several handlers, e.g., SavepointHandlers.</description>
      <version>1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">docs..includes.generated.rest.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-29 01:00:00" id="9107" opendate="2018-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document timer coalescing for ProcessFunctions</summary>
      <description>In a ProcessFunction, registering timers for each event via ctx.timerService().registerEventTimeTimer() using times like ctx.timestamp() + timeout will get a millisecond accuracy and may thus create one timer per millisecond which may lead to some overhead in the TimerService.This problem can be mitigated by using timer coalescing if the desired accuracy of the timer can be larger than 1ms. A timer firing at full seconds only, for example, can be realised like this:coalescedTime = ((ctx.timestamp() + timeout) / 1000) * 1000;ctx.timerService().registerEventTimeTimer(coalescedTime);As a result, only a single timer may exist for every second since we do not add timers for timestamps that are already there.This should be documented in the ProcessFunction docs.</description>
      <version>1.3.0,1.4.0,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-29 01:00:00" id="9108" opendate="2018-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>invalid ProcessWindowFunction link in Document</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.side.output.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-29 01:00:00" id="9109" opendate="2018-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add flink modify command to documentation</summary>
      <description>We should add documentation for the flink modify command.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-2 01:00:00" id="9121" opendate="2018-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Flip-6 prefixes from code base</summary>
      <description>We should remove all Flip-6 prefixes and other references from the code base since it is not a special case but the new default architecture. Instead we should prefix old code with legacy.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManagerTest.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Flip6YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNHighAvailabilityITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnConfigurationITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.TestingYarnClusterDescriptor.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.AbstractOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerProcessFailureStreamingRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureBatchRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.operators.RemoteEnvironmentITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.AutoParallelismITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorLiveITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointIT.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.MiniClusterResource.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.AbstractTestBase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.category.OldAndFlip6.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.category.Flip6.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.environment.LocalStreamEnvironmentITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.RemoteStreamEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.LocalStreamEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.Flip6LocalStreamEnvironment.java</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellLocalStartupITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskCancelAsyncProducerConsumerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.slot.TimerServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.NetworkBufferCalculationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcConnectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.FencedRpcEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.AsyncCallsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.MessageSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.MainThreadValidationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerIdPathParameterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.RestResponseMarshallingTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.RestRequestMarshallingTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.MessageParametersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.TaskManagerMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.SubtaskMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.MetricsFilterParameterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.JobVertexMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.JobMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.JobManagerMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.AbstractMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.MetricsHandlerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AbstractMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.BlobServerPortHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotProtocolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerJobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.JobLeaderIdServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.KvStateRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.MiniClusterITCase.java</file>
      <file type="M">.travis.yml</file>
      <file type="M">docs.monitoring.rest.api.md</file>
      <file type="M">docs.ops.config.md</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.DefaultCLI.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.Flip6DefaultCLI.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.Flip6StandaloneClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.StandaloneClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.LocalExecutor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.RemoteExecutor.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendPackageProgramTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendTestBase.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.DefaultCLITest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.jobmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-taskmanager.sh</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-libraries.flink-streaming-python.src.main.java.org.apache.flink.streaming.python.api.environment.PythonEnvironmentFactory.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.main.java.org.apache.flink.queryablestate.client.proxy.KvStateClientProxyImpl.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.client.proxy.KvStateClientProxyImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SharedSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SimpleSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.Slot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.CoLocationConstraint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.KvStateRegistry.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.FileArchivedExecutionGraphStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.MiniDispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.heartbeat.HeartbeatManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.nonha.standalone.StandaloneHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerSlotSharingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlotsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.AvailableSlotsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.DualKeyMapTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolRpcTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolSchedulingTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-4 01:00:00" id="9128" opendate="2018-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for scheduleRunAsync for FencedRpcEndpoints</summary>
      <description>Currently, the FencedRpcEndpoint cannot send a scheduleRunAsync message because it is not properly wrapped in a LocalFencedMessage. Due to this, the message will be dropped and the code will not be executed.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.AsyncCallsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-4 01:00:00" id="9131" opendate="2018-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable spotbugs on travis</summary>
      <description>The misc profile that also runs spotbugs is consistently timing out on travis at the moment.The spotbugs plugin is a major contributor to the compilation time, for example it doubles the compile time for flink-runtime.I suggest to temporarily disable spotbugs, and re-enable it at a lter point when we figure out the daily cron jobs.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-5 01:00:00" id="9140" opendate="2018-4-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>simplify scalastyle configurations</summary>
      <description>Simplifying &lt;check .....&gt;&lt;/check&gt; to &lt;check ... /&gt;</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.scalastyle-config.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-6 01:00:00" id="9144" opendate="2018-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Spilling batch job hangs</summary>
      <description>A user on the mailing list reported that his batch job stops to run with Flink 1.5 RC1: https://lists.apache.org/thread.html/43721934405019e7255fda627afb7c9c4ed0d04fb47f1c8f346d4194@%3Cdev.flink.apache.org%3E</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferBuilderTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartition.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.allround.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-6 01:00:00" id="9145" opendate="2018-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Website build is broken</summary>
      <description>The javadoc generation fails with a dependency-convergence error in flink-json:[WARNING] Dependency convergence error for commons-beanutils:commons-beanutils:1.8.0 paths to dependency are:+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-commons-configuration:commons-configuration:1.7 +-commons-digester:commons-digester:1.8.1 +-commons-beanutils:commons-beanutils:1.8.0and+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-commons-configuration:commons-configuration:1.7 +-commons-beanutils:commons-beanutils:1.8.3[WARNING] Dependency convergence error for org.codehaus.janino:commons-compiler:3.0.7 paths to dependency are:+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-org.codehaus.janino:janino:3.0.7 +-org.codehaus.janino:commons-compiler:3.0.7and+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-org.apache.calcite:calcite-core:1.16.0 +-org.codehaus.janino:commons-compiler:2.7.6[WARNING] Dependency convergence error for commons-lang:commons-lang:2.6 paths to dependency are:+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-commons-configuration:commons-configuration:1.7 +-commons-lang:commons-lang:2.6and+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-org.apache.calcite:calcite-core:1.16.0 +-net.hydromatic:aggdesigner-algorithm:6.0 +-commons-lang:commons-lang:2.4[WARNING] Dependency convergence error for org.codehaus.janino:janino:3.0.7 paths to dependency are:+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-org.codehaus.janino:janino:3.0.7and+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-org.apache.calcite:calcite-core:1.16.0 +-org.codehaus.janino:janino:2.7.6and+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-org.codehaus.janino:janino:3.0.7[WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:Failed while enforcing releasability. See above detailed error message.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-libraries.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-8 01:00:00" id="9147" opendate="2018-4-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PrometheusReporter jar does not include Prometheus dependencies</summary>
      <description>The PrometheusReporter seems to lack the shaded Prometheus dependencies.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-12 01:00:00" id="9158" opendate="2018-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set default FixedRestartDelayStrategy delay to 0s</summary>
      <description>Set default FixedRestartDelayStrategy delay to 0s.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-12 01:00:00" id="9159" opendate="2018-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sanity check default timeout values</summary>
      <description>Check that the default timeout values for resource release are sanely chosen.</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.configuration.MesosOptions.java</file>
      <file type="M">docs..includes.generated.mesos.configuration.html</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs..includes.generated.slot.manager.configuration.html</file>
      <file type="M">docs..includes.generated.resource.manager.configuration.html</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-12 01:00:00" id="9160" opendate="2018-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make subclasses of RuntimeContext internal that should be internal</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.util.AbstractRuntimeUDFContext.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-12 01:00:00" id="9163" opendate="2018-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden e2e tests&amp;#39; signal traps and config restoration during abort</summary>
      <description>Signal traps on certain systems, e.g. Linux, may be called concurrently when the trap is caught during its own execution. In that case, our cleanup may just be wrong and may also overly eagerly delete flink-conf.yaml.</description>
      <version>1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka010.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-5-16 01:00:00" id="9181" opendate="2018-4-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SQL Client documentation page</summary>
      <description>The current implementation of the SQL Client implementation needs documentation for the upcoming 1.5 release. </description>
      <version>None</version>
      <fixedVersion>1.5.0,1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.Rowtime.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-16 01:00:00" id="9186" opendate="2018-4-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable dependency convergence for flink-libraries</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.pom.xml</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-libraries.flink-sql-client.pom.xml</file>
      <file type="M">flink-libraries.flink-ml.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-17 01:00:00" id="9188" opendate="2018-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide a mechanism to configure AmazonKinesisClient in FlinkKinesisConsumer</summary>
      <description>It should be possible to control the ClientConfiguration to set socket timeout and other properties.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-4-18 01:00:00" id="9212" opendate="2018-4-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port SubtasksAllAccumulatorsHandler to new REST endpoint</summary>
      <description/>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-19 01:00:00" id="9216" opendate="2018-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix comparator violation</summary>
      <description/>
      <version>1.3.3,1.4.2,1.5.0</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.JSONGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-23 01:00:00" id="9236" opendate="2018-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Apache Parent POM 20</summary>
      <description>Flink is still using Apache Parent POM 18. Apache Parent POM 20 is out.This will also fix Javadoc generation with JDK 10+</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-libraries.flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-24 01:00:00" id="9246" opendate="2018-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HistoryServer job overview broken</summary>
      <description>The jobs overview URL was changed from jobsoverview to jobs/overview. The handlers and UI was properly adjusted, but the HistoryServer was not.As a result the job overview isn't merged properly, causing only a single job to be displayed.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-24 01:00:00" id="9249" opendate="2018-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add convenience profile for skipping non-essential plugins</summary>
      <description>When compiling Flink devs can already set a variety of command line options to speed up the process, for example skipping checkstyle. We also do the same thing on travis.However, not only is it difficult to keep track of all possible options, it is also tedious to write and obfuscates the actual command.I propose adding a fast profile that skips non-essential plugins, including: rat checkstyle scalastyle enforcer japicmp javadoc</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-27 01:00:00" id="9265" opendate="2018-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Prometheus version</summary>
      <description>We're using 0.0.26Latest release is 2.2.1This issue is for upgrading the Prometheus version</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-30 01:00:00" id="9274" opendate="2018-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add thread name to Kafka Partition Discovery</summary>
      <description>For debugging, threads should have names to filter on and get a quick overview. The Kafka partition discovery thread(s) currently don't have any name assigned.</description>
      <version>1.4.0,1.4.1,1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-2 01:00:00" id="9284" opendate="2018-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update CLI page</summary>
      <description>The CLI page must be updated for 1.5.The examples using the -m option must be updated to use 8081.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-2 01:00:00" id="9285" opendate="2018-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update REST API page</summary>
      <description>The REST API must be updated for 1.5.The Available requests section still predominantly lists legacy calls. These should be either removed or moved to the bottom, and explicitly marked as legacy.The developing section must be updated.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.rest.api.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-7-12 01:00:00" id="932" opendate="2014-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Semantic Properties from Project Joins</summary>
      <description>The Project Joins do not create semantic properties right now, which looses optimization potential.</description>
      <version>None</version>
      <fixedVersion>0.6-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">stratosphere-java.src.main.java.eu.stratosphere.api.java.operators.ProjectOperator.java</file>
      <file type="M">stratosphere-java.src.main.java.eu.stratosphere.api.java.operators.JoinOperator.java</file>
      <file type="M">stratosphere-java.src.main.java.eu.stratosphere.api.java.operators.CrossOperator.java</file>
      <file type="M">stratosphere-java.src.main.java.eu.stratosphere.api.java.functions.SemanticPropUtil.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-9 01:00:00" id="9323" opendate="2018-5-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move checkstyle configuration to plugin management</summary>
      <description>The checkstyle plugin configuration is currently duplicated in 4 modules. We should instead move the configuration into the plugin-management section.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-optimizer.pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-11 01:00:00" id="9337" opendate="2018-5-11 00:00:00" resolution="Done">
    <buginformation>
      <summary>Implement AvroDeserializationSchema</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.resources.flink-1.3-avro-type-serializer-snapshot</file>
      <file type="M">flink-formats.flink-avro.src.test.resources.flink-1.3-avro-type-serialized-data</file>
      <file type="M">flink-formats.flink-avro.src.test.resources.avro.user.avsc</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.utils.TestDataGenerator.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.utils.AvroTestUtils.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.BackwardsCompatibleAvroSerializerTest.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroSerializer.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroRowDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-11 01:00:00" id="9338" opendate="2018-5-11 00:00:00" resolution="Done">
    <buginformation>
      <summary>Implement RegistryAvroDeserializationSchema</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-14 01:00:00" id="9353" opendate="2018-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Kubernetes integration</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-end-to-end-tests.README.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-14 01:00:00" id="9354" opendate="2018-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>print execution times for end-to-end tests</summary>
      <description>We need to modify the end-to-end scripts to include the time it takes for a test to run.We currently don't have any clue how long a test actually runs for.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.externalized.checkpoints.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-14 01:00:00" id="9358" opendate="2018-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Closing of unestablished RM connections can cause NPE</summary>
      <description>When closing an unestablished RM connection, a NPE is thrown. The reason is that we try to unmonitor a non-existing heartbeat target.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-15 01:00:00" id="9368" opendate="2018-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Python API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-15 01:00:00" id="9372" opendate="2018-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo on Elasticsearch website link (elastic.io --&gt; elastic.co)</summary>
      <description>Typo on website link in Elasticsearch Java Docs (elastic.io --&gt; elastic.co)</description>
      <version>1.4.1,1.4.2,1.5.0,1.5.1</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSink.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-16 01:00:00" id="9380" opendate="2018-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failing end-to-end tests should not clean up logs</summary>
      <description>Some of the end-to-end tests clean up their logs also in the failure case. This makes debugging and understanding the problem extremely difficult. Ideally, the scripts says where it stored the respective logs.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-17 01:00:00" id="9387" opendate="2018-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Several log message errors in queryable-state module</summary>
      <description/>
      <version>1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.main.java.org.apache.flink.queryablestate.client.proxy.KvStateClientProxyHandler.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.AbstractServerHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-17 01:00:00" id="9392" opendate="2018-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add @FunctionalInterface annotations to all core functional interfaces</summary>
      <description>The @FunctionalInterface annotation should be added to all SAM interfaces in order to prevent accidentally breaking them (as non SAMs).We had a case of that before for the SinkFunction which was compatible through default methods, but incompatible for users that previously instantiated that interface through a lambda.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.ReduceFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.Partitioner.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.MapPartitionFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.MapFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.JoinFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.GroupReduceFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.GroupCombineFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FoldFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FlatMapFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FlatJoinFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FilterFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.CrossFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.CombineFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.CoGroupFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.BroadcastVariableInitializer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-19 01:00:00" id="9402" opendate="2018-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis consumer validation incorrectly requires aws.region property</summary>
      <description>AwsClientBuilder says: Only one of Region or EndpointConfiguration may be set.But KinesisUtil still thinks: The AWS region ('aws.region') must be set in the config.This doesn't affect configuration based on region, but makes testing with Kinesalite impossible. The Flink code needs to match the new opinion in AWS SDK (probably changed with . recent update). </description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-22 01:00:00" id="9408" opendate="2018-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retry JM-RM connection in case of explicit disconnect</summary>
      <description>The JM should try to reconnect to the RM not only in the case of a heartbeat timeout but also in case of an explicit disconnect.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.EstablishedResourceManagerConnection.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-23 01:00:00" id="9415" opendate="2018-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove reference to StreamingMultipleProgramsTestBase in docs</summary>
      <description/>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.testing.md</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.StreamingOperatorsITCase.scala</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.ScalaStreamingMultipleProgramsTestBase.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-23 01:00:00" id="9416" opendate="2018-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make job submission retriable operation in case of a ongoing leader election</summary>
      <description>When starting a session cluster, it can happen that the job submission fails if the REST server endpoint has already gained leadership but if the leadership election for the Dispatcher is still ongoing. In such a case, we receive a error response saying that the leader election is still ongoing and fail the job submission. I think it would be nicer to also make the submission step a retriable operation in order to avoid this race condition.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-6-16 01:00:00" id="942" opendate="2014-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Config key "env.java.opts" does not effect JVM args</summary>
      <description>Setting custom args for the JVM as in env.java.opts: -Dio.netty.leakDetectionLevel=paranoidhas no effect for the started JVMs.A fix is coming up.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">stratosphere-dist.src.main.stratosphere-bin.bin.taskmanager.sh</file>
      <file type="M">stratosphere-dist.src.main.stratosphere-bin.bin.jobmanager.sh</file>
      <file type="M">stratosphere-dist.src.main.stratosphere-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-23 01:00:00" id="9420" opendate="2018-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for SQL IN sub-query operator in streaming</summary>
      <description>In FLINK-6094 we implemented non-windowed inner joins. The Table API &amp; SQL should now support the IN operator for sub-queries in streaming. Batch support has been added in FLINK-4565. We need to add unit tests, an IT case, and update the docs about that.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.UnsupportedOpsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.subquery.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-4-23 01:00:00" id="9429" opendate="2018-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstart E2E not working locally</summary>
      <description>The quickstart e2e test is not working locally. It seems as if the job does not produce anything into Elasticsearch. Furthermore, the test does not terminate with control-C.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-single-test.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-29 01:00:00" id="9467" opendate="2018-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>No Watermark display on Web UI</summary>
      <description>Watermark is currently not shown on the web interface,  because it still queries for watermark using the old metric name `currentLowWatermark` instead of the new ones `currentInputWatermark` and `currentOutputWatermark` </description>
      <version>1.5.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.hs.index.js</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.jobs.ctrl.coffee</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-30 01:00:00" id="9476" opendate="2018-5-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lost sideOutPut Late Elements in CEP Operator</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CepOperatorTestUtilities.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.PatternStream.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.SelectTimeoutCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.SelectCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.FlatSelectTimeoutCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.FlatSelectCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CEPOperatorUtils.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep-scala.src.main.scala.org.apache.flink.cep.scala.PatternStream.scala</file>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-1 01:00:00" id="9488" opendate="2018-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create common entry point for master and workers</summary>
      <description>To make the container setup easier, we should provide a single cluster entry point which uses leader election to become either the master or a worker which runs the TaskManager.</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.entrypoint.ClusterConfigurationParserFactoryTest.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-6-3 01:00:00" id="9508" opendate="2018-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>General Spell Check on Flink Docs</summary>
      <description>Fixing Flink docs misspelling </description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.upgrading.md</file>
      <file type="M">docs.ops.security-ssl.md</file>
      <file type="M">docs.ops.filesystems.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.dev.execution.configuration.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-4 01:00:00" id="9518" opendate="2018-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSL setup Docs config example has wrong keys password</summary>
      <description>In creating keystores and turststore section password is set to password but in setup config section it is abc123</description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.security-ssl.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-6 01:00:00" id="9532" opendate="2018-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Overview of Jobs Documentation Incorrect</summary>
      <description>Link"Jobs, grouped by status, each with a small summary of its status."This statement is incorrect as per the new response format.</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-6 01:00:00" id="9539" opendate="2018-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate flink-shaded 4.0</summary>
      <description>With the recent release of flink-shaded 4.0 we should bump the versions for all dependencies (except netty which is handled in FLINK-3952).We can now remove the exclusions from the jackson dependencies as they are now properly hidden.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-7 01:00:00" id="9549" opendate="2018-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix FlickCEP Docs broken link and minor style changes</summary>
      <description/>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-8 01:00:00" id="9555" opendate="2018-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support table api in scala shell</summary>
      <description>It would be nice to have table api available in scala shell so that user can experience table api in interactive way. </description>
      <version>1.5.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.start-script.start-scala-shell.sh</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
      <file type="M">flink-scala-shell.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-13 01:00:00" id="9576" opendate="2018-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong contiguity documentation</summary>
      <description>Example for the contiguity is first of all wrong, and second of all misleading: To illustrate the above with an example, a pattern sequence "a+ b" (one or more "a"’s followed by a "b") with input "a1", "c", "a2", "b" will have the following results:Strict Contiguity: {a2 b} – the "c" after "a1" causes "a1" to be discarded.Relaxed Contiguity: {a1 b} and {a1 a2 b} – "c" is ignored.Non-Deterministic Relaxed Contiguity: {a1 b}, {a2 b}, and {a1 a2 b}.For looping patterns (e.g. oneOrMore() and times()) the default is relaxed contiguity. If you want strict contiguity, you have to explicitly specify it by using the consecutive() call, and if you want non-deterministic relaxed contiguity you can use the allowCombinations() call. Results for the relaxed contiguity are wrong plus they do not clearly explains the internal contiguity of kleene closure.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-13 01:00:00" id="9578" opendate="2018-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow to define an auto watermark interval in SQL Client</summary>
      <description>Currently it is not possible to define an auto watermark interval in a non-programmatic way for the SQL Client.</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.EnvironmentTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.PropertyStrings.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Execution.java</file>
      <file type="M">flink-libraries.flink-sql-client.conf.sql-client-defaults.yaml</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-6-15 01:00:00" id="9595" opendate="2018-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add instructions to docs about ceased support of KPL version used in Kinesis connector</summary>
      <description>The KPL version used in the Kinesis connector for FlinkKinesisProducer is not longer supported by AWS Kinesis Streams: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Flink-1-4-and-below-STOPS-writing-to-Kinesis-after-June-12th-td22687.html#a22701We should add a notice about this to the Kinesis connectors, and how to bypass it by specifying the KPL version when building the Kinesis connector.</description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-21 01:00:00" id="9629" opendate="2018-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Datadog metrics reporter does not have shaded dependencies</summary>
      <description>flink-metrics-datadog-1.5.0.jar does not contain shaded dependencies for okhttp3 and okio</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-8-25 01:00:00" id="9655" opendate="2018-6-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Externalized checkpoint E2E test fails on travis</summary>
      <description>https://travis-ci.org/zentol/flink-ci/builds/396395491 ==============================================================================Running 'Resuming Externalized Checkpoint after terminal failure (file, sync) end-to-end test'==============================================================================Flink dist directory: /home/travis/build/zentol/flink-ci/flink/build-targetTEST_DATA_DIR: /home/travis/build/zentol/flink-ci/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-47420177246Starting cluster.Starting standalonesession daemon on host travis-job-b06f9adc-9d05-4569-b4bc-af11e6a0bc6e.Starting taskexecutor daemon on host travis-job-b06f9adc-9d05-4569-b4bc-af11e6a0bc6e.Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Dispatcher REST endpoint is up.Running externalized checkpoints test, with ORIGINAL_DOP=file NEW_DOP=false and STATE_BACKEND_TYPE=false STATE_BACKEND_FILE_ASYNC=true STATE_BACKEND_ROCKSDB_INCREMENTAL=false SIMULATE_FAILURE=false ...Job () is running.Waiting for job (1) to have at least completed checkpoints ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-27 01:00:00" id="9666" opendate="2018-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>short-circuit logic should be used in boolean contexts</summary>
      <description>short-circuit logic should be used in boolean contexts</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.UnilateralSortMerger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.NormalizedKeySorter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReOpenableMutableHashTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SlotSharingGroupAssignment.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.operators.CartesianProductDescriptor.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.TwoInputNode.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.StringValue.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.runtime.PojoComparator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-27 01:00:00" id="9681" opendate="2018-6-27 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Make sure minRetentionTime not equal to maxRetentionTime</summary>
      <description>Currently, for a group by(or other operators), if minRetentionTime equals to maxRetentionTime, the group by operator will register a timer for each record coming at different time which cause performance problem. The reasoning for having two parameters is that we can avoid to register many timers if we have more freedom when to discard state. As min equals to max cause performance problem it is better to make sure these two parameters are not same.Any suggestions are welcome.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.operators.ProcessFunctionWithCleanupStateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.operators.KeyedProcessFunctionWithCleanupStateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.StateCleaningCountTriggerHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.OverWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.NonWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.queryConfig.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Execution.java</file>
      <file type="M">docs.dev.table.streaming.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-4-3 01:00:00" id="9712" opendate="2018-7-3 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support enrichment joins in Flink SQL/Table API</summary>
      <description>As described here:https://docs.google.com/document/d/1KaAkPZjWFeu-ffrC9FhYuxE6CIKsatHTTxyrxSBR8Sk/edit?usp=sharing</description>
      <version>1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.index.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-3 01:00:00" id="9713" opendate="2018-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support versioned joins in planning phase</summary>
      <description>Queries like:SELECT o.amount * r.rate FROM Orders AS o, LATERAL TABLE (Rates(o.rowtime)) AS r WHERE o.currency = r.currencyshould evaluate to valid plan with versioned joins plan node.</description>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.TemporalTableJoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-3 01:00:00" id="9714" opendate="2018-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support versioned joins with processing time</summary>
      <description>Queries like:SELECT o.amount * r.rate FROM Orders AS o, LATERAL TABLE (Rates(o.proctime)) AS r WHERE o.currency = r.currencyshould work for processing time</description>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.utils.StreamingWithStateTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.TemporalTableJoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamTemporalTableJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalTemporalTableJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-3 01:00:00" id="9715" opendate="2018-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support versioned joins with event time</summary>
      <description>Queries like:SELECT o.amount * r.rate FROM Orders AS o, LATERAL TABLE (Rates(o.rowtime)) AS r WHERE o.currency = r.currencyshould work with event time</description>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.TemporalJoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.TemporalTableJoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamTemporalJoinToCoProcessTranslator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-3 01:00:00" id="9730" opendate="2018-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>avoid access static via class reference</summary>
      <description>&amp;#91;code refactor&amp;#93; access static via class reference</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.AbstractServerHandler.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.async.AsyncIOExample.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-4 01:00:00" id="9735" opendate="2018-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Potential resource leak in RocksDBStateBackend#getDbOptions</summary>
      <description>Here is related code: if (optionsFactory != null) { opt = optionsFactory.createDBOptions(opt); }opt, an DBOptions instance, should be closed before being rewritten.getColumnOptions has similar issue.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-4 01:00:00" id="9737" opendate="2018-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow to define Table Version Functions from SQL environment file</summary>
      <description>It should be possible to define TVF from SQL environment file.</description>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.FunctionService.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.DescriptorProperties.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-factory.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-factory-services-file</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSourceFactory.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSinkFactory.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.EnvironmentTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.SessionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ResultStore.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.UserDefinedFunction.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.SourceSink.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Source.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Sink.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.PropertyStrings.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Execution.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Environment.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Deployment.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.ConfigUtil.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-libraries.flink-sql-client.conf.sql-client-defaults.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-4 01:00:00" id="9738" opendate="2018-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide a way to define Table Version Functions in Table API</summary>
      <description/>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableFunctionScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.fieldExpression.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.table.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-4 01:00:00" id="9742" opendate="2018-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose Expression.resultType to public</summary>
      <description>I have use case of TableSource which requires custom implementation of TimestampExtractor. To ensure new TimestampExtractor to cover more general use cases, accessing Expression.resultType is necessary, but its scope is now defined as package private for "org.apache.flink".Below is the implementation of custom TimestampExtractor which leverages Expression.resultType, hence had to place it to org.apache.flink package (looks like a hack).class IsoDateStringAwareExistingField(val field: String) extends TimestampExtractor { override def getArgumentFields: Array[String] = Array(field) override def validateArgumentFields(argumentFieldTypes: Array[TypeInformation[_]]): Unit = { val fieldType = argumentFieldTypes(0) fieldType match { case Types.LONG =&gt; // OK case Types.SQL_TIMESTAMP =&gt; // OK case Types.STRING =&gt; // OK case _: TypeInformation[_] =&gt; throw ValidationException( s"Field '$field' must be of type Long or Timestamp or String but is of type $fieldType.") } } override def getExpression(fieldAccesses: Array[ResolvedFieldReference]): Expression = { val fieldAccess: Expression = fieldAccesses(0) fieldAccess.resultType match { case Types.LONG =&gt; // access LONG field fieldAccess case Types.SQL_TIMESTAMP =&gt; // cast timestamp to long Cast(fieldAccess, Types.LONG) case Types.STRING =&gt; Cast(Cast(fieldAccess, SqlTimeTypeInfo.TIMESTAMP), Types.LONG) } }}It would be better to just make Expression.resultType public to cover other cases as well. (I'm not sure other methods would be also better to be public as well.)</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionUtils.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-6 01:00:00" id="9771" opendate="2018-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Show Plan" option under Submit New Job in WebUI not working</summary>
      <description>Show Plan button under Submit new job in WebUI not working.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobPlanInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobPlanInfo.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-6 01:00:00" id="9772" opendate="2018-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation of Hadoop API outdated</summary>
      <description>It looks like the documentation of the Hadoop Compatibility is somewhat outdated? At least the text and examples in section Using Hadoop InputFormats mention methodsenv.readHadoopFile and env.createHadoopInputwhich do not exist anymore since 1.4.0.   </description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.index.md</file>
      <file type="M">docs.dev.batch.hadoop.compatibility.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-9 01:00:00" id="9781" opendate="2018-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>scala-maven-plugin fails on java 9</summary>
      <description>https://travis-ci.org/zentol/flink/jobs/40171125811:10:02.157 [INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ flink-runtime_2.11 ---11:10:04.861 [INFO] /home/travis/build/zentol/flink/flink-runtime/src/main/java:-1: info: compiling11:10:04.862 [INFO] /home/travis/build/zentol/flink/flink-runtime/src/main/scala:-1: info: compiling11:10:04.862 [INFO] Compiling 1486 source files to /home/travis/build/zentol/flink/flink-runtime/target/classes at 153113460486211:10:06.135 [ERROR] error: java.lang.NoClassDefFoundError: javax/tools/ToolProvider11:10:06.135 [INFO] at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)11:10:06.135 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.135 [INFO] at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)11:10:06.135 [INFO] at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)11:10:06.135 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.136 [INFO] at scala.collection.Iterator$class.foreach(Iterator.scala:891)11:10:06.136 [INFO] at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)11:10:06.136 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.136 [INFO] at scala.collection.AbstractIterable.foreach(Iterable.scala:54)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.136 [INFO] at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)11:10:06.136 [INFO] at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)11:10:06.136 [INFO] at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)11:10:06.136 [INFO] at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)11:10:06.136 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:71)11:10:06.136 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:39)11:10:06.136 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)11:10:06.136 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)11:10:06.136 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)11:10:06.136 [INFO] at scala.tools.nsc.Global$Run.&lt;init&gt;(Global.scala:1242)11:10:06.136 [INFO] at scala.tools.nsc.Driver.doCompile(Driver.scala:31)11:10:06.136 [INFO] at scala.tools.nsc.MainClass.doCompile(Main.scala:23)11:10:06.136 [INFO] at scala.tools.nsc.Driver.process(Driver.scala:51)11:10:06.136 [INFO] at scala.tools.nsc.Driver.main(Driver.scala:64)11:10:06.136 [INFO] at scala.tools.nsc.Main.main(Main.scala)11:10:06.136 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)11:10:06.136 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)11:10:06.136 [INFO] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)11:10:06.137 [INFO] at java.base/java.lang.reflect.Method.invoke(Method.java:564)11:10:06.137 [INFO] at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)11:10:06.137 [INFO] at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)11:10:06.137 [INFO] java.lang.reflect.InvocationTargetException11:10:06.137 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)11:10:06.137 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)11:10:06.137 [INFO] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)11:10:06.137 [INFO] at java.base/java.lang.reflect.Method.invoke(Method.java:564)11:10:06.137 [INFO] at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)11:10:06.137 [INFO] at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)11:10:06.137 [ERROR] Caused by: java.lang.NoClassDefFoundError: javax/tools/ToolProvider11:10:06.137 [INFO] at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)11:10:06.137 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.137 [INFO] at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)11:10:06.138 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.138 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.138 [INFO] at scala.collection.Iterator$class.foreach(Iterator.scala:891)11:10:06.138 [INFO] at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)11:10:06.138 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.138 [INFO] at scala.collection.AbstractIterable.foreach(Iterable.scala:54)11:10:06.148 [INFO] at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)11:10:06.148 [INFO] at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.149 [INFO] at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)11:10:06.149 [INFO] at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)11:10:06.149 [INFO] at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)11:10:06.149 [INFO] at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)11:10:06.149 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:71)11:10:06.149 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:39)11:10:06.151 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)11:10:06.153 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)11:10:06.153 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)11:10:06.153 [INFO] at scala.tools.nsc.Global$Run.&lt;init&gt;(Global.scala:1242)11:10:06.153 [INFO] at scala.tools.nsc.Driver.doCompile(Driver.scala:31)11:10:06.153 [INFO] at scala.tools.nsc.MainClass.doCompile(Main.scala:23)11:10:06.172 [INFO] at scala.tools.nsc.Driver.process(Driver.scala:51)11:10:06.172 [INFO] at scala.tools.nsc.Driver.main(Driver.scala:64)11:10:06.172 [INFO] at scala.tools.nsc.Main.main(Main.scala)11:10:06.172 [INFO] ... 6 more11:10:06.196 [INFO] ------------------</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-9 01:00:00" id="9785" opendate="2018-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add remote addresses to LocalTransportException instances</summary>
      <description>LocalTransportException instantiations do not always add the remote address into the exception message. Having this will ease debugging and make correlating different log files much easier.There always is an address associated with these exceptions but it does not seem like this is used anywhere for now and in that case it is also the local address (looks like its intended to be the error source's address).</description>
      <version>1.4.2,1.5.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-8-10 01:00:00" id="9795" opendate="2018-7-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Mesos documentation for flip6</summary>
      <description>Mesos documentation would benefit from an overhaul after flip6 became the default cluster management model.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.configuration.MesosOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">docs..includes.generated.mesos.task.manager.configuration.html</file>
      <file type="M">docs..includes.generated.mesos.configuration.html</file>
      <file type="M">docs..includes.generated.high.availability.zookeeper.configuration.html</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-11 01:00:00" id="9805" opendate="2018-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HTTP Redirect to Active JM in Flink CLI</summary>
      <description>Flink CLI allows specifying job manager address via --jobmanager flag. However, in HA mode the JM can change and then standby JM does HTTP redirect to the active one. However, during deployment via flink CLI with --jobmanager flag option the CLI does not redirect to the active one. Thus fails to submit job with "Could not complete the operation. Number of retries has been exhausted"  Proposal:Honor JM HTTP redirect in case leadership changes in flink CLI with --jobmanager flag active. </description>
      <version>1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-11 01:00:00" id="9806" opendate="2018-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a canonical link element to documentation HTML</summary>
      <description>Flink has suffered for a while with non-optimal SEO for its documentation, meaning a web search for a topic covered in the documentation often produces results for many versions of Flink, even preferring older versions since those pages have been around for longer.Using a canonical link element (see references) may alleviate this by informing search engines about where to find the latest documentation (i.e. pages hosted under https://ci.apache.org/projects/flink/flink-docs-master/).I think this is at least worth experimenting with, and if it doesn't cause problems, even backporting it to the older release branches to eventually clean up the Flink docs' SEO and converge on advertising only the latest docs (unless a specific version is specified).References: https://moz.com/learn/seo/canonicalization https://yoast.com/rel-canonical/ https://support.google.com/webmasters/answer/139066?hl=en https://en.wikipedia.org/wiki/Canonical_link_element</description>
      <version>1.5.0</version>
      <fixedVersion>1.3.4,1.4.3,1.5.3,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs..config.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-25 01:00:00" id="983" opendate="2014-6-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebClient makes names of functions unreadable</summary>
      <description>The named of Functions are truncated and abbreviated like "Properties of ...reeDistribution, delimiter: ) - ID = 7".There should be a place where the names are printed fully, not abbreviated/truncated.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.resources.web-docs.js.graphCreator.js</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-16 01:00:00" id="9865" opendate="2018-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-hadoop-compatibility should assume Hadoop as provided</summary>
      <description>The flink-hadoop-compatibility project as a compile scope dependency on Hadoop (flink-hadoop-shaded). Because of that, the hadoop dependencies are pulled into the user application.Like in other Hadoop-dependent modules, we should assume that Hadoop is provided in the framework classpath already.</description>
      <version>1.5.0,1.5.1</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hcatalog.pom.xml</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  <bug fixdate="2018-10-26 01:00:00" id="9975" opendate="2018-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Netty dependency of Hadoop &gt;= 2.7 is not relocated</summary>
      <description>Previously, in flink-shaded-hadoop, we also relocate Netty (org.jboss.netty) to not conflict with user code. Since Hadoop 2.7 the Netty version they depend on has been upgraded and we missed relocating io.netty accordingly.</description>
      <version>1.4.2,1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-29 01:00:00" id="9990" opendate="2018-7-29 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add regexp_extract supported in TableAPI and SQL</summary>
      <description>regex_extract is a very useful function, it returns a string based on a regex pattern and a index.For example : regexp_extract('foothebar', 'foo(.*?)(bar)', 2) // returns 'bar.'It is provided as a UDF in Hive, more details please see&amp;#91;1&amp;#93;.&amp;#91;1&amp;#93;: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.sql.ScalarSqlFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.functions.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-29 01:00:00" id="9991" opendate="2018-7-29 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add regexp_replace supported in TableAPI and SQL</summary>
      <description>regexp_replace is a very userful function to process String. For example :regexp_replace("foobar", "oo|ar", "") //returns 'fb.'It is supported as a UDF in Hive, more details please see&amp;#91;1&amp;#93;.&amp;#91;1&amp;#93;: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF </description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.sql.ScalarSqlFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.functions.md</file>
    </fixedFiles>
  </bug>
</bugrepository>