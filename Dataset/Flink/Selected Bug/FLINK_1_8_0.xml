<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2018-11-5 01:00:00" id="10791" opendate="2018-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide end-to-end test for Kafka 0.11 connector</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-6 01:00:00" id="10801" opendate="2018-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix sql client integrate elasticsearch connector test failure</summary>
      <description>It usually reports : FAIL SQL Client Elasticsearch Upsert: Output hash mismatch. Got 6187222e109ee9222e6b2f117742070c, expected 982cb32908def9801e781381c1b8a8db.head hexdump of actual:0000000 { \n " h i t s " : { \n 0000010 " t o t a l " : 3 , \n0000020 " m a x _ s c o r e " 0000030 : 1 . 0 , \n " h i t s0000040 " : [ \n { \n 0000050 " _ i n d e x " :0000060 " m y _ u s e r s " , \n 0000070 " _ t y p e " : "0000080 u s e r " , \n "0000090 _ i d " : " 1 _ B o b "00000a0 , \n " _ s c o r00000b0 e " : 1 . 0 , \n 00000bathe actual hash means : { "hits" : { "total" : 3, "max_score" : 1.0, "hits" : [ { "_index" : "my_users", "_type" : "user", "_id" : "1_Bob ", "_score" : 1.0, "_source" : { "user_id" : 1, "user_name" : "Bob ", "user_count" : 1 } }, { "_index" : "my_users", "_type" : "user", "_id" : "22_Alice", "_score" : 1.0, "_source" : { "user_id" : 22, "user_name" : "Alice", "user_count" : 1 } }, { "_index" : "my_users", "_type" : "user", "_id" : "42_Greg ", "_score" : 1.0, "_source" : { "user_id" : 42, "user_name" : "Greg ", "user_count" : 3 } } ] }}the expected hash code means : { "hits" : { "total" : 3, "max_score" : 1.0, "hits" : [ { "_index" : "my_users", "_type" : "user", "_id" : "1_Bob ", "_score" : 1.0, "_source" : { "user_id" : 1, "user_name" : "Bob ", "user_count" : 2 } }, { "_index" : "my_users", "_type" : "user", "_id" : "22_Alice", "_score" : 1.0, "_source" : { "user_id" : 22, "user_name" : "Alice", "user_count" : 1 } }, { "_index" : "my_users", "_type" : "user", "_id" : "42_Greg ", "_score" : 1.0, "_source" : { "user_id" : 42, "user_name" : "Greg ", "user_count" : 3 } } ] }}It seems that the user count for "Bob" is off by 1.The speculation is due to the premature acquisition of aggregated statistics from Elasticsearch. </description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-18 01:00:00" id="10921" opendate="2018-11-18 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Prioritize shard consumers in Kinesis Consumer by event time</summary>
      <description>Shard consumer threads currently emit records directly. In order to align shards by event time, decouple shard consumer threads and emitter with a queue, as described in &amp;#91;1&amp;#93;.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/ac41718246ad8f6098efaf7dbf5f7182d60abdc473e8bf3c96ef5968@%3Cdev.flink.apache.org%3E</description>
      <version>None</version>
      <fixedVersion>1.8.1,1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.TestableKinesisDataFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.DynamoDBStreamsDataFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ConsumerConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-19 01:00:00" id="10922" opendate="2018-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor the placement of the Flink Kafka connector end to end test module</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka.src.main.java.org.apache.flink.streaming.examples.kafka.KafkaExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-base.src.main.java.org.apache.flink.streaming.examples.kafka.base.RollingAdditionMapper.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-base.src.main.java.org.apache.flink.streaming.examples.kafka.base.KafkaExampleUtil.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-base.src.main.java.org.apache.flink.streaming.examples.kafka.base.KafkaEventSchema.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-base.src.main.java.org.apache.flink.streaming.examples.kafka.base.KafkaEvent.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-base.src.main.java.org.apache.flink.streaming.examples.kafka.base.CustomWatermarkExtractor.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-base.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.11.src.main.java.org.apache.flink.streaming.examples.kafka.Kafka011Example.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.11.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.10.src.main.scala.org.apache.flink.streaming.scala.examples.kafka.Kafka010Example.scala</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.10.src.main.java.org.apache.flink.streaming.examples.kafka.Kafka010Example.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.10.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka011.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka010.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kafka011-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kafka010-test.src.main.scala.org.apache.flink.streaming.scala.kafka.test.Kafka010Example.scala</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kafka010-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kafka-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kafka-test-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-19 01:00:00" id="10932" opendate="2018-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Initial flink-kubernetes module with empty implementation</summary>
      <description>Initial the skeleton module to start native kubernetes integration. </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-20 01:00:00" id="10951" opendate="2018-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable enforcing of YARN container virtual memory limits in tests</summary>
      <description>DescriptionThe Jepsen YARN tests sporadically fail because TM containers are exceeding their virtual memory limits:Closing TaskExecutor connection container_1541436244107_0001_01_000005 because: Container [pid=32403,containerID=container_1541436244107_0001_01_000005] is running beyond virtual memory limits. Current usage: 970.2 MB of 2 GB physical memory used; 4.2 GB of 4.2 GB virtual memory used. Killing container.By default YARN enforces a virtual memory limit of 2.1 times the requested physical memory. However, in my experiments, the virtual memory of a JVM process running the ClusterEntryPoint (without submitting job) is already in the region of 3.3 GB. Hence, the virtual memory enforcement should be disabled.Acceptance Criteria yarn.nodemanager.vmem-check-enabled is false in yarn-site.xml</description>
      <version>1.8.0</version>
      <fixedVersion>1.6.3,1.7.0,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.hadoop.clj</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-22 01:00:00" id="10992" opendate="2018-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen: Do not use /tmp as HDFS Data Directory</summary>
      <description>dfs.name.dir and dfs.data.dir should not be located in /tmp. The directories might get deleted unintentionally, which can cause test failures.</description>
      <version>1.6.2,1.7.0,1.8.0</version>
      <fixedVersion>1.6.3,1.7.0,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.utils.clj</file>
      <file type="M">flink-jepsen.src.jepsen.flink.hadoop.clj</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-1-27 01:00:00" id="11012" opendate="2018-11-27 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce abstract superclass for filesystem IT cases</summary>
      <description>IT cases for filesystems are somewhat similar as they are using write/read cycles and querying meta data like directory listing. We could avoid code duplication now and for the future by introducing an abstract superclass for those it cases that outlines the tests.</description>
      <version>1.8.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.test.java.org.apache.flink.fs.osshadoop.HadoopOSSFileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.java</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.pom.xml</file>
      <file type="M">docs.ops.deployment.oss.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-12-12 01:00:00" id="11144" opendate="2018-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Tests runnable on Java 9</summary>
      <description>When using Java 9, tests should run when invoking mvn clean install -DfastAcceptance criteria: Project compiles with mvn clean install -Dfast -DskipTests Tests are runnable but may not pass</description>
      <version>1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-1-14 01:00:00" id="11161" opendate="2018-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to import java packages in scala-shell</summary>
      <description/>
      <version>1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.start-script.start-scala-shell.sh</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-18 01:00:00" id="11191" opendate="2018-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exception in code generation when ambiguous columns in MATCH_RECOGNIZE</summary>
      <description>Query:SELECT *FROM TickerMATCH_RECOGNIZE ( PARTITION BY symbol, price ORDER BY proctime MEASURES A.symbol AS symbol, A.price AS price PATTERN (A) DEFINE A AS symbol = 'a') AS Tthrows a cryptic exception from the code generation stack that the output arity is wrong. We should add early validation and throw a meaningful exception. I've also created a calcite ticket to fix it on calcite's side: CALCITE-2747</description>
      <version>1.7.0,1.8.0</version>
      <fixedVersion>1.7.2,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.match.MatchRecognizeValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamMatchRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-20 01:00:00" id="11207" opendate="2018-12-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Apache commons-compress from 1.4.1 to 1.18</summary>
      <description>There is at least one security vulnerability in the current version that we should address by upgrading to 1.18+:https://app.snyk.io/vuln/SNYK-JAVA-ORGAPACHECOMMONS-32473</description>
      <version>1.5.5,1.6.2,1.7.0,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2-uber.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-31 01:00:00" id="11235" opendate="2018-12-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch connector leaks threads if no connection could be established</summary>
      <description>elasticsearch transport sink init steps1, create client thread2, try to check every host:port3, if each host:port is unreachable, while throw RuntimeExceptionbut, because of throw RuntimeException, the client can not close, so causing thread leaktransport client code```TransportClient transportClient = new PreBuiltTransportClient(settings);for (TransportAddress transport : ElasticsearchUtils.convertInetSocketAddresses(transportAddresses)) { transportClient.addTransportAddress(transport);}// verify that we actually are connected to a clusterif (transportClient.connectedNodes().isEmpty()) { throw new RuntimeException("Elasticsearch client is not connected to any Elasticsearch nodes!");}return transportClient;}```thread leakthread dump </description>
      <version>1.6.3,1.7.1,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.Elasticsearch5ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.Elasticsearch2ApiCallBridge.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-3 01:00:00" id="11262" opendate="2019-1-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump jython-standalone to 2.7.1</summary>
      <description>Due to security issue: https://ossindex.sonatype.org/vuln/7a4be7b3-74f5-4a9b-a24f-d1fd80ed6bbca</description>
      <version>1.6.3,1.7.1,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-libraries.flink-streaming-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-libraries.flink-streaming-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-8 01:00:00" id="11288" opendate="2019-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add separate flink-ml module for building fat jar</summary>
      <description>Similar to sql-connectors the flink-ml fat jar , that is included in flink-dist, should be built in a separate module so that we can add proper licensing to it.</description>
      <version>1.7.1,1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-libraries.pom.xml</file>
      <file type="M">flink-libraries.flink-ml.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-8 01:00:00" id="11289" opendate="2019-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework example module structure to account for licensing</summary>
      <description>Some of our example bundle additional dependencies (like kafka). The example jars (that are deployed to maven central and included in flink-dist) should have proper licensing.Otherwise we would have to disable deployment of these jars, and keep the flink-dist NOTICE file in sync with example dependencies manually.One way to do this would be to add a separate module for each jar, or at the very least for those example that bundle dependencies.</description>
      <version>1.7.1,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-twitter.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-1-24 01:00:00" id="11422" opendate="2019-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add builder for creating StreamTask mocks</summary>
      <description/>
      <version>1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorLatencyMetricsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamOperatorChainingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamSourceContextIdleDetectionTests.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-24 01:00:00" id="11427" opendate="2019-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Protobuf parquet writer implementation</summary>
      <description>Right now only ParquetAvroWriters exist to create ParquetWriterFactory. We want to implement a protobuf ParquetProtoWriters to create ParquetWriterFactory.  I am happy to submit a PR if this approach sounds good . </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">docs.dev.connectors.streamfile.sink.zh.md</file>
      <file type="M">docs.dev.connectors.streamfile.sink.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-29 01:00:00" id="11442" opendate="2019-1-29 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Upgrade OSS SDK Version</summary>
      <description>Upgrade oss sdk version to exclude org.json dependency.&amp;#91;INFO&amp;#93; +- com.aliyun.oss:aliyun-sdk-oss:jar:3.1.0:compile&amp;#91;INFO&amp;#93; | +- org.apache.httpcomponents:httpclient:jar:4.5.3:compile&amp;#91;INFO&amp;#93; | | &amp;#45; org.apache.httpcomponents:httpcore:jar:4.4.6:compile&amp;#91;INFO&amp;#93; | +- org.jdom:jdom:jar:1.1:compile&amp;#91;INFO&amp;#93; | +- com.sun.jersey:jersey-json:jar:1.9:compile&amp;#91;INFO&amp;#93; | | +- org.codehaus.jettison:jettison:jar:1.1:compile&amp;#91;INFO&amp;#93; | | | &amp;#45; stax:stax-api:jar:1.0.1:compile&amp;#91;INFO&amp;#93; | | +- com.sun.xml.bind:jaxb-impl:jar:2.2.3-1:compile&amp;#91;INFO&amp;#93; | | | &amp;#45; javax.xml.bind:jaxb-api:jar:2.2.2:compile&amp;#91;INFO&amp;#93; | | | +- javax.xml.stream:stax-api:jar:1.0-2:compile&amp;#91;INFO&amp;#93; | | | &amp;#45; javax.activation:activation:jar:1.1:compile&amp;#91;INFO&amp;#93; | | +- org.codehaus.jackson:jackson-jaxrs:jar:1.8.3:compile&amp;#91;INFO&amp;#93; | | &amp;#45; org.codehaus.jackson:jackson-xc:jar:1.8.3:compile&amp;#91;INFO&amp;#93; | +- com.aliyun:aliyun-java-sdk-core:jar:3.4.0:compile&amp;#91;INFO&amp;#93; | | &amp;#45; org.json:json:jar:20170516:compile&amp;#91;INFO&amp;#93; | +- com.aliyun:aliyun-java-sdk-ram:jar:3.0.0:compile&amp;#91;INFO&amp;#93; | +- com.aliyun:aliyun-java-sdk-sts:jar:3.0.0:compile&amp;#91;INFO&amp;#93; | &amp;#45; com.aliyun:aliyun-java-sdk-ecs:jar:4.2.0:compile The license of org.json:json:jar:20170516:compile is JSON License, which cannot be included.https://www.apache.org/legal/resolved.html#json</description>
      <version>1.8.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-30 01:00:00" id="11469" opendate="2019-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix example in "Tuning Checkpoints and Large State" documentation</summary>
      <description>Sample code for subtitle Tuning RocksDB in Tuning Checkpoints and Large State is wrong  Affects Version：All versions after 1.1  </description>
      <version>1.6.2,1.6.3,1.6.4,1.7.0,1.7.1,1.7.2,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-2-5 01:00:00" id="11535" opendate="2019-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client jar does not contain table-api-java</summary>
      <description>The SQL Client jar does not package flink-table-api-java.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-12 01:00:00" id="11585" opendate="2019-2-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prefix matching in ConfigDocsGenerator can result in wrong assignments</summary>
      <description>There are some cases where the key-prefix matching does not work as intended: given the prefixes "a.b" and "a.b.c.d", then an option with a key "a.b.c.X" will be assigned to the default groups instead of "a.b" given a prefix "a.b", an option "a.c.b" will be matched to that group instead of the default</description>
      <version>1.6.3,1.7.1,1.8.0</version>
      <fixedVersion>1.6.4,1.7.3,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocGeneratorTest.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-13 01:00:00" id="1159" opendate="2014-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Case style anonymous functions not supported by Scala API</summary>
      <description>In Scala it is very common to define anonymous functions of the following form{case foo: Bar =&gt; foobar(foo)case _ =&gt; throw new RuntimeException()}These case style anonymous functions are not supported yet by the Scala API. Thus, one has to write redundant code to name the function parameter.What works is the following pattern, but it is not intuitive for someone coming from Scala:dataset.map{ _ match{ case foo:Bar =&gt; ... }}</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.JoinedStreams.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.ConnectedStreams.scala</file>
      <file type="M">docs.apis.streaming.index.md</file>
      <file type="M">docs.apis.java8.md</file>
      <file type="M">docs.apis.batch.index.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-8-16 01:00:00" id="11634" opendate="2019-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "State Backends" page into Chinese</summary>
      <description>doc locates in flink/docs/dev/stream/state/state_backens.md</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-16 01:00:00" id="11636" opendate="2019-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "State Schema Evolution" into Chinese</summary>
      <description>doc locates in flink/docs/dev/stream/state/schema_evolution.mdhttps://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/state/schema_evolution.html</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.state.schema.evolution.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-16 01:00:00" id="11637" opendate="2019-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Checkpoints" page into Chinese</summary>
      <description>doc locates in flink/docs/ops/state/checkpoints.md</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.checkpoints.zh.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-10-20 01:00:00" id="11665" opendate="2019-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink fails to remove JobGraph from ZK even though it reports it did</summary>
      <description>We recently have seen the following issue with Flink 1.5.5:Given Flink Job ID 1d24cad26843dcebdfca236d5e3ad82a: 1- A job is activated successfully and the job graph added to ZK:Added SubmittedJobGraph(1d24cad26843dcebdfca236d5e3ad82a, null) to ZooKeeper.2- Job is deactivated, Flink reports that the job graph has been successfully removed from ZK and the blob is deleted from the blob server (in this case S3):Removed job graph 1d24cad26843dcebdfca236d5e3ad82a from ZooKeeper.3- JM is later restarted, Flink for some reason attempts to recover the job that it reported earlier it has removed from ZK but since the blob has already been deleted the JM goes into a crash loop. The only way to recover it manually is to remove the job graph entry from ZK:Recovered SubmittedJobGraph(1d24cad26843dcebdfca236d5e3ad82a, null). andorg.apache.flink.fs.s3presto.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 1BCDFD83FC4546A2), S3 Extended Request ID: OzZtMbihzCm1LKy99s2+rgUMxyll/xYmL6ouMvU2eo30wuDbUmj/DAWoTCs9pNNCLft0FWqbhTo= (Path: s3://blam-state-staging/flink/default/blob/job_1d24cad26843dcebdfca236d5e3ad82a/blob_p-c51b25cc0b20351f6e32a628bb6e674ee48a273e-ccfa96b0fd795502897c73714185dde3)My question is under what circumstances would this happen? this seems to happen very infrequently but since the consequence is severe (JM crash loop) we'd like to understand how it would happen.This all seems a little similar to https://issues.apache.org/jira/browse/FLINK-9575 but that issue is reported fixed in Flink 1.5.2 and we are already on Flink 1.5.5</description>
      <version>1.5.5,1.6.4,1.7.2,1.8.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DispatcherRunnerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DispatcherRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDispatcherRunnerImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DispatcherRunnerImplNGFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-4-20 01:00:00" id="11670" opendate="2019-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SUSPEND/TERMINATE calls to REST API</summary>
      <description>Exposes the SUSPEND/TERMINATE functionality to the user through the REST API.</description>
      <version>1.8.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.util.FakeClusterClient.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.TimestampITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingDispatcherGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendStopTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.MiniClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.StopOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-20 01:00:00" id="11679" opendate="2019-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Blink SQL planner and runtime modules</summary>
      <description>As mentioned in FLIP-32, we will create separate modules while performing the Blink SQL merge. As part of this issue we will create flink-table-planner-blink and flink-table-runtime-blink modules.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.stage.sh</file>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-20 01:00:00" id="11686" opendate="2019-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enforce spaces around lambdas</summary>
      <description>Enforce spaces around lambdas so that ()-&gt;x would be rejected. We commonly add spaces around lambdas, might as well enforce it properly.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.checkstyle.xml</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-21 01:00:00" id="11713" opendate="2019-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy mode from documentation</summary>
      <description>Legacy mode cannot be used anymore but docs still mention it:mode: Execution mode of Flink. Possible values are legacy and new. In order to start the legacy components, you have to specify legacy (DEFAULT: new). </description>
      <version>1.7.2,1.8.0</version>
      <fixedVersion>1.7.3,1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.configuration.MesosOptions.java</file>
      <file type="M">docs..includes.generated.mesos.configuration.html</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
      <file type="M">docs.monitoring.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-25 01:00:00" id="11743" opendate="2019-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sticky E2E tests failed on travis</summary>
      <description>https://travis-ci.org/apache/flink/builds/497743828Running 'Local recovery and sticky scheduling end-to-end test'==============================================================================TEST_DATA_DIR: /home/travis/build/apache/flink/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-02822890481Flink dist directory: /home/travis/build/apache/flink/flink/flink-dist/target/flink-1.8-SNAPSHOT-bin/flink-1.8-SNAPSHOTRunning local recovery test with configuration:       parallelism: 4       max attempts: 10       backend: rocks       incremental checkpoints: true       kill JVM: trueStarting zookeeper daemon on host travis-job-82bee23f-b097-4080-9be9-556bd13fbfe1.Starting HA cluster with 1 masters.used deprecated key `jobmanager.heap.mb`, please replace with key `jobmanager.heap.size`Starting standalonesession daemon on host travis-job-82bee23f-b097-4080-9be9-556bd13fbfe1.used deprecated key `taskmanager.heap.mb`, please replace with key `taskmanager.heap.size`Starting taskexecutor daemon on host travis-job-82bee23f-b097-4080-9be9-556bd13fbfe1.Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Dispatcher REST endpoint is up.Started TM watchdog with PID 25095.Starting execution of programProgram execution finishedJob with JobID d5eec8ff1a3f52d89d9c0a14d26afb5f has finished.Job Runtime: 1698 msFAILURE: Found 15 failed attempt(s) for local recovery of correctly scheduled task(s).</description>
      <version>1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestorePrepareResult.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.BackendBuildingException.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-25 01:00:00" id="11745" opendate="2019-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TTL end-to-end test restores from the savepoint after the job cancelation</summary>
      <description>The state TTL end-to-end test currently cancels the first running job, takes savepoint and starts the job again from stratch without using the savepoint. The second job should start from the previously taken savepoint.</description>
      <version>1.6.4,1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.6.5,1.7.3,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.stream.state.ttl.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-4-5 01:00:00" id="11822" opendate="2019-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce Flink metadata handlers</summary>
      <description>Calcite has defined various metadata handlers(e.g. RowCoun, Selectivity and provided default implementation(e.g. RelMdRowCount, RelMdSelectivity). However, the default implementation can't completely meet our requirements, e.g. some of its logic is incomplete，and some RelNodes are not considered.There are two options to meet our requirements:option 1. Extends from default implementation, overrides method to improve its logic, add new methods for new RelNode. The advantage of this option is we just need to focus on the additions and modifications. However, its shortcomings are also obvious: we have no control over the code of non-override methods in default implementation classes especially when upgrading the Calcite version.option 2. Extends from metadata handler interfaces, reimplement all the logic. Its shortcomings are very obvious, however we can control all the code logic that's what we want.so we choose option 2!In this jira, all Flink metadata handles will be introduced, including calcite builtin metadata handlers:FlinkRelMdPercentageOriginalRow,FlinkRelMdNonCumulativeCost,FlinkRelMdCumulativeCost,FlinkRelMdRowCount,FlinkRelMdSize,FlinkRelMdSelectivity,FlinkRelMdDistinctRowCoun,FlinkRelMdPopulationSize,FlinkRelMdColumnUniqueness,FlinkRelMdUniqueKeys,FlinkRelMdDistribution,and flink extented metadata handlers:FlinkRelMdColumnInterval,FlinkRelMdFilteredColumnInterval,FlinkRelMdColumnNullCount,FlinkRelMdColumnOriginNullCount,FlinkRelMdUniqueGroups,FlinkRelMdModifiedMonotonicity</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.MaxAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.MinAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkRelOptClusterFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.package.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.Rank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalDataStreamTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalIntersect.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalMinus.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalOverWindow.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableFunctionScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalUnion.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkHepProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkVolcanoProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecOverWindowAggRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.StreamExecDeduplicateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.TwoStageOptimizedAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.FlinkRelOptTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.stats.FlinkStatistic.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.trait.FlinkRelDistribution.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelMdUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RankProcessStrategy.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SingleRowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.SingleRowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.RankITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-5 01:00:00" id="11827" opendate="2019-3-5 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce DataFormatConverters to convert internal data format and java format</summary>
      <description>We have introduced a new internal data format, but if user interact with Source, sink and Udx, they prefer to use the traditional Java data format.So we introduce DataFormat Converters to convert the internal efficient binary data format into more general Java data format when source, sink and UDX are used.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryArray.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-6 01:00:00" id="11836" opendate="2019-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update NOTICE-binary and licenses-binary for Flink 1.8.0</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.collect.license.files.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-3-8 01:00:00" id="11858" opendate="2019-3-8 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce block compressor/decompressor for batch table runtime</summary>
      <description>Introduce block compressor/decompressor API to support future compression of the spilled data in batch table runtime. The compressor/decompressor works on block one at a time. A block is a piece of continuous data stored either in `byte[]` or `ByteBuffer`. We will first support lz4 compression algorithm.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-13 01:00:00" id="11892" opendate="2019-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port conflict when running nightly end-to-end tests</summary>
      <description>When I do the end-to-end check according to `https://github.com/apache/flink/tree/master/flink-end-to-end-tests`. I got the follows problem:1. Executed command， and the message of console as follows:FLINK_DIR=/Users/jincheng/work/FlinkRelease/1.8/flink-1.8.0/flink-dist/target/flink-1.8.0-bin/flink-1.8.0export FLINK_DIRsh flink-end-to-end-tests/run-nightly-tests.sh......Starting taskexecutor daemon on host jinchengsunjcs-iMac.local.Dispatcher REST endpoint is up.Job (180a3cfc35d549417e5807520d7402f9) is running.Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...2. Log info:2019-03-13 07:56:33,670 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator         - Remoting shut down.2019-03-13 07:56:33,673 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Could not start cluster entrypoint StandaloneSessionClusterEntrypoint.org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint StandaloneSessionClusterEntrypoint.at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:190)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:535)at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:65)Caused by: java.net.BindException: Could not start actor system on any port in port range 6123at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:172)at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:112)at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:87)at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.createRpcService(AkkaRpcServiceUtils.java:84)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createRpcService(ClusterEntrypoint.java:296)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:264)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:216)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:172)at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:171)3. environmentMacOS: 10.14.3 Java version "1.8.0_151"jinchengsunjcs-iMac:~ jincheng$ echo $0-bash</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.README.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-13 01:00:00" id="11896" opendate="2019-3-13 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce stream physical nodes</summary>
      <description>This issues aims to introduce flink stream physical RelNode, such as StreamExecCalc, StreamExecExchange, StreamExecExpand etc.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkOptimizeContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamPhysicalRel.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecGroupAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.WatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.Rank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.LogicalWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableConfig.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-3-14 01:00:00" id="11918" opendate="2019-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecated Window and Rename it to GroupWindow</summary>
      <description>OverWindow and Window are confusing in the API, and mentioned that we want to rename it to GroupWindow for many times.  So,  here just a suggestion, how about Deprecated the Window in release-1.8, since we should create a new RC2 for release 1.8. If we do not do that the Window will keep existing for almost half a year. I create this JIRA, and link to release-1.8 vote mail thread, ask RM's options. If all of you do not agree, I'll close the JIRA, otherwise, we can open the new PR for Depercated the window. What do you think?</description>
      <version>1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.UpdatingPlanCheckerTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RetractionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.OverWindowStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.GroupWindowStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.AggregateStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.windows.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.table.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.windows.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.java.windows.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-18 01:00:00" id="11950" opendate="2019-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing dependencies in NOTICE file of flink-dist.</summary>
      <description>Add Missing dependencies in NOTICE file of flink-dist. </description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-3-26 01:00:00" id="12013" opendate="2019-3-26 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support calc and correlate in blink</summary>
      <description>1.Support filter and projection in calc, e.g.: select a + b, c from T where a &gt; 5;2.Support udf in calc.3.Support udtf in correlate.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.type.RowType.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.type.InternalTypeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BaseRowTypeInfo.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryString.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.StreamTestSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.ValuesITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.FlinkTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.DataStreamTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecBoundedStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.SinkCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.scala.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.scala.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-4-1 01:00:00" id="12076" opendate="2019-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for generating optimized logical plan for simple group aggregate on batch</summary>
      <description>To distinguish Window aggregate and Over aggregate, we call the `GROUP BY` aggregate as group aggregate.This issue only involves batch group aggregate, stream group aggregate will be finished in new issue due to retraction derivation.complex group aggregate(including GROUP SETS and DISTINCT) will also be finished in new issue.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.ValuesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.UnionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.ValuesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.UnionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.SingleRowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableConfig.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.DeclarativeAggregateFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.AvgAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.ExpressionBuilder.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.api.PlannerConfigOptions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-1 01:00:00" id="12077" opendate="2019-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce HashJoinOperator and LongHashJoinGenerator to blink</summary>
      <description>Introduce HashJoinOperator to support all join type.Introduce LongHashJoinGenerator to do some performance optimization when key is long using LongHybridHashTable.HashJoinOperator and LongHashJoinOperator can be used in both shuffle hash join and broadcast hash join.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedProjection.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.OperatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-1 01:00:00" id="12079" opendate="2019-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for generating optimized logical plan for join on batch</summary>
      <description>Currently, there are 3 types of batch physical join nodes: BatchExecHashJoin, BatchExecSortMergeJoin and BatchExecNestedLoopJoin. This issue aims to add rules to convert logical join to appropriate physical join. a logical join can be converted to BatchExecHashJoin if there exists at least one equal-join condition and ShuffleHashJoin or BroadcastHashJoin are enabled, can be converted to BatchExecSortMergeJoin if there exists at least one equal-join condition and SortMergeJoin is enabled, can be converted to BatchExecNestedLoopJoin if NestedLoopJoin is enabled or one of join input sides returns at most a single row.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.package.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableConfig.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.api.PlannerConfigOptions.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-10-9 01:00:00" id="12147" opendate="2019-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>InfluxDB throws errors if metric value is NaN/infinity</summary>
      <description>when use influxDB store metric datas, and metric kafka performance. There has some POSITIVE_INFINITY or NEGATIVE_INFINITY   data in the values, but influxDB no support it, so it alway some error.</description>
      <version>1.8.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-influxdb.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-4-12 01:00:00" id="12168" opendate="2019-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support e2e limit, sortLimit, rank, union in blink batch</summary>
      <description>Support limit and add limit it cases to blink batch.Support sortLimit and add sortLimit it cases to blink batch.Support rank and add rank it cases to blink batch.Support union and add union it cases to blink batch.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.OuterJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.DecimalITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CorrelateITCase2.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.util.BaseRowTestUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.codegen.SortCodeGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecUnion.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.SortCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-4-13 01:00:00" id="12185" opendate="2019-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for generating optimized logical plan for join on stream</summary>
      <description>This issue aims to supports generating optimized plan for join on stream. The query will be converted to window join if join condition contains window bounds, otherwise will be converted to normal join.e.g.Queries similar to the following should be window join:SELECT t1.a, t2.b FROM MyTable t1 JOIN MyTable2 t2 ON t1.a = t2.a AND t1.proctime BETWEEN t2.proctime - INTERVAL '1' HOUR AND t2.proctime + INTERVAL '1' HOUR</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkLogicalRankRuleForRangeEndTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.ShuffledHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RankUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-15 01:00:00" id="12200" opendate="2019-4-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>[Table API] Support UNNEST for MAP types</summary>
      <description>In case if the input dataset has the following schema :Row(a: Integer, b: Long, c: Map&lt;String, String&gt;)I would like to have the ability to execute the SQL query like:SELECT a, k, v FROM src, UNNEST(c) as m (k,v)Currently, the UNNEST operator is supported only for ARRAY and MULTISET I would like to propose adding the support of UNNEST functionality for MAP types.</description>
      <version>1.7.2,1.7.3,1.8.0,1.8.1</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.ExplodeFunctionUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalUnnestRule.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-7-20 01:00:00" id="12277" opendate="2019-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for catalog</summary>
      <description>Add documentation for unified catalog APIs and catalogs.The ticket for corresponding Chinese documentation is FLINK-13086</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-22 01:00:00" id="12283" opendate="2019-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API does not allow non-static inner class as UDF</summary>
      <description>See details here https://lists.apache.org/thread.html/9ecec89ba1225dbd6b3ea2466a910ad9685a42a4672b449f6ee13565@%3Cuser.flink.apache.org%3E</description>
      <version>1.8.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RexProgramExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PushFilterIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.call.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.SelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.call.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.CallExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.UserDefinedFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.TableFunctionDefinition.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.TableAggregateFunctionDefinition.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.ScalarFunctionDefinition.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.AggregateFunctionDefinition.java</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.test.scala.org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImplTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.TableEnvironmentMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.OperationTreeBuilder.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.functions.UserDefinedFunctionHelper.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolverRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.ExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImplTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-5-23 01:00:00" id="12299" opendate="2019-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reject negative auto watermark intervals</summary>
      <description>In any scenario, `autoWatermarkInterval` should not be less than or equal to zero.First of all, this does not correspond to the meaning of `autoWatermarkInterval`.Second, in the case where `autoWatermarkInterval` is less than 0, we will not be able to register ourselves in `TimestampsAndPeriodicWatermarksOperator#open`, which will result in the water level of this stream being kept at the lowest level.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-24 01:00:00" id="12306" opendate="2019-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the name of variable "log" to upper case "LOG"</summary>
      <description>Change the name of variable "log" from lower case to upper case "LOG" to correspond to other class files.</description>
      <version>1.7.2,1.8.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.MainThreadValidatorUtil.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-4-24 01:00:00" id="12323" opendate="2019-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy ActorGateway implementations</summary>
      <description>Remove the legacy ActorGateway based implementations.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskInputSplitProviderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.TaskInputSplitProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayTaskManagerActions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayResultPartitionConsumableNotifier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayPartitionProducerStateChecker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayKvStateRegistryListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayKvStateLocationOracle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayGlobalAggregateManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayCheckpointResponder.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-11 01:00:00" id="1234" opendate="2014-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hadoop2 profile default</summary>
      <description>As per mailing list discussion.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.generate.specific.pom.sh</file>
      <file type="M">tools.deploy.to.maven.sh</file>
      <file type="M">tools.change-version</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-addons.pom.xml</file>
      <file type="M">flink-addons.flink-yarn.pom.xml</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.pom.xml</file>
      <file type="M">docs.building.md</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-26 01:00:00" id="12342" opendate="2019-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn Resource Manager Acquires Too Many Containers</summary>
      <description>In currently implementation of YarnFlinkResourceManager, it starts to acquire new container one by one when get request from SlotManager. The mechanism works when job is still, say less than 32 containers. If the job has 256 container, containers can't be immediately allocated and appending requests in AMRMClient will be not removed accordingly. We observe the situation that AMRMClient ask for current pending request + 1 (the new request from slot manager) containers. In this way, during the start time of such job, it asked for 4000+ containers. If there is an external dependency issue happens, for example hdfs access is slow. Then, the whole job will be blocked without getting enough resource and finally killed with SlotManager request timeout.Thus, we should use the total number of container asked rather than pending request in AMRMClient as threshold to make decision whether we need to add one more resource request.</description>
      <version>1.6.4,1.7.2,1.8.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">docs..includes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-26 01:00:00" id="12343" opendate="2019-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow set file.replication in Yarn Configuration</summary>
      <description>Currently, FlinkYarnSessionCli upload jars into hdfs with default 3 replications. From our production experience, we find that 3 replications will block big job (256 containers) to launch, when the HDFS is slow due to big workload for batch pipelines. Thus, we want to make the factor customizable from FlinkYarnSessionCli by adding an option.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnFileStageTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNITCase.java</file>
      <file type="M">docs..includes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-1 01:00:00" id="12386" opendate="2019-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support mapping BinaryType, VarBinaryType, CharType, VarCharType, and DecimalType between Flink and Hive in HiveCatalog</summary>
      <description>Flink right now doesn't have a 1:1 data type mapping for Hive's CHAR, VARCHAR, DECIMAL, MAP, STRUCT. Needs to add the mapping once FLIP-37 &amp;#91;rework Flink type system&amp;#93; is finished</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogGenericMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTypeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-2 01:00:00" id="12388" opendate="2019-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the production readiness checklist</summary>
      <description>The production readiness checklist has grown organically over the years, and while it provides valuable information, the content does not flow cohesively as it has been worked on by a number of users. We should improve the overall structure and readability of the checklist and also update any outdated information.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.production.ready.zh.md</file>
      <file type="M">docs.ops.production.ready.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-5-2 01:00:00" id="12390" opendate="2019-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fully migrate to asm6</summary>
      <description>We currently use a mix of asm5/asm6, let's migrate completely so we can drop a dependency and remove a module from flink-shaded.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.DependencyVisitor.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ClosureCleaner.java</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractionUtils.java</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-2 01:00:00" id="12391" opendate="2019-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add timeout to transfer.sh</summary>
      <description>The upload to transfer.sh frequently does not work, but causes the build to timeout, like seen here: https://api.travis-ci.org/v3/job/527020650/log.txt</description>
      <version>None</version>
      <fixedVersion>1.8.1,1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-5 01:00:00" id="12401" opendate="2019-5-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support incremental emit under AccRetract mode for non-window streaming FlatAggregate on Table API</summary>
      <description>As described in Flip-29, there are two output modes for non-window streaming flatAggregate. One is emitting with full values, the other is emitting with incremental values. FLINK-10977 supports the former one, this jira is going to support the latter one.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.UserDefinedTableAggFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.TableAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.GroupTableAggProcessFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.generated.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.TableAggregateFunction.java</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-5 01:00:00" id="12402" opendate="2019-5-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make validation error message for CallExpression more user friendly</summary>
      <description>Currently, the error message for CallExpression validation may not display the root cause which may confuse our users. Take the following test as an example: @Test def testSimpleSelectAllWithAs(): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = StreamTableEnvironment.create(env) StreamITCase.testResults = mutable.MutableList() val ds = StreamTestData.getSmall3TupleDataStream(env).toTable(tEnv, 'a, 'b, 'c) .select('a, 'b.log as 'b, 'c) val results = ds.toAppendStream[Row] results.addSink(new StreamITCase.StringSink[Row]) env.execute() val expected = mutable.MutableList( "1,1,Hi", "2,2,Hello", "3,2,Hello world") assertEquals(expected.sorted, StreamITCase.testResults.sorted) }The error message is:org.apache.flink.table.api.ValidationException: Invalid arguments [log(b), 'b'] for function: asFrom the error message, it shows there is something wrong with the `as` function. However, the root cause is the log function can only accept a double parameter while b is a long number.To make it more user friendly, it would be better to display the root cause error message.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ResolveCallByArgumentsRule.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-1-9 01:00:00" id="12461" opendate="2019-5-9 00:00:00" resolution="Workaround">
    <buginformation>
      <summary>Document binary compatibility situation with Scala beyond 2.12.8</summary>
      <description>When using 1.8 with Scala 2.12.8 and trying to parse a scala.Map (not a java.util.Map), I get a: java.lang.ClassNotFoundException: scala.math.Ordering$$anon$9To reproduce: start with the Scala Maven archetype (org.apache.flink:flink-quickstart-scala:1.8.0) in the POM, set the scala.version to 2.12.8 and the scala.binary.version to 2.12 in StreamingJob, add: env.fromElements[Map&amp;#91;String, Int&amp;#93;]()It works with Scala 2.12.7 (well, without putting anything in the job, it fails with "No operators defined in streaming topology", which is expected).I suspect this is linked to the binary incompatiblity of 2.12.8 with 2.12.7 (see the release note of 2.12.8), so compiling Flink with 2.12.8 instead of 2.12.7 might be enough (although it might stop working with 2.12.7?)</description>
      <version>1.8.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.flinkDev.building.zh.md</file>
      <file type="M">docs.flinkDev.building.md</file>
      <file type="M">docs.dev.project-configuration.zh.md</file>
      <file type="M">docs.dev.project-configuration.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-5-10 01:00:00" id="12487" opendate="2019-5-10 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce planner rules to rewrite expression and merge calc</summary>
      <description>This issue aims to introduce planner rules to rewrite expression and merge calc, rules include:1. ConvertToNotInOrInRule, that converts a cascade of predicates to IN or NOT_IN,e.g. converts predicate (x = 1 OR x = 2 OR x = 3 OR x = 4) AND y = 5 to predicate x IN (1, 2, 3, 4) AND y = 5 converts predicate (x &lt;&gt; 1 AND x &lt;&gt; 2 AND x &lt;&gt; 3 AND x &lt;&gt; 4) AND y = 5 to predicate x NOT IN (1, 2, 3, 4) AND y = 52. RewriteCoalesceRule, that rewrites Coalesce to Case When3. FlinkCalcMergeRule, that is copied from Calcite CalcMergeRule, and it will simplify the merged program</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkAggregateExpandDistinctAggregatesRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGeneratorContext.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-5-14 01:00:00" id="12508" opendate="2019-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expand Documentation on Testing DataStream API programs</summary>
      <description>The documentation under ApplicationDevelopment -&gt; Streaming -&gt; Testing should be extended by a more detailed discussion of Flink's TestHarnesses as well as the `MiniClusterWithClientResource`.</description>
      <version>1.8.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.testing.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-21 01:00:00" id="12566" opendate="2019-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove row interval type</summary>
      <description>The row interval type just adds additional complexity and prevents SQL queries from supporting count windows. A regular BIGINT type is sufficient to represent a count.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTableAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.overOffsets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.literals.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.call.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.AggregateOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.OverWindowResolverRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.typeutils.RowIntervalTypeInfo.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.ValueLiteralExpression.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.ApiExpressionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-21 01:00:00" id="12568" opendate="2019-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement OutputFormat to write Hive tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTypeUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-22 01:00:00" id="12590" opendate="2019-5-22 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Replace http links in documentation</summary>
      <description/>
      <version>1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.md</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.StreamingJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.BatchJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.StreamingJob.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.BatchJob.java</file>
      <file type="M">flink-python.README.md</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.java.org.apache.flink.examples.java.misc.CollectionExecutionExample.java</file>
      <file type="M">flink-end-to-end-tests.README.md</file>
      <file type="M">flink-dist.src.main.flink-bin.README.txt</file>
      <file type="M">docs..config.yml</file>
      <file type="M">docs.README.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.zh.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.getting-started.tutorials.local.setup.zh.md</file>
      <file type="M">docs.getting-started.tutorials.local.setup.md</file>
      <file type="M">docs.getting-started.tutorials.flink.on.windows.zh.md</file>
      <file type="M">docs.getting-started.tutorials.flink.on.windows.md</file>
      <file type="M">docs.getting-started.tutorials.datastream.api.zh.md</file>
      <file type="M">docs.getting-started.tutorials.datastream.api.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.zh.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.md</file>
      <file type="M">docs.dev.connectors.filesystem.sink.zh.md</file>
      <file type="M">docs.dev.connectors.filesystem.sink.md</file>
      <file type="M">.github.CONTRIBUTING.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-24 01:00:00" id="12617" opendate="2019-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>StandaloneJobClusterEntrypoint should default to random JobID for non-HA setups</summary>
      <description/>
      <version>1.8.0</version>
      <fixedVersion>1.8.1,1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPointTest.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfigurationParserFactoryTest.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfigurationParserFactory.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-1 01:00:00" id="12703" opendate="2019-6-1 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce metadata handlers on SEMI/ANTI join and lookup join</summary>
      <description>In FLINK-11822, we have introduced all Flink metadata handlers, several RelNode s (e.g. look up join) have not be implemented. So this issue aims to introduce all metadata handlers on SEMI/ANTI join and lookup join.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.MetadataTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueGroupsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdSizeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdSelectivityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdRowCountTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdPopulationSizeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdPercentageOriginalRowsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdDistinctRowCountTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnUniquenessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnOriginNullCountTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnNullCountTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnIntervalTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueGroups.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdSize.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdModifiedMonotonicity.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnUniqueness.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnOriginNullCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnNullCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnInterval.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-4 01:00:00" id="12715" opendate="2019-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive-1.2.1 build is broken</summary>
      <description>Some Hive util methods used are not compatible between Hive-2.3.4 and Hive-1.2.1.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV2.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV1.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-4 01:00:00" id="12716" opendate="2019-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an interactive shell for Python Table API</summary>
      <description>We should add an interactive shell for the Python Table API. It will have the similar functionality like the Scala Shell.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.pyflink.java.gateway.py</file>
      <file type="M">flink-python.pyflink.find.flink.home.py</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.pyflink-gateway-server.sh</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgram.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-4 01:00:00" id="12717" opendate="2019-6-4 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add windows support for PyFlink</summary>
      <description>The aim of this JIRA is to allow Python users to develop PyFlink programs in windows. Users should be able to run a simple PyFlink program in the IDE(via minicluster) for debugging purposes.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.bin.pyflink-gateway-server.sh</file>
      <file type="M">flink-python.src.main.resources.pyflink-udf-runner.sh</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.ResourceUtil.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.ProcessPythonEnvironmentManager.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonGatewayServer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonFunctionFactory.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonEnvUtils.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">flink-python.pyflink.python.callback.server.py</file>
      <file type="M">flink-python.pyflink.java.gateway.py</file>
      <file type="M">flink-python.pyflink.gen.protos.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.process.mode.boot.py</file>
      <file type="M">flink-python.pyflink.find.flink.home.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.state.backend.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-4 01:00:00" id="12723" opendate="2019-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adds a wiki page about setting up a Python Table API development environment</summary>
      <description>We should add a wiki page showing how to set up a Python Table API development environment to help contributors who are interested in the Python Table API to join in easily.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.flinkDev.ide.setup.zh.md</file>
      <file type="M">docs.flinkDev.ide.setup.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-5 01:00:00" id="12756" opendate="2019-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>migrate HiveCatalog from TypeInformation-based old type system to DataType-based new type system</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogGenericMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTypeUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-6 01:00:00" id="12758" opendate="2019-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add flink-ml-lib module</summary>
      <description>The Jira introduces a new module "flink-ml-lib" under flink-ml-parent.The flink-ml-lib is planned in the roadmap in FLIP-39, as the code base of library implementations of FlinkML. This Jira only aims to create the module, and algorithms will be added in separate Jira in the future. For more details, please refer to [FLIP39 design doc|https://docs.google.com/document/d/1StObo1DLp8iiy0rbukx8kwAJb0BwDZrQrMWub3DzsEo]</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-ml-parent.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-12-6 01:00:00" id="12766" opendate="2019-6-6 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Dynamically allocate TaskExecutor’s managed memory to slots.</summary>
      <description>This step is a temporal workaround for release 1.9 to meet the basic usability requirements of batch functions from Blink.</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.ResourceSpecTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.ResourceSpec.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.ResourceProfileTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.resources.Resource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-6 01:00:00" id="12767" opendate="2019-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support user defined connectors/format</summary>
      <description>Currently, only built-in connectors such as FileSystem/Kafka/ES are supported and only built-in formats such as OldCSV/JSON/Avro/CSV/ are supported. We should also provide a convenient way for the connectors/formats that are not built-in supported.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.table.descriptors.py</file>
      <file type="M">flink-python.pyflink.java.gateway.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-10 01:00:00" id="12787" opendate="2019-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow to specify directory in option -pyfs</summary>
      <description>Current only files can be specified in option `-pyfs`, we want to improve it allow also specify directories in option `-pyfs`. </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.client.PythonEnvUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.client.PythonDriverTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.client.PythonEnvUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.client.PythonDriver.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-26 01:00:00" id="1279" opendate="2014-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default partitioning setting for low parallelism stream sources from forward to distribute</summary>
      <description>The default partitioning setting for all operators including stream sources are forward at the moment. In many cases sources have lower parallelism therefore the forward connection might cause that only a subset of the operators will receive data.Partitioning from source to the first operators should be changed to distribute data to all parallel instances of the first operators, when the source parallelism is lower.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.partitioner.ForwardPartitionerTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.partitioner.DistributePartitionerTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.PrintTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.StreamPartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.ShufflePartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.GlobalPartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.ForwardPartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.FieldsPartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.DistributePartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.BroadcastPartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.JobGraphBuilder.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.IterativeDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-12 01:00:00" id="12812" opendate="2019-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set resource profiles for task slots</summary>
      <description/>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.ResourceProfileTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.ResourceSpec.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-12 01:00:00" id="12814" opendate="2019-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support a traditional and scrolling view of result (non-interactive)</summary>
      <description>In table mode, we want to introduce a non-interactive view (so-called FinalizedResult), which submit SQL statements(DQLs) in attach mode with a user defined timeout, fetch results until the job finished/failed/timeout or interrupted by user(Ctrl+C), and output them in a non-interactive way (the behavior in change-log mode is under discussion)</description>
      <version>1.8.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.utils.TerminalUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ResultDescriptor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ResultStore.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ExecutionEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-12-26 01:00:00" id="1282" opendate="2014-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update layout of documentation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.default.html</file>
      <file type="M">docs..includes.footer.md</file>
      <file type="M">docs.css.custom.css</file>
      <file type="M">docs..includes.navbar.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-12 01:00:00" id="12821" opendate="2019-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug that fix time quantifier can not be the last element of a pattern</summary>
      <description>Currently, exception "Greedy quantifiers are not allowed as the last element of a Pattern yet. Finish your pattern with either a simple variable or reluctant quantifier." will be thrown for patterns such as "a{2}". Actually greedy property is not meaningful for this kind of pattern.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.match.PatternTranslatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamMatch.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-6-24 01:00:00" id="12957" opendate="2019-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix thrift and protobuf dependency examples in documentation</summary>
      <description>The examples in the docs are not up-to-date anymore and should be updated.</description>
      <version>1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.7.3,1.8.1,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.custom.serializers.md</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-6-26 01:00:00" id="12990" opendate="2019-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Date type doesn&amp;#39;t consider the local TimeZone</summary>
      <description>Currently, the python DateType is converted by an `int` which indicates the days passed since 1970-1-1 and then the Java side will create a Java Date by call `new Date(days * 86400)`. As we know that the Date constructor expected milliseconds since 1970-1-1 00:00:00 GMT and so we should convert `days * 86400` to GMT milliseconds.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.python.PythonTableUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-27 01:00:00" id="13017" opendate="2019-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Broken and irreproducible dockerized docs build</summary>
      <description>The build tools around docs/docker seem broken and (on my machine) give errors like the following while it is working on a colleague's machine:bash: /etc/bash_completion.d/git-prompt.sh: No such file or directorybash: __git_ps1: command not found/usr/bin/env: 'ruby.ruby2.5': No such file or directorybash: __git_ps1: command not foundReason seems to be that your whole user's $HOME is mounted (writable!) into the docker container. We should just mount the docs directory to get builds which are independent from the host system (making them reproducible) not have the commands in the container affect the host</description>
      <version>1.6.4,1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.docker.run.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-8-1 01:00:00" id="13045" opendate="2019-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move Scala expression DSL to flink-table-api-scala</summary>
      <description>Moves the implicit conversions to the target module.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.package.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.package.scala</file>
      <file type="M">flink-table.flink-table-api-scala.pom.xml</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.TableConversions.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.DataStreamConversions.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.DataSetConversions.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.pom.xml</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-7-11 01:00:00" id="13227" opendate="2019-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Asynchronous I/O for External Data Access" page into Chinese</summary>
      <description>The page url is https://ci.apache.org/projects/flink/flink-docs-master/dev/stream/operators/asyncio.htmlThe markdown file is located in flink/docs/dev/stream/operators/asyncio.md</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.asyncio.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-22 01:00:00" id="13347" opendate="2019-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>should handle new JoinRelType(SEMI/ANTI) in switch case</summary>
      <description>Calcite 1.20 introduces SEMI &amp; ANTI to JoinRelType, blink planner &amp; flink planner should handle them in each switch case</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamJoinToCoProcessTranslator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonCorrelate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-31 01:00:00" id="13519" opendate="2019-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch Connector sample code for Scala on version 6.x will not work</summary>
      <description>The Scala example in the documentation for the Elasticsearch Connector, version 6.x, will not work. The class ElasticsearchSinkFunction&amp;#91;String&amp;#93; requires a RuntimeContext and a RequestIndexer, which the example omits.Also, type needs to be in inverse quotes as it's a Scala keyword.It should look like the following:def process(element: String, ctx: RuntimeContext, indexer: RequestIndexer) { val json = new java.util.HashMap[String, String] json.put("data", element) val rqst: IndexRequest = Requests.indexRequest .index("testarindex") .`type`("_doc") .source(json) indexer.add(rqst) } </description>
      <version>1.7.2,1.8.0,1.8.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-8-1 01:00:00" id="13540" opendate="2019-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DDL do not support key of properties contains number or "-"</summary>
      <description>But many connector have key of properties contains number or "-", like kafka..So as long as we don't solve this problem, it's hard for users to use these connectors.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkDDLDataTypeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-9-29 01:00:00" id="13893" opendate="2019-8-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>S3 tests are failing due to missing jaxb dependency</summary>
      <description>The (hadoop/presto) S3 filesystems make use of javax.xml.bind but does not declare a dependency on jaxb-api and relies on the JDK containing this class. This is no longer the case on Java 11; we have to add it as a dependency and bundle it in the jar.16:20:12.975 [ERROR] Tests run: 3, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 5.691 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase16:20:12.992 [ERROR] testResumeAfterCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 5.067 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.IOException&gt; but was&lt;java.lang.NoClassDefFoundError&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit(HadoopS3RecoverableWriterExceptionITCase.java:165)Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit(HadoopS3RecoverableWriterExceptionITCase.java:165)16:20:12.992 [ERROR] testResumeWithWrongOffset(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 0.194 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.IOException&gt; but was&lt;java.lang.NoClassDefFoundError&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset(HadoopS3RecoverableWriterExceptionITCase.java:185)Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset(HadoopS3RecoverableWriterExceptionITCase.java:185)https://api.travis-ci.org/v3/job/577860512/log.txt</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-29 01:00:00" id="13894" opendate="2019-8-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add TaskExecutor log link to subtask view</summary>
      <description>As FLINK-13868 has added ResourceId in rest api of subatsk. So we can according to it add log link for Taskamanager which subtask  allocated.screenshot  </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-subtask.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-2 01:00:00" id="13941" opendate="2019-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent data-loss by not cleaning up small part files from S3.</summary>
      <description/>
      <version>1.8.0,1.8.1,1.9.0</version>
      <fixedVersion>1.8.2,1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-13 01:00:00" id="1395" opendate="2015-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Jodatime support to Kryo</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.KryoGenericTypeSerializerTest.scala</file>
      <file type="M">flink-dist.src.main.flink-bin.NOTICE</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.TupleSerializerTest.scala</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.KryoGenericTypeSerializerTest.scala</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.KryoGenericTypeSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.AbstractGenericTypeSerializerTest.java</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.LICENSE</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.SerializerTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-5 01:00:00" id="13973" opendate="2019-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpoint recovery failed after user set uidHash</summary>
      <description>Checkpoint recovery failed after user set uidHash, the possible reasons are as follows:If altOperatorID is not null, operatorState will be obtained by altOperatorID and will not be given</description>
      <version>1.8.0,1.8.1,1.9.0,1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-6 01:00:00" id="13995" opendate="2019-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix shading of the licence information of netty</summary>
      <description>The license filter isn't actually filtering anything. It should be META-INF/license/**.The first filter seems to be outdated btw.Multiple modules affected.&lt;filter&gt; &lt;artifact&gt;io.netty:netty&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/maven/io.netty/**&lt;/exclude&gt; &lt;!-- Only some of these licenses actually apply to the JAR and have been manuallyplaced in this module's resources directory. --&gt; &lt;exclude&gt;META-INF/license&lt;/exclude&gt; &lt;!-- Only parts of NOTICE file actually apply to the netty JAR and have been manuallycopied into this modules's NOTICE file. --&gt; &lt;exclude&gt;META-INF/NOTICE.txt&lt;/exclude&gt; &lt;/excludes&gt;&lt;/filter&gt;</description>
      <version>1.8.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-9-9 01:00:00" id="14005" opendate="2019-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 2.2.0</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV230.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-9 01:00:00" id="14006" opendate="2019-9-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add doc for how to using Java UDFs in Python API</summary>
      <description>Currently, user can not find out the doc for  how to using Java UDFs in Python API. So we should add the detail doc. In https://ci.apache.org/projects/flink/flink-docs-master/dev/table/udfs.html only describe how to using the APIs, but do not mention how to add the JARs to the class path.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-9 01:00:00" id="14007" opendate="2019-9-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add doc for how to using Java user-defined source/sink in Python API</summary>
      <description>Currently, user can not find out the doc for how to using Java user-defined source/sink in Python API. So we should add the detail doc. </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sourceSinks.zh.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-7 01:00:00" id="14334" opendate="2019-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ElasticSearch docs refer to non-existent ExceptionUtils.containsThrowable</summary>
      <description/>
      <version>1.8.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ActionRequestFailureHandler.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraFailureHandler.java</file>
      <file type="M">docs.dev.connectors.elasticsearch.zh.md</file>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-10-15 01:00:00" id="14398" opendate="2019-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Further split input unboxing code into separate methods</summary>
      <description>In one of our production pipelines, we have a table with 1200+ columns.  At runtime, it failed due to a method inside the generated code exceeding 64kb when compiled to bytecode.After we investigated the generated code, it appeared that the map method inside a generated RichMapFunction was too long. See attached file (codegen.example.txt).In the problematic map method, result setters were correctly split into individual methods and did not have the largest footprint.However, there were also 1000+ input unboxing expressions inside reusableInputUnboxingExprs, which, individually were not trivial and when flattened linearly in the map function here, pushed the method size beyond 64kb in bytecode.We think it is worthwhile to split these input unboxing code snippets into individual methods.  We were able to verify, in our production environment, that splitting input unboxing code snippets into individual methods resolves the issue.  Would love to hear thoughts from the team and find the best path to fix it.</description>
      <version>1.8.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.FunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.CollectorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-18 01:00:00" id="14445" opendate="2019-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python module build failed when making sdist</summary>
      <description>From the description of error-log from building python module in travis, it seems invocation failed for sdist-make and then the phase of building python module exited.The instance log: https://api.travis-ci.com/v3/job/246710918/log.txt</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-7 01:00:00" id="14639" opendate="2019-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metrics User Scope docs refer to wrong class</summary>
      <description>Hi , i think it should be `MetricGroup{{#addGroup(String key, String value)}}` </description>
      <version>1.8.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-26 01:00:00" id="14954" opendate="2019-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Publish OpenAPI specification of REST Monitoring API</summary>
      <description>Hello,Flink provides a very helpful REST Monitoring API.OpenAPI is convenient standard to generate clients in a variety of language for REST API documented according to their specification. In this case, clients would be helpful to automate management of Flink clusters.Currently, there is no "official" OpenAPI specification of Flink REST Monitoring API. Some have written by users, but their consistency across Flink releases is uncertain.I think it would be beneficial to have an OpenAPI specification provided and maintained by the Flink project. Kind regards, </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.json.SerializedThrowableSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.async.AsynchronousOperationResult.java</file>
      <file type="M">flink-docs.README.md</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">docs.content.docs.ops.rest.api.md</file>
      <file type="M">docs.content.zh.docs.ops.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-5 01:00:00" id="15080" opendate="2019-12-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deploy OSS filesystem to maven central</summary>
      <description>Just noticed that the OSS filesystem isn't being deployed.I see no reason why this is the case; we are deploying all other artifacts and it just makes it more difficult to access older versions (since you'd have to splice them out of a distribution).</description>
      <version>1.8.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-30 01:00:00" id="16862" opendate="2020-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove example url in quickstarts</summary>
      <description>Our quickstarts define an example url http:/myorganization.org, which as it turns out is a real website.We should just remove tag.</description>
      <version>1.8.0</version>
      <fixedVersion>1.9.3,1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-13 01:00:00" id="17107" opendate="2020-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CheckpointCoordinatorConfiguration#isExactlyOnce() is inconsistent with StreamConfig#getCheckpointMode()</summary>
      <description>CheckpointCoordinatorConfiguration#isExactlyOnce() is inconsistent with StreamConfig#getCheckpointMode() when checkpoint is disabled. CheckpointCoordinatorConfiguration#isExactlyOnce() returns true if checkpoint mode is  EXACTLY_ONCE mode and return false if checkpoint mode is AT_LEAST_ONCE while StreamConfig#getCheckpointMode() will always return AT_LEAST_ONCE which means always not exactly once.</description>
      <version>1.6.3,1.7.2,1.8.0,1.9.0,1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-20 01:00:00" id="17254" opendate="2020-4-20 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Improve the PyFlink documentation and examples to use SQL DDL for source/sink definition</summary>
      <description>Currently there are two ways to register a table sink/source in PyFlink table API:1) TableEnvironment.connect2) TableEnvironment.sql_updateI think it's better to provide documentation and examples on how to use 2) in PyFlink.</description>
      <version>None</version>
      <fixedVersion>1.9.4,1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.examples.batch.word.count.py</file>
      <file type="M">docs.getting-started.walkthroughs.python.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-20 01:00:00" id="17256" opendate="2020-4-20 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Suppport keyword arguments in the PyFlink Descriptor API</summary>
      <description>Keyword arguments is a very commonly used feature in Python. We should support it in the PyFlink Descriptor API to make the API more user friendly for Python users.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.table.descriptors.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-21 01:00:00" id="19009" opendate="2020-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>wrong way to calculate the "downtime" metric</summary>
      <description>Currently the way to calculate the Flink system metric "downtime"  is not consistent with the description in the doc, now the downtime is actually the current timestamp minus the time timestamp when the job started.   But Flink doc (https://flink.apache.org/gettinghelp.html) obviously describes the time as the current timestamp minus the timestamp when the job failed. I believe we should update the code this metric as the Flink doc shows. The easy way to solve this is using the current timestamp to minus the latest uptime timestamp.</description>
      <version>1.7.2,1.8.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.metrics.DownTimeGauge.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-7-21 01:00:00" id="19010" opendate="2020-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a system metric to show the checkpoint restore time</summary>
      <description>Now the system metric only shows the downtime when failure happens. It would be interesting to see the time to restore the checkpoint, so users can better understand the bottleneck of failure recovery.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroup.java</file>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-9 01:00:00" id="19173" opendate="2020-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Pandas Batch Group Aggregation Function Operator</summary>
      <description>Add Pandas Batch Group Aggregation Function Operator</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperatorBase.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.datastream.runtime.operators.python.DataStreamTwoInputPythonStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.datastream.runtime.operators.python.DataStreamPythonStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.datastream.runtime.operators.python.DataStreamPythonReduceFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
    </fixedFiles>
  </bug>
  
</bugrepository>