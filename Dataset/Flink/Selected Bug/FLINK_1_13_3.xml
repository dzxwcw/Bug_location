<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2020-1-17 01:00:00" id="20188" opendate="2020-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation for new File Source</summary>
      <description/>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.execution.mode.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.content.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.text.files.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.parquet.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.azure.table.storage.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.execution.mode.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.file.sink.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-1 01:00:00" id="23571" opendate="2021-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The internal query-start options missed when convert exec graph to transformation</summary>
      <description>The internal query-start configuration options is missed when convert exec graph to transformation, please see:// org.apache.flink.table.planner.delegation.PlannerBase translateToPlan(execGraph: ExecNodeGraph)</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-8 01:00:00" id="24213" opendate="2021-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deadlock in QueryableState Client</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23750&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&amp;l=15476 Found one Java-level deadlock:Sep 08 11:12:50 =============================Sep 08 11:12:50 "Flink Test Client Event Loop Thread 0":Sep 08 11:12:50 waiting to lock monitor 0x00007f4e380309c8 (object 0x0000000086b2cd50, a java.lang.Object),Sep 08 11:12:50 which is held by "main"Sep 08 11:12:50 "main":Sep 08 11:12:50 waiting to lock monitor 0x00007f4ea4004068 (object 0x0000000086b2cf50, a java.lang.Object),Sep 08 11:12:50 which is held by "Flink Test Client Event Loop Thread 0"</description>
      <version>1.14.0,1.12.6,1.13.3</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.ServerConnection.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-16 01:00:00" id="24310" opendate="2021-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>A bug in the BufferingSink example in the doc</summary>
      <description>The following line in the BufferingSink on this page has a bug:if (bufferedElements.size() == threshold) {It should be &gt;= instead of == , because when restoring from a checkpoint during downscaling, the task may get more elements than the threshold. </description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.state.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.state.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-21 01:00:00" id="24608" opendate="2021-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sinks built with the unified sink framework do not receive timestamps when used in Table API</summary>
      <description>All sinks built with the unified sink framework extract the timestamp from the internal StreamRecord. The Table API does not facilitate the timestamp field in the StreamRecord but extracts the timestamp from the actual data. We either have to use a dedicated operator before all the sinks to simulate the behavior or allow a customizable timestamp extraction during the sink translation.</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.match.RowtimeProcessFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-1-28 01:00:00" id="24686" opendate="2021-10-28 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Make doc clear on AsyncFunction::timeout() overriding</summary>
      <description>Sometimes, a user overrides AsyncFunction::timeout() with an empty method or with only logging code. This causes the timeout does not signal back to the framework and job stuck especially when using orderedWait(). Opening this Jira to make the doc clear on this.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.operators.asyncio.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.asyncio.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-2 01:00:00" id="24739" opendate="2021-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>State requirements for Flink&amp;#39;s application mode in the documentation</summary>
      <description>If I am not mistaken, then Flink won't ship jars when using the application mode because it assumes the jars to be on the classpath. If this is true, then we should make this requirement a bit more prominent in the deployment documentation because currently it is only subtly hinted at.Alternatively, we could enable Flink to ship the user code jars to its components also when using the application mode.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-2 01:00:00" id="24740" opendate="2021-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update testcontainers dependency to v1.16.2</summary>
      <description>We should update our testcontainers dependency to the latest version, which is 1.16.2Main benefits (based on https://github.com/testcontainers/testcontainers-java/releases) Better startup performance for all containers Faster Cassandra startup Host port access for containers (make hosts ports accessible to containers, even after the container has started) New Azure Cosmos DB module</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-2 01:00:00" id="24742" opendate="2021-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL client add info about key strokes to docs</summary>
      <description>SQL client supports key strokes from jline.Unfortunately there is no docs about that in jline however there is source from which it could be found &amp;#91;1&amp;#93;here it is a list of most useful key strokes which are already supported by all existing Flink SQL client Key-Stroke Description `alt-b` Backward word `alt-f` Forward word `alt-c` Capitalize word `alt-l` Lowercase word `alt-u` Uppercase word `alt-d` Kill word `alt-n` History search forward `alt-p` History search backward `alt-t` Transpose words `ctrl-a` To the beginning of line `ctrl-e` To the end of line `ctrl-b` Backward char `ctrl-f` Forward char `ctrl-d` Delete char `ctrl-h` Backward delete char `ctrl-t` Transpose chars `ctrl-i` Invoke completion `ctrl-j` Submit a query `ctrl-m` Submit a query `ctrl-k` Kill the line to the right from the cursor `ctrl-w` Kill the line to the left from the cursor `ctrl-u` Kill the whole line `ctrl-l` Clear screen `ctrl-n` Down line from history `ctrl-p` Up line from history `ctrl-r` History incremental search backward `ctrl-s` History incremental search forward &amp;#91;1&amp;#93; https://github.com/jline/jline3/blob/997496e6a6338ca5d82c7dec26f32cf089dd2838/reader/src/main/java/org/jline/reader/impl/LineReaderImpl.java#L5907</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-5-8 01:00:00" id="24820" opendate="2021-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Examples in documentation for value1 IS DISTINCT FROM value2 are wrong</summary>
      <description>Currently it is stated in docs for value1 IS DISTINCT FROM value2E.g., 1 IS NOT DISTINCT FROM NULL returns TRUE; NULL IS NOT DISTINCT FROM NULL returns FALSE.In fact they return opposite values.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-1-12 01:00:00" id="24884" opendate="2021-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink flame graph webui bug</summary>
      <description>i can not compile success when i port the flame graph feature to our low version of flink.but it is success in the high version of flink  </description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.@types.d3-flame-graph.index.d.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-23 01:00:00" id="25022" opendate="2021-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassLoader leak with ThreadLocals on the JM when submitting a job through the REST API</summary>
      <description>If a job is submitted using the REST API's /jars/:jarid/run endpoint, user code has to be executed on the JobManager and it is doing this in a couple of (pooled) dispatcher threads like Flink-DispatcherRestEndpoint-thread-*.If the user code is using thread locals (and not cleaning them up), they may remain in the thread with references to the ChildFirstClassloader of the job and thus leaking that.We saw this for the jsoniter scala library at the JM which creates ThreadLocal instances but doesn't remove them, but it can actually happen with any user code or (worse) library used in user code. There are a few workarounds a user can use, e.g. putting the library in Flink's lib/ folder or submitting via the Flink CLI, but these may actually not be possible to use, depending on the circumstances. A proper fix should happen in Flink by guarding against any of these things in the dispatcher threads. We could, for example, spawn a separate thread for executing the user's main() method and once the job is submitted exit that thread and destroy all thread locals along with it.</description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebSubmissionExtension.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarMessageParameters.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-25 01:00:00" id="25053" opendate="2021-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to use the usrlib to load code in the user code class loader</summary>
      <description>With FLINK-13993 we introduced the usrlib directory that can be used to load code in the user code class loader. This functionality has not been properly documented so that it is very hard to use. I would suggest to change this so that our users can benefit from this cool feature.</description>
      <version>1.14.0,1.12.5,1.13.3,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-12-29 01:00:00" id="25096" opendate="2021-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make errors happened during JobMaster initialization accessible through the exception history</summary>
      <description>Currently we are using flink version 1.13.2 and as per the flink documentation we should get all exceptions through exceptions api in exceptionHistory tag. While running few scenarios we observed that the below two exceptions are not coming in exceptionHistory tag but are coming in root-exception tag.Exception 1 - caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: java.io.FileNotFoundException: Cannot find checkpoint or savepoint file/directory 'C:\Users\abc\Documents\checkpoints\a737088e21206281db87f6492bcba074' on file system 'file'.Exception 2 - Caused by: java.lang.IllegalStateException: Failed to rollback to checkpoint/savepoint file:/mnt/c/Users/abc/Documents/checkpoints/a737088e21206281db87f6492bcba074/chk-144.Please find the attachment for the logs of above exceptions.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcessTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionGraphInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.exceptionhistory.RootExceptionHistoryEntry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-1 01:00:00" id="25118" opendate="2021-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add vertex index as prefix in vertex name</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.PipelineOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pipeline.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-3 01:00:00" id="25160" opendate="2021-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make doc clear: tolerable-failed-checkpoints counts consecutive failures</summary>
      <description>According to the code, tolerable-failed-checkpoints counts the consecutive failures. We should make this clear in the doc config </description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.checkpointing.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-12-3 01:00:00" id="25161" opendate="2021-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update dependency for japicmp-maven-plugin</summary>
      <description>compiliation with jdk 17 fails like belowupdate of jaxb-impl to 2.3.1 helpsjava.security.PrivilegedActionException: java.lang.NoSuchMethodException: sun.misc.Unsafe.defineClass(java.lang.String,[B,int,int,java.lang.ClassLoader,java.security.ProtectionDomain) at java.base/java.security.AccessController.doPrivileged(AccessController.java:573) at com.sun.xml.bind.v2.runtime.reflect.opt.Injector.&lt;clinit&gt;(Injector.java:197) at com.sun.xml.bind.v2.runtime.reflect.opt.AccessorInjector.prepare(AccessorInjector.java:81) at com.sun.xml.bind.v2.runtime.reflect.opt.OptimizedAccessorFactory.get(OptimizedAccessorFactory.java:125) at com.sun.xml.bind.v2.runtime.reflect.Accessor$GetterSetterReflection.optimize(Accessor.java:402) at com.sun.xml.bind.v2.runtime.reflect.TransducedAccessor$CompositeTransducedAccessorImpl.&lt;init&gt;(TransducedAccessor.java:235) at com.sun.xml.bind.v2.runtime.reflect.TransducedAccessor.get(TransducedAccessor.java:175) at com.sun.xml.bind.v2.runtime.property.AttributeProperty.&lt;init&gt;(AttributeProperty.java:91) at com.sun.xml.bind.v2.runtime.property.PropertyFactory.create(PropertyFactory.java:108) at com.sun.xml.bind.v2.runtime.ClassBeanInfoImpl.&lt;init&gt;(ClassBeanInfoImpl.java:181) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.getOrCreate(JAXBContextImpl.java:514) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.&lt;init&gt;(JAXBContextImpl.java:331) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.&lt;init&gt;(JAXBContextImpl.java:139) at com.sun.xml.bind.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1156) at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:165) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:568) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:297) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:286) at javax.xml.bind.ContextFinder.find(ContextFinder.java:409) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:721) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:662) at japicmp.output.xml.XmlOutputGenerator.createXmlDocumentAndSchema(XmlOutputGenerator.java:119) at japicmp.output.xml.XmlOutputGenerator.generate(XmlOutputGenerator.java:70) at japicmp.maven.JApiCmpMojo.generateXmlOutput(JApiCmpMojo.java:866) at japicmp.maven.JApiCmpMojo.executeWithParameters(JApiCmpMojo.java:149) at japicmp.maven.JApiCmpMojo.execute(JApiCmpMojo.java:125)</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-12-6 01:00:00" id="25173" opendate="2021-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce CatalogLock</summary>
      <description>Currently, only HiveCatalog can provide this catalog lock./** * An interface that allows source and sink to use global lock to some transaction-related things. */@Internalpublic interface CatalogLock extends Closeable {     /** Run with catalog lock. The caller should tell catalog the database and table name. */    &lt;T&gt; T runWithLock(String database, String table, Callable&lt;T&gt; callable) throws Exception;     /** Factory to create {@link CatalogLock}. */    interface Factory extends Serializable {        CatalogLock create();    }} And we need a interface to set lock to source&amp;sink by catalog:/** * Source and sink implement this interface if they require {@link CatalogLock}. This is marked as * internal. If we need lock to be more general, we can put lock factory into {@link * DynamicTableFactory.Context}. */@Internalpublic interface RequireCatalogLock {     void setLockFactory(CatalogLock.Factory lockFactory);} {{}}</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveDynamicTableFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2021-8-10 01:00:00" id="25252" opendate="2021-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable Kafka E2E tests on Java 11</summary>
      <description>The Java Kafka E2E tests are currently not run on Java 11. We should check what the actual issue is and whether it can be resolved (e.g., by a Kafka server version bump):</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SmokeKafkaITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-13 01:00:00" id="25278" opendate="2021-12-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Azure failed due to unable to transfer jar from confluent maven repo</summary>
      <description>Dec 12 00:46:45 [ERROR] Failed to execute goal on project flink-avro-confluent-registry: Could not resolve dependencies for project org.apache.flink:flink-avro-confluent-registry:jar:1.13-SNAPSHOT: Could not transfer artifact io.confluent:common-utils:jar:5.5.2 from/to confluent (https://packages.confluent.io/maven/): transfer failed for https://packages.confluent.io/maven/io/confluent/common-utils/5.5.2/common-utils-5.5.2.jar: Connection reset -&gt; [Help 1]Dec 12 00:46:45 [ERROR] Dec 12 00:46:45 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.Dec 12 00:46:45 [ERROR] Re-run Maven using the -X switch to enable full debug logging.Dec 12 00:46:45 [ERROR] Dec 12 00:46:45 [ERROR] For more information about the errors and possible solutions, please read the following articles:Dec 12 00:46:45 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionExceptionDec 12 00:46:45 [ERROR] Dec 12 00:46:45 [ERROR] After correcting the problems, you can resume the build with the commandDec 12 00:46:45 [ERROR] mvn &lt;goals&gt; -rf :flink-avro-confluent-registry https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27994&amp;view=logs&amp;j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&amp;t=9c1ddabe-d186-5a2c-5fcc-f3cafb3ec699&amp;l=8812</description>
      <version>1.13.3</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.alibaba-mirror-settings.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-5-29 01:00:00" id="26909" opendate="2022-3-29 00:00:00" resolution="Done">
    <buginformation>
      <summary>Allow setting parallelism to -1 from CLI</summary>
      <description>When we start the job by command with args "-p $parallelism", the error is thrown with "The parallelism must be a positive number: -1".Since we can use AdaptiveBatch with config parallelism.default: -1, we should support it from Cli.</description>
      <version>None</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
    </fixedFiles>
  </bug>
</bugrepository>