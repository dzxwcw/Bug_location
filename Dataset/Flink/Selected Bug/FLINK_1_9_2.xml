<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  <bug fixdate="2019-11-7 01:00:00" id="14641" opendate="2019-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix description of metric `fullRestarts`</summary>
      <description>The metric `fullRestarts` counts both full restarts and fine grained restarts since 1.9.2.We should update the metric description doc accordingly.We need to pointing out the the metric counts full restarts in 1.9.1 or earlier versions, and turned to count all kinds of restarts since 1.9.2.</description>
      <version>1.9.2,1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-2-2 01:00:00" id="15010" opendate="2019-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temp directories flink-netty-shuffle-* are not cleaned up</summary>
      <description>Starting a Flink cluster with 2 TMs and stopping it again will leave 2 temporary directories (and not delete them): flink-netty-shuffle-&lt;uid&gt;</description>
      <version>1.9.0,1.9.1,1.9.2</version>
      <fixedVersion>1.9.3,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.FileChannelManagerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-3 01:00:00" id="15864" opendate="2020-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jackson-databind dependency to 2.10.1 for security reasons</summary>
      <description>The module flink-kubernetes defines an explicit dependency on jackson-databind:2.9.8. This is problematic since this jackson version contains security vulnerabilities. See FLINK-14104 for more information.If possible, I would suggest to remove the explicit version tag and to rely on the parent's dependency management.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-4 01:00:00" id="15897" opendate="2020-2-4 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Defer the deserialization of the Python UDF execution results</summary>
      <description>Currently, the Python UDF execution results are deserialized and then buffered in a collection when received from the Python worker. The deserialization could be deferred when sending the execution results to the downstream operator. That's to say, it buffers the serialized bytes instead of the deserialized Java objects in the buffer. This could reduce the memory footprint of the Java operator.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PassThroughPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.PythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.BaseRowPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.AbstractPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.PythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.BaseRowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.AbstractPythonFunctionRunner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-20 01:00:00" id="1591" opendate="2015-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove window merge before flatten as an optimization</summary>
      <description>After a Window Reduce or Map transformation there is always a merge step when the transformation was parallel or grouped.This merge step should be removed when the windowing operator is followed by flatten to avoid unnecessary bottlenecks in the program.This feature should be added as an optimization step to the WindowingOptimizer class.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.WindowingOptimzier.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.StreamGraph.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-2-6 01:00:00" id="15929" opendate="2020-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>test_set_requirements_with_cached_directory failed on travis</summary>
      <description>The Python tests "test_set_requirements_with_cached_directory" is instable. It failed on travis with the following exception:E Caused by: org.apache.beam.vendor.guava.v26_0_jre.com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Process died with exit code 0E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2050)E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.get(LocalCache.java:3952)E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974)E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958)E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4964)E at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory.&lt;init&gt;(DefaultJobBundleFactory.java:211)E at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory.&lt;init&gt;(DefaultJobBundleFactory.java:202)E at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory.forStage(DefaultJobBundleFactory.java:185)E at org.apache.flink.python.AbstractPythonFunctionRunner.open(AbstractPythonFunctionRunner.java:179)E at org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator$ProjectUdfInputPythonScalarFunctionRunner.open(AbstractPythonScalarFunctionOperator.java:193)E at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.open(AbstractPythonFunctionOperator.java:139)E at org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator.open(AbstractPythonScalarFunctionOperator.java:143)E at org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperator.open(BaseRowPythonScalarFunctionOperator.java:86)E at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1007)E at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)E at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)E at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)E at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)E at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)E at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)E at java.lang.Thread.run(Thread.java:748)E Caused by: java.lang.IllegalStateException: Process died with exit code 0E at org.apache.beam.runners.fnexecution.environment.ProcessManager$RunningProcess.isAliveOrThrow(ProcessManager.java:74)E at org.apache.beam.runners.fnexecution.environment.ProcessEnvironmentFactory.createEnvironment(ProcessEnvironmentFactory.java:125)E at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:178)E at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:162)E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3528)E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2277)E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154)E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2044)E ... 20 moreinstance: https://api.travis-ci.org/v3/job/646511525/log.txt</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.tox.ini</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-7 01:00:00" id="15953" opendate="2020-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job Status is hard to read for some Statuses</summary>
      <description>The job status RESTARTING is rendered in a white font on white background which makes it hard to read (see attachments).</description>
      <version>1.9.2,1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.config.ts</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-2-23 01:00:00" id="16242" opendate="2020-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BinaryGeneric serialization error cause checkpoint failure</summary>
      <description>The serialization error occurs from time to time when we're using RoaringBitmap as the accumulator of a UDAF.I've attached the screenshot of the error.</description>
      <version>1.9.2</version>
      <fixedVersion>1.9.3,1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.BaseRowSerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.BaseRowSerializer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-3-3 01:00:00" id="16410" opendate="2020-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PrometheusReporterEndToEndITCase fails with ClassNotFoundException</summary>
      <description>Logs: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5883&amp;view=logs&amp;j=b1623ac9-0979-5b0d-2e5e-1377d695c991&amp;t=e7804547-1789-5225-2bcf-269eeaa37447[INFO] Running org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.005 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase[ERROR] testReporter(org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase) Time elapsed: 0.005 s &lt;&lt;&lt; ERROR!java.lang.NoClassDefFoundError: org/apache/flink/runtime/rest/messages/RequestBody at org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.&lt;init&gt;(PrometheusReporterEndToEndITCase.java:119)Caused by: java.lang.ClassNotFoundException: org.apache.flink.runtime.rest.messages.RequestBody at org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.&lt;init&gt;(PrometheusReporterEndToEndITCase.java:119)[INFO]</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-6 01:00:00" id="16471" opendate="2020-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>develop PostgresCatalog</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.dialect.JDBCDialects.java</file>
      <file type="M">flink-connectors.flink-jdbc.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.test.resources.log4j2-test.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-6 01:00:00" id="16472" opendate="2020-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>support precision of timestamp and time data types</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.catalog.PostgresCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.catalog.PostgresCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-6 01:00:00" id="16473" opendate="2020-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add documentation for JDBCCatalog and PostgresCatalog</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
      <file type="M">docs.dev.table.catalogs.zh.md</file>
      <file type="M">docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-7 01:00:00" id="16485" opendate="2020-3-7 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support vectorized Python UDF in the batch mode of old planner</summary>
      <description>Currently, vectorized Python UDF is only supported in the batch/stream mode for the blink planner and stream mode for the old planner. The aim of this Jira is to add support in the batch mode for the old planner.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetPythonCalc.scala</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonTableFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonStatelessFunctionFlatMap.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-7 01:00:00" id="16486" opendate="2020-3-7 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add documentation for vectorized Python UDF</summary>
      <description>As the title described, the aim of this JIRA is to add documentation for vectorized Python UDF.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.python.index.zh.md</file>
      <file type="M">docs.dev.table.python.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-20 01:00:00" id="16694" opendate="2020-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resuming Externalized Checkpoint end-to-end test failed on travis</summary>
      <description>Running 'Resuming Externalized Checkpoint (rocks, incremental, scale down) end-to-end test' failed on travis (release-1.9 branch) with the error:The job exceeded the maximum log length, and has been terminated.https://api.travis-ci.org/v3/job/664469537/log.txthttps://travis-ci.org/github/apache/flink/builds/664469494This error is probably because of metrics logging and it's masking some other underlying issue, potentially FLINK-16695 as they happened in the same build.</description>
      <version>1.9.2</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.externalized.checkpoints.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-18 01:00:00" id="1720" opendate="2015-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate ScalaDoc in Scala sources into overall JavaDoc</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils.pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.pom.xml</file>
      <file type="M">flink-staging.flink-hcatalog.pom.xml</file>
      <file type="M">flink-staging.flink-expressions.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-16 01:00:00" id="17763" opendate="2020-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>No log files when starting scala-shell</summary>
      <description>I see the following error when starting scala shell. Starting Flink Shell:ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2</description>
      <version>1.9.2,1.10.0</version>
      <fixedVersion>1.9.4,1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.start-script.start-scala-shell.sh</file>
    </fixedFiles>
  </bug>
</bugrepository>